<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Notes on</title><link>https://alonelysheep.github.io/quartz-blog/notes/</link><description>Recent content in Notes on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 22 Sep 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://alonelysheep.github.io/quartz-blog/notes/index.xml" rel="self" type="application/rss+xml"/><item><title>'That being so' &amp; 'With that said' vs 'Having said this'</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.9/That-being-so-With-that-said-vs-Having-said-this/</link><pubDate>Thu, 22 Sep 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.9/That-being-so-With-that-said-vs-Having-said-this/</guid><description>因而, 尽管如此, 话虽如此 2022-09-22 Tags: #English
&amp;lsquo;That being so&amp;rsquo; 类似于汉语里面的&amp;rsquo;因而&amp;rsquo;, 前后两个句子是衔接关系, 或者因果关系 &amp;lsquo;With that said&amp;rsquo; 类似于汉语里面的&amp;rsquo;话虽如此&amp;rsquo;, 表示句子 A 和句子 B 之间出现了一个 gap, 但是否定意思没有&amp;rsquo;Having said that&amp;rsquo;那么的强烈 &amp;lsquo;Having said that&amp;rsquo; 类似于&amp;rsquo;尽管如此, 话虽如此&amp;rsquo; 表示前后句子出现了一个冲突.</description></item><item><title>Vision Transformer (ViT)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.9/Vision-Transformer-ViT/</link><pubDate>Thu, 22 Sep 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.9/Vision-Transformer-ViT/</guid><description>Vision Transformer 2022-09-22 Tags: #ViT
How the Vision Transformer works in a nutshell1 The total architecture is called Vision Transformer (ViT in short).</description></item><item><title>indention in YAML</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.9/indention-in-YAML/</link><pubDate>Mon, 05 Sep 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.9/indention-in-YAML/</guid><description>YAML里面的缩进 2022-09-05 Tags: #YAML #Frontmatter
YAML(YAML Ain&amp;rsquo;t Markup Language)可以在 Markdown 里面用于存储原始数据. 但是 YAML 里面的缩进却难住了我.
一个 YAML 的例子如下:
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 doe:&amp;#34;a deer, a female deer&amp;#34;ray:&amp;#34;a drop of golden sun&amp;#34;pi:3.</description></item><item><title>了解信用卡</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.7/%E4%BA%86%E8%A7%A3%E4%BF%A1%E7%94%A8%E5%8D%A1/</link><pubDate>Mon, 11 Jul 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.7/%E4%BA%86%E8%A7%A3%E4%BF%A1%E7%94%A8%E5%8D%A1/</guid><description>了解信用卡 2022-07-11 Tags: #DailyLife #CreditCard
信用卡是什么? 信用卡（英语：Credit Card），是一种非现金交易付款的方式，是银行业提供的信贷服务。 与一般的信用卡与借记卡、提款卡不同，信用卡在消费时不会直接扣除用户的资金，而是等到账单日时再进行还款。 信用卡的历史 根据维基百科，最早的信用支付出现于 19 世纪末的资本主义重镇英国，大约在十九世纪八十年代，针对有钱人购买昂贵的奢侈品却没有随身携带那么多钱，英国服装业发展出所谓的信用制度，利用记录卡，购物的时候可以及早带流行商品回去，旅游业与商业部门也都跟随这个潮流抢占商机。
但当时的卡片仅能进行在特定场所的短期商业赊借行为，款项还是要随用随付，不能长期拖欠，也没有授信额度，完全是依赖富裕人口的资本信用而设计。
20 世纪 50 年代，第一张针对大众的信用卡出现，美国曼哈顿信贷专家麦克纳马拉（Frank McNamara）在饭店用餐，由于没有带足够的钱，只能让太太送钱过来。这让他觉得很狼狈，于是组织了“食客俱乐部”（英语：Diners Club，即为大来卡），任何人获准成为会员后，带一张就餐记账卡到指定 27 间餐厅就可以记账消费，不必付现金，这就是最早的信用卡。</description></item><item><title>有效地背诵GRE单词</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.7/%E6%9C%89%E6%95%88%E5%9C%B0%E8%83%8C%E8%AF%B5GRE%E5%8D%95%E8%AF%8D/</link><pubDate>Sat, 09 Jul 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.7/%E6%9C%89%E6%95%88%E5%9C%B0%E8%83%8C%E8%AF%B5GRE%E5%8D%95%E8%AF%8D/</guid><description>有效地背诵 GRE 单词 2022-07-09 Bathos Bathos is a coined word(from Greek bathys, “deep”), which means unsuccessful, and therefore ludicrous, attempt to portray pathos in art, i.</description></item><item><title>数据预处理-周期性数据(时间等)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.6/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-%E5%91%A8%E6%9C%9F%E6%80%A7%E6%95%B0%E6%8D%AE%E6%97%B6%E9%97%B4%E7%AD%89/</link><pubDate>Tue, 14 Jun 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.6/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-%E5%91%A8%E6%9C%9F%E6%80%A7%E6%95%B0%E6%8D%AE%E6%97%B6%E9%97%B4%E7%AD%89/</guid><description>周期性数据的预处理 2022-06-14 Tags: #MachineLearning #DataPreprocessing #CyclicFeatureEncoding
Source: Three Approaches to Encoding Time Information as Features for ML Models | NVIDIA Technical Blog</description></item><item><title>Un- is not always negative - Ravel &amp; Unravel</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.6/Un-is-not-always-negative-Ravel-Unravel/</link><pubDate>Mon, 13 Jun 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.6/Un-is-not-always-negative-Ravel-Unravel/</guid><description>Ravel 和 Unravel 有相同的意思 2022-06-13 Tags: #English
When &amp;lsquo;Un-&amp;rsquo; Isn&amp;rsquo;t Negative | Merriam-Webster
These are three instances in which a verb beginning with un- means the same as, rather than the negative or opposite of, its stem.</description></item><item><title>JS散度</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.6/JS%E6%95%A3%E5%BA%A6/</link><pubDate>Wed, 08 Jun 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.6/JS%E6%95%A3%E5%BA%A6/</guid><description>Jensen–Shannon divergence 2022-06-08 Tags: #JSDivergence
KL 散度是不对称的, 这使得在训练过程中可能出现一些问题，所以我们在 KL 散度基础上引入 JS 散度 定义 The Jensen–Shannon divergence (JSD) $M_+^1( A ) × M_+^1(A) → [ 0 , ∞ )$ 1is a symmetrized and smoothed version of the Kullback–Leibler divergence $D ( P ∥ Q )$ .</description></item><item><title>Imbalanced Data - 数据不均衡</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.6/Imbalanced-Data-%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%9D%87%E8%A1%A1/</link><pubDate>Tue, 07 Jun 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.6/Imbalanced-Data-%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%9D%87%E8%A1%A1/</guid><description>数据不均衡 2022-06-07 Tags: #DataPreprocessing
什么是数据不均衡 A classification data set with skewed class proportions is called imbalanced. Classes that make up a large proportion of the data set are called majority classes.</description></item><item><title>从二项分布到泊松分布再到指数分布-From Binomial Distribution to Poisson Distribution to Exponential Distribution</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.5/%E4%BB%8E%E4%BA%8C%E9%A1%B9%E5%88%86%E5%B8%83%E5%88%B0%E6%B3%8A%E6%9D%BE%E5%88%86%E5%B8%83%E5%86%8D%E5%88%B0%E6%8C%87%E6%95%B0%E5%88%86%E5%B8%83-From-Binomial-Distribution-to-Poisson-Distribution-to-Exponential-Distribution/</link><pubDate>Sun, 22 May 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.5/%E4%BB%8E%E4%BA%8C%E9%A1%B9%E5%88%86%E5%B8%83%E5%88%B0%E6%B3%8A%E6%9D%BE%E5%88%86%E5%B8%83%E5%86%8D%E5%88%B0%E6%8C%87%E6%95%B0%E5%88%86%E5%B8%83-From-Binomial-Distribution-to-Poisson-Distribution-to-Exponential-Distribution/</guid><description>From Binomial Distribution to Poisson Distribution to Exponential Distribution 2022-05-22 Tags: #Math/Probability #PoissonDistribution #BinomialDistribution #ExponentialDistribution
这两个回答讲的挺好: 泊松分布的现实意义是什么，为什么现实生活多数服从于泊松分布？ - 马同学的回答 - 知乎 https://www.</description></item><item><title>Jupyter_Notebook-使用代理</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.5/Jupyter_Notebook-%E4%BD%BF%E7%94%A8%E4%BB%A3%E7%90%86/</link><pubDate>Wed, 18 May 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.5/Jupyter_Notebook-%E4%BD%BF%E7%94%A8%E4%BB%A3%E7%90%86/</guid><description>如何在Jupyter Notebook里面使用代理 2022-05-18 Tags: #Jupyter #Python #Proxy
方案一: 使用 IPython 启动脚本 根据 这个回答 以及下面评论, IPython 可以在启动之前运行一些代码. 我们可以借助这个功能来为 Jupyter Notebook 设置代理 Windows 的启动脚本默认在以下目录: C:\Users\[用户名]\.</description></item><item><title>F1_Score</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.5/F1_Score/</link><pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.5/F1_Score/</guid><description>F1 Score 2022-05-05 Tags: #F1Score
The traditional F-measure or balanced F-score (F1 score) is the Harmonic_Mean-调和平均数 of Precision Recall: $$F_{1}=\frac{2}{\text { recall }^{-1}+\text { precision }^{-1}}=2 \cdot \frac{\text { precision } \cdot \text { recall }}{\text { precision }+\text { recall }}=\frac{\text { tp }}{t p+\frac{1}{2}(\mathrm{fp}+\mathrm{fn})}$$</description></item><item><title>Precision Recall and Accuracy</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.5/Precision-Recall-and-Accuracy/</link><pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.5/Precision-Recall-and-Accuracy/</guid><description>Precision, Recall &amp;amp; Accuracy 2022-05-05 Tags: #Precision #Recall
圈圈是模型的预测: 圈圈里面是预测的 Positive, 圈圈外面是预测的 Negative 方方是真实的情况: 左半边是真实的 Positive(实心点点), 右半边是真实的 Negative(空心点点). 1 Precision 是预测的 Positive 有多少是对的: 即查准率.</description></item><item><title>端到端学习-End_to_End_Learning-E2E</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.5/%E7%AB%AF%E5%88%B0%E7%AB%AF%E5%AD%A6%E4%B9%A0-End_to_End_Learning-E2E/</link><pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.5/%E7%AB%AF%E5%88%B0%E7%AB%AF%E5%AD%A6%E4%B9%A0-End_to_End_Learning-E2E/</guid><description>端到端学习 2022-05-05 Tags: #EndtoEndLearning #MachineLearning #DeepLearning
端到端的学习就是省略中间步骤，直接从输入得到输出结果。 Pro &amp;amp; Con Pro 不用人为设计中间步骤, 减少了工作量, 并且避免人为添加的步骤给模型带来不好的 归纳偏置. Con 需要足够多的数据才能达到较好的效果</description></item><item><title>Harmonic_Mean-调和平均数</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.5/Harmonic_Mean-%E8%B0%83%E5%92%8C%E5%B9%B3%E5%9D%87%E6%95%B0/</link><pubDate>Tue, 03 May 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.5/Harmonic_Mean-%E8%B0%83%E5%92%8C%E5%B9%B3%E5%9D%87%E6%95%B0/</guid><description>Harmonic Mean 2022-05-03 Tags: #HarmonicMean #Math
用 $H$ 表示两个数的调和平均数, 则: $$\frac{1}{H}=\frac{1}{2}\left(\frac{1}{x_{1}}+\frac{1}{x_{2}}\right)$$ 显式地表示为: $$H=\frac{2 x_{1} x_{2}}{x_{1}+x_{2}}$$
直观理解 用紫色线段 $H$ 表示 $a, b$ 的Harmonic Mean: 其中:</description></item><item><title>离岸人民币与在岸人民币</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.5/%E7%A6%BB%E5%B2%B8%E4%BA%BA%E6%B0%91%E5%B8%81%E4%B8%8E%E5%9C%A8%E5%B2%B8%E4%BA%BA%E6%B0%91%E5%B8%81/</link><pubDate>Tue, 03 May 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.5/%E7%A6%BB%E5%B2%B8%E4%BA%BA%E6%B0%91%E5%B8%81%E4%B8%8E%E5%9C%A8%E5%B2%B8%E4%BA%BA%E6%B0%91%E5%B8%81/</guid><description>汇率计算中两种不同的人民币 2022-05-03 Tags: #ExchangeRate #CHY
其实简单来说就是，在岸价是在国内换汇的汇率，离岸价是在国外换汇的汇率。
离岸人民币(CNH) 离岸——在中国境外经营人民币业务(CHY offshore)。央行开放香港以及其他国家进行人民币交易的汇率就叫离岸人民币，而2010年中国香港实施的人民币离岸交易(CNH)已经是泛指海外离岸人民币交易。
目前主要的人民币离岸市场在香港，新加坡、伦敦、台湾也在积极发展人民币离岸市场。一般来说，国外希望人民币大幅升值，这样有利于打开中国市场，但国内希望增加出口，所以离岸汇率高于在岸汇率。
在岸人民币(CHY) 在岸——在国内经营的人民币业务。央行授权中国外汇中心于每个工作日上午对外公布当日人民币兑换美元、欧元、日元、港币汇率的中间价作为当日银行间即期外汇市场以及银行柜台交易汇率的参考价格，这就叫在岸人民币。 区别 在岸人民币市场发展的时间比较长、规模比较大、受到的管制也比较多，在岸汇率受到央妈政策影响比较大。而离岸则相反，他的汇率变化主要是受到国际因素影响比较多，通常离岸人民币市场更能充分反映市场对人民币供需。 离岸人民币、在岸人民币和人民币中间价的区别，你分得清楚吗？ — 搜航网 CNY vs CNH - Differences Between Two Types of Renminbi - Wise, formerly TransferWise</description></item><item><title>D2L-75-BERT</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-75-BERT/</link><pubDate>Sat, 30 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-75-BERT/</guid><description>Bidirectional Encoder Representations from Transformers (BERT) 2022-04-30 Tags: #BERT #Transformer #DeepLearning
Motivation 构建一个通用的语言模型 在计算机视觉领域中, 我们能对一个已经训练好的大型网络进行微调(Fine-tune), 以较小的计算成本和网络改动就能获得很好的模型. BERT就是期望能够构建一个足够强大的预训练模型(Pretrained), 来适配各种各样的任务. 结合两个现有架构的优点: ELMo &amp;amp; GPT GPT: task-agnostic 其实在 BERT 以前, OpenAI 已经提出了 GPT （Generative Pre-Training，生成式预训练）模型, 试图提供一种既考虑上下文语意(context-sensitive), 又能适配多任务(task-agnostic1)的模型</description></item><item><title>D2L-76-BERT - Pretrain</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-76-BERT-Pretrain/</link><pubDate>Sat, 30 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-76-BERT-Pretrain/</guid><description>BERT: Pretrain 2022-04-30 Tags: #BERT #Pretrain #DeepLearning #Transformer
Pretrain Tasks Task 1 - Masked Language Modeling Motivation 语言模型(Language Model) 在输出时是从左到右进行的, 使用左侧的上下文来预测未知词元。</description></item><item><title>D2L-77-BERT - Fine-tune</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-77-BERT-Fine-tune/</link><pubDate>Sat, 30 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-77-BERT-Fine-tune/</guid><description>BERT: Fine-tune 2022-04-30 Tags: #BERT #Fine-tune #DeepLearning #Transformer
预训练好 BERT 以后, 我们只需要对模型进行很小的改动即可适配很多任务. 在 Finetuning 的时候, 新增的输出部分是从头开始训练的, 而 BERT 主体部分是在 pre-train 的基础上进行训练的.</description></item><item><title>D2L-71-Multi-Head_Attention</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-71-Multi-Head_Attention/</link><pubDate>Wed, 27 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-71-Multi-Head_Attention/</guid><description>多头注意力 2022-04-27 Tags: #Attention #Multi-headAttention #DeepLearning
多头注意力就是对 Query, Key, Value 进行一些线性变换, 并行地计算多个注意力, 期望模型能学习到多样化的依赖关系. Another way of seeing it: 1 模型构建 下面我们给出 Multi-Head Attention 的形象化表示:</description></item><item><title>D2L-73-Positional_Encoding</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-73-Positional_Encoding/</link><pubDate>Wed, 27 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-73-Positional_Encoding/</guid><description>位置编码: 将位置信息加入数据 2022-04-27 Tags: #PositionalEncoding #Self-Attention #DeepLearning
为了使用序列的顺序信息，我们通过在输入表示中添加 位置编码（positional encoding）来注入绝对的或相对的位置信息。
我觉得D2L讲的很深入很好了: 10.6. Self-Attention and Positional Encoding</description></item><item><title>D2L-74-Transformer</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-74-Transformer/</link><pubDate>Wed, 27 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-74-Transformer/</guid><description>Transformer 2022-04-27 Tags: #Transformer #Attention #DeepLearning
1
Transformer 是一个纯基于 Attention 的 Encoder Decoder 架构模型
Hugging Face Explorable Transformer: exBERT</description></item><item><title>D2L-72-Self-Attention</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-72-Self-Attention/</link><pubDate>Tue, 26 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-72-Self-Attention/</guid><description>自注意力 2022-04-26 Tags: #Self-Attention #Attention #DeepLearning
Attention 机制可以抽象为:1 $$\begin{align} \textit{Attention}(Q,K,V) = V\cdot\textit{softmax}\space (\textit{score}(Q, K)) \end{align}$$ 自注意力就是 $Q = K = V$ , 也就是同一个序列同时作为 Query, Key 和 Value.</description></item><item><title>Difference between Purple and Violet</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/Difference-between-Purple-and-Violet/</link><pubDate>Sat, 23 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/Difference-between-Purple-and-Violet/</guid><description>Difference between Purple and Violet 2022-04-23 Tags: #Purple #Violet #English #Color
Purple是复色光(Blue and Red 1:1)激发的颜色, 而Violet是单色光激发的颜色 因而看起来Purple要Red一点, 饱和度要高一点. In depth reading: Difference between ‘violet’ and ‘purple’</description></item><item><title>D2L-70-Seq2Seq with Attention - Bahdanau Attention</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-70-Seq2Seq-with-Attention-Bahdanau-Attention/</link><pubDate>Fri, 22 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-70-Seq2Seq-with-Attention-Bahdanau-Attention/</guid><description>含注意力机制的Seq2Seq 2022-04-22 Tags: #Seq2Seq #Attention #DeepLearning #RNN
Motivation 在 Seq2Seq模型里面, Encoder向Decoder传递的仅仅是最后一个时间步的隐状态, 也就是上下文变量 $\mathbf c= \mathbf{h}_T$, 我们假设里面已经包含了输入序列的所有信息: 但这样每一步Decoder的输入都是原序列的一个&amp;quot;全局, 笼统的总结&amp;quot;, 这是不太合理的: 在下图中, 在翻译&amp;quot;Knowledge&amp;quot;的的时候, 显然&amp;quot;力量&amp;quot;这个词是不太重要的.</description></item><item><title>RNN中output和hidden_state的区别</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/RNN%E4%B8%ADoutput%E5%92%8Chidden_state%E7%9A%84%E5%8C%BA%E5%88%AB/</link><pubDate>Fri, 22 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/RNN%E4%B8%ADoutput%E5%92%8Chidden_state%E7%9A%84%E5%8C%BA%E5%88%AB/</guid><description>Difference between output and hidden_state in RNN 2022-04-22 Tags: #RNN
首先要将RNN理解为一个二维的网络, 它不仅可能有多个隐藏层, 还在时间维度上有多个时间步. output是 最后一层隐藏层 在 每一个时间步 的状态</description></item><item><title>D2L-67-Attention Scoring Function</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-67-Attention-Scoring-Function/</link><pubDate>Thu, 21 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-67-Attention-Scoring-Function/</guid><description>注意力评分函数 2022-04-21 Tags: #Attention #DeepLearning
抽取出Attention Pooling里面都有的Softmax部分, 我们可以将注意力机制的设计简化为Attention Scoring Function的设计. 形式化的表达如下:
query $\mathbf{q} \in \mathbb{R}^q$,</description></item><item><title>D2L-68-Additive Attention</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-68-Additive-Attention/</link><pubDate>Thu, 21 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-68-Additive-Attention/</guid><description>加性注意力 2022-04-21 Tags: #Attention #DeepLearning
一般来说，当Query和Key是不同长度的矢量时，我们可以使用Additive Attention来作为Scoring Function。
给定查询 $\mathbf{q} \in \mathbb{R}^q$ 和键 $\mathbf{k} \in \mathbb{R}^k$，加性注意力（additive attention）的评分函数(Scoring Function)为 $$a(\mathbf q, \mathbf k) = \mathbf w_v^\top \text{tanh}(\mathbf W_q\mathbf q + \mathbf W_k \mathbf k) \in \mathbb{R},$$</description></item><item><title>D2L-69-Scaled Dot-Product Attention</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-69-Scaled-Dot-Product-Attention/</link><pubDate>Thu, 21 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-69-Scaled-Dot-Product-Attention/</guid><description>缩放的点积注意力 2022-04-21 Tags: #Attention #DeepLearning
1
相比Additive Attention, 使用点积可以得到计算效率更高的Scoring Function. 但是点积操作要求查询和键具有相同的长度 $d$。
我们知道 内积可以衡量两个向量之间的相似程度, 所以我们可以这样解读缩放点积注意力:</description></item><item><title>D2L-62-BLEU (Bilingual Evaluation Understudy)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-62-BLEU-Bilingual-Evaluation-Understudy/</link><pubDate>Wed, 20 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-62-BLEU-Bilingual-Evaluation-Understudy/</guid><description>BLEU (Bilingual Evaluation Understudy) 2022-04-20 Tags: #BLEU #DeepLearning
BLEU 是一种用于评价输出序列质量的评价指标, 其特点在于它考虑到了序列长度和预测难度的关系.
BLEU 通过综合&amp;quot;不同n-gram在结果中的成功次数&amp;quot;来评价最终质量的好坏.
定义 $$ \exp\left(\min\left(0, 1 - \frac{\mathrm{len}{\text{label}}}{\mathrm{len}{\text{pred}}}\right)\right) \prod_{n=1}^k p_n^{1/2^n}$$</description></item><item><title>D2L-64-Kernel Regression</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-64-Kernel-Regression/</link><pubDate>Wed, 20 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-64-Kernel-Regression/</guid><description>Nadaraya-Watson Kernel Regression 2022-04-20 Tags: #KernelRegression #Nonparametric #Attention #MachineLearning
Intuition Definition $$f(x) = \sum_{i=1}^n \frac{K(x - x_i)}{\sum_{j=1}^n K(x - x_j)} y_i$$</description></item><item><title>D2L-65-Attention Cues &amp; Attention Mechanisms</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-65-Attention-Cues-Attention-Mechanisms/</link><pubDate>Wed, 20 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-65-Attention-Cues-Attention-Mechanisms/</guid><description>注意力信号 与 注意力机制 2022-04-20 Tags: #Attention #DeepLearning
Attention Cue Attention Cue分为两种: nonvolitional cue 和 volitional cue. Your volition is the power you have to decide something for yourself.</description></item><item><title>D2L-66-Kernel Regression and Attention</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-66-Kernel-Regression-and-Attention/</link><pubDate>Wed, 20 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-66-Kernel-Regression-and-Attention/</guid><description>Kernel Regression And Attention 2022-04-20 Tags: #KernelRegression #Attention #DeepLearning
Nadaraya-Watson kernel regression is an example of machine learning with attention mechanisms.</description></item><item><title>Indexing a tensor or ndarray with `None`</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/Indexing-a-tensor-or-ndarray-with-None/</link><pubDate>Wed, 20 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/Indexing-a-tensor-or-ndarray-with-None/</guid><description>None as index 2022-04-20 Tags: #Numpy #PyTorch
None in index is equivalent to unsqueeze() Similar to NumPy you can insert a singleton dimension (&amp;ldquo;unsqueeze&amp;rdquo; a dimension) by indexing this dimension with None.</description></item><item><title>D2L-60-Encoder-Decoder</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-60-Encoder-Decoder/</link><pubDate>Tue, 19 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-60-Encoder-Decoder/</guid><description>编码器-解码器架构 2022-04-19 Tags: #EncoderDecoder #RNN #DeepLearning
Encoder-Decoder将模型分为两部分, 使得我们可以先用编码器处理不规则的输入, 然后再将输出送入Decoder得到最终结果.
Encoder-Decoder是一种抽象的模型架构, 可以有许多不同的实现方式.
有的时候Decoder也需要Input, 所以上图也可以表示成下面的样子:</description></item><item><title>D2L-61-Sequence to Sequence Learning - Seq2Seq</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-61-Sequence-to-Sequence-Learning-Seq2Seq/</link><pubDate>Tue, 19 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-61-Sequence-to-Sequence-Learning-Seq2Seq/</guid><description>Seq2Seq: 序列到序列模型 2022-04-19 Tags: #Seq2Seq #EncoderDecoder #DeepLearning #RNN
Seq2Seq也就是Sequence to Sequence, 顾名思义, 它实现的是一种序列到另一种序列的转换(比如从英语到中文). Seq2Seq符合 Encoder-Decoder架构 总览 如上图所示, 首先Encoder输入长度可变的序列， 并将其转换为固定形状的隐状态。然后隐状态输入Decoder, 解码器根据隐状态和输入来生成最后的输出.</description></item><item><title>Variant or Variance</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/Variant-or-Variance/</link><pubDate>Tue, 19 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/Variant-or-Variance/</guid><description>Variant or Variance 2022-04-19 Tags: #English
They are pretty much the same.
Very strictly speaking, variation is change, and a variant is one of the forms resulting from the change.</description></item><item><title>Viterbi Algorithm</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/Viterbi-Algorithm/</link><pubDate>Tue, 19 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/Viterbi-Algorithm/</guid><description>维特比算法 2022-04-19 Tags: #Viterbi
Viterbi 算法是一个用于求解最佳路径的动态规划算法。
Viterbi 算法常常用于HMM模型里面，用于寻找最有可能产生观测事件序列的维特比路径——隐含状态序列
下面这个回答解释的很清楚了： 如何通俗地讲解 Viterbi 算法？ https://www.zhihu.com/question/20136144/answer/763021768</description></item><item><title>D2L-57-LSTM-长短期记忆网络</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-57-LSTM-%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C/</link><pubDate>Mon, 18 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-57-LSTM-%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C/</guid><description>Long Short-Term Memory 2022-04-18 Tags: #LSTM #DeepLearning #RNN
LSTM是最早用于解决长期依赖问题的一种RNN. 它比GRU复杂, 但是设计思想是一样的. 有趣的是, LSTM(1997)比GRU(2014)早出现近20年.
LSTM和GRU一样, 使用了不同的门(Gate)来控制上一个隐状态在下一个隐状态里面的占比, 也就是有选择地来混合&amp;quot;长期记忆&amp;quot;和&amp;quot;短期记忆&amp;quot;, 这也是其名称的由来.</description></item><item><title>D2L-58-深度循环神经网络</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-58-%E6%B7%B1%E5%BA%A6%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link><pubDate>Mon, 18 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-58-%E6%B7%B1%E5%BA%A6%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</guid><description>Deep Recurrent Neural Networks 2022-04-18 Tags: #RNN #DeepLearning
和MLP与CNN中一样, 我们可以通过添加更多的层来增强网络的表达能力. 但不同的是, 增加的每一层都需要在时间步上展开, 就像上图一样. 具体的来说, 除了边缘部分外, 每一个隐状态 $H^{(l)}_t$ 同时接受上一层同一时间步的 $\textcolor{red}{H^{(l-1)}t}$ 和同一层上一时间步的 $\textcolor{red}{H^{(l)}{t-1}}$ 作为输入, 并且输出到下一层同一时间步的 $\textcolor{royalblue}{H^{(l+1)}t}$ 和同一层下一时间步的 $\textcolor{royalblue}{H^{(l)}{t+1}}$ 用GRU或LSTM的隐状态代替上图中的隐状态，便得到深度GRU或深度LSTM。 形式化定义 假设在时间步 $t$ 有一个小批量的输入数据 $\mathbf{X}_t \in \mathbb{R}^{n \times d}$（样本数：$n$，每个样本中的输入数：$d$）。</description></item><item><title>D2L-59-双向循环神经网络</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-59-%E5%8F%8C%E5%90%91%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link><pubDate>Mon, 18 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-59-%E5%8F%8C%E5%90%91%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</guid><description>Bidirectional Recurrent Neural Networks 2022-04-18 Tags: #RNN #DeepLearning #BidirectionalRNN
双向神经网络增加了反向扫描的隐藏层, 使网络拥有了&amp;quot;前瞻能力&amp;quot; 正向层和反向层的输入是相同的, 是并行进行的, 最后正向和反向的结果一起生成输出. 在D2L教程里面将正向反向扫描的过程和隐马尔科夫模型动态规划的正向与反向传递1进行了类比: 这种转变集中体现了现代深度网络的设计原则： 首先使用经典统计模型的函数依赖类型，然后将其参数化为通用形式。</description></item><item><title>凸组合 - Convex Combination</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/%E5%87%B8%E7%BB%84%E5%90%88-Convex-Combination/</link><pubDate>Mon, 18 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/%E5%87%B8%E7%BB%84%E5%90%88-Convex-Combination/</guid><description>Convex Combination 2022-04-18 Tags: #NonlinearProgreamming #Math #ConvexCombination
A convex combination of points $\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \ldots, \mathbf{x}^{(k)} \in \mathbb{R}^{n}$ is a &amp;ldquo;weighted average&amp;rdquo;: a linear combination $$ \lambda_{1} \mathbf{x}^{(1)}+\lambda_{2} \mathbf{x}^{(2)}+\cdots+\lambda_{k} \mathbf{x}^{(k)} $$ where $\lambda_{1}+\lambda_{2}+\cdots+\lambda_{k}=1$ and $\lambda_{1}, \ldots, \lambda_{k} \geq 0$</description></item><item><title>前馈神经网络(Feedforward neural network)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CFeedforward-neural-network/</link><pubDate>Tue, 12 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CFeedforward-neural-network/</guid><description>前馈神经网络: Feedforward Neural Network 2022-04-12 Tags: #FeedforwardNeuralNetwork
多层前馈神经网络的结构如下图所示： 每层神经元与下一层神经元全部互连 同层神经元之间不存在连接 跨层神经元之间也不存在连接</description></item><item><title>D2L-56-门控循环单元GRU</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-56-%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83GRU/</link><pubDate>Sun, 03 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-56-%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83GRU/</guid><description>Gated Recurrent Units (GRU) 2022-04-03 Tags: #GRU #RNN #DeepLearning
GRU在RNN的基础上添加了&amp;quot;门&amp;quot;(Gate), 针对性地解决了RNN里面存在的以下问题: 长期依赖问题: 序列早期的部分可能对未来所有观测值都有非常重要的影响, 我们需要能够保留序列早期信息的网络结构. GRU里面体现在: 重置门减少重置, 更新门更多地保留上一个隐状态 序列里面可能有干扰信息, 我们需要能够跳过(遗忘)这些信息的机制 GRU里面体现在: 更新门更多地保留上一个隐状态 序列里面可能有逻辑中断, 比如一本书里面章节的变化往往会导致主题的变化.</description></item><item><title>Latex White Spaces</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/Latex-White-Spaces/</link><pubDate>Sun, 03 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/Latex-White-Spaces/</guid><description>Latex 里面的空格 2022-04-03 Tags: #Latex
There are a number of horizontal spacing macros for LaTeX :
\, inserts a .</description></item><item><title>D2L-54-Gradient Clipping-梯度剪裁</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-54-Gradient-Clipping-%E6%A2%AF%E5%BA%A6%E5%89%AA%E8%A3%81/</link><pubDate>Sat, 02 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-54-Gradient-Clipping-%E6%A2%AF%E5%BA%A6%E5%89%AA%E8%A3%81/</guid><description>Gradient Clipping 2022-04-02 Tags: #GradientClipping
梯度剪裁是预防梯度爆炸的一种方法, 它直接给梯度设置一个上限. $$\mathbf{g} \leftarrow \min \left(1, \frac{\theta}{|\mathbf{g}|}\right) \mathbf{g}$$
上面的写法有点绕, 因为为了保持梯度 $\mathbf{g}$ 的方向不变, 剪裁时需要作用于 $\mathbf{g}$ 的每一个分量, 整体上来说其实就是: $$\mathbf{g} \leftarrow \min \left(|\mathbf{g}|, \theta \frac{\mathbf{g}}{|\mathbf{g}|}\right)$$ 相比直接减小学习率，Clipping是分段的, 可以只在梯度较大时加以限制.</description></item><item><title>D2L-55-在时间上反向传播</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-55-%E5%9C%A8%E6%97%B6%E9%97%B4%E4%B8%8A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/</link><pubDate>Sat, 02 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-55-%E5%9C%A8%E6%97%B6%E9%97%B4%E4%B8%8A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/</guid><description>Backpropagation Through Time 2022-04-02 Tags: #Backpropagation #RNN
和正向传播的时候一样, RNN在反向传播的时候需要在时间步上面进行迭代, 这可能导致梯度问题. 下面我们先大概分析在&amp;quot;时间上&amp;quot;反向传播的不同之处, 然后简要介绍一些缓解梯度问题的训练方法, 最后, 我们详细的分析一下在时间上反向传播的细节问题. 这篇笔记以 8.7. Backpropagation Through Time — Dive into Deep Learning 为基础.</description></item><item><title>汉字排检法</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/%E6%B1%89%E5%AD%97%E6%8E%92%E6%A3%80%E6%B3%95/</link><pubDate>Sat, 02 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/%E6%B1%89%E5%AD%97%E6%8E%92%E6%A3%80%E6%B3%95/</guid><description>汉字排检法 2022-04-02 Tags: #ChineseCharacters
汉字作为象形文字, 在信息化的过程中面临着先天的困难.
其实除了拼音, 汉字是有许多索引方式的, 这些方法统称汉字排检法. 比如过去的四角号码检字法.
其实相比基于语音的拼音, 四角号码就像五笔一样, 你即使认不到这个字, 也可以输入, 这是基于字形的检索方法的优势. 汉字排检法_百度百科</description></item><item><title>递推公式 $a_{t}=b_{t}+c_{t}a_{t-1}$ 转通项公式</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/%E9%80%92%E6%8E%A8%E5%85%AC%E5%BC%8F-a_tb_t+c_ta_t-1-%E8%BD%AC%E9%80%9A%E9%A1%B9%E5%85%AC%E5%BC%8F/</link><pubDate>Sat, 02 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/%E9%80%92%E6%8E%A8%E5%85%AC%E5%BC%8F-a_tb_t+c_ta_t-1-%E8%BD%AC%E9%80%9A%E9%A1%B9%E5%85%AC%E5%BC%8F/</guid><description>递推公式 $a_{t}=b_{t}+c_{t}a_{t-1}$ 转通项公式 2022-04-02 Tags: #Math
$$\begin{aligned} a_{t}=b_{t} &amp;amp;+c_{t} a_{t-1} \ &amp;amp;+ c_{t}\left(b_{t-1}+c_{t-1} a_{t-2}\right) \ &amp;amp;\hspace{4.25em}+c_{t-1}\left(b_{t-2}+c_{t-2} a_{t-3}\right) \ &amp;amp;\hspace{13em}\vdots \ &amp;amp;\hspace{12.</description></item><item><title>D2L-53-循环神经网络RNN</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-53-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN/</link><pubDate>Fri, 01 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-53-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN/</guid><description>Recurrent Neural Networks 2022-04-01 Tags: #RNN #DeepLearning #NeuralNetwork
Motivation 基于马尔可夫假设的N元语法（n-gram）需要存储大量的参数。在 $n$ 逐渐增大的过程中，n-gram模型的参数大小 $|W|$ 与序列长度 $n$ 是指数关系：$$|W|=|\mathcal{V}|^n $$ ($|\mathcal{V}|$ 是单词的数目) 因此, 我们将目光转向了 隐变量自回归模型.</description></item><item><title>cmd_powershell_bash-Comparison</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/cmd_powershell_bash-Comparison/</link><pubDate>Fri, 18 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/cmd_powershell_bash-Comparison/</guid><description>CMD / Powershell / bash: Comparison 2022-03-18 Tags: #Terminal #OperatingSystem
Bash是Unix系统的, 而前两个是Windows的 Powershell是CMD的升级版</description></item><item><title>D2L-49-文本预处理-Text Preprocessing</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-49-%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86-Text-Preprocessing/</link><pubDate>Tue, 08 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-49-%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86-Text-Preprocessing/</guid><description>文本预处理 2022-03-08 Tags: #Preprocessing
动手是最好的学习: 8.2. 文本预处理 — 动手学深度学习 2.0.0-beta0 documentation
一些常见的操作:
将文本作为字符串加载到内存中。 将字符串拆分为词元（如单词和字符）。Tokenize 建立一个词表，将拆分的词元映射到数字索引。 将文本转换为数字索引序列，方便模型操作。 语料 in English: corpus</description></item><item><title>D2L-50-语言模型-传统模型的不足</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-50-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-%E4%BC%A0%E7%BB%9F%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8D%E8%B6%B3/</link><pubDate>Tue, 08 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-50-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-%E4%BC%A0%E7%BB%9F%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8D%E8%B6%B3/</guid><description>语言模型 2022-03-08 Tags: #LanguageModel
传统模型 语言模型的输出是一个文本序列: $$x_{t},\space x_{t-1},\space \ldots\space ,\space x_{1}$$ 为了生成有意义的序列, 我们希望模拟语料库里面的语句, 生成概率 $P\left(x_{1}, x_{2}, \ldots, x_{T}\right)$ 最高的语句.</description></item><item><title>D2L-51-语言的统计特征</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-51-%E8%AF%AD%E8%A8%80%E7%9A%84%E7%BB%9F%E8%AE%A1%E7%89%B9%E5%BE%81/</link><pubDate>Tue, 08 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-51-%E8%AF%AD%E8%A8%80%E7%9A%84%E7%BB%9F%E8%AE%A1%E7%89%B9%E5%BE%81/</guid><description>语言的统计特征 2022-03-08 Tags: #Zipf_Law
n元语法 n-gram 我们将涉及一个、两个和三个变量的概率公式的模型分别称为 “一元语法”（unigram）、“二元语法”（bigram）和“三元语法”（trigram）模型. $$\begin{aligned} &amp;amp;P\left(x_{1}, x_{2}, x_{3}, x_{4}\right)=P\left(x_{4}\right) P\left(x_{3}\right) P\left(x_{2}\right) P\left(x_{1}\right) \ &amp;amp;P\left(x_{1}, x_{2}, x_{3}, x_{4}\right)=P\left(x_{4} \mid x_{3}\right) P\left(x_{3} \mid x_{2}\right) P\left(x_{2} \mid x_{1}\right) P\left(x_{1}\right) \ &amp;amp;P\left(x_{1}, x_{2}, x_{3}, x_{4}\right)=P\left(x_{4} \mid x_{2}, x_{3}\right)P\left(x_{3} \mid x_{1}, x_{2}\right) P\left(x_{2} \mid x_{1}\right) P\left(x_{1}\right) \end{aligned}$$</description></item><item><title>D2L-52-读取长序列数据的两种方法</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-52-%E8%AF%BB%E5%8F%96%E9%95%BF%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95/</link><pubDate>Tue, 08 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-52-%E8%AF%BB%E5%8F%96%E9%95%BF%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95/</guid><description>读取长序列数据的两种方法 2022-03-08 Tags: #SequentialData #DataPreprocessing
尽管序列数据本质上是连续的, 我们在处理数据的时候也希望将其分为小批量, 方便模型读取。 设我们将使用神经网络来训练语言模型， 模型中的网络一次处理具有预定义长度 （例如n个时间步）的一个小批量序列。 我们应该从上图中选择哪一个序列呢？ 事实上，他们都一样好。 然而，如果我们只选择一个偏移量， 那么用于训练网络的、所有可能的子序列的覆盖范围将是有限的。 因此，我们可以从随机偏移量开始划分序列， 以同时获得覆盖性（coverage）和随机性（randomness）。 下面，我们将描述如何实现随机采样（random sampling）和 顺序分区（sequential partitioning）策略。 随机采样 下图中 Batchsize=2, 也就是说每个Batch里面有两个序列, 一个红色的, 一个蓝色的.</description></item><item><title>D2L-46-DenseNet</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-46-DenseNet/</link><pubDate>Mon, 07 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-46-DenseNet/</guid><description>DenseNet 2022-03-07 Tags: #DenseNet #DeepLearning #CNN ResNet极大地改变了如何参数化深层网络中函数的观点。 稠密连接网络（DenseNet）在某种程度上是ResNet的逻辑扩展。
PDF(zotero://select/items/@huang2017densely)
数学直觉: 从ResNet到DenseNet 某个函数在 $x=0$ 处的泰勒展开为: $$f(x)=f(0)+f^{\prime}(0) x+\frac{f^{\prime \prime}(0)}{2 !</description></item><item><title>D2L-47-序列信息</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-47-%E5%BA%8F%E5%88%97%E4%BF%A1%E6%81%AF/</link><pubDate>Mon, 07 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-47-%E5%BA%8F%E5%88%97%E4%BF%A1%E6%81%AF/</guid><description>序列信息 2022-03-07 Tags: #SequentialData
数据分布的不同 对于图像或者表格数据， 我们通常都假设所有样本是独立同分布的1。 然而，大多数的数据都有序列性。 例如，文章中的单词是按顺序写的，如果顺序被随机地重排，就很难理解文章原始的意思。 同样，视频中的图像帧、对话中的音频信号以及网站上的浏览行为都是有顺序的。 因此，针对此类数据而设计特定模型，可能效果会更好。 实际情景 在接收一个序列作为输入的时候， 我们通常期望猜测这个序列的后续。 例如预测股市的波动、 患者的体温曲线或者赛车所需的加速度。 我们需要能够处理这些数据的特定模型。 相关模型 如果说卷积神经网络可以有效地处理空间信息， 那么 循环神经网络（recurrent neural network，RNN）则可以更好地处理序列信息。 #todo</description></item><item><title>D2L-48-序列模型-Sequence_Models</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-48-%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B-Sequence_Models/</link><pubDate>Mon, 07 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-48-%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B-Sequence_Models/</guid><description>序列模型 - Sequence Models 2022-03-07 Tags: #SequenceModel #SequentialData
两种流行的序列模型是自回归模型和隐变量自回归模型 预测问题 假设我们想要根据前 $t-1$ 天的股票价格预测今天的股票价格 $x_t$, 这个问题可以抽象为： $$x_{t} \sim P\left(x_{t} \mid x_{t-1}, \ldots, x_{1}\right)$$</description></item><item><title>D2L-45-ResNet</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-45-ResNet/</link><pubDate>Sun, 06 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-45-ResNet/</guid><description>ResNet 残差网络 2022-03-06 Tags: #ResNet #CNN #DeepLearning
ResNet在网络中引入了残差连接的思想, 简单的改变带来了很棒的效果.
残差连接让每一层很容易地包含了原始函数1, 这样能保证新增加的每一层都能包含原来的最优解, 进一步在原来的基础上继续改进.
Motivation 函数类的角度 我们定义 $\mathcal{F}$ 是某个模型能够拟合的所有函数构成的函数类.</description></item><item><title>D2L-43-GoogLeNet(Inception)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-43-GoogLeNetInception/</link><pubDate>Sat, 05 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-43-GoogLeNetInception/</guid><description>GoogLeNet 2022-03-05 Tags: #DeepLearning #CNN #GoogLeNet-Inception GoogLeNet是一个含并行连结的网络, 其核心组成部分为&amp;quot;Inception块&amp;quot;.
Inception块组合使用了不同大小的卷积核, 试图用现有的稠密结构(Dense Components)来构建一个&amp;quot;最佳的局部稀疏网络&amp;quot;.
局部: 多个Inception块拼接构成最后的GoogLeNet 稀疏: 也就是具有随机性的结构1 GoogLeNet还具有较高的计算效率, 这主要得益于Inception块里面不含全连接层.</description></item><item><title>D2L-44-Batch_Normalization-批量归一化</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-44-Batch_Normalization-%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96/</link><pubDate>Sat, 05 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-44-Batch_Normalization-%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96/</guid><description>Batch Normalization 2022-03-05 Tags: #BatchNormalization #Normalization #DeepLearning #Regularization
批量归一化是一种加速收敛的方法.
批量归一化作用于每一个mini-Batch, 先将这个Batch归一化, 然后再做一个统一的偏移与拉伸.
最后这个偏移和拉伸的量是一个可以学习的超参数 对于全连接层, BN作用于每一个特征</description></item><item><title>D2L-42-NiN</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-42-NiN/</link><pubDate>Fri, 04 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-42-NiN/</guid><description>Network in Network - NiN 2022-03-04 Tags: #DeepLearning #NiN #CNN
用卷积代替全连接 动机 全连接层很贵 (参数很多) 一层卷积层需要的参数为:
卷积层参数大小的计算 卷积层后面第一个全连接层的参数为: $$in_Channel\times in_Height\times in_Width\times num_of_Hidden_Units$$</description></item><item><title>Inanimate Whose</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Inanimate-Whose/</link><pubDate>Fri, 04 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Inanimate-Whose/</guid><description>Inanimate Whose 2022-03-04 Tags: #English
我们可以用Whose指代不是人的动物, 也可以用于非生命的物体(Inanimate Whose1就是这种情况).
例句: That&amp;rsquo;s the car whose alarm keeps waking us up at night.</description></item><item><title>D2L-41-VGG</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-41-VGG/</link><pubDate>Thu, 03 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-41-VGG/</guid><description>VGG 2022-03-03 Tags: #DeepLearning #VGG #CNN
模块化是VGG网络最重要的思想. 模块化进一步带来了自由性, 不同的块配置可以带来不同的模型表现. 规范化 - 模块化 与芯片设计中工程师从放置晶体管到逻辑元件再到逻辑块的过程类似，神经网络架构的设计也逐渐变得更加抽象。研究人员开始从单个神经元的角度思考问题，发展到整个层，现在又转向块，重复层的模式。 使用块的设计同样让模型更加简洁. VGG块 VGG将 AlexNet 里面三层连续的卷积拿出来, 抽象成VGG块, 作为构建网络的基础模式.</description></item><item><title>D2L-40-AlexNet</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-40-AlexNet/</link><pubDate>Wed, 02 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-40-AlexNet/</guid><description>AlexNet 2022-03-02 Tags: #DeepLearning #AlexNet #CNN #ImageNet
模型解析 对比LeNet 最重要的是, AlexNet导致了计算机视觉方法论的改变: 从核方法到深度神经网络, 开启了神经网络的第二次热潮 1 对比LeNet, AlexNet的主要特点有: 输入图片更 &amp;ldquo;大&amp;rdquo;, 网络结构更 &amp;ldquo;深&amp;rdquo;, 每层通道更 &amp;ldquo;多&amp;rdquo;, 滑动窗口更 &amp;ldquo;大&amp;rdquo;(核函数和池化层) 使用了ReLU作为激活函数 池化层采用了Max Pooling 使用了丢弃法(Dropout) 作为正则化方法2, 而 LeNet只采用了 权重衰减 AlexNet在训练前进行了数据增强 3</description></item><item><title>D2L-39-LeNet</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-39-LeNet/</link><pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-39-LeNet/</guid><description>LeNet 2022-03-01 Tags: #LeNet #DeepLearning #CNN #NeuralNetwork
架构 1
总体来看，LeNet（LeNet-5）由两个部分组成：
卷积编码器：由两个卷积层组成; 全连接层密集块：由三个全连接层组成。 每个卷积块中的基本单元是一个卷积层(含一个sigmoid激活函数) 和一个Avg Pooling 层。</description></item><item><title>为什么Softmax回归不用MSE</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE/</link><pubDate>Mon, 28 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE/</guid><description>为什么Softmax (或者Logistic) 不用MSE作为损失函数? 2022-02-28 Tags: #DeepLearning #MachineLearning #SoftmaxRegression #LogisticRegression #CostFunction #MeanSquareError #CrossEntropy
回顾:
MSE假设样本误差i.i.d., 并且服从正态分布, 最小化MSE等价于极大似然估计. 通常用于回归问题. MSE基于输出与真实值的欧氏距离.</description></item><item><title>D2L-34-卷积层 - 填充 - Padding</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-34-%E5%8D%B7%E7%A7%AF%E5%B1%82-%E5%A1%AB%E5%85%85-Padding/</link><pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-34-%E5%8D%B7%E7%A7%AF%E5%B1%82-%E5%A1%AB%E5%85%85-Padding/</guid><description>Padding - 填充 2022-02-27 Tags: #DeepLearning #CNN #Padding
It&amp;rsquo;s always nice to have an interactive example:
Convolution Visualizer CNN Explainer 卷积操作会使图像尺寸变小, 填充 (Padding) 可以减缓这个过程.</description></item><item><title>D2L-35-卷积层 - 步幅 - Stride</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-35-%E5%8D%B7%E7%A7%AF%E5%B1%82-%E6%AD%A5%E5%B9%85-Stride/</link><pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-35-%E5%8D%B7%E7%A7%AF%E5%B1%82-%E6%AD%A5%E5%B9%85-Stride/</guid><description>Stride - 步幅 2022-02-27 Tags: #DeepLearning #CNN #Stride
It&amp;rsquo;s always nice to have an interactive example:
Convolution Visualizer CNN Explainer 卷积操作会使图像尺寸变小, 增大步幅 (Stride) 可以加快这个过程.</description></item><item><title>D2L-36-1x1卷积层</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-36-1x1%E5%8D%B7%E7%A7%AF%E5%B1%82/</link><pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-36-1x1%E5%8D%B7%E7%A7%AF%E5%B1%82/</guid><description>$1×1$ 卷积层 2022-02-27 Tags: #CNN #DeepLearning #Convolution
$1×1$ 卷积，即 $k_h=k_w=1$，它虽然不能提取相关特征, 但是却能融合图像的不同通道, 也是一种很受欢迎的网络结构.
它相当于输入形状为 $n_{h} n_{w} \times c_{i}$ , 权重为 $c_{o} \times c_{i}$ 的全连接层</description></item><item><title>D2L-37-CNN的计算复杂度</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-37-CNN%E7%9A%84%E8%AE%A1%E7%AE%97%E5%A4%8D%E6%9D%82%E5%BA%A6/</link><pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-37-CNN%E7%9A%84%E8%AE%A1%E7%AE%97%E5%A4%8D%E6%9D%82%E5%BA%A6/</guid><description>CNN的计算复杂度 2022-02-27 Tags: #DeepLearning #CNN #ComputationalComplexity
前面我们提到过利用傅里叶变换可以快速地计算卷积: 数值计算 卷积与傅里叶变换
动手学深度学习里面给出了一个例子, 说明卷积的计算复杂度其实还是很高的, 只是参数的存储开销较小.</description></item><item><title>D2L-38-池化层-Pooling_Layer</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-38-%E6%B1%A0%E5%8C%96%E5%B1%82-Pooling_Layer/</link><pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-38-%E6%B1%A0%E5%8C%96%E5%B1%82-Pooling_Layer/</guid><description>池化层/汇聚层 - Pooling Layer 2022-02-27 Tags: #PoolingLayer #DeepLearning #CNN
首先，池化层为什么叫“池化层” Collins Dictionary - Pool 7. verb
If a group of people or organizations pool their money, knowledge, or equipment, they share it or put it together so that it can be used for a particular purpose.</description></item><item><title>D2L-32-Convolution-卷积</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-32-Convolution-%E5%8D%B7%E7%A7%AF/</link><pubDate>Sat, 26 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-32-Convolution-%E5%8D%B7%E7%A7%AF/</guid><description>卷积 - Convolution 2022-02-26 Tags: #DeepLearning #Convolution
关键点:
Convolution Determines the Output of a System for any Input1</description></item><item><title>D2L-33-卷积神经网络CNN</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-33-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/</link><pubDate>Sat, 26 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-33-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/</guid><description>Convolutional Neural Network - 卷积神经网络 2022-02-26 Tags: #CNN #DeepLearning #Convolution
MLP的不足 随着图像分辨率的提高, MLP显露出以下不足:
假设我们的图像分辨率为 $1920\times 1080$, 那么一张图片就有 $2,073,600$ 个像素点, 假设和输入层相连的隐藏层有 $1000$ 个单元, 那么光是第一个全连接层就有大约 $2\times10^9$ ($20$ 亿) 个参数, 训练这样的网络是难以想象的, 况且这还只是网络的第一层.</description></item><item><title>归纳偏置-Inductive bias - learning bias</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E5%BD%92%E7%BA%B3%E5%81%8F%E7%BD%AE-Inductive-bias-learning-bias/</link><pubDate>Sat, 26 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E5%BD%92%E7%BA%B3%E5%81%8F%E7%BD%AE-Inductive-bias-learning-bias/</guid><description>Inductive Bias - 归纳偏置 / 归纳偏好 2022-02-26 Tags: #DeepLearning #MachineLearning
当学习器去预测其未遇到过的输入的结果时，会做一些假设（Mitchell, 1980）。而学习算法中的归纳偏置（Inductive bias）则是这些假设的集合。1
一个典型的归纳偏置例子是 奥卡姆剃刀，它假设最简单而又一致的假设是最佳的。这里的一致是指学习器的假设会对所有样本产生正确的结果。
Machine Learning - Mitchell Chapter 2.</description></item><item><title>Passage_Paragraph-Difference</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Passage_Paragraph-Difference/</link><pubDate>Tue, 22 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Passage_Paragraph-Difference/</guid><description>Passage Paragraph 都是&amp;quot;段落&amp;quot;, 有什么区别吗? 2022-02-22 Tags: #English
Paragraph is a section of a document that is usually indicated by an indent on the left hand side of the paper.</description></item><item><title>D2L-26-环境和分布偏移</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-26-%E7%8E%AF%E5%A2%83%E5%92%8C%E5%88%86%E5%B8%83%E5%81%8F%E7%A7%BB/</link><pubDate>Sat, 19 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-26-%E7%8E%AF%E5%A2%83%E5%92%8C%E5%88%86%E5%B8%83%E5%81%8F%E7%A7%BB/</guid><description>Environment and Distribution Shift 2022-02-19 Tags: #DeepLearning #DistributionShift #CovariateShift
环境是变化的，数据也是。有时通过将基于模型的决策引入环境，我们可能会破坏模型1。我们需要合理地调整模型来适应这种可能的变化。 分布偏移的类型 Covariate Shift - 协变量偏移 顾名思义，就是输入数据（特征、协变量）的分布发生了偏移。也就是说，输入变得不一样了，原来是真实的猫猫狗狗， 现在变成了卡通的猫猫狗狗！ 形式化地说, 就是输入数据的分布 $P(X)$ 发生了变化, 但是在输入确定之后, 最终输出的标签分布 $P(y\space |\space X)$ 不变.</description></item><item><title>D2L-27-Computation-层和块</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-27-Computation-%E5%B1%82%E5%92%8C%E5%9D%97/</link><pubDate>Sat, 19 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-27-Computation-%E5%B1%82%E5%92%8C%E5%9D%97/</guid><description>深度学习计算: 使用层 和块 2022-02-19 Tags: #DeepLearning #Computation #PyTorch
这一章主要介绍框架的使用细节, 最好的方法就是结合代码示例, 边运行边理解. 这里我们记录一些容易忽略的要点. 在线代码实例: 5.1. 层和块 层和块: 定义 &amp;ldquo;层&amp;quot;具有三个特征:</description></item><item><title>D2L-28-Computation-参数管理</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-28-Computation-%E5%8F%82%E6%95%B0%E7%AE%A1%E7%90%86/</link><pubDate>Sat, 19 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-28-Computation-%E5%8F%82%E6%95%B0%E7%AE%A1%E7%90%86/</guid><description>深度学习计算: 参数管理 2022-02-19 Tags: #DeepLearning #Computation #Parameter #PyTorch
在线代码实例: 5.2. 参数管理 本节主要有以下内容：
访问参数，用于调试、诊断和可视化。 参数初始化。 在不同模型组件间共享参数。(保持某几个层的参数是同步的) 延后初始化 ¶ 深度学习框架无法判断网络的输入维度是什么。 这里的诀窍是框架的 延后初始化（defers initialization）， 即直到数据第一次通过模型传递时，框架才会动态地推断出每个层的大小。 这个在MXNET 和 Tensorflow 里面有, PyTorch还不太完善, 不过LazyLinear可以达到类似的功能</description></item><item><title>D2L-29-Computation-自定义层</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-29-Computation-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B1%82/</link><pubDate>Sat, 19 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-29-Computation-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B1%82/</guid><description>深度学习计算: 自定义一个层 2022-02-19 Tags: #DeepLearning #Computation #PyTorch
在线代码实例: 5.4. 自定义层 我们可以通过基本层类设计自定义层。这允许我们定义灵活的新层。 在自定义层定义完成后，我们就可以在任意环境和网络架构中调用该自定义层。 层可以有局部参数，这些参数可以通过内置函数创建。</description></item><item><title>D2L-30-Computation-读写文件</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-30-Computation-%E8%AF%BB%E5%86%99%E6%96%87%E4%BB%B6/</link><pubDate>Sat, 19 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-30-Computation-%E8%AF%BB%E5%86%99%E6%96%87%E4%BB%B6/</guid><description>深度学习计算: 读写文件 2022-02-19 Tags: #DeepLearning #Computation #PyTorch
在线代码实例: 5.5. 读写文件 我们可以保存一个张量, 或者张量的字典和列表 我们可以通过参数字典保存和加载网络的全部参数, 但是Pytorch中, 模型的定义需要用其他方法来保存.</description></item><item><title>D2L-31-Computation-购买与使用GPU</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-31-Computation-%E8%B4%AD%E4%B9%B0%E4%B8%8E%E4%BD%BF%E7%94%A8GPU/</link><pubDate>Sat, 19 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-31-Computation-%E8%B4%AD%E4%B9%B0%E4%B8%8E%E4%BD%BF%E7%94%A8GPU/</guid><description>深度学习计算: 购买与使用GPU 2022-02-19 Tags: #DeepLearning #Computation #PyTorch #GPU
购买与搭建计算平台: 16.4. 选择服务器和GPU
Pytorch使用GPU: 5.6. GPU — 动手学深度学习</description></item><item><title>为什么参数不能初始化为同一个常数</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%82%E6%95%B0%E4%B8%8D%E8%83%BD%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%BA%E5%90%8C%E4%B8%80%E4%B8%AA%E5%B8%B8%E6%95%B0/</link><pubDate>Sat, 19 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%82%E6%95%B0%E4%B8%8D%E8%83%BD%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%BA%E5%90%8C%E4%B8%80%E4%B8%AA%E5%B8%B8%E6%95%B0/</guid><description>深度学习: 参数化所固有的对称性 2022-02-19 Tags: #DeepLearning
4.8.1.3. 打破对称性 对于一个多层感知机, 假设隐藏层只有两个单元, 输出层只有一个输出单元。 想象一下，如果我们将隐藏层的所有参数初始化为 $W^{(1)}=c$， $c$ 为常量，会发生什么？ 在这种情况下，在前向传播期间，两个隐藏单元采用相同的输入和参数， 产生相同的激活，该激活被送到输出单元。 在反向传播期间，根据参数 $W^{(1)}$ 对输出单元进行微分， 得到一个梯度，其元素都取相同的值。 因此，在一次梯度下降（例如，小批量随机梯度下降）之后， $W^{(1)}$ 的所有元素仍然有相同的值。 而每一次迭代永远都不会打破对称性，这也意味着隐藏层的行为就好像只有一个单元, 我们永远也无法利用多层网络强大的表达能力。 请注意，虽然小批量随机梯度下降不会打破这种对称性，但暂退法 (Dropout-丢弃法) 正则化可以。</description></item><item><title>D2L-25-让训练更加稳定-Xavier初始化</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-25-%E8%AE%A9%E8%AE%AD%E7%BB%83%E6%9B%B4%E5%8A%A0%E7%A8%B3%E5%AE%9A-Xavier%E5%88%9D%E5%A7%8B%E5%8C%96/</link><pubDate>Fri, 18 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-25-%E8%AE%A9%E8%AE%AD%E7%BB%83%E6%9B%B4%E5%8A%A0%E7%A8%B3%E5%AE%9A-Xavier%E5%88%9D%E5%A7%8B%E5%8C%96/</guid><description>让训练更加稳定 2022-02-18 Tags: #DeepLearning
要点 因为我们无法改变 梯度问题的根本原因, 所以我们的目标是将梯度的值控制在一个合理的范围内. 改进方向 将乘法变加法: 如ResNet, LSTM
归一化: 梯度归一化, Gradient Clipping-梯度剪裁</description></item><item><title>D2L-24-数值稳定性</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-24-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7/</link><pubDate>Thu, 17 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-24-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7/</guid><description>深度学习里面的数值稳定性 2022-02-17 Tags: #DeepLearning #NumericalComputing
问题的由来 数值稳定性的问题发生在反向传播的时候. 对于一个很深的模型, 计算在损失 $\ell$ 关于第 $t$ 层权重 $\mathbf{W_t}$ 的梯度的时候, 如果第 $t$ 层关于输出较远, 则结果由许多矩阵乘法构成, 这会导致梯度爆炸或者梯度消失.</description></item><item><title>D2L-23-Dropout-丢弃法</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-23-Dropout-%E4%B8%A2%E5%BC%83%E6%B3%95/</link><pubDate>Mon, 14 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-23-Dropout-%E4%B8%A2%E5%BC%83%E6%B3%95/</guid><description>Dropout - 丢弃法(暂退法) 2022-02-14 Tags: #Dropout #Regularization #DeepLearning
1
Dropout就是在前向传播过程计算每一内部层的同时注入噪声, 从而提高模型的平滑性, 减少过拟合. 实现方式 实现的关键是要以一种无偏(不改变期望)的方式注入噪声.</description></item><item><title>Norm in Regularization - Intuition</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Norm-in-Regularization-Intuition/</link><pubDate>Mon, 14 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Norm-in-Regularization-Intuition/</guid><description>Norm in Regularization - Intuition 2022-02-14 Tags: #Norm #Regularization #DeepLearning #MachineLearning
L2 Norm $\ell_{2}$ in Regularization L2 Norm 的等高线是圆形的</description></item><item><title>Regularization-正则化</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Regularization-%E6%AD%A3%E5%88%99%E5%8C%96/</link><pubDate>Mon, 14 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Regularization-%E6%AD%A3%E5%88%99%E5%8C%96/</guid><description>Regularization 2022-02-14 Tags: #Regularization #DeepLearning
Definition Regularization: any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error.</description></item><item><title>好的预测模型的特质</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E5%A5%BD%E7%9A%84%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%B9%E8%B4%A8/</link><pubDate>Mon, 14 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E5%A5%BD%E7%9A%84%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%B9%E8%B4%A8/</guid><description>&amp;ldquo;好&amp;quot;的预测模型的特征 2022-02-14 Tags: #DeepLearning
泛化性的角度:
我们期待“好”的预测模型能在未知的数据上有很好的表现： 经典泛化理论认为，为了缩小训练和测试性能之间的差距，应该以简单的模型为目标。 简单性以较小维度的形式展现. 此外, $L_2$ 正则化的有效性也说明, 参数的范数也代表了一种有用的简单性度量。 简单性的另一个角度是平滑性，即函数不应该对其输入的微小变化敏感。 例如，当我们对图像进行分类时，我们预计向像素添加一些随机噪声应该是基本无影响的。 1995年，克里斯托弗·毕晓普证明了 具有输入噪声的训练等价于Tikhonov正则化。 这项工作用数学证实了“要求函数光滑”和“要求函数对输入的随机噪声具有适应性”之间的联系。</description></item><item><title>D2L-15-Perceptron</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-15-Perceptron/</link><pubDate>Sat, 12 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-15-Perceptron/</guid><description>Perceptron - 感知机 2022-02-12 Tags: #MachineLearning #Perceptron</description></item><item><title>D2L-16-线性模型的问题</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-16-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%97%AE%E9%A2%98/</link><pubDate>Sat, 12 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-16-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%97%AE%E9%A2%98/</guid><description>线性模型存在的问题 2022-02-12 Tags: #DeepLearning
线性意味着 &amp;ldquo;单调性&amp;rdquo; 假设： 输出和输入以相同的速度变化.
但是有很多问题虽然是单调的, 但是并不是线性&amp;quot;匀速&amp;quot;变化的 对策: 对数据进行预处理，使线性变得更合理，如进行对数变换。 但是很多情况也不是单调的。 例如，我们想要根据体温预测死亡率。 对于体温高于37摄氏度的人来说，温度越高风险越大。 然而，对于体温低于37摄氏度的人来说，温度越高风险就越低。</description></item><item><title>D2L-17-MLP-多层感知机</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-17-MLP-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/</link><pubDate>Sat, 12 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-17-MLP-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/</guid><description>Multilayer Perceptron 2022-02-12 Tags: #MultilayerPerceptron #DeepLearning #Perceptron
隐藏层 从线性到非线性 用$\mathbf{X, H, O}$ 分别代表输入层, 隐藏层和输出层, 带偏置的模型可以表示如下: $$\begin{aligned} &amp;amp;\mathbf{H}=\mathbf{X} \mathbf{W}^{(1)}+\mathbf{b}^{(1)} \ &amp;amp;\mathbf{O}=\mathbf{H} \mathbf{W}^{(2)}+\mathbf{b}^{(2)} \end{aligned}$$</description></item><item><title>D2L-18-激活函数-Activation_Functions</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-18-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-Activation_Functions/</link><pubDate>Sat, 12 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-18-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-Activation_Functions/</guid><description>常用激活函数 2022-02-12 Tags: #DeepLearning #ActivationFunction
1
ReLU 修正线性单元（Rectified Linear Unit，ReLU） ReLU就是一个 $max(0,x)$ 函数. ReLU是分段线性的 ReLU的变体通过设置一个线性项, 使得负轴的一些信息得到保留(Parameterized ReLU) $\mathbf{pReLU}(x)=max(0,x)+α\space min(0,x).</description></item><item><title>D2L-20-训练误差与泛化误差</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-20-%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE%E4%B8%8E%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE/</link><pubDate>Sat, 12 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-20-%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE%E4%B8%8E%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE/</guid><description>训练误差与泛化误差 2022-02-12 Tags: #DeepLearning
这一小节写的挺好的: 训练误差和泛化误差¶ 定义 训练误差（training error）是指， 模型在训练数据集上计算得到的误差。
泛化误差（generalization error）是指， 模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。
因为我们不能得到无限多的样本, 所以我们只能估计泛化误差.</description></item><item><title>D2L-21-模型容量</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-21-%E6%A8%A1%E5%9E%8B%E5%AE%B9%E9%87%8F/</link><pubDate>Sat, 12 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-21-%E6%A8%A1%E5%9E%8B%E5%AE%B9%E9%87%8F/</guid><description>模型容量(复杂度) 2022-02-12 Tags: #DeepLearning
Link: Part.17_Overfitting_Underfitting(ML_Andrew.Ng.)
概念 模型容量就是模型的复杂度, 也就是一个模型的Variance. 模型容量和数据的复杂度应该对应, 两者的不匹配容易导致过拟合与欠拟合问题 下图是两种损失随着模型复杂度的变化(注意横轴代表的是不同复杂度的模型, 不要和训练的Loss曲线搞混). 可以看到对于同一个数据集, 过于复杂的模型虽然有着更小的训练损失, 但是有了更大的泛化损失, 则说明模型过分契合训练集了, 学习到了训练集的一些误差等等, 太过于灵活(High variance), 出现了过拟合 而如果模型过于简单, 则训练损失和泛化损失都很高, 说明模型能力不够, 学习不到足够的知识, 模型对于数据的偏见(Bias)太高了 模型容量 估计模型容量 我们很难比较不同类型模型的容量差别: 比如树模型和神经网络.</description></item><item><title>D2L-22-权重衰减</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-22-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/</link><pubDate>Sat, 12 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-22-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/</guid><description>权重衰减 2022-02-12 Tags: #Regularization #DeepLearning
权重衰减就是利用 $\ell_{2}$ 范数进行 正则化, 避免过拟合 权重衰减是通过减小目标参数(weights)的大小来实现正则化的, 这也是其名称的由来. 参数的范数代表了一种有用的简单性度量。1 Links:</description></item><item><title>VC维-VC_Dimension</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/VC%E7%BB%B4-VC_Dimension/</link><pubDate>Sat, 12 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/VC%E7%BB%B4-VC_Dimension/</guid><description>Vapnik–Chervonenkis dimension 2022-02-12 Tags: #DeepLearning #StatisticalLearning
在VC理论中，VC维是对一个可学习分类函数空间的能力（复杂度，表示能力等）的衡量。它定义为算法能“打散”的点集的势的最大值。
对于线性分类器:
VC维可以衡量训练误差和泛化误差的间隔, 但是在深度学习中, 我们很难计算一个模型的VC维</description></item><item><title>Cross_Entropy-交叉熵</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5/</link><pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5/</guid><description>Cross Entropy - 交叉熵 2022-02-11 Tags: #InformationTheory #DeepLearning
Intuition 熵是编码一个事件所需要的最短平均长度 $$\begin{aligned}H(p)&amp;amp;=\sum_{x_{i}} p\left(x_{i}\right) \log \frac{1}{p\left(x_{i}\right)} \ &amp;amp;=-\sum_{x} p(x) \log p(x) \end{aligned}$$</description></item><item><title>D2L-13-Softmax_Regression</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-13-Softmax_Regression/</link><pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-13-Softmax_Regression/</guid><description>Softmax 回归 2022-02-11 Tags: #SoftmaxRegression #MachineLearning #Classification #MulticlassClassification
Softmax回归解决的是多分类问题1, 它可以看作是二分类的 Logistic_Regression的推广. Softmax函数
Softmax回归 Softmax回归就是在线性回归的基础上套上一个Softmax函数, 取输出结果中概率最大的项作为预测结果. 交叉熵作为损失函数 D2L-14-Cross Entropy as Loss</description></item><item><title>D2L-14-Cross Entropy as Loss</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss/</link><pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss/</guid><description>交叉熵作为损失函数 2022-02-11 Tags: #CostFunction #DeepLearning #CrossEntropy
在作为损失函数的时候, 构成 交叉熵的概率分布为: 真实分布: $P^*$ 模型输出: $P$ 作为损失函数, 交叉熵的作用是 衡量模型输出与真实值的差距, 作为优化算法的优化对象, 还需要尽量简洁, 减少训练模型的开销.</description></item><item><title>KL_Divergence-KL散度</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/KL_Divergence-KL%E6%95%A3%E5%BA%A6/</link><pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/KL_Divergence-KL%E6%95%A3%E5%BA%A6/</guid><description>Kullback–Leibler divergence 2022-02-11 Tags: #Math/Probability #DeepLearning
1
KL散度可以衡量两个概率分布之间的相似性
KL散度也称为相对熵
Wikipedia: In mathematical statistics, the Kullback–Leibler divergence, $D _{KL} ( P ∥ Q )$ (also called relative entropy), is a statistical distance: a measure of how one probability distribution Q is different from a second, reference probability distribution P.</description></item><item><title>Likelihood_Function-似然函数</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Likelihood_Function-%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0/</link><pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Likelihood_Function-%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0/</guid><description>Likelihood Function - 似然函数 2022-02-11 Tags: #Math/Statistics #MachineLearning
对于某个(某组)随机变量 $X$, 我们通过采样获得了数据集 $x$ :
似然函数$\mathcal{L}(\theta \mid x)$就是在某个参数(parameter) $\theta$ 下, 现有数据 $x$ 出现的概率大小, 也就是说: $$\mathcal{L}(\theta \mid x) = P(X=x\mid\theta)$$ $P(X=x\mid\theta)$ 也常常写作 $p_{\theta}(x)=P_{\theta}(X=x)=P(X=x\space ;\theta)$</description></item><item><title>Logit</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Logit/</link><pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Logit/</guid><description>Logit: a confusing term 2022-02-11 Tags: #Math #DeepLearning #SoftmaxRegression
Ref: machine learning - What is the meaning of the word logits in TensorFlow?</description></item><item><title>Relation_between_Softmax_and_Logistic_Regression</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Relation_between_Softmax_and_Logistic_Regression/</link><pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Relation_between_Softmax_and_Logistic_Regression/</guid><description>Softmax 与 Logistic 回归的联系 2022-02-11 Tags: #SoftmaxRegression #LogisticRegression #Classification #MulticlassClassification
Ref: Unsupervised Feature Learning and Deep Learning Tutorial
二分类的 Softmax回归形式如下: $$h_{\theta}(x)=\frac{1}{\exp \left(\theta^{(1) \top} x\right)+\exp \left(\theta^{(2) \top} x^{(i)}\right)}\left[\begin{array}{c} \exp \left(\theta^{(1) \top} x\right) \ \exp \left(\theta^{(2) \top} x\right) \end{array}\right]$$</description></item><item><title>Softmax_Regression_is_Over-parameterized</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Softmax_Regression_is_Over-parameterized/</link><pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Softmax_Regression_is_Over-parameterized/</guid><description>Softmax Regression is Over-parameterized 2022-02-11 Tags: #SoftmaxRegression
Ref: Unsupervised Feature Learning and Deep Learning Tutorial
将Softmax回归里面的参数全部变化一个相同的值, 结果不变: $$\begin{aligned} P\left(y^{(i)}=k \mid x^{(i)} ; \theta\right) &amp;amp;=\frac{\exp \left(\left(\theta^{(k)}-\psi\right)^{\top} x^{(i)}\right)}{\sum_{j=1}^{K} \exp \left(\left(\theta^{(j)}-\psi\right)^{\top} x^{(i)}\right)} \ &amp;amp;=\frac{\exp \left(\theta^{(k) \top} x^{(i)}\right) \exp \left(-\psi^{\top} x^{(i)}\right)}{\sum_{j=1}^{K} \exp \left(\theta^{(j) \top} x^{(i)}\right) \exp \left(-\psi^{\top} x^{(i)}\right)} \ &amp;amp;=\frac{\exp \left(\theta^{(k) \top} x^{(i)}\right)}{\sum_{j=1}^{K} \exp \left(\theta^{(j) \top} x^{(i)}\right)} \end{aligned}$$</description></item><item><title>Dummy_Variables</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/Dummy_Variables/</link><pubDate>Thu, 10 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/Dummy_Variables/</guid><description>Dummy Variable 2022-02-10 Tags: #Math/Statistics
Dummy variable (statistics) - Wikipedia
In statistics and econometrics, particularly in regression analysis, a dummy variable is one that takes only the value 0 or 1 to indicate the absence or presence of some categorical effect that may be expected to shift the outcome.</description></item><item><title>Kronecker delta - 克罗内克δ函数</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/Kronecker-delta-%E5%85%8B%E7%BD%97%E5%86%85%E5%85%8B%CE%B4%E5%87%BD%E6%95%B0/</link><pubDate>Thu, 10 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/Kronecker-delta-%E5%85%8B%E7%BD%97%E5%86%85%E5%85%8B%CE%B4%E5%87%BD%E6%95%B0/</guid><description>Kronecker delta 2022-02-10 Tags: #Math
$$\delta_{ij} = \left{\begin{matrix} 1 &amp;amp; (i=j) \ 0 &amp;amp; (i \ne j) \end{matrix}\right.$$
在线性代数中，单位矩阵可以写作 $\left(\delta_{i j}\right)_{i, j=1}^{n}$</description></item><item><title>仿射变换-Affine_Transformation</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2-Affine_Transformation/</link><pubDate>Thu, 10 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2-Affine_Transformation/</guid><description>仿射变换 Affine Transformation 2022-02-10 Tags: #Math/LinearAlgebra
仿射变换就是平移后的线性变换:
1 Here is an Interaction: Affine transformations / Kjerand Pedersen / Observable 有趣的是, 我们可以在高维度通过线性变换来完成仿射变换2 3</description></item><item><title>One-hot_Encoding-独热编码</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/One-hot_Encoding-%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81/</link><pubDate>Wed, 09 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/One-hot_Encoding-%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81/</guid><description>独热编码 One-hot Encoding 2022-02-09 Tags: #One-hot #DeepLearning #Encoding
$$\begin{array}{ll} apple &amp;amp;=\quad [\space 1\quad 0\quad 0\space ] \ banana &amp;amp;=\quad [\space 0\quad 1\quad 0\space ] \ pineapple &amp;amp;=\quad [\space 0\quad 0\quad 1\space ] \end{array}$$</description></item><item><title>D2L-11-泛化(Generalization)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-11-%E6%B3%9B%E5%8C%96Generalization/</link><pubDate>Tue, 08 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-11-%E6%B3%9B%E5%8C%96Generalization/</guid><description>Generalization: 泛化 2022-02-08 Tags: #MachineLearning #DeepLearning
线性回归恰好是一个在整个域中只有一个最小值的学习问题。 1但是对于像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。
深度学习实践者很少会去花费大力气寻找这样一组参数，使得在_训练集_上的损失达到最小。 事实上，更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失， 这一挑战被称为_泛化_（generalization）。
可以证明, 正则后的线性回归损失函数MSE依然是凸的: 正则项不影响线性回归损失函数的凸性&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>D2L-12-Predication_or_Inference-Difference</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-12-Predication_or_Inference-Difference/</link><pubDate>Tue, 08 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-12-Predication_or_Inference-Difference/</guid><description>Prediction or Inference? The Difference 2022-02-08 Tags: #DeepLearning #Math/Statistics #Inference #Prediction
在深度学习里面, 给定特征估计目标的过程通常称为_预测_（prediction）或_推断_（inference）。 但是, 虽然 推断 这个词已经成为深度学习的标准术语，但其实 推断 这个词有些用词不当。 在统计学中，推断 更多地表示基于数据集估计参数。1 当深度学习从业者与统计学家交谈时，术语的误用经常导致一些误解。2 如Bayesian Inference: Bayesian_Estimation(Inference)贝叶斯估计&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>D2L-10-小批量随机梯度下降</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-10-%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</link><pubDate>Wed, 02 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-10-%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</guid><description>小批量随机梯度下降(Mini-Batch)是深度学习默认的求解方法 2022-02-02 Tags: #MachineLearning #GradientDescent #DeepLearning #Optimization
Different_Gradient_Descent_Methods 注意有两个点: 小批量(Mini-Batch), 随机(Stochastic) 梯度下降 其中:
小批量是因为在整个数据集上面训练一次又慢又贵 同时小批量还能从多个相似的数据点中选一个代表来计算, 节约了计算资源 但是样本不能太小, 太小的样本不适合用GPU并行计算 随机是选取小样本的方法: 随机选取</description></item><item><title>D2L-5-拓展链式法则</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-5-%E6%8B%93%E5%B1%95%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99/</link><pubDate>Wed, 02 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-5-%E6%8B%93%E5%B1%95%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99/</guid><description>拓展的求导链式法则 2022-02-02 Tags: #Math #Derivative
从标量到向量, 不仅符号需要对应上, 矩阵的形状也需要对应上 $$\begin{align} &amp;amp;\frac{\partial y}{\partial \mathbf{x}}=\frac{\partial y}{\partial u} \frac{\partial u}{\partial \mathbf{x}}\ &amp;amp;\small{(1, n)}\quad{(1,1)(1,n)} \end{align}$$</description></item><item><title>D2L-6-计算图</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-6-%E8%AE%A1%E7%AE%97%E5%9B%BE/</link><pubDate>Wed, 02 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-6-%E8%AE%A1%E7%AE%97%E5%9B%BE/</guid><description>计算图 2022-02-02 Tags: #MachineLearning #DeepLearning
将计算表示为一个无环图 例子 线性回归: 计算图有两种构造方法: 显式构造 主要应用于: Tensorflow/Theano/MXNet 例子: 1 2 3 4 5 from mxnet import sym a = sym.</description></item><item><title>D2L-7-自动求导</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-7-%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC/</link><pubDate>Wed, 02 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-7-%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC/</guid><description>自动求导 2022-02-02 Tags: #DeepLearning
在机器学习里面, 深度学习框架可以帮我们自动求导, 计算梯度.
自动求导的两种方式 基于链式法则, 求导有两种顺序:
正向累积 $$\frac{\partial y}{\partial x}=\frac{\partial y}{\partial u_{n}}\left(\frac{\partial u_{n}}{\partial u_{n-1}}\left(\ldots\left(\frac{\partial u_{2}}{\partial u_{1}} \frac{\partial u_{1}}{\partial x}\right)\right)\right)$$ 反向累积、又称反向传递 $$ \frac{\partial y}{\partial x}=\left(\left(\left(\frac{\partial y}{\partial u_{n}} \frac{\partial u_{n}}{\partial u_{n-1}}\right) \ldots\right) \frac{\partial u_{2}}{\partial u_{1}}\right) \frac{\partial u_{1}}{\partial x} $$ 反向累积/传播 反向传播分为两个阶段: 正向阶段和反向阶段</description></item><item><title>D2L-9-梯度下降的方向</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-9-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E6%96%B9%E5%90%91/</link><pubDate>Wed, 02 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-9-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E6%96%B9%E5%90%91/</guid><description>梯度下降的方向是梯度的反方向 2022-02-02 Tags: #GradientDescent #DeepLearning #MachineLearning
梯度是一个函数增长最快的方向, 通常我们都是想获得损失函数的最小值, 所以需要沿着梯度的反方向来移动.
注意这并不是一定的, 梯度下降/上升只是一种优化方法而已, 如果我们想要优化的目标函数取得最大值, 那么就应该沿着梯度的方向变化.</description></item><item><title>D2L-2-Tensor数据操作</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-2-Tensor%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C/</link><pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-2-Tensor%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C/</guid><description>Tensor数据操作 2022-02-01 Tags: #Tensor #DeepLearning
[行, 列] 用冒号可以表示范围, 即一个子区域 注意还可以用双冒号间隔选择, 双冒号后的数字为间隔的周期</description></item><item><title>D2L-3-亚导数</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-3-%E4%BA%9A%E5%AF%BC%E6%95%B0/</link><pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-3-%E4%BA%9A%E5%AF%BC%E6%95%B0/</guid><description>亚导数 2022-02-01 Tags: #Math
将导数拓展到了不可微的函数: 举例:</description></item><item><title>D2L-4-矩阵求导</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-4-%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/</link><pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-4-%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/</guid><description>矩阵求导 2022-02-01 Tags: #Math #Matrix
矩阵的求导一直很让人头疼😖 之前的笔记: 矩阵的求导 李沐这次的讲解方式不太一样，是从标量逐步推广到矩阵，还蛮清晰的。 从标量到向量 其中 $\Large{\frac{\partial y}{\partial x}, \frac{\partial \mathbf y}{\partial x}}$都很好理解, 尤其需要注意的是当求导的自变量$\mathbf x$为向量的时候, 为 $$\mathbf{x}=\left[\begin{array}{c} x_{1} \ x_{2} \ \vdots \ x_{n} \end{array}\right] \quad \frac{\partial y}{\partial \mathbf{x}}=\left[\frac{\partial y}{\partial x_{1}}, \frac{\partial y}{\partial x_{2}}, \ldots, \frac{\partial y}{\partial x_{n}}\right]$$ 结果变成了一个行向量.</description></item><item><title>Einstein Notation</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/Einstein-Notation/</link><pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/Einstein-Notation/</guid><description>Einstein Notation 2022-02-01 Tags: #Tensor #EinsteinNotation
Einstein notation - Wikipedia 一种求和符号的简化书写方式, 在张量运算中经常使用, 由爱因斯坦发明.</description></item><item><title>矩阵的不同乘法-Hadamard-Kronecker</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/%E7%9F%A9%E9%98%B5%E7%9A%84%E4%B8%8D%E5%90%8C%E4%B9%98%E6%B3%95-Hadamard-Kronecker/</link><pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/%E7%9F%A9%E9%98%B5%E7%9A%84%E4%B8%8D%E5%90%8C%E4%B9%98%E6%B3%95-Hadamard-Kronecker/</guid><description>矩阵的不同乘积 2022-02-01 Tags: #Matrix #Math
一般的矩阵乘法 Hadamard Product $\odot$ 对应位置的元素相乘 $$ \begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\ a_{21} &amp;amp; a_{22} &amp;amp; a_{23}\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33} \end{bmatrix} \circ \begin{bmatrix} b_{11} &amp;amp; b_{12} &amp;amp; b_{13}\ b_{21} &amp;amp; b_{22} &amp;amp; b_{23}\ b_{31} &amp;amp; b_{32} &amp;amp; b_{33} \end{bmatrix} = \begin{bmatrix} a_{11}, b_{11} &amp;amp; a_{12}, b_{12} &amp;amp; a_{13}, b_{13}\ a_{21}, b_{21} &amp;amp; a_{22}, b_{22} &amp;amp; a_{23}, b_{23}\ a_{31}, b_{31} &amp;amp; a_{32}, b_{32} &amp;amp; a_{33}, b_{33} \end{bmatrix}$$</description></item><item><title>D2L-1-What_is_a_tensor</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-1-What_is_a_tensor/</link><pubDate>Tue, 25 Jan 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-1-What_is_a_tensor/</guid><description>What is a tensor? 2022-01-25 Tags: #Tensor #DeepLearning
最初, 张量是在物理和数学里面的一个概念, 后来深度学习借用了这个名词, 但是意义有所改变.
在数学与物理学的语境里面, &amp;ldquo;Tensor&amp;quot;是一个抽象的概念, 用于表示在坐标变换下的一种不变量, 比如广义相对论中, 坐标的变换会引起观测的时空的变换。而爱因斯坦张量（Einstein tensor）是广义相对论中用来描述时空曲率的一个张量, 不随坐标的变换而变换.</description></item><item><title>Bayesian_Estimation(Inference)贝叶斯估计</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Bayesian_EstimationInference%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/</link><pubDate>Mon, 27 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Bayesian_EstimationInference%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/</guid><description>贝叶斯估计 2021-12-27 Tags: #Bayes #MachineLearning
贝叶斯估计 有点难, 还要进一步学习
贝叶斯估计是一种参数估计方法, 不只局限于设计贝叶斯分类器 贝叶斯估计的核心在于用新的样本来更新旧的Prior, 一起得到一个PostPrior的参数的概率分布, 合并的过程利用的是贝叶斯分布. 贝叶斯估计和极大似然估计的最大不同就是贝叶斯估计的是参数可能的概率分布, 而不是一个确定的值. 通过对这个概率分布进行积分, 我们可以平均地得到所有情况下最可能出现的参数. 这两篇文章写的很好:</description></item><item><title>Maximum_Likelihood_Estimation-极大似然估计</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Maximum_Likelihood_Estimation-%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/</link><pubDate>Sat, 25 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Maximum_Likelihood_Estimation-%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/</guid><description>极大似然估计 MLE 2021-12-25 Tags: #MachineLearning #Math/Statistics
Links: Likelihood_Function-似然函数
假设样本 $X$ 服从已知的概率分布(比如正态分布)
极大似然估计就是要找一个参数 $\hat\theta$, 使似然函数 $\mathcal{L}(\theta \mid X)$ 取得最大值$$i.e.\quad \hat{\theta}=\operatorname{argmax}_{\theta \in \Theta} \mathcal{L}(\theta \mid X)$$ 极大似然估计认为: 最佳的参数 $\hat\theta$ 最可能使取样结果为现在的 $x$, 也就是说, 概率$P(X=x\mid \theta)$最大: $$\hat{\theta}=\operatorname{argmax}_{\theta \in \Theta} P(X=x\mid \theta)$$</description></item><item><title>参数估计-Parameter_Estimation</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1-Parameter_Estimation/</link><pubDate>Sat, 25 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1-Parameter_Estimation/</guid><description>参数估计 2021-12-25 Tags: #MachineLearning #ParameterEstimation #Math/Statistics
在设计分类器或者进行回归预测的时候, 我们需要知道目标问题的概率分布情况. 但是通常我们能得到的数据只是一些特例(即训练样本). 为了对问题进行建模, 我们不仅需要确定合适的概率分布模型, 还需要根据训练样本确定模型里面的具体参数. 参数估计就是在模型已知的情况下得到最优参数的过程.
对于贝叶斯分类器, 估计先验概率$P(\omega_i)$通常不是很困难. 难点在于估计类条件概率密度$p(x|\omega_i)$, 这是因为:</description></item><item><title>mention(note) something in passing - 顺便提到</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/mentionnote-something-in-passing-%E9%A1%BA%E4%BE%BF%E6%8F%90%E5%88%B0/</link><pubDate>Fri, 24 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/mentionnote-something-in-passing-%E9%A1%BA%E4%BE%BF%E6%8F%90%E5%88%B0/</guid><description>mention/note something in passing 2021-12-24 Tags: #English
mention/note something in passing | meaning of mention/note something in passing in Longman Dictionary of Contemporary English | LDOCE</description></item><item><title>多元高斯分布-Mutivariate_Gaussian</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83-Mutivariate_Gaussian/</link><pubDate>Fri, 24 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83-Mutivariate_Gaussian/</guid><description>Multivariate Gaussian 2021-12-24 Tags: #GaussianDistribution #Math/Probability
正态分布_高斯分布_Normal_Distribution-Gaussian_Distribution
$$p(x ; \mu, \Sigma)=\frac{1}{(2 \pi)^{n / 2}|\Sigma|^{1 / 2}} \exp \left(-\frac{1}{2}(x-\mu)^{T} \Sigma^{-1}(x-\mu)\right)$$</description></item><item><title>正态分布的判别函数</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E7%9A%84%E5%88%A4%E5%88%AB%E5%87%BD%E6%95%B0/</link><pubDate>Fri, 24 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E7%9A%84%E5%88%A4%E5%88%AB%E5%87%BD%E6%95%B0/</guid><description>Discriminant Function of Gaussian 2021-12-24 Tags: #MachineLearning #DiscriminantFunction #GaussianDistribution
下面是学习Duda模式分类第二章做的简单的笔记, 有时间应该进一步梳理
高斯分布的判别函数(贝叶斯分类器)的一个常见形式是把Bayes定理的分子取下来, 再取对数.
即以下形式: $$g_{i}(\mathbf{x})=\ln p\left(\mathbf{x} \mid \omega_{i}\right)+\ln P\left(\omega_{i}\right)$$</description></item><item><title>Bayesian Decision Theory - Part1</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Bayesian-Decision-Theory-Part1/</link><pubDate>Tue, 21 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Bayesian-Decision-Theory-Part1/</guid><description>贝叶斯决策论 - Part1 2021-12-21 Tags: #MachineLearning #Bayes
贝叶斯决策其实就是把生活中我们基于直觉和常识的决策方法形式化了, 并加以进一步地推广. 贝叶斯决策综合考量每种情况的概率和决策带来的代价. 贝叶斯决策假设问题可以用概率分布的形式来刻画, 属于贝叶斯学派的一种方法. Intro A Sad Case 假如圣诞老人打包了100盒糖果, 其中20盒是巧克力($\omega_1$), 80盒是水果硬糖($\omega_2$), 现在他从里面随机挑了一盒给小企鹅, 盒子的颜色和重量都一样, 那么小企鹅得到的是巧克力还是水果硬糖呢?</description></item><item><title>Cross Validation</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Cross-Validation/</link><pubDate>Sun, 19 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Cross-Validation/</guid><description>Cross Validation 2021-12-19 Tags: #MachineLearning
交叉验证是一种评价模型好坏的方法. 设置验证集的目的在于减少样本给模型带来的Bias, 即我们想要找到一个普遍适用的模型, 而不是只在训练集上表现很好的模型. 交叉验证是一种增大验证集, 充分利用数据的方法. 交叉验证的方法 Cross-validation (statistics) - Wikipedia Machine Learning Fundamentals: Cross Validation - YouTube 简单的来说, 交叉验证会把训练集随机分成几个部分.</description></item><item><title>Understanding Bayes' Theorem</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Understanding-Bayes-Theorem/</link><pubDate>Sun, 19 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Understanding-Bayes-Theorem/</guid><description>Understanding Bayes&amp;rsquo; Theorem 2021-12-19 Tags: #Math/Probability #Bayes
单从形式上来说, Bayes定理是十分简单的. 但是如果我们结合实际问题与一点几何直觉, Bayes定理可以从两个独特的角度来直观理解:
Update of Prior Beliefs &amp;amp; Change of Perspective Bayes&amp;rsquo; Theorem: Statement $$P(A \mid B)=\frac{P(B \mid A) P(A)}{P(B)}$$</description></item><item><title>Covariance-协方差</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Covariance-%E5%8D%8F%E6%96%B9%E5%B7%AE/</link><pubDate>Sat, 11 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Covariance-%E5%8D%8F%E6%96%B9%E5%B7%AE/</guid><description>Covariance 2021-12-11 Tags: #Math/Statistics
期望值分别为 $E(X)=\mu$ 与 $E(Y)=\nu$ 的两个随机变量 X 与 Y 之间的协方差定义为: $$\begin{aligned} \operatorname{cov}(X,Y)&amp;amp;=\mathrm{E}((X-\mu)(Y-\nu))\ &amp;amp;=\mathrm{E}(X \cdot Y-\nu X-\mu Y +\mu\nu)\ &amp;amp;=\mathrm{E}(X \cdot Y)-\nu \mathrm{E}(X)-\mu \mathrm{E}(Y) +\mu\nu\ &amp;amp;=\mathrm{E}(X \cdot Y)-\mu\nu-\mu\nu +\mu\nu\ &amp;amp;=\mathrm{E}(X \cdot Y)-\mu \nu \end{aligned}$$</description></item><item><title>OS-12-内存管理</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/OS-12-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/</link><pubDate>Sat, 11 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/OS-12-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/</guid><description>内存管理 2021-12-11 Tags: #OperatingSystem/Memory
程序运行不仅需要处理机, 还需要有地方来保存代码与数据, 这一节我们谈论计算机在运行时, 这些这些数据是如何管理的.
目标 电脑的内存总是有限的, 我们想要用有限的内存满足更多的程序的需要, 并且还想这些程序运行得越快越好. 在容量上, 为了营造一个内存&amp;quot;无限大&amp;quot;的假象, 我们可以从&amp;quot;开源&amp;quot;, &amp;ldquo;节流&amp;quot;两方面来入手: 节流就是要减少不必要的内存浪费. 分段把程序分成大小不同的段, 避免了程序内部大段的空白.</description></item><item><title>Dilemma&amp;lemma</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Dilemmalemma/</link><pubDate>Fri, 10 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Dilemmalemma/</guid><description>The relation between Dilemma and lemma 2021-12-10 Tags: #English #Etymology #Latin
di: two, 两个 lemma: premise 假定, 先决条件 在两个(不好的)可能性中间做出抉择, 即进退两难.</description></item><item><title>in situ</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/in-situ/</link><pubDate>Thu, 09 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/in-situ/</guid><description>in situ 2021-12-09 Tags: #English #Latin
在原地
In situ ( /ɪn ˈsɪtjuː, - ˈsaɪtjuː, - ˈsiː-/; often not italicized in English) is a Latin phrase that translates literally to &amp;ldquo;on site&amp;rdquo; or &amp;ldquo;in position.</description></item><item><title>Chernoff Bounds</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Chernoff-Bounds/</link><pubDate>Sat, 04 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Chernoff-Bounds/</guid><description>Chernoff Bounds 2021-12-04 Tags: #Math/Statistics
probabilitycourse.com - Chernoff Bounds
zotero:@ChernoffBounds(zotero://select/items/@ChernoffBounds)
The generic Chernoff bound for a random variable $X$ is attained by applying Markov&amp;rsquo;s inequality to $e^{tX}$.</description></item><item><title>Chi-Squared_Distribution-卡方分布</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Chi-Squared_Distribution-%E5%8D%A1%E6%96%B9%E5%88%86%E5%B8%83/</link><pubDate>Sat, 04 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Chi-Squared_Distribution-%E5%8D%A1%E6%96%B9%E5%88%86%E5%B8%83/</guid><description>Chi-Squared Distribution 2021-12-04 Tags: #Math/Statistics
A very Good Website Chi-squared distribution - Wikipedia
重要结论 $Z_{1},Z_{2},\cdots,Z_{n}$ are independent standard normal random variables,</description></item><item><title>Markov's and Chebyshev's Inequalities</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Markovs-and-Chebyshevs-Inequalities/</link><pubDate>Sat, 04 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Markovs-and-Chebyshevs-Inequalities/</guid><description>Markov and Chebyshev Inequalities 2021-12-04 Tags: #Math/Statistics
FileLink(zotero://select/items/@InequalitiesMarkov)
Markov&amp;rsquo;s Inequality $X$ 是一个非负的随机变量. 对于任意正实数 $a$ , 有 $$ P(X \geq a) \leq \frac{E(X)}{a} $$</description></item><item><title>Moment Generating Function-MGF</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Moment-Generating-Function-MGF/</link><pubDate>Sat, 04 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Moment-Generating-Function-MGF/</guid><description>Moment Generating Function - MGF 2021-12-04 Tags: #Math/Statistics
This article covers it all. Moment Generating Function Explained | by Aerin Kim | Towards Data Science</description></item><item><title>Union_Bound-布尔不等式-Boole's_inequality</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Union_Bound-%E5%B8%83%E5%B0%94%E4%B8%8D%E7%AD%89%E5%BC%8F-Booles_inequality/</link><pubDate>Sat, 04 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Union_Bound-%E5%B8%83%E5%B0%94%E4%B8%8D%E7%AD%89%E5%BC%8F-Booles_inequality/</guid><description>Union Bound: 布尔不等式 2021-12-04 Tags: #Math/Statistics
This website explained it well: The Union Bound and Extension Intuition $$\begin{aligned} P(A \cup B) &amp;amp;=P(A)+P(B)-P(A \cap B) \ &amp;amp; \leq P(A)+P(B) .</description></item><item><title>Johnson Lindenstrauss Lemma - Publish Version</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Johnson-Lindenstrauss-Lemma-Publish-Version/</link><pubDate>Fri, 03 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Johnson-Lindenstrauss-Lemma-Publish-Version/</guid><description>Johnson Lindenstrauss Lemma 2021-12-03 Tags: #MachineLearning #Math
对于高维数据，我们能够在降维的过程中保留其大部分的几何特征，即使降维的幅度非常大。 这是徐亦达老师让我们学习的第一个主题
1
Study Materials MIT 6.</description></item><item><title>et al.</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/et-al./</link><pubDate>Tue, 30 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/et-al./</guid><description>et al. 2021-11-30 Tags: #Latin
And others; to complete a list, especially of persons, as authors of a published work. From Latin, abbreviation of et aliī (“and others”) Usage notes Formally preferred by some over etc.</description></item><item><title>OS-11_中断</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/OS-11_%E4%B8%AD%E6%96%AD/</link><pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/OS-11_%E4%B8%AD%E6%96%AD/</guid><description>中断 2021-11-28 Tags: #OperatingSystem
我觉得我对UNIX进程管理一直迷迷糊糊的原因就是还是对中断与上下文切换迷迷糊糊, 学习的顺序没有理清.
我们来重新梳理一下:
硬件 UNIX v6++ 建立在x86平台上面, 下面的叙述都是基于 x86 的 中断控制器 中断需要硬件支持, 即中断控制器, 用于处理外设中断</description></item><item><title>Invertable-word</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/Invertable-word/</link><pubDate>Thu, 18 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/Invertable-word/</guid><description>Invertable 2021-11-18 Tags: #English
我突然意识到 &amp;ldquo;Invertable&amp;rdquo; 其实是 &amp;ldquo;invert&amp;rdquo; + &amp;ldquo;able&amp;rdquo;, 即可以求逆的, 你可以求它的 &amp;ldquo;Inverse&amp;rdquo; 它的词根并不是 in</description></item><item><title>MIT_18.065-Part_11-SVD_&amp;_Linear_System</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_11-SVD__Linear_System/</link><pubDate>Thu, 18 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_11-SVD__Linear_System/</guid><description>SVD &amp;amp; Linear System 2021-11-18 Tags: #LinearRegression #SVD #Math/LinearAlgebra
$Ax=b$ 对于这个线性约束方程组: $$Ax=b$$
只有在A可逆的方阵的时候, 才有唯一解: $$x=A^{-1}b$$
而在A为其他形状的时候, 常常不能够简单的利用$A^{-1}$来求解这个方程组
Under-determined: (不定方程) 这时我们没有足够的约束来限制x, x常常有无穷解 换一个看法, 这可以看作因为$Row(A)$没有填满$R^n$, 所以我们可以在每一个解里面加上一部分核空间里面的向量$x_{kernel}$, 同时不影响方程的成立: $$\begin{aligned}&amp;amp;A(x+x_{kernel})=b \\Rightarrow &amp;amp;Ax+Ax_{kernel}=b\\Rightarrow &amp;amp;Ax+0=b\end{aligned}$$ Over-determined: (超定方程) 在这个情况下, 我们有太多限制来限制x, 所以有可能出现矛盾, 导致x没有解.</description></item><item><title>MIT_18.065-Part_10-SVD_in_Action</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_10-SVD_in_Action/</link><pubDate>Wed, 17 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_10-SVD_in_Action/</guid><description>SVD in Action 2021-11-17 Tags: #SVD #Math/LinearAlgebra
Economy SVD 在被分解的矩阵A特别&amp;quot;瘦高&amp;quot;的时候(m&amp;raquo;n), 我们可以只取$U$的前n列, 因为后面的&amp;quot;重要性&amp;quot;不大.
Application Digital Watermark Hands-on Tips Plot how the information varies SVD Method of Snapshots A different way to compute SVD if the data is so large that you can&amp;rsquo;t store it into memory at once.</description></item><item><title>内积和相关性的联系-Dot(Inner)_Product_&amp;_Correlation</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/%E5%86%85%E7%A7%AF%E5%92%8C%E7%9B%B8%E5%85%B3%E6%80%A7%E7%9A%84%E8%81%94%E7%B3%BB-DotInner_Product__Correlation/</link><pubDate>Tue, 16 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/%E5%86%85%E7%A7%AF%E5%92%8C%E7%9B%B8%E5%85%B3%E6%80%A7%E7%9A%84%E8%81%94%E7%B3%BB-DotInner_Product__Correlation/</guid><description>Inner Product &amp;amp; Correlation 2021-11-16 Tags: #Math #InnerProduct
Source: David Joyce&amp;rsquo;s answer to Is there any relation between &amp;lsquo;correlation of two signals&amp;rsquo; and &amp;lsquo;dot product of two vectors&amp;rsquo;?</description></item><item><title>矩阵相乘-关于维度的新视角</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%98-%E5%85%B3%E4%BA%8E%E7%BB%B4%E5%BA%A6%E7%9A%84%E6%96%B0%E8%A7%86%E8%A7%92/</link><pubDate>Tue, 16 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%98-%E5%85%B3%E4%BA%8E%E7%BB%B4%E5%BA%A6%E7%9A%84%E6%96%B0%E8%A7%86%E8%A7%92/</guid><description>关于矩阵相乘维度关系的新视角 2021-11-16 Tags: #Math/LinearAlgebra
!</description></item><item><title>酉矩阵为什么叫酉矩阵</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/%E9%85%89%E7%9F%A9%E9%98%B5%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%AB%E9%85%89%E7%9F%A9%E9%98%B5/</link><pubDate>Tue, 16 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/%E9%85%89%E7%9F%A9%E9%98%B5%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%AB%E9%85%89%E7%9F%A9%E9%98%B5/</guid><description>酉矩阵为什么叫酉矩阵 2021-11-16 Tags: #Math/LinearAlgebra #English
酉矩阵里面的&amp;quot;酉&amp;ldquo;其实是字母$U$的音译 （又译作幺正矩阵，英语：unitary matrix）
unitary adjective uk /ˈjuː.nɪ.tər.i/ us /ˈjuː.nɪ.ter.i/
of a system of local government in the UK in which official power is given to one organization that deals with all matters in a local area instead of to several organizations that each deal with only a few matters （英国地方政府）集权制的，单一自治体的</description></item><item><title>MIT_18.065-Part_7-Eigenvalues and Eigenvectors</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_7-Eigenvalues-and-Eigenvectors/</link><pubDate>Sun, 14 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_7-Eigenvalues-and-Eigenvectors/</guid><description>Eigenvalues and Eigenvectors 2021-11-14 Tags: #Math/LinearAlgebra #Math/LinearAlgebra/Eigenvalue
特征值的一些性质 相似矩阵 [[notes/2021/2021.11/理解相似矩阵]]
相似矩阵有相同的特征值 $$P^{-1}AP = B$$ 假设矩阵$B$有特征值$\lambda$: $$Bx=\lambda x$$ 则 $$\begin{aligned}P^{-1}APx&amp;amp;=\lambda x \ &amp;amp;\Rightarrow \ APx&amp;amp;=P\lambda x \ &amp;amp;\Rightarrow \ A(Px)&amp;amp;=\lambda (Px)\end{aligned}$$</description></item><item><title>MIT_18.065-Part_8-Positive Definite and Semidefinite Matrices</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_8-Positive-Definite-and-Semidefinite-Matrices/</link><pubDate>Sun, 14 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_8-Positive-Definite-and-Semidefinite-Matrices/</guid><description>Positive Definite and Semi-definite Matrices 2021-11-14 Tags: #Math/LinearAlgebra
Positive Definite Matrices are the Best of the Symmetric Matrices. 五个判别条件 同时也是正定矩阵的重要性质: Positive Eigenvalues 所有的特征值都是正数 Energy $x^TSx&amp;gt;0$, $\forall x\neq 0$ 有正的&amp;quot;能量&amp;quot;, 这点后面会详述 $S=A^TA$, A has Independent Columns S可以被分解为一个矩阵的转置与自己的乘积 All leading Determinants &amp;gt; 0 All Pivots in Elimination &amp;gt; 0 Energy (在视频里面没有找到详细的定义, 网上也没有相关的资料, 应当是一个直观的概念)</description></item><item><title>MIT_18.065-Part_9-Singular Value Decomposition-SVD</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_9-Singular-Value-Decomposition-SVD/</link><pubDate>Sun, 14 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_9-Singular-Value-Decomposition-SVD/</guid><description>Singular Value Decomposition 2021-11-14 Tags: #Math/LinearAlgebra #SVD
1
SVD将任意矩阵$A$分解成了三个部分: $U$ 的列是相互正交的. - Left Singular Vectors $\Sigma$ 的对角线上面是递减的奇异值. - Singular Values $V^T$ 的行是相互正交的.</description></item><item><title>Roll-Pitch-Yaw</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/Roll-Pitch-Yaw/</link><pubDate>Sun, 14 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/Roll-Pitch-Yaw/</guid><description>Roll Pitch &amp;amp; Yaw 2021-11-14 Tags: #English
三维空间里面的三个旋转方向
Roll, Pitch, and Yaw | How Things Fly</description></item><item><title>理解相似矩阵</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/%E7%90%86%E8%A7%A3%E7%9B%B8%E4%BC%BC%E7%9F%A9%E9%98%B5/</link><pubDate>Sun, 14 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/%E7%90%86%E8%A7%A3%E7%9B%B8%E4%BC%BC%E7%9F%A9%E9%98%B5/</guid><description>理解相似矩阵 2021-11-14 Tags: #Math/LinearAlgebra
设 $A,B$ 都是 $n$ 阶矩阵，若有可逆矩阵 $P$ , 使得 $B=P^{-1}AP$ , 则称$B$是$A$的相似矩阵。
相似矩阵是同一个线性变换在不同基向量下的不同矩阵表示.</description></item><item><title>MIT_18.065-Part_5-Four_Subspaces</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_5-Four_Subspaces/</link><pubDate>Sat, 13 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_5-Four_Subspaces/</guid><description>4 Subspaces 2021-11-13 Tags: #Math/LinearAlgebra
What are they? The Four Subspaces are:
$A$的列空间 $Col(A)$ $A$的行空间 $Row(A)=Col(A^T)$ $A$的核(零空间) $N(A) \Rightarrow Ax=0$ $A^T$的核 $N(A^T) \Rightarrow A^Ty=0$ 为了与课本一致, 我们将$Col(A)$简称为$C(A)$</description></item><item><title>MIT_18.065-Part_6-Orthonormal Columns in Q Give Q'Q = I</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_6-Orthonormal-Columns-in-Q-Give-QQ-I/</link><pubDate>Sat, 13 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_6-Orthonormal-Columns-in-Q-Give-QQ-I/</guid><description>Orthonormal Columns in Q Give Q&amp;rsquo;Q = I 2021-11-13 Tags: #Math/LinearAlgebra
orthogonal: 正交的 orthonormal: Ortho(gonal) + normal, 即又正交又是单位向量, 长度为1.</description></item><item><title>somebody is golden</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/somebody-is-golden/</link><pubDate>Sat, 13 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/somebody-is-golden/</guid><description>somebody is golden 2021-11-13 Tags: #
informal used to say that someone is in a very good situation and is likely to be successful If the right editor looks at your article, you’re golden.</description></item><item><title>MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning,</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT-18.065-Matrix-Methods-in-Data-Analysis-Signal-Processing-and-Machine-Learning/</link><pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT-18.065-Matrix-Methods-in-Data-Analysis-Signal-Processing-and-Machine-Learning/</guid><description>MIT 18.065 - Matrix Methods in Data Analysis, Signal Processing, and Machine Learning, 2021-11-12 Tags: #Matrix #Math/LinearAlgebra #Math
学习这门课的主要动力是线性代数的知识在大二的一年内已经有所遗忘了, 并且在学习机器学习期间常常设计线性代数与矩阵的相关知识, 所以想要有针对性地深入学习与复习一下.</description></item><item><title>MIT_18.065-Part_1-A_Column_Space_Perspective</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_1-A_Column_Space_Perspective/</link><pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_1-A_Column_Space_Perspective/</guid><description>The Column Space of A Contains All Vectors Ax 2021-11-12 Tags: #Math/LinearAlgebra
Video Link: 1. The Column Space of A Contains All Vectors Ax - YouTube</description></item><item><title>MIT_18.065-Part_2-Matrix_Factorization</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_2-Matrix_Factorization/</link><pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_2-Matrix_Factorization/</guid><description>矩阵分解 2021-11-12 Tags: #Matrix #Math/LinearAlgebra #Math
我们看待矩阵乘积的新方式有助于我们理解数据科学里面对矩阵的各种分解. 我们常常需要发掘一个矩阵$A$里面隐藏的信息, 而通过将$A$分解为$CR$, 我们可以观察A里面最基本的组成部分: 秩为1的矩阵: $col_k(C)\space row_k(R)$ 下面列举重要的分解, 在详细论述后将补充相应细节</description></item><item><title>MIT_18.065-Part_4-LU_Factorization</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_4-LU_Factorization/</link><pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_4-LU_Factorization/</guid><description>LU 分解 2021-11-12 Tags: #Math/LinearAlgebra #Math #Matrix
$A=L U$
Key Idea: 怎样从A = Sum of Rank 1 Matrices的角度来理解这个分解?</description></item><item><title>Balance_of_trade-净出口-贸易余额</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/Balance_of_trade-%E5%87%80%E5%87%BA%E5%8F%A3-%E8%B4%B8%E6%98%93%E4%BD%99%E9%A2%9D/</link><pubDate>Thu, 11 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/Balance_of_trade-%E5%87%80%E5%87%BA%E5%8F%A3-%E8%B4%B8%E6%98%93%E4%BD%99%E9%A2%9D/</guid><description>Balance of trade 2021-11-11 Tags: #Economy
净出口，或称贸易余额，是指一国在一定时间内的出口总值与其进口总值的差额。
$\text{净出口} = Sum(\text{出口}) - Sum(\text{进口})$ 净出口为正值时，称为贸易黑字、贸易顺差、贸易盈余或出超。
净出口为负值时，称为贸易赤字、贸易逆差或入超。
美国净出口的变化 从图中可见, 美国长时间处于贸易逆差状态, 即: 销往美国的商品总值&amp;gt;美国出口的贸易总值 在2008经济危机期间, 美国贸易逆差减小很多 近年来逐渐回落 全球净出口余额状态 1980年－2008年平均 中美差异明显</description></item><item><title>Part.30_Dimensionality_Reduction(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/Part.30_Dimensionality_ReductionML_Andrew.Ng./</link><pubDate>Thu, 11 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/Part.30_Dimensionality_ReductionML_Andrew.Ng./</guid><description>Dimensionality Reduction - 降维 2021-11-11 Tags: #MachineLearning #DimensionalityReduction
Motivation Data Compression 将数据维数减少后, 可以节约存储数据的空间
Visualization 高维数据降维后才能可视化, 而可视化有助于我们理解高维数据的内在含义.
Principal Component Analysis [[notes/2021/2021.</description></item><item><title>Part.31_Principal_Component_Analysis(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/Part.31_Principal_Component_AnalysisML_Andrew.Ng./</link><pubDate>Thu, 11 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/Part.31_Principal_Component_AnalysisML_Andrew.Ng./</guid><description>Principal Component Analysis - 主成分分析 2021-11-11 Tags: #MachineLearning #DimensionalityReduction #PCA
基本步骤 Step 0 - Data Preprocessing Normalization 归一化 Standardization 标准化 PCA依赖于欧氏距离, 所以预处理数据可以让降维效果更好.</description></item><item><title>Quid pro quo - Latin Phrase</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/Quid-pro-quo-Latin-Phrase/</link><pubDate>Thu, 11 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/Quid-pro-quo-Latin-Phrase/</guid><description>Quid pro quo 2021-11-11 Tags: #Latin #English
Quid pro quo (&amp;lsquo;what for what&amp;rsquo; in Latin) is a Latin phrase used in English to mean an exchange of goods or services, in which one transfer is contingent upon the other; &amp;ldquo;a favor for a favor&amp;rdquo;.</description></item><item><title>Odyssey-奥德赛</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Odyssey-%E5%A5%A5%E5%BE%B7%E8%B5%9B/</link><pubDate>Sat, 06 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Odyssey-%E5%A5%A5%E5%BE%B7%E8%B5%9B/</guid><description>Odyssey - 奥德赛 2021-11-06 Tags: #Greek #Poems #Homer #Literature
《奥德赛》（古希腊语：Ὀδύσσεια，转写：Odýsseia，英语：Odyssey）是古希腊最重要的两部史诗之一（另一部是《伊利亚特》）。《奥德赛》延续了《伊利亚特》的故事情节，由盲诗人荷马所作。这部史诗是西方文学的奠基之作，是除《伊利亚特》外现存最古老的西方文学作品。
It follows the Greek hero Odysseus, king of Ithaca, and his journey home after the Trojan War.</description></item><item><title>住房自有率-House_Ownership_Rate</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/%E4%BD%8F%E6%88%BF%E8%87%AA%E6%9C%89%E7%8E%87-House_Ownership_Rate/</link><pubDate>Sat, 06 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/%E4%BD%8F%E6%88%BF%E8%87%AA%E6%9C%89%E7%8E%87-House_Ownership_Rate/</guid><description>住房自有率 - House Ownership Rate 2021-11-06 Tags: #HousingMarket #Index
即拥有住房的人口占总人口的比率 不同国家的统计方式不同 List of countries by home ownership rate</description></item><item><title>Compiler-4_LR分析</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Compiler-4_LR%E5%88%86%E6%9E%90/</link><pubDate>Fri, 05 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Compiler-4_LR%E5%88%86%E6%9E%90/</guid><description>LR 分析 2021-11-05 Tags: #Compiler #Course
在自底向上语法分析中, 如何寻找句柄是关键问题 Compiler-4-1_什么是 LR 分析
Compiler-4-2_LR(0)_Parse
Compiler-4-3_SLR_parse
Compiler-4-4_LR(1)_分析</description></item><item><title>ad_hoc-Latin_Phrase</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/ad_hoc-Latin_Phrase/</link><pubDate>Thu, 04 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/ad_hoc-Latin_Phrase/</guid><description>Ad hoc 2021-11-04 Tags: #Latin #English
adjective uk/ˌæd ˈhɒk/ us/ˌæd ˈhɑːk/
made or happening only for a particular purpose or need, not planned before it happens 特别的；专门的；临时安排的 an ad hoc committee/meeting 特别委员会／会议 We deal with problems on an ad hoc basis (= as they happen).</description></item><item><title>Compiler-2_Bottom-Up_Parsing-自底向上分析</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Compiler-2_Bottom-Up_Parsing-%E8%87%AA%E5%BA%95%E5%90%91%E4%B8%8A%E5%88%86%E6%9E%90/</link><pubDate>Sat, 30 Oct 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Compiler-2_Bottom-Up_Parsing-%E8%87%AA%E5%BA%95%E5%90%91%E4%B8%8A%E5%88%86%E6%9E%90/</guid><description>Bottom-Up Parsing 2021-10-30 Tags: #Compiler #Course
自底向上分析是一种语法分析方法, 它从语法树的下边缘(即一堆终结符)开始, 逐步向上构建这个句子的推导过程
一般来说, 自底向上分析比自上而下分析要更强大, 同时也更复杂.
Shift-reduce parsing1 移位-规约分析(Shift-Reduce Parsing)是自底向上分析的主流方法 有许多不同的移位规约分析方法: 比如: &amp;ldquo;算符优先分析&amp;rdquo;和&amp;quot;LR分析&amp;quot;都属于移位规约分析.</description></item><item><title>Compiler-3_算符优先分析</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Compiler-3_%E7%AE%97%E7%AC%A6%E4%BC%98%E5%85%88%E5%88%86%E6%9E%90/</link><pubDate>Sat, 30 Oct 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Compiler-3_%E7%AE%97%E7%AC%A6%E4%BC%98%E5%85%88%E5%88%86%E6%9E%90/</guid><description>Operator-precedence grammar 2021-10-30 Tags: #Compiler #Course #FormalLanguage
算符优先文法 算符优先文法(OPG)是一种有特殊性质的上下文无关文法(CFG)
它的特殊性质表现为:
产生式右部不能为空 (即没有 $P\rightarrow \varepsilon$) 产生式右边不能有两个连续的非终结符 (即没有 $P\rightarrow \cdots AB\cdots$) 上述规则使得我们可以定义终结符之间的&amp;quot;优先级&amp;quot;(Precedence), ( 为什么?</description></item><item><title>Dot_Product_and_Linear_Transformation-向量内积与线性变换</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Dot_Product_and_Linear_Transformation-%E5%90%91%E9%87%8F%E5%86%85%E7%A7%AF%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2/</link><pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Dot_Product_and_Linear_Transformation-%E5%90%91%E9%87%8F%E5%86%85%E7%A7%AF%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2/</guid><description>内积与线性变换 2021-10-29 Tags: #Math/LinearAlgebra #LinearTransformation #DotProduct #Vector
向量内积即多维空间到一维空间的线性变换. 将第二个向量变换到第一个向量的方向上去 如果第一个向量的长度为1, 那么就是第二个向量到第一个向量方向上的投影变换 Highlights 视频</description></item><item><title>为什么方差的分母常常是n-1</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%96%B9%E5%B7%AE%E7%9A%84%E5%88%86%E6%AF%8D%E5%B8%B8%E5%B8%B8%E6%98%AFn-1/</link><pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%96%B9%E5%B7%AE%E7%9A%84%E5%88%86%E6%AF%8D%E5%B8%B8%E5%B8%B8%E6%98%AFn-1/</guid><description>为什么方差的分母常常是$n-1$? 2021-10-29 Tags: #Math/Statistics #Variance
按照定义, 方差的分母的确应该是$n$
但是因为我们用样本的均值$\overline X$代替了数学期望$\mu$, 而这个$\overline X$是有误差的, $\frac 1 n \rightarrow \frac 1 {(n-1)}$是为了消除这个误差.</description></item><item><title>协方差矩阵_Covariance_Matrix</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5_Covariance_Matrix/</link><pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5_Covariance_Matrix/</guid><description>Covariance Matrix 2021-10-29 Tags: #Matrix #Math/Statistics
https://janakiev.com/blog/covariance-matrix/
Variance, Covariance Variance measures the variation of a single random variable (like height of a person in a population) $$\sigma_{x}^{2}=\mathbb E \left(\frac{1}{n-1} \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}\right)$$ Link: 为什么方差的分母常常是n-1</description></item><item><title>拉格朗日乘数-Lagrange_Multiplier</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0-Lagrange_Multiplier/</link><pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0-Lagrange_Multiplier/</guid><description>拉格朗日乘数 2021-10-29 Tags: #Math #Optimization
Intuition $$\mathcal{L}(x, y, \lambda)=f(x, y)-\lambda g(x, y)$$ 在一个三维曲面($f(x,y)$)上面画了一条曲线($g(x,y)$), 求这条曲线上面的最低点.
Explanation 中英文的维基百科已经解释的十分直观与清晰了:
Chinese English</description></item><item><title>Part.29_Fisher_Linear_Discriminant(Pattern_Classification-Chapter_4)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Part.29_Fisher_Linear_DiscriminantPattern_Classification-Chapter_4/</link><pubDate>Thu, 28 Oct 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Part.29_Fisher_Linear_DiscriminantPattern_Classification-Chapter_4/</guid><description>Fisher Linear Discriminant 2021-10-28 Tags: #MachineLearning #PatternClassification #Course #DimensionalityReduction
通过降维进行分类, 降到一维即为线性判别. 其实, 线性判别分析 (LDA)就是对Fisher线性判别的归纳.1 Motivation Curse of Dimensionality - 模型的表现随着维度的增加而变坏, 而且根据设计者的3维直觉, 无法很好的解决高维度的问题.</description></item><item><title>CMU15-445_3_Lecture_Note</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/CMU15-445_3_Lecture_Note/</link><pubDate>Wed, 20 Oct 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/CMU15-445_3_Lecture_Note/</guid><description>03 - Database Storage I 2021-10-20 Tags: #Database
Outline Different Layers of the whole system, One layer at a time, from bottom to top.</description></item><item><title>梯度的方向是哪个方向-为什么梯度是函数增长最快的方向</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/%E6%A2%AF%E5%BA%A6%E7%9A%84%E6%96%B9%E5%90%91%E6%98%AF%E5%93%AA%E4%B8%AA%E6%96%B9%E5%90%91-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%A2%AF%E5%BA%A6%E6%98%AF%E5%87%BD%E6%95%B0%E5%A2%9E%E9%95%BF%E6%9C%80%E5%BF%AB%E7%9A%84%E6%96%B9%E5%90%91/</link><pubDate>Thu, 14 Oct 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/%E6%A2%AF%E5%BA%A6%E7%9A%84%E6%96%B9%E5%90%91%E6%98%AF%E5%93%AA%E4%B8%AA%E6%96%B9%E5%90%91-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%A2%AF%E5%BA%A6%E6%98%AF%E5%87%BD%E6%95%B0%E5%A2%9E%E9%95%BF%E6%9C%80%E5%BF%AB%E7%9A%84%E6%96%B9%E5%90%91/</guid><description>为什么梯度是函数增长最快的方向 2021-10-14 Tags: #Math
看这篇文章可快速回顾高数: https://zhuanlan.zhihu.com/p/38525412
Key Idea 对于任意方向一个增量 $\Delta z=f\left(x_{0}+t \cos \alpha, y_{0}+t \sin \alpha\right)-f\left(x_{0}, y_{0}\right)$
函数沿此方向的变化率为: $$ \lim {t \rightarrow 0^{+}} \frac{f\left(x{0}+t \cos \alpha, y_{0}+t \sin \alpha\right)-f\left(x_{0}, y_{0}\right)}{t}=f_{x}\left(x_{0}, y_{0}\right) \cos \alpha+f_{y}\left(x_{0}, y_{0}\right) \sin \alpha $$</description></item><item><title>Part.28_Cost_Function-Neural_Network(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Part.28_Cost_Function-Neural_NetworkML_Andrew.Ng./</link><pubDate>Sat, 09 Oct 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Part.28_Cost_Function-Neural_NetworkML_Andrew.Ng./</guid><description>Cost Function - Neural Network 2021-10-09 Tags: #MachineLearning #NeuralNetwork #CostFunction
Basic Concepts $$\left{\left(x^{(1)}, y^{(1)}\right), \left(x^{(2)}, y^{(2)}\right), \ldots, \left(x^{(m)}, y^{(m)}\right)\right}$$</description></item><item><title>Call_Stack(Stack_Frame)的构成</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Call_StackStack_Frame%E7%9A%84%E6%9E%84%E6%88%90/</link><pubDate>Thu, 07 Oct 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Call_StackStack_Frame%E7%9A%84%E6%9E%84%E6%88%90/</guid><description>Call Stack 2021-10-07 Tags: #Stack #OperatingSystem #Assembly
两个非常好的视频:</description></item><item><title>Part.27_Locally_Weighted_Linear_Regression(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.27_Locally_Weighted_Linear_RegressionML_Andrew.Ng./</link><pubDate>Thu, 30 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.27_Locally_Weighted_Linear_RegressionML_Andrew.Ng./</guid><description>Locally Weighted Linear Regression 2021-09-30 Tags: #MachineLearning #LinearRegression
Abbreviation: LWR
上图展现了Underfitting &amp;amp; Overfitting的情况，而 Locally weighted linear regression (LWR) is an algorithm which, assuming there is sufficient training data, makes the choice of features less critical.</description></item><item><title>ROC_and_AUC_Graph</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/ROC_and_AUC_Graph/</link><pubDate>Thu, 30 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/ROC_and_AUC_Graph/</guid><description>ROC AUC Clearly Explained 2021-09-30 Tags: #MachineLearning #ROC #AUC
ROC: Receiver Operator Characteristic 用来判断哪一个是最好的Classification Threshold. AUC: the area under the curve, 用来判断哪一个是最好的模型</description></item><item><title>Codecademy_SQL_Tutorial-Notes-Database</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Codecademy_SQL_Tutorial-Notes-Database/</link><pubDate>Sun, 26 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Codecademy_SQL_Tutorial-Notes-Database/</guid><description>SQL Tutorial 2021-09-26 Tags: #Database #SQL #SQLite
This course use SQLite 复习材料:
SQL Commands - Glossary CheatSheet 下面只叙述CheatSheet里面没有的内容</description></item><item><title>De facto</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/De-facto/</link><pubDate>Sun, 26 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/De-facto/</guid><description>De facto 2021-09-26 Tags: #English #Latin
In law and government, de facto (/deɪ ˈfæktoʊ, di-, də-/ day FAK-toh, dee -⁠; Latin: de facto [deː ˈfaktoː], &amp;ldquo;in fact&amp;rdquo;) describes practices that exist in reality, even though they are not officially recognized by laws.</description></item><item><title>Underline_in_Markdown_MD下划线</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Underline_in_Markdown_MD%E4%B8%8B%E5%88%92%E7%BA%BF/</link><pubDate>Sat, 18 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Underline_in_Markdown_MD%E4%B8%8B%E5%88%92%E7%BA%BF/</guid><description>Markdown里面的下划线 2021-09-18 Tags: #Markdown #HTML
&amp;lt;ins&amp;gt; &amp;lt;/ins&amp;gt; Example:
1 &amp;lt;ins&amp;gt;This line is UNDERLINED!&amp;lt;/ins&amp;gt; This line is UNDERLINED!</description></item><item><title>Part.26_Probabilistic_Interpretation_of_MSE(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.26_Probabilistic_Interpretation_of_MSEML_Andrew.Ng./</link><pubDate>Thu, 16 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.26_Probabilistic_Interpretation_of_MSEML_Andrew.Ng./</guid><description>均方差的合理性 - 概率解释 2021-09-16 Tags: #MachineLearning #Math/Statistics #MeanSquareError #CostFunction
之前的一些讨论 Mean_Squared_Error-均方误差 Why_do_cost_functions_use_the_square_error CS229 - Probabilistic Interpretation 独立同分布-IID [[notes/2021/2021.</description></item><item><title>正态分布_高斯分布_Normal_Distribution-Gaussian_Distribution</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83_%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_Normal_Distribution-Gaussian_Distribution/</link><pubDate>Thu, 16 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83_%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_Normal_Distribution-Gaussian_Distribution/</guid><description>正态分布 2021-09-16 Tags: #Math/Statistics
概率密度函数 正态分布, 概率密度函数: $$f(x)=\frac{1}{\sigma \sqrt{2 \pi}} e^{\Large -\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{2}}$$ or $$f(x)=\frac{1}{\sigma \sqrt{2 \pi}} \exp\left(-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{2}\right)$$
重要性质 Mean $(\mu)$ and standard deviation $(\sigma)$ $$ \begin{aligned} &amp;amp;\mu=E(X)=\int_{-\infty}^{\infty} x p(x) d x \ &amp;amp;\sigma^{2}=E\left{(X-\mu)^{2}\right}=\int_{-\infty}^{\infty}(x-\mu)^{2} p(x) d x \end{aligned} $$</description></item><item><title>独立同分布-IID</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/%E7%8B%AC%E7%AB%8B%E5%90%8C%E5%88%86%E5%B8%83-IID/</link><pubDate>Thu, 16 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/%E7%8B%AC%E7%AB%8B%E5%90%8C%E5%88%86%E5%B8%83-IID/</guid><description>独立同分布 Independent and identically distributed 2021-09-16 Tags: #Math/Statistics
定义 在概率论与统计学中，独立同分布（英语：Independent and identically distributed，或称独立同分配，缩写为iid、 i.i.d.、IID）是指一组随机变量中每个变量的概率分布都相同，且这些随机变量互相独立.</description></item><item><title>Part.23_Forward_Propagation-Neural_Network(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.23_Forward_Propagation-Neural_NetworkML_Andrew.Ng./</link><pubDate>Sun, 12 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.23_Forward_Propagation-Neural_NetworkML_Andrew.Ng./</guid><description>Forward Propagation 2021-09-12 Tags: #NeuralNetwork #MachineLearning
Vectorized Implementation $$\begin{aligned} a_{1}^{(2)} &amp;amp;=g\left(\Theta_{10}^{(1)} x_{0}+\Theta_{11}^{(1)} x_{1}+\Theta_{12}^{(1)} x_{2}+\Theta_{13}^{(1)} x_{3}\right) \ a_{2}^{(2)} &amp;amp;=g\left(\Theta_{20}^{(1)} x_{0}+\Theta_{21}^{(1)} x_{1}+\Theta_{22}^{(1)} x_{2}+\Theta_{23}^{(1)} x_{3}\right) \ a_{3}^{(2)} &amp;amp;=g\left(\Theta_{30}^{(1)} x_{0}+\Theta_{31}^{(1)} x_{1}+\Theta_{32}^{(1)} x_{2}+\Theta_{33}^{(1)} x_{3}\right) \ \end{aligned}$$ 我们把Sigmoid函数里面的部分用$z$代替: $$\begin{aligned} &amp;amp;a_{1}^{(2)}=g\left(z_{1}^{(2)}\right) \ &amp;amp;a_{2}^{(2)}=g\left(z_{2}^{(2)}\right) \ &amp;amp;a_{3}^{(2)}=g\left(z_{3}^{(2)}\right) \end{aligned}$$ 同时: $$\begin{aligned} &amp;amp;X = \left[\begin{array}{cccc} x_0 \x_1 \x_2 \x_3 \ \end{array}\right] =a^{(1)} \end{aligned}$$</description></item><item><title>Part.24_Neural_Network-Examples(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.24_Neural_Network-ExamplesML_Andrew.Ng./</link><pubDate>Sun, 12 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.24_Neural_Network-ExamplesML_Andrew.Ng./</guid><description>Examples of Neural Network 2021-09-12 Tags: #NeuralNetwork #MachineLearning
我们可以通过以下直观的组合过程, 体会神经网络利用Linearity构建Non-Linearity的过程. OR function AND function NOT function Putting together -&amp;gt; XNOR Function</description></item><item><title>Part.25_Multiclass_Classification-Neural_Network(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.25_Multiclass_Classification-Neural_NetworkML_Andrew.Ng./</link><pubDate>Sun, 12 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.25_Multiclass_Classification-Neural_NetworkML_Andrew.Ng./</guid><description>Multiclass Classification 2021-09-12 Tags: #MachineLearning #NeuralNetwork
We can define our set of resulting classes as y: $$ y^{(i)}=\left[\begin{array}{l} 1 \ 0 \ 0 \ 0 \end{array}\right],\left[\begin{array}{l} 0 \ 1 \ 0 \ 0 \end{array}\right],\left[\begin{array}{l} 0 \ 0 \ 1 \ 0 \end{array}\right],\left[\begin{array}{l} 0 \ 0 \ 0 \ 1 \end{array}\right] $$ Each $y^{(i)}$ represents a different image corresponding to either a car, pedestrian, truck, or motorcycle.</description></item><item><title>i.e._Meaning</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/i.e._Meaning/</link><pubDate>Sat, 11 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/i.e._Meaning/</guid><description>Latin Expression: i.e. 2021-09-11 Tags: #English
id est (i.e.) that is (literally &amp;ldquo;it is&amp;rdquo;)
&amp;ldquo;That is (to say)&amp;rdquo; in the sense of &amp;ldquo;that means&amp;rdquo; and &amp;ldquo;which means&amp;rdquo;, or &amp;ldquo;in other words&amp;rdquo;, &amp;ldquo;namely&amp;rdquo;, or sometimes &amp;ldquo;in this case&amp;rdquo;, depending on the context.</description></item><item><title>Part.20_Regularized_Logistic_Regression(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.20_Regularized_Logistic_RegressionML_Andrew.Ng./</link><pubDate>Sat, 11 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.20_Regularized_Logistic_RegressionML_Andrew.Ng./</guid><description>Regularized Logistic Regression 2021-09-11 Tags: #MachineLearning #LogisticRegression #Regularization
回顾一下没有正则化的情况 损失函数 更简洁的形式 $$\begin{align} J(\theta) &amp;amp;=-\frac{1}{m} \sum_{i=1}^{m}\left[y^{(i)} \log \left(h_{\theta}\left(x^{(i)}\right)\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]\ \end{align}$$ or 向量化的 $$\begin{aligned} h&amp;amp;=g(X \theta) \ J(\theta)&amp;amp;=-\frac{1}{m} \cdot\left[y^{T} \log (h)+(1-y)^{T} \log (1-h)\right] \end{aligned}$$</description></item><item><title>Part.21_Neural_Network_Introduction(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.21_Neural_Network_IntroductionML_Andrew.Ng./</link><pubDate>Sat, 11 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.21_Neural_Network_IntroductionML_Andrew.Ng./</guid><description>Neural Network - Introduction 2021-09-11 Tags: #NeuralNetwork #MachineLearning
Non-linear Hypotheses 所以随着特征数量的提高, 如果需要更加复杂的假设函数, 特征数量会爆炸式地增长. 比如在图片处理的时候, 每一个像素都是一个特征(RGB彩色像素甚至是三个特征), 那么特征的数目将会是十分庞大的: The “one learning algorithm” hypothesis Neurons in the Brain 树突: Dendrite 轴突: Axon 神经脉冲: Spike</description></item><item><title>Part.22_Model_Representation-Neural_Network(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.22_Model_Representation-Neural_NetworkML_Andrew.Ng./</link><pubDate>Sat, 11 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.22_Model_Representation-Neural_NetworkML_Andrew.Ng./</guid><description>Model Representation - NN 2021-09-11 Tags: #NeuralNetwork #MachineLearning
Hypothesis 一个神经元(Neuron / Activation Unit)的输出计算公式由如下公式给出: $$h_\Theta(x)= a = g(x_0\theta_0+x_1\theta_1+\cdots+x_n\theta_n)$$ 是线性的. ($g(x)$是Sigmoid Function)</description></item><item><title>正则项不影响Logistic回归损失函数凸性</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/%E6%AD%A3%E5%88%99%E9%A1%B9%E4%B8%8D%E5%BD%B1%E5%93%8DLogistic%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%87%B8%E6%80%A7/</link><pubDate>Sat, 11 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/%E6%AD%A3%E5%88%99%E9%A1%B9%E4%B8%8D%E5%BD%B1%E5%93%8DLogistic%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%87%B8%E6%80%A7/</guid><description>正则项不影响Logistic回归损失函数凸性 2021-09-11 Tags: #MachineLearning #LogisticRegression #Regularization #ConvexOptimization #CostFunction
首先, 没有加正则项的二阶导数如下 那么只需要计算正则项的二阶导数 $$\begin{align} J(\theta)&amp;amp;=P(\theta)+\frac\lambda{2m}\sum^n_{i=1}\theta_i^2 \end{align}$$
$$\begin{aligned} \frac{\partial^2}{\partial \theta_{j}^2} \left(\frac\lambda{2m}\sum^n_{i=1}\theta_i^2\right)&amp;amp;= \frac{\lambda}{m}\frac{\partial}{\partial \theta_{j}} \theta_{j}\ &amp;amp;=\frac{\lambda}{m}&amp;gt;0 \end{aligned}$$ 所以损失函数还是凸的</description></item><item><title>证明Logistic回归的损失函数是凸函数</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/%E8%AF%81%E6%98%8ELogistic%E5%9B%9E%E5%BD%92%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%98%AF%E5%87%B8%E5%87%BD%E6%95%B0/</link><pubDate>Sat, 11 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/%E8%AF%81%E6%98%8ELogistic%E5%9B%9E%E5%BD%92%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%98%AF%E5%87%B8%E5%87%BD%E6%95%B0/</guid><description>证明Logistic回归的损失函数是凸函数 2021-09-11 Tags: #MachineLearning #LogisticRegression #ConvexOptimization #CostFunction
证明 原函数 Part.13_Cost_Function-Logistic_Regression(ML_Andrew.Ng.) $$\begin{aligned} h&amp;amp;=g(X \theta) \ J(\theta)&amp;amp;=-\frac{1}{m} \cdot\left[y^{T} \log (h)+(1-y)^{T} \log (1-h)\right] \end{aligned}$$</description></item><item><title>Part.18_Regularization_Intuition(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.18_Regularization_IntuitionML_Andrew.Ng./</link><pubDate>Fri, 10 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.18_Regularization_IntuitionML_Andrew.Ng./</guid><description>Regularization: Intuition 2021-09-10 Tags: #MachineLearning #Regularization
如果我们约束的参数&amp;quot;加大权重&amp;quot;, 那么在优化的时候就会重点最小化那些加了权重的参数. E.g. $$ \theta_{0}+\theta_{1} x+\theta_{2} x^{2}+\theta_{3} x^{3}+\theta_{4} x^{4} $$ We&amp;rsquo;ll want to eliminate the influence of $\theta_{3} x^{3}$ and $\theta_{4} x^{4}$.</description></item><item><title>Part.19_Regularized_Linear_Regression(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.19_Regularized_Linear_RegressionML_Andrew.Ng./</link><pubDate>Fri, 10 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.19_Regularized_Linear_RegressionML_Andrew.Ng./</guid><description>Regularization &amp;amp; Linear Regression 2021-09-10 Tags: #MachineLearning #Regularization #GradientDescent #LinearRegression #NormalEquation
Regularization &amp;amp; Gradient Descent 添加了正则项之后有两点需要注意:
$\theta_0$需要单独处理 (不需要正则约束, 损失函数不一样) $\theta_1 \sim \theta_n$ 因为需要正则化, 损失函数$J(\theta)$发生了变化, 梯度需要重新计算 正则项不影响线性回归损失函数的凸性 同时考虑上面两点, 梯度下降更新公式变为了:</description></item><item><title>正则项不影响线性回归损失函数的凸性</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/%E6%AD%A3%E5%88%99%E9%A1%B9%E4%B8%8D%E5%BD%B1%E5%93%8D%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%87%B8%E6%80%A7/</link><pubDate>Fri, 10 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/%E6%AD%A3%E5%88%99%E9%A1%B9%E4%B8%8D%E5%BD%B1%E5%93%8D%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%87%B8%E6%80%A7/</guid><description>正则项不影响线性回归损失函数的凸性 2021-09-10 Tags: #MachineLearning #Regularization #GradientDescent #LinearRegression #ConvexOptimization
Question: 加上正则项以后函数还是凸的吗? 梯度下降还适用吗? 还是适用的, 证明如下 首先, 如何证明一个函数为凸函数? 如果$f$是二阶可微的，那么如果$f$的定义域是凸集，并且$\forall x\in dom(f), \nabla^2 f(x)\geqslant0$，那么$f$ 就是一个凸函数.</description></item><item><title>正则项会消除正规方程法可能的不可逆性</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/%E6%AD%A3%E5%88%99%E9%A1%B9%E4%BC%9A%E6%B6%88%E9%99%A4%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E6%B3%95%E5%8F%AF%E8%83%BD%E7%9A%84%E4%B8%8D%E5%8F%AF%E9%80%86%E6%80%A7/</link><pubDate>Fri, 10 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/%E6%AD%A3%E5%88%99%E9%A1%B9%E4%BC%9A%E6%B6%88%E9%99%A4%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E6%B3%95%E5%8F%AF%E8%83%BD%E7%9A%84%E4%B8%8D%E5%8F%AF%E9%80%86%E6%80%A7/</guid><description>Normal Equation Non-invertibility &amp;amp; Regularization 2021-09-10 Tags: #NormalEquation #Regularization
$(X^{T} X+\lambda \cdot L)$的内部如下图所示: 最关键的位置就是最左上角的那个地方, 如果那个位置不为0 (或者第一行/列上某个位置不为零, 也可以移过去), 那么容易知道这个矩阵一定可逆(满秩), 因为L就是单位矩阵除去第一个1. 如果上面第一行第一列元素全部为零, 那么一定是因为第一个特征($\theta_0$对应的特征)构成的向量为零向量(X里面黄色的部分), 但是如果是这样, 将这一列与其他特征交换, 便可在乘积矩阵的第一行/列得到非零的元素(因为不可能有两个特征都是零向量, 如果有, 那么便是多余的向量, 删除便可(emm这样一想好像零向量本来就是多余的)) 从而得知最后的矩阵加和一定是满秩的, 证毕.</description></item><item><title>CMU15-445_1_Lecture_Note</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/CMU15-445_1_Lecture_Note/</link><pubDate>Wed, 08 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/CMU15-445_1_Lecture_Note/</guid><description>Course Intro &amp;amp; Relational Algebra 2021-09-08 Tags: #Database
Overview 这门课是关于数据库的DBMS的设计和实现的, 而不是关于如何使用一个数据库的
2 classes per week, supplementary reading materials.</description></item><item><title>Why_printf_is_called_printf</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Why_printf_is_called_printf/</link><pubDate>Mon, 06 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Why_printf_is_called_printf/</guid><description>Why printf is called printf 2021-09-06 Tags: #Programming #English
What f stands for in printf ? The title of section 7.</description></item><item><title>Norm_of_a_Vector-Matrix</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Norm_of_a_Vector-Matrix/</link><pubDate>Fri, 20 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Norm_of_a_Vector-Matrix/</guid><description>向量/矩阵的范数 2021-08-20 Tags: #Norm #Math #MachineLearning #Regularization
https://zh.wikipedia.org/wiki/%E8%8C%83%E6%95%B0</description></item><item><title>Part.17_Overfitting_Underfitting(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.17_Overfitting_UnderfittingML_Andrew.Ng./</link><pubDate>Fri, 20 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.17_Overfitting_UnderfittingML_Andrew.Ng./</guid><description>Overfitting Underfitting 2021-08-20 Tags: #Overfitting #Underfitting #MachineLearning
Underfitting Underfitting 的另一 种表述是这个模型有 &amp;ldquo;High Bias&amp;rdquo;, 直观上理解, 这个模型对数据集有着先入为主的&amp;quot;偏见&amp;quot;, &amp;ldquo;不允许&amp;quot;数据集为二次的, 导致预测效果不好. Bias=Preconception
原因: 模型太简单/使用的特征太少</description></item><item><title>Part.11_Classification(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.11_ClassificationML_Andrew.Ng./</link><pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.11_ClassificationML_Andrew.Ng./</guid><description>Classification 2021-08-19 Tags: #MachineLearning #Classification
分类问题最简单的情况是二分类问题(Binary Classification), 更一般的情况是多分类问题.
分类问题与回归问题最大的不同是其对输出的要求是离散的, 线性函数/回归在分类问题上面不适用.</description></item><item><title>Part.12_Logistic_Regression(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.12_Logistic_RegressionML_Andrew.Ng./</link><pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.12_Logistic_RegressionML_Andrew.Ng./</guid><description>Logistic Regression 2021-08-19 Tags: #LogisticRegression #MachineLearning #Classification
Logistic Function Hypothesis Representation 我们可以通过对线性回归的方法进行一些小改动来匹配回归问题, 在线性回归的时候, $h(x)$的输出与分类问题的&amp;quot;值域&amp;quot;偏差较大, 比如在二分类问题里面, 要求$y=0\space or\space 1$, 但是$h(x)$会输出大于一或者小于零的数.</description></item><item><title>Part.13_Cost_Function-Logistic_Regression(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.13_Cost_Function-Logistic_RegressionML_Andrew.Ng./</link><pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.13_Cost_Function-Logistic_RegressionML_Andrew.Ng./</guid><description>Cost Function - Logistic Regression 2021-08-19 Tags: #CostFunction #LogisticRegression #MachineLearning
Representation 如果我们采用 线性回归的损失函数: 均方误差, 那么因为Logistic 回归的$h(x)$里面有形式很复杂的Logistic函数, 损失函数将不再是 凸函数, 将会很难最小化, 所以我们需要考虑另外的损失函数形式:</description></item><item><title>Part.14_Logistic_Regression&amp;Gradient_Descent(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.14_Logistic_RegressionGradient_DescentML_Andrew.Ng./</link><pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.14_Logistic_RegressionGradient_DescentML_Andrew.Ng./</guid><description>Logistic Regression &amp;amp; Gradient Descent 2021-08-19 Tags: #LogisticRegression #GradientDescent #MachineLearning
Gradient Descent: Cost Function: 推导 损失函数里面的$g(x)$为Logistic函数, Logistic的导函数为: $$\begin{aligned} \frac {d}{dx}g(x)&amp;amp;=g(x)\left(1-g(x)\right)\ \end{aligned}$$</description></item><item><title>Part.15_Advanced_Optimization(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.15_Advanced_OptimizationML_Andrew.Ng./</link><pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.15_Advanced_OptimizationML_Andrew.Ng./</guid><description>Advanced Optimization 2021-08-19 Tags: #Octave #MachineLearning #GradientDescent #LinearRegression #LogisticRegression
More sophisticated, faster way to optimize parameters: Conjugate gradient BFGS L-BFGS Link:其他Gradient_Descent Different_Gradient_Descent_Methods</description></item><item><title>Part.16_MulticlassClassification-One_vs_Rest(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.16_MulticlassClassification-One_vs_RestML_Andrew.Ng./</link><pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.16_MulticlassClassification-One_vs_RestML_Andrew.Ng./</guid><description>One vs Rest 2021-08-19 Tags: #MulticlassClassification #Classification #MachineLearning
AKA: One vs All MulticlassClassification $$\begin{aligned} &amp;amp;y \in{0,1 \ldots n} \ &amp;amp;h_{\theta}^{(0)}(x)=P(y=0 \mid x ; \theta) \ &amp;amp;h_{\theta}^{(1)}(x)=P(y=1 \mid x ; \theta) \ &amp;amp;\cdots \ &amp;amp;h_{\theta}^{(n)}(x)=P(y=n \mid x ; \theta) \ &amp;amp;\text { prediction }=\max {i}\left(h{\theta}^{(i)}(x)\right) \end{aligned}$$</description></item><item><title>Sigmoid-Definition</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Sigmoid-Definition/</link><pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Sigmoid-Definition/</guid><description>Sigmoid 2021-08-19 Tags: #Sigmoid
Sigmoid means resembling the lower-case Greek letter sigma (uppercase Σ, lowercase σ, lowercase in word-final position ς) or the Latin letter S1</description></item><item><title>Sigmoid_Function</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Sigmoid_Function/</link><pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Sigmoid_Function/</guid><description>Sigmoid Function 2021-08-19 Tags: #Sigmoid #MachineLearning #ActivationFunction
什么是Sigmoid函数? Sigmoid的含义是像S型的, 所以Sigmoid函数便是具有S形状的一类函数.
Sigmoid函数把整个实数域上的任意数映射到一个有限的区间里面: $(0,1)$
在分类问题里面, it&amp;rsquo;s useful for transforming an arbitrary-valued function into a function better suited for classification.</description></item><item><title>Part.10_Octave_Tutorial(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.10_Octave_TutorialML_Andrew.Ng./</link><pubDate>Wed, 18 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.10_Octave_TutorialML_Andrew.Ng./</guid><description>Octave Tutorial 2021-08-18 Tags: #Octave #MachineLearning
还是要在实践中学习Octave 为什么吴恩达说Octave比Python好呢? 或许这里涉及到编程与建模的区别? 建模的目的是快速实现一个模型, 像 Matlab Octave Labview之类的软件的目标就是快速建模, 而编程语言像是Python之类的, 他们的目的偏向于 建立一个模型的可靠的应用实例, 不仅要实现, 还需要可靠, 性能需要优化 但是像IPython Console, Jupyter Notebook之类的交互式编程界面是否已经打破了这两个之间的隔阂?</description></item><item><title>How_to_Work_Hard-Paul_Graham</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/How_to_Work_Hard-Paul_Graham/</link><pubDate>Mon, 16 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/How_to_Work_Hard-Paul_Graham/</guid><description>How to Work Hard 2021-08-16 Tags: #Essay #PaulGraham #Translation
Chinese Translation [[notes/2021/2021.8/如何努力工作_Paul_Graham]]
English Version June 2021
It might not seem there&amp;rsquo;s much to learn about how to work hard.</description></item><item><title>如何努力工作_Paul_Graham</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/%E5%A6%82%E4%BD%95%E5%8A%AA%E5%8A%9B%E5%B7%A5%E4%BD%9C_Paul_Graham/</link><pubDate>Mon, 16 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/%E5%A6%82%E4%BD%95%E5%8A%AA%E5%8A%9B%E5%B7%A5%E4%BD%9C_Paul_Graham/</guid><description>如何勤奋工作 2021-08-16 Tags: #Essay #PaulGraham #Translation
2021.6 关于如何勤奋工作似乎没什么好学的. 任何上过学的人都知道勤奋工作需要付出些什么, 即使他们选择不去这么做. 要知道有些12岁的小孩也工作得非常勤奋. 如今， 每当思考这个问题的时候, 我总会发现自己相比学生时期有了更多的感悟.
我的感悟之一是: 如果你想要做点了不起的事情, 那么努力工作是必须的. 在童年的时候, 我还对此不太确定: 那时学校里不同功课的难度并不相同, 有的功课不费什么力气就能做好.</description></item><item><title>矩阵的求导</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E7%9A%84%E6%B1%82%E5%AF%BC/</link><pubDate>Mon, 16 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E7%9A%84%E6%B1%82%E5%AF%BC/</guid><description>对矩阵的求导_Matrix_Derivative 2021-08-16 Tags: #Matrix #Derivative #Calculus #MachineLearning
在学习吴恩达机器学习CS229的时候为了推导Normal Equation的公式, 接触到了函数对于矩阵的求导, 因为许久没有接触微积分, 并且知识跨度太大, 许久没有看懂, 故在此笔记中慢慢梳理. Learning Materials: Pili HU, Matrix Calculus, https://github.</description></item><item><title>矩阵迹的性质</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E8%BF%B9%E7%9A%84%E6%80%A7%E8%B4%A8/</link><pubDate>Mon, 16 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E8%BF%B9%E7%9A%84%E6%80%A7%E8%B4%A8/</guid><description>矩阵迹的性质 2021-08-16 Tags: #Trace #Matrix #Math
标量可以直接套上迹： $a=\operatorname{tr}(a)$
$\mathrm{tr}AB = \mathrm{tr}BA$ ^tracecommutative
左边: $$ \begin{align} &amp;amp;\sum^n_i a_{1i}b_{i1}+\sum^n_i a_{2i}b_{i2}+\cdots+\sum^n_i a_{mi}b_{im} \ = &amp;amp;\sum^m_j\sum^n_ia_{ji}b_{ij} \ = &amp;amp;\sum^m_i\sum^n_j a_{ij}b_{ji} \end{align} $$ 右边: $$ \begin{align} &amp;amp;\sum^m_i b_{1i}a_{i1}+\sum^m_i b_{2i}a_{i2}+\cdots+\sum^m_i b_{ni}a_{in} \ = &amp;amp;\sum^n_j\sum^m_i b_{ji}a_{ij} \ = &amp;amp;\sum^m_i\sum^n_j a_{ij}b_{ji} \end{align} $$</description></item><item><title>Part.9_Normal_Equation(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.9_Normal_EquationML_Andrew.Ng./</link><pubDate>Sat, 14 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.9_Normal_EquationML_Andrew.Ng./</guid><description>Normal Equation 2021-08-14 Tags: #MachineLearning #NormalEquation #LinearRegression
Normal Equation 是解 线性回归(Linear Regression) 问题的一种代数方法. Definition The value of $\theta$ that minimizes $J(\theta)$ can be given in closed form by the equation $$ \theta=\left(X^{T} X\right)^{-1} X^{T} \vec{y} $$</description></item><item><title>Part.7_Feature_Scaling(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.7_Feature_ScalingML_Andrew.Ng./</link><pubDate>Fri, 06 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.7_Feature_ScalingML_Andrew.Ng./</guid><description>Feature Scaling 2021-08-06 Tags: #MachineLearning #FeatureEngineering
1
深入阅读的链接: https://sebastianraschka.com/Articles/2014_about_feature_scaling.html
When to Use 在梯度下降的时候, 缩放数据可以让梯度变化更平滑 If an algorithm uses gradient descent, then the difference in ranges of features will cause different step sizes for each feature.</description></item><item><title>Part.8_Train_Gradient_Descent(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.8_Train_Gradient_DescentML_Andrew.Ng./</link><pubDate>Fri, 06 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.8_Train_Gradient_DescentML_Andrew.Ng./</guid><description>Train Gradient Descent 2021-08-06 Tags: #GradientDescent #MachineLearning
判断收敛(Convergence)的方法 画出Cost Function - Iteration图, 平缓后收敛 相邻周期变化值小于一个很小的值$\Delta$ 寻找正常的学习率 只要学习率$\alpha$足够小, 损失函数一定是递减的(可以严格证明) 如果学习率波动或者递增, 常常是因为学习率过大 学习率过大也有一定几率导致收敛缓慢 学习率过小会导致收敛过慢 合适的方法是类似于二分法的思路, 用一系列的值去尝试, e.</description></item><item><title>Relation_Between_Linear_Regression&amp;Gradient_Descent_梯度下降和线性回归的关系</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Relation_Between_Linear_RegressionGradient_Descent_%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%92%8C%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E5%85%B3%E7%B3%BB/</link><pubDate>Thu, 05 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Relation_Between_Linear_RegressionGradient_Descent_%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%92%8C%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E5%85%B3%E7%B3%BB/</guid><description>梯度下降法和线性回归的关系 2021-08-05 Tags: #MachineLearning #LinearRegression #GradientDescent
1 2 graph TD A([梯度下降])--&amp;gt;B([梯度下降+平方损失])--&amp;gt;C([梯度下降+平方损失+线性回归]) 梯度下降法公式 $$ \begin{array}{l} \text { repeat until convergence }{\ \begin{array}{cc} \theta_{j}:=\theta_{j}-\alpha \frac{\Large\partial}{\Large\partial \Large\theta_{j}} J\left(\theta_{0},\cdots ,\theta_{n}\right) &amp;amp; \text { (simultaneously update } j=0, \cdots ,j=n) \end{array}\ \text { } } \end{array} $$ 梯度下降 + Cost Function=平方损失 $$J\left(\theta_{0},\cdots ,\theta_{n}\right)=\frac{1}{2 m} \sum_{i=1}^{m}\left(\hat{y}^{(i)}-y^{(i)}\right)^{2}=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}$$ 所以</description></item><item><title>Part.6_Linear_Algerba_Review(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.6_Linear_Algerba_ReviewML_Andrew.Ng./</link><pubDate>Wed, 04 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.6_Linear_Algerba_ReviewML_Andrew.Ng./</guid><description>Linear Algebra Review 2021-08-04 Tags: #Math #Math/LinearAlgebra
Scalar: 标量, A physical quantity that is completely described by its magnitude.</description></item><item><title>Here_goes_nothing!_meaning&amp;etymology</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Here_goes_nothing_meaningetymology/</link><pubDate>Mon, 02 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Here_goes_nothing_meaningetymology/</guid><description>Here goes nothing! 2021-08-02 Tags: #English
ref.1 Indicates a lack of confidence or certainty about the activity about to be tried.</description></item><item><title>Part.4_Cost_Function_Intuition(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.4_Cost_Function_IntuitionML_Andrew.Ng./</link><pubDate>Mon, 02 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.4_Cost_Function_IntuitionML_Andrew.Ng./</guid><description>Cost Function Intuition: Linear Regression 2021-08-02 Tags: #MachineLearning #CostFunction #LinearRegression
2-Dimension Intuition 首先简化一下我们的问题, 现在只有三个数据点$(1,1),(2,2),(3,3)$, 我们的Hypothesis Function$:h=\theta_1 x$ 只有一个参数$\theta_1$表示斜率, Cost Function还是: $$ J\left(\theta_{0}, \theta_{1}\right)=\frac{1}{2 m} \sum_{i=1}^{m}\left(\hat{y}^{(i)}-y^{(i)}\right)^{2}=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2} $$ 则我们大概可以把Cost Function的变化过程表示成这样: 可以看出, 斜率为1的时候Cost Function有最小值1, 此时Hypothesis最优.</description></item><item><title>Part.5_Gradient_Descent(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.5_Gradient_DescentML_Andrew.Ng./</link><pubDate>Mon, 02 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.5_Gradient_DescentML_Andrew.Ng./</guid><description>Gradient Descent 2021-08-02 Tags: #MachineLearning #GradientDescent
梯度下降是一种最小化损失函数的标准方法 So we have our hypothesis function and we have a way of measuring how well it fits into the data.</description></item><item><title>凸优化与线性回归问题</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/%E5%87%B8%E4%BC%98%E5%8C%96%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98/</link><pubDate>Mon, 02 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/%E5%87%B8%E4%BC%98%E5%8C%96%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98/</guid><description>Gradient Descent &amp;amp; Convex Optimization / 凸优化 2021-08-02 Tags: #MachineLearning #ConvexOptimization #Math
在 这里(和下面的引用里面), 我们特殊的线性规划的损失函数一定是一个凸函数, 那么在其他情况下, 线性规划还是凸函数吗, 线性规划问题会陷入局部最优的问题中去吗?
Note that, while gradient descent can be susceptible to local minima in general, the optimization problem we have posed here for linear regression has only one global, and no other local, optima; thus gradient descent always converges (assuming the learning rate α is not too large) to the global minimum.</description></item><item><title>Mean_Squared_Error_均方误差</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Mean_Squared_Error_%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE/</link><pubDate>Sat, 31 Jul 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Mean_Squared_Error_%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE/</guid><description>Mean Squared Error 2021-07-31 Tags: #MachineLearning #CostFunction
Mean Square Error: 平均平方误差, 简称均方差, MSE, 又称 Mean Squared Deviation (MSD)
均方差的形式很简单, 但是也有许多问题值得思考</description></item><item><title>Part.3_Linear_Regression(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.3_Linear_RegressionML_Andrew.Ng./</link><pubDate>Sat, 31 Jul 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.3_Linear_RegressionML_Andrew.Ng./</guid><description>Linear Regression 2021-07-31 Tags: #MachineLearning #SelfLearning
Model Representation Supervised Learning Regression Problem Structure 基于训练集, 我们希望通过学习算法得到一个Hypothesis函数$h$, 在房价预测问题上. 输入房子的大小, 得到估计的价格.</description></item><item><title>Why_do_cost_functions_use_the_square_error</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error/</link><pubDate>Sat, 31 Jul 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error/</guid><description>Why do cost functions use the square error? 2021-07-31 Tags: #MachineLearning #CostFunction #MeanSquareError
Reference: StackExchange: why-do-cost-functions-use-the-square-error?
StackExchange上面一个关于均方差的一个很好的解释, 翻译如下:
Question: I&amp;rsquo;m just getting started with some machine learning, and until now I have been dealing with linear regression over one variable.</description></item><item><title>拉普拉斯分布与高斯分布的联系_Relation_of_Laplace_distribution _and_Gaussian_distribution</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%88%86%E5%B8%83%E4%B8%8E%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E7%9A%84%E8%81%94%E7%B3%BB_Relation_of_Laplace_distribution-_and_Gaussian_distribution/</link><pubDate>Sat, 31 Jul 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%88%86%E5%B8%83%E4%B8%8E%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E7%9A%84%E8%81%94%E7%B3%BB_Relation_of_Laplace_distribution-_and_Gaussian_distribution/</guid><description>Gaussian distribution, Laplace distribution: The Relation 2021-07-31 Tags: #Math/Statistics #GaussianDistribution #LaplaceDistribution
拉普拉斯分布, 概率密度函数: Look at the formula for the PDF in the infobox &amp;ndash; it&amp;rsquo;s just the Gaussian with $|\boldsymbol{x}-\boldsymbol{\mu}|$ instead of $(\boldsymbol{x}-\boldsymbol{\mu})^{2}$)1</description></item><item><title>Difference_between_Git Bash_Git_CMD</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.7/Difference_between_Git-Bash_Git_CMD/</link><pubDate>Tue, 27 Jul 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.7/Difference_between_Git-Bash_Git_CMD/</guid><description>Git Bash or Git Cmd? 2021-07-27 Tags: #Git
它们是什么? 在为Windows Terminal设置Git界面的时候遇到了这个问题, Git提供了三种操控方式: Git GUI, Git Bash和Git Cmd, 第一个是图形界面, 那么后面两个命令行界面有什么区别呢?
Bash是Unix Shell的一种 Unix，一种操作系统，Linux是Unix的一种 Shell，“为用户提供用户界面”的软件，比如Windows里面的Cmd</description></item><item><title>Part.1_Supervised_Learning(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.1_Supervised_LearningML_Andrew.Ng./</link><pubDate>Tue, 27 Jul 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.1_Supervised_LearningML_Andrew.Ng./</guid><description>Supervised Learning-Introduction 2021-07-27 Tags: #MachineLearning #SupervisedLearning
What is supervised learning? Supervised learning refers to the fact that we gave the algorithm a dataset in which &amp;ldquo;Right Answers&amp;rdquo; were given.</description></item><item><title>Part.2_Unsupervised_Learning(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.2_Unsupervised_LearningML_Andrew.Ng./</link><pubDate>Tue, 27 Jul 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.2_Unsupervised_LearningML_Andrew.Ng./</guid><description>Unsupervised Learning-Introduction 2021-07-27 Tags: #MachineLearning #UnsupervisedLearning
What is unsupervised learning? In Unsupervised Learning, we&amp;rsquo;re given data that looks different than data that looks like this that doesn&amp;rsquo;t have any labels or that all has the same label or really no labels.</description></item><item><title>Function_Procedure_Difference_(存储)过程和函数的区别</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/Function_Procedure_Difference_%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B%E5%92%8C%E5%87%BD%E6%95%B0%E7%9A%84%E5%8C%BA%E5%88%AB/</link><pubDate>Fri, 25 Jun 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/Function_Procedure_Difference_%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B%E5%92%8C%E5%87%BD%E6%95%B0%E7%9A%84%E5%8C%BA%E5%88%AB/</guid><description>Difference Between Function and Procedure Link
函数有1个返回值,而存储过程是通过参数返回的,可以有多个或者没有
E.g. 1 2 3 ZZ x, a, n; x = InvMod(a, n); _// functional form_ InvMod(x, a, n); _// procedural form_</description></item><item><title>Hamming_Distance_汉明距离</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/Hamming_Distance_%E6%B1%89%E6%98%8E%E8%B7%9D%E7%A6%BB/</link><pubDate>Fri, 25 Jun 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/Hamming_Distance_%E6%B1%89%E6%98%8E%E8%B7%9D%E7%A6%BB/</guid><description>Hamming Distance / 汉明距离 汉明距离是对于两个相同长度的字符串而言, the number of positions at which the corresponding symbols are different(相同的位置上对应字符不同的位置个数)
图例 带色线条是路径示意
Two example distances: 100→011 has distance 3; 010→111 has distance 2</description></item><item><title>2个3x3卷积核堆叠后等价于一个5x5卷积核</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/2%E4%B8%AA3x3%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%A0%86%E5%8F%A0%E5%90%8E%E7%AD%89%E4%BB%B7%E4%BA%8E%E4%B8%80%E4%B8%AA5x5%E5%8D%B7%E7%A7%AF%E6%A0%B8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/2%E4%B8%AA3x3%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%A0%86%E5%8F%A0%E5%90%8E%E7%AD%89%E4%BB%B7%E4%BA%8E%E4%B8%80%E4%B8%AA5x5%E5%8D%B7%E7%A7%AF%E6%A0%B8/</guid><description>VGG16相比AlexNet的一个改进是采用连续的几个3x3的卷积核代替AlexNet中的较大卷积核（11x11，7x7，5x5）。
对于给定的感受野（与输出有关的输入图片的局部大小），采用堆积的小卷积核优于大的卷积核，因为多层非线性层可以增加网络深度来保证学习更复杂的模式，而且代价还比较小（参数更少）。 代价更小: [[notes/2022/2022.2/对于等价的网络, 小的卷积核参数更少]] 简单来说，在VGG中，使用了3个3x3卷积核来代替7x7卷积核，使用了2个3x3卷积核来代替5x5卷积核，这样做的主要目的是在保证具有相同感知野的条件下，提升了网络的深度，在一定程度上提升了神经网络的效果。
为什么使用2个3x3卷积核可以来代替5x5卷积核 5x5卷积看做一个小的全连接网络在5x5区域滑动，我们可以先用一个3x3的滤波器卷积，然后再用一个全连接层连接这个3x3卷积输出. 同时, 这个全连接层我们也可以看做一个3x3卷积层。这样我们就可以用两个3x3卷积级联（叠加）起来代替一个 5x5卷积。</description></item><item><title>A Fancy Example of SVD</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/A-Fancy-Example-of-SVD/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/A-Fancy-Example-of-SVD/</guid><description>==Example==
Consider the $4 × 5$ matrix
$$\mathbf{M} = \begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 2 \</description></item><item><title>CAPTCHA</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/CAPTCHA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/CAPTCHA/</guid><description>完全自动图灵测试: Completely Automated Public Turing test to tell Computer and Humans Apart</description></item><item><title>Compiler-4-1_什么是 LR 分析</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Compiler-4-1_%E4%BB%80%E4%B9%88%E6%98%AF-LR-%E5%88%86%E6%9E%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Compiler-4-1_%E4%BB%80%E4%B9%88%E6%98%AF-LR-%E5%88%86%E6%9E%90/</guid><description>什么是 LR 分析 LR 分析方法是一种自底向上的分析方法: 它从终结符开始, 从左到右, 逐步寻找句柄进行归约. 从这个角度看, LR 分析相当于一种高效的句柄查找方法.
LR 分析比 LL 分析更强大.
LL (k) 分析技术的一个弱点是，它在仅仅看到右部的前 k 个单词时就必须预测要使用的是哪一个产生式。另一种更有效的分析方法是 LR (k) 分析，它可以将这种判断推迟至己看到与正在考虑的这个产生式的整个右部对应的输入单词以后（多于 K 个单词） 。1</description></item><item><title>Compiler-4-2_LR(0)_Parse</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Compiler-4-2_LR0_Parse/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Compiler-4-2_LR0_Parse/</guid><description>LR (0) Parse 在分析 LR (0) 语法时, 我们只需要观察栈顶的元素, 根据栈顶元素来决定如何移进/归约. 这也是 LR (0) 里面&amp;quot;0&amp;quot;的含义 - 0 look ahead. LR (0) 项目 - LR (0) Item LR 分析表的构造以&amp;quot;LR Item&amp;quot;为基础, &amp;ldquo;LR Item&amp;quot;可以理解为&amp;quot;在寻找句柄的过程中的一种推断&amp;rdquo;.</description></item><item><title>Compiler-4-3_SLR_parse</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Compiler-4-3_SLR_parse/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Compiler-4-3_SLR_parse/</guid><description>SLR parse 在 LR (0) 里面, Reduce 完全占据了一个状态, 这造成了极大的浪费: 回忆 LL 分析里面的 FOLLOW 集合: FOLLOW (A) = {可以立即跟在 A 后面的所有终结符} 一个状态需要 Reduce 代表着出现了一个完全识别句柄的项: $P\rightarrow\alpha\space \cdot$, 但是仔细想想: 我们完全可以再向后多看一个符号, 只有在下一个符号属于 FOLLOW (P) 的时候, 我们才应该规约, 如上图所示, 要是 b 不在 FOLLOW (P) 里面的话, α就不应该是 $P\rightarrow\alpha$ 的句柄了, 应该继续 shift, 读进 b, 寻找句柄 $Q\rightarrow\alpha\space b\cdots$ SLR (1) 分析法就是利用 Look ahead, 看看下一个符号和规约产生式的非终结符是不是一致的, 只有在一致的时候才进行规约, 消除了部分的冲突.</description></item><item><title>Compiler-4-4_LR(1)_分析</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Compiler-4-4_LR1_%E5%88%86%E6%9E%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Compiler-4-4_LR1_%E5%88%86%E6%9E%90/</guid><description>LR (1) 分析 LR (1) 分析在一开始构建 DFA 的时候便考虑到了前瞻符号, 使得其在结构上更为强大.
LR (1) 项 在 LR (0) 项的基础上, LR (1) 项添加了前瞻符号: $$A\rightarrow \alpha\cdot \beta, a $$</description></item><item><title>Compiler-4-5_LALR(1)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Compiler-4-5_LALR1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Compiler-4-5_LALR1/</guid><description>LALR (1) LR (1) 的分析表通常十分巨大, 为了在节省空间的同时保留 LR (1) 大部分的优点, 我们常常采用 LALR (1) 分析方法, LALR 代表 Look-Ahead LR.
LR (1) 分析的 DFA 里面常常有两个状态的产生式完全相同, 只有前瞻符号不同.</description></item><item><title>Cross_Entropy_Loss_Input_Format-交叉熵损失函数输入格式</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.5/Cross_Entropy_Loss_Input_Format-%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%BE%93%E5%85%A5%E6%A0%BC%E5%BC%8F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.5/Cross_Entropy_Loss_Input_Format-%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%BE%93%E5%85%A5%E6%A0%BC%E5%BC%8F/</guid><description>PyTorch 标签不需要变成独热编码:
Keras Kears有两种形式:
Categorical Cross Entropy Doc: Use this crossentropy loss function when there are two or more label classes.</description></item><item><title>D2L-19-验证集_or_测试集</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-19-%E9%AA%8C%E8%AF%81%E9%9B%86_or_%E6%B5%8B%E8%AF%95%E9%9B%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-19-%E9%AA%8C%E8%AF%81%E9%9B%86_or_%E6%B5%8B%E8%AF%95%E9%9B%86/</guid><description>验证 数据集: Validation set, 是训练的时候调整参数的依据
训练时被多次使用 测试 数据集: Test set, 是最终测试模型性能的数据
训练完后只测试一次 这两个词常常被混淆, 但是有着很重要的区别:</description></item><item><title>D2L-63-Beam Search</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-63-Beam-Search/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-63-Beam-Search/</guid><description>束搜索 2022-04-20 Tags: #BeamSearch #DynamicProgramming
在 Seq2Seq里面预测的时候, 我们直接就将上一步预测概率最大的选项输入到下一个时间步, 其实这是一种贪心策略: 最大化当前时间步的预测概率. 而贪心算法常常不能找到全局的最优解, 我们能怎样改进呢? 贪心 Greedy Search 我们先来评估一下贪心算法的时间复杂度, 我们需要计算 $T$ 个时间步的 $|\mathcal{Y}|$ 个概率, 总的时间复杂度为: $$\mathcal{O}({T}\cdot\left|\mathcal{Y}\right|)$$</description></item><item><title>D2L-8-线性模型可以看作单层神经网络</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-8-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E5%8F%AF%E4%BB%A5%E7%9C%8B%E4%BD%9C%E5%8D%95%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-8-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E5%8F%AF%E4%BB%A5%E7%9C%8B%E4%BD%9C%E5%8D%95%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</guid><description>$$y=w_{1} x_{1}+w_{2} x_{2}+\ldots+w_{n} x_{n}+b$$ Links:
Part.3_Linear_Regression(ML_Andrew.Ng.) Part.24_Neural_Network-Examples(ML_Andrew.Ng.)</description></item><item><title>Different_Gradient_Descent_Methods</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Different_Gradient_Descent_Methods/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Different_Gradient_Descent_Methods/</guid><description>Batch Gradient Descent, 批梯度下降, BGD 每一次把所有数据都用来更新参数, Use ALL the training examples. $$ \begin{array}{l} \text { repeat until convergence }{\ \begin{array}{cc} &amp;amp;\theta_{j}:=\theta_{j}-\alpha \frac 1 m \sum_{i=1}^{m} \left(h_{\theta}(x^{(i)})-y^{(i)}\right) x_j^{(i)} \end{array}\ \text { } } \\ \text { (simultaneously update } j=0, \cdots ,j=n) \end{array} $$</description></item><item><title>Diffie-Hellman问题</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/Diffie-Hellman%E9%97%AE%E9%A2%98/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/Diffie-Hellman%E9%97%AE%E9%A2%98/</guid><description>Diffie-Hellman问题 两个Diffie-Hellman问题 Computation Diffie-Hellman / CDH 即给定一个基数与两个指数, 计算合并的指数
给定$(\alpha, \alpha^b, \alpha^c)$, 求 $\alpha^{bc}$ Decision Diffie-Hellman / DDH 即给你三个指数, 让你判断最后一个是不是前两个的合并</description></item><item><title>Entropy-熵</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Entropy-%E7%86%B5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Entropy-%E7%86%B5/</guid><description>Entropy - 熵 2022-02-11
Tags: #InformationTheory
理解 熵是Surprise的期望 Entropy (for data science) Clearly Explained!!! - YouTube
熵是对事件复杂度的衡量, 即我们最少需要多少信息才能完整地描述这个事件 Intuitively Understanding the Shannon Entropy - YouTube</description></item><item><title>GeoGebra Embed</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.5/GeoGebra-Embed/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.5/GeoGebra-Embed/</guid><description>1 &amp;lt;iframe src=&amp;#34;https://www.geogebra.org/calculator/jzuwutfr?embed&amp;#34; width=&amp;#34;800&amp;#34; height=&amp;#34;600&amp;#34; allowfullscreen style=&amp;#34;border: 1px solid #e4e4e4;border-radius: 4px;&amp;#34; frameborder=&amp;#34;0&amp;#34;&amp;gt;&amp;lt;/iframe&amp;gt; 只需要在链接后添加 ?embed 即可</description></item><item><title>Get ahead of oneself</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/Get-ahead-of-oneself/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/Get-ahead-of-oneself/</guid><description>get ahead of yourself to do something too early, or before you are ready or prepared
That last game suggests that we have been getting ahead of ourselves in praising the team&amp;rsquo;s progress.</description></item><item><title>Gilbert Strang 深入浅出机器学习的矩阵知识</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.10/Gilbert-Strang-%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%9F%A9%E9%98%B5%E7%9F%A5%E8%AF%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.10/Gilbert-Strang-%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%9F%A9%E9%98%B5%E7%9F%A5%E8%AF%86/</guid><description>MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning, MIT_18.065-Part_1-A_Column_Space_Perspective MIT_18.065-Part_2-Matrix_Factorization MIT_18.065-Part_3-A_Different_Perspectvie_of_Matrix_Multiplication-An_Example MIT_18.</description></item><item><title>Hash函数_Pt.1_数据完整性</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/Hash%E5%87%BD%E6%95%B0_Pt.1_%E6%95%B0%E6%8D%AE%E5%AE%8C%E6%95%B4%E6%80%A7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/Hash%E5%87%BD%E6%95%B0_Pt.1_%E6%95%B0%E6%8D%AE%E5%AE%8C%E6%95%B4%E6%80%A7/</guid><description>4.1 Hash 函数与数据完整性 Hash 函数的主要目的就是为了保持数据的完整性 与&amp;quot;加密&amp;quot;不同, hash函数的目的不是为了让一个消息对其他人&amp;quot;不可知,不可理解&amp;quot;, 而是为了保证这条信息没有被篡改, 为了&amp;quot;不变质&amp;quot;地保存一条信息, 即保持这条信息原来的模样
Hash函数有两种, 一种是带密钥的, 一种是不带密钥的, 两种在应用场景上有一些区别
不带密钥的Hash函数可以在一段时间内验证数据的完整性。 想象一个Oracle(先知, 谕示器), 你提供给他一条信息, 她总会返回给你一个独特的信息，通过比对这个信息，你就能够知道自己的信息从上次询问到现在之间是否被篡改了（但是这个信息本身并没有什么实际含义）
注意, 你必须要安全的保存 $y=h(x)$, 否则坏蛋可以同时更换你的 $x$ -&amp;gt; $x^\prime$ 和 $y$ -&amp;gt; $y^\prime=h(x^\prime)$, 你在询问的时候依然有 $h(x^\prime)=y^\prime$, 满足校验条件.</description></item><item><title>Hash函数_Pt.2_攻击方法_安全性</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/Hash%E5%87%BD%E6%95%B0_Pt.2_%E6%94%BB%E5%87%BB%E6%96%B9%E6%B3%95_%E5%AE%89%E5%85%A8%E6%80%A7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/Hash%E5%87%BD%E6%95%B0_Pt.2_%E6%94%BB%E5%87%BB%E6%96%B9%E6%B3%95_%E5%AE%89%E5%85%A8%E6%80%A7/</guid><description>Hash函数的攻击方法/安全性 密码学原理与实践 page99
理想的安全性:随机谕示模型(Random Oracle Model) 完全的随机对应性:
从名称来理解, 即理想的Hash函数相当于一个&amp;quot;先知&amp;quot;, 能够对每一个$x$给出一个完全随机的$hash(x)$, 并且计算$hash(x)$的唯一方法便是询问Oracle(谕示器)
在这个假设下,有如下定理:
密码学原理与实践 page94
直观理解:</description></item><item><title>Hash函数_Pt.3_迭代Hash函数</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/Hash%E5%87%BD%E6%95%B0_Pt.3_%E8%BF%AD%E4%BB%A3Hash%E5%87%BD%E6%95%B0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/Hash%E5%87%BD%E6%95%B0_Pt.3_%E8%BF%AD%E4%BB%A3Hash%E5%87%BD%E6%95%B0/</guid><description>迭代Hash函数 迭代Hash函数的基本结构 迭代Hash函数是一种用有限长度Hash函数来处理无限长度的数据的方法
下面这张图形象地表示了迭代Hash函数地加密步骤: 这样的构造方法可以概括为以下的三个步骤:
我们以有限的Hash函数$compress$为基础 $$ {0,1}^{m+t}\longrightarrow {0,1}^{m} $$ 这是一个将长度从$m+t$缩减到$m$的有限长度hash函数
预处理 这一步将明文切分成长度为t的小段. &amp;ndash;&amp;gt;最后剩余的怎么办?
在末尾添加整个字符串的长度
如果还不够,用0补足 &amp;ndash;&amp;gt;为什么要添加字符串的长度?</description></item><item><title>Hash函数_Pt.4_安全Hash算法_SHA-1</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/Hash%E5%87%BD%E6%95%B0_Pt.4_%E5%AE%89%E5%85%A8Hash%E7%AE%97%E6%B3%95_SHA-1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/Hash%E5%87%BD%E6%95%B0_Pt.4_%E5%AE%89%E5%85%A8Hash%E7%AE%97%E6%B3%95_SHA-1/</guid><description>SHA-1 Tags: #Cryptography #Math #Course #Hash
SHA-1是一个具有160bit消息摘要的迭代Hash函数
主要思想 SHA-1 的分组大小是 512bit, 意味着每一次迭代处理 512bit 的数据
SHA-1建立在对比特串面向字的操作上, 意味着在处理512bit的时候是每次32bit, 一共16次, 一共80次. (为什么变多了? 因为在循环里面需要将16个字扩充到80个字, 如下图)</description></item><item><title>Hash函数_Pt.5_消息认证码_MAC</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/Hash%E5%87%BD%E6%95%B0_Pt.5_%E6%B6%88%E6%81%AF%E8%AE%A4%E8%AF%81%E7%A0%81_MAC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/Hash%E5%87%BD%E6%95%B0_Pt.5_%E6%B6%88%E6%81%AF%E8%AE%A4%E8%AF%81%E7%A0%81_MAC/</guid><description>消息认证码 MAC Message Authentication Code 消息认证码是什么 生成方式: 带密钥的Hash函数, Hash值可以不用在安全的信道上传输, 只需要开始的时候协商一个密钥$K$就可以了
用于核验在不安全信道上传输的消息有没有被修改
不安全的构造方式 一个很直观的方式便是把密钥$K$加到明文x里面,一起Hash, 既然攻击者不知道$K$是什么, 他也应该无法计算$h_K(x)=h(x_K). (x_K$为加入了K的x)
但是以下论证告诉我们一些简单的构造方式并不安全, 攻击者即使不知道K是什么, 也可以利用一个有效对$(x,h_K(x))$计算新的有效对$(x^\prime,h_K(x^\prime)$
直接把K作为$IV$ (IV 即 Compress 的初始输入 见迭代Hash函数的基本结构)</description></item><item><title>Latex Colored Text</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/Latex-Colored-Text/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/Latex-Colored-Text/</guid><description>使用颜色名 $$\textcolor{red}{Sample}$$ 1 \textcolor{red}{Sample} 使用 rgb $$\textcolor[rgb]{0.5,0.2,0.8}{text}$$ 1 \textcolor[rgb]{r,g,b}{text} 其中{r,g,b}代表red、green和blue三种颜色的组合，取值范围为[0-1]</description></item><item><title>Linear_Regression&amp;Gradient_Descent</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Linear_RegressionGradient_Descent/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Linear_RegressionGradient_Descent/</guid><description>%%下面这里我一来就想要写一个最普适的情况, 但是弄得能难懂, 吴恩达在这里比我讲的清晰多了%%
把梯度下降方法应用到我们的线性回归问题里面, 可以得到我们Hypothesis函数参数更新的方法(如何求Cost Function最小值Minimal的方法): $$\begin{align*} \text{repeat until convergence: } \lbrace &amp;amp; \newline \theta_0 := &amp;amp; \theta_0 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)}) \newline \theta_1 := &amp;amp; \theta_1 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m}\left((h_\theta(x^{(i)}) - y^{(i)}) x^{(i)}_1\right) \newline \rbrace&amp;amp; \end{align*}$$</description></item><item><title>LU分解的例子</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/LU%E5%88%86%E8%A7%A3%E7%9A%84%E4%BE%8B%E5%AD%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/LU%E5%88%86%E8%A7%A3%E7%9A%84%E4%BE%8B%E5%AD%90/</guid><description>将一个简单的$3×3$矩阵A进行LU分解： $$ A= \begin{bmatrix} 1 &amp;amp; 2 &amp;amp; 3 \ 2 &amp;amp; 5 &amp;amp; 7 \ 3 &amp;amp; 5 &amp;amp; 3 \ \end{bmatrix} $$</description></item><item><title>MIT_18.065-Part_3-A_Different_Perspectvie_of_Matrix_Multiplication-An_Example</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_3-A_Different_Perspectvie_of_Matrix_Multiplication-An_Example/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_3-A_Different_Perspectvie_of_Matrix_Multiplication-An_Example/</guid><description>Math/LinearAlgebra #Matrix #Math $$S=Q \Lambda Q^{\mathrm{T}}$$
其中S是一个对称矩阵, $S=S^{\mathrm{T}}$
Q的行向量是S的特征向量, 这些特征向量相互正交 $$Q=\left[\begin{array}{ccc} \mid &amp;amp; &amp;amp; \mid \ q_{1} &amp;amp; \ldots &amp;amp; q_{n} \ \mid &amp;amp; &amp;amp; \mid \end{array}\right]$$</description></item><item><title>Normal_Equation_Proof_2_Matrix_Method</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Normal_Equation_Proof_2_Matrix_Method/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Normal_Equation_Proof_2_Matrix_Method/</guid><description>首先补充一点矩阵的知识:求导、迹的性质 矩阵的求导和矩阵的迹是密不可分的
矩阵的求导 矩阵的求导
矩阵的迹 矩阵迹的性质
证明中需要的一些其他性质 结合矩阵的求导, 还有以下性质:
$$\nabla_{A} \operatorname{tr} A B =B^{T}$$ 结合 这里对$\operatorname{tr} A B$的推导, 可以看出对于矩阵$A$每一个位置单独求偏导, 都会得到$b_{ji}$, 即$B^T$对应的位置.</description></item><item><title>OK_should_be_in_capital letters(or_okay)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/OK_should_be_in_capital-lettersor_okay/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/OK_should_be_in_capital-lettersor_okay/</guid><description>English OK/okay is a word with uncertain origin. Both &amp;ldquo;OK&amp;rdquo; and &amp;ldquo;okay&amp;rdquo; is acceptable, but ok is only used for convenience.</description></item><item><title>Ring Hollow</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Ring-Hollow/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Ring-Hollow/</guid><description>ring/sound hollow English If something someone says rings hollow, it does not sound true or sincere. 显得虚假，听起来不诚恳</description></item><item><title>Sliver Bullet</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Sliver-Bullet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Sliver-Bullet/</guid><description>Silver bullet English a simple solution to a complicated problem 银弹（指针对复杂问题的简单解决办法），良方，高招 In folklore, a bullet cast from silver is often one of the few weapons that are effective against a werewolf or witch.</description></item><item><title>Softmax函数</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Softmax%E5%87%BD%E6%95%B0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Softmax%E5%87%BD%E6%95%B0/</guid><description>Softmax函数 直观理解 Softmax函数的作用隐藏在它关于域的映射关系里: $$\sigma: \mathbb{R}^{K} \rightarrow(0,1)^{K}$$ 可以看到, Softmax函数将 $K$ 维向量中每一个元素的取值范围由 $\mathbb{R}$ 压缩到 $(0,1)$ , 并且还保证了所有元素加起来等于 $1$, 这就意味着我们可以将每一个元素看作一个概率.
也就是说:</description></item><item><title>SVD Intuition</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/SVD-Intuition/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/SVD-Intuition/</guid><description>Section 1 Source:
Singular Value Decomposition (SVD) and Image Compression - YouTube The Notebook in the link below is a good demonstration of what each rank 1 matrices represents: GitHub - singular_value_decomposition Section 2 Podcast: Gilbert Strang&amp;rsquo;s Feeling about Singular Value Decomposition - YouTube</description></item><item><title>Xavier初始化的详细例子</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Xavier%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E8%AF%A6%E7%BB%86%E4%BE%8B%E5%AD%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Xavier%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E8%AF%A6%E7%BB%86%E4%BE%8B%E5%AD%90/</guid><description>以MLP为例
假设: $w_{i, j}^{s}$ 是 i.i.d, 那么 $\mathbb{E}\left[w_{i, j}^{s}\right]=0, \operatorname{Var}\left[w_{i, j}^{s}\right]=\gamma_{s}$ $h_{i}^{s-1}$ 独立于 $w_{i, j}^{s}$ 假设没有激活函数, 即: $$\mathbf{h}^{s}=\mathbf{W}^{s} \mathbf{h}^{s-1}, \text { 这里 } \mathbf{W}^{s} \in \mathbb{R}^{n_{s} \times n_{s-1}}$$ 正向均值 $$\mathbb{E}\left[h_{i}^{s}\right]=\mathbb{E}\left[\sum_{j} w_{i, j}^{s} h_{j}^{s-1}\right]=\sum_{j} \mathbb{E}\left[w_{i, j}^{s}\right] \mathbb{E}\left[h_{j}^{s-1}\right]=0$$</description></item><item><title>为什么反向传播比前向传播更高效</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%AF%94%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%9B%B4%E9%AB%98%E6%95%88/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%AF%94%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%9B%B4%E9%AB%98%E6%95%88/</guid><description>找到了一个很好的解释:
为什么说反向传播算法很高效？要回答这个问题，让我们来考虑另一种计算梯度的方式。设想现在是神经网络研究的早期阶段，大概是在上世纪50年代或60年代左右，并且你是第一个想到使用梯度下降方法来进行训练的人！但是要实现这个想法，你需要一种计算代价函数梯度的方式。你回想了你目前关于演算的知识，决定试一下是否能用链式法则来计算梯度。但是琢磨了一会之后发现，代数计算看起来非常复杂，你也因此有些失落。所以你尝试着寻找另一种方法。你决定把代价单独当做权重的函数$C=C(w)$（我们一会再来讨论偏置）。将权重写作$w1,w2,…$，并且要对某个权重计算$∂C/∂w_j$。一个很明显的计算方式是使用近似： $$\frac{\partial C}{\partial w_{j}} \approx \frac{C\left(w+\epsilon e_{j}\right)-C(w)}{\epsilon}$$ 其中$\epsilon$是一个大于零的极小数, $e_j$是第$j$个方向上的单位向量。换句话说，我们可以通过计算两个差距很小的$w_j$的代价，然后利用上面的等式来估计$∂C/∂w_j$。我们可以利用相同的思想来对偏置求偏导$∂C/∂b$。
这种方式看起来很不错。它的概念很简单，实现起来也很简单，只需要几行代码。当然了，他看起来要比使用链式法则来计算梯度靠谱多了！
然而遗憾的是，虽然这种方式看起来很美好，但当用代码实现之后就会发现，它实在是太慢了。要理解其中的原因的话，设想在我们的神经网络中有一百万个权重，对于每一个不同的权重$w_j$，为了计算$C(w+ϵe_j)$，我们需要计算$∂C/∂w_j$。这意味着为了计算梯度，我们需要计算一百万次代价函数，进而对于每一个训练样例，都需要在神经网络中前向传播一百万次。我们同样需要计算$C(w)$，因此总计需要一百万零一次前向传播。
反向传播的优点在于它仅利用一次前向传播就可以同时计算出所有的偏导$∂C/∂w_j$，随后也仅需要一次反向传播。大致来说，反向传播算法所需要的总计算量与两次前向传播的计算量基本相等（这应当是合理的，但若要下定论的话则需要更加细致的分析。合理的原因在于前向传播时主要的计算量在于权重矩阵的乘法计算，而反向传播时主要的计算量在于权重矩阵转置的乘法。很明显，它们的计算量差不多）。这与基于等式(46)的方法所需要的一百万零一次前向传播相比，虽然反向传播看起来更复杂一些，但它确实更更更更更快。
这种加速方式在1986年首次被人们所重视，极大地拓展了神经网络能够适用的范围，也导致了神经网络被大量的应用。当然了，反向传播算法也不是万能的。在80年代后期，人们终于触及到了性能瓶颈，在利用反向传播算法来训练深度神经网络（即具有很多隐含层的网络）时尤为明显。在本书后面的章节中我们将会看到现代计算机以及一些非常聪明的新想法是如何让反向传播能够用来训练深度神经网络的。
Source: 为什么说反向传播算法很高效 · 神经网络与深度学习 原文无公式, 对照 英文原文: Neural networks and deep learning添加了公式</description></item><item><title>代换密码（Substitution Cipher）与置换密码（Permutation Cipher）</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/%E4%BB%A3%E6%8D%A2%E5%AF%86%E7%A0%81Substitution-Cipher%E4%B8%8E%E7%BD%AE%E6%8D%A2%E5%AF%86%E7%A0%81Permutation-Cipher/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/%E4%BB%A3%E6%8D%A2%E5%AF%86%E7%A0%81Substitution-Cipher%E4%B8%8E%E7%BD%AE%E6%8D%A2%E5%AF%86%E7%A0%81Permutation-Cipher/</guid><description>代换密码与置换密码（Substitution Cipher &amp;amp; Permutation Cipher） Tags: #Math #Cryptography #Course
分不清楚这两个完全是翻译的锅
置换（不是置换密码）Permutation（Not Permutation Cipher) 首先置换是数学上的一种操作，是对一组确定的元素进行重新排列
元素不变 只改变顺序 Wikipedia:</description></item><item><title>关于特征值的一个结论</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/%E5%85%B3%E4%BA%8E%E7%89%B9%E5%BE%81%E5%80%BC%E7%9A%84%E4%B8%80%E4%B8%AA%E7%BB%93%E8%AE%BA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/%E5%85%B3%E4%BA%8E%E7%89%B9%E5%BE%81%E5%80%BC%E7%9A%84%E4%B8%80%E4%B8%AA%E7%BB%93%E8%AE%BA/</guid><description>If $A$ is $m$ by $n$ and $B$ is $n$ by $m$, then $AB$ and $BA$ have the same nonzero eigenvalues.</description></item><item><title>关于秩的一个结论</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/%E5%85%B3%E4%BA%8E%E7%A7%A9%E7%9A%84%E4%B8%80%E4%B8%AA%E7%BB%93%E8%AE%BA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/%E5%85%B3%E4%BA%8E%E7%A7%A9%E7%9A%84%E4%B8%80%E4%B8%AA%E7%BB%93%E8%AE%BA/</guid><description>$Rank(A)=Rank(A^TA)=Rank(AA^T)$
证明:
一方面: $Ax=0\Rightarrow A^TAx=0$
另一方面: $A^TAx=0\Rightarrow x\cdot0=x\cdot A^TAx=0$
$x\cdot A^TAx=0\Rightarrow x^TA^TAx=0$
$x^TA^TAx=0\Rightarrow (Ax)^TAx=0 \Rightarrow ||Ax||^2=0\Rightarrow Ax=0$
所以$A$与$A^TA$核空间相同，所以秩相等</description></item><item><title>卷积层参数大小的计算</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%8F%82%E6%95%B0%E5%A4%A7%E5%B0%8F%E7%9A%84%E8%AE%A1%E7%AE%97/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%8F%82%E6%95%B0%E5%A4%A7%E5%B0%8F%E7%9A%84%E8%AE%A1%E7%AE%97/</guid><description> 输入通道数决定了每一个卷积核的&amp;quot;厚度&amp;quot; 输出通道数决定了卷积核的&amp;quot;个数&amp;quot; 卷积核的大小则和输出的尺寸密切相关 其实 层数(厚度) 和 面积(尺寸) 没有什么联系, 是两个比较独立的参数</description></item><item><title>可视化损失函数的困难</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%9B%B0%E9%9A%BE/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%9B%B0%E9%9A%BE/</guid><description>可视化损失函数的困难 我们先来看看Loss Function的形式: $$L_{\theta}\space (\hat{y},\space y)=\cdots$$ 里面有四个组成部分: $L, \theta, \hat{y}$ 和 $y$
需要牢记的一点是: 选择Loss Function $L$ 的时候选择的是&amp;quot;计算方式&amp;quot;, 也就是我们怎样计算模型输出 $\hat y$ 和真实值 $y$, 而损失函数实际上是关于权重 $\theta$ 的函数, 我们在反向传播的时候是对学习目标 $\theta$ 求梯度, 而直观的梯度下降法也是在 $L$ 关于 $\theta$ 的图像上逐步下降的.</description></item><item><title>图灵归约 Turing Reduction</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/%E5%9B%BE%E7%81%B5%E5%BD%92%E7%BA%A6-Turing-Reduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/%E5%9B%BE%E7%81%B5%E5%BD%92%E7%BA%A6-Turing-Reduction/</guid><description>把还没解决的问题归约到已经解决的问题上
用已经解决的问题去解决还没解决的问题
密码学原理与实践 Page 167
假设我们已经存在一个解决问题 A 的算法 $G(x)$
一个A到B的图灵归约即利用$G(x)$构造一个解决问题B的算法$H(x)$, 并且$H(x)$是多项式时间的.
https://zhuanlan.zhihu.com/p/194313998 这篇文章译自 reductions-and-jokes</description></item><item><title>对于等价的网络, 小的卷积核参数更少</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E5%AF%B9%E4%BA%8E%E7%AD%89%E4%BB%B7%E7%9A%84%E7%BD%91%E7%BB%9C-%E5%B0%8F%E7%9A%84%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%8F%82%E6%95%B0%E6%9B%B4%E5%B0%91/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E5%AF%B9%E4%BA%8E%E7%AD%89%E4%BB%B7%E7%9A%84%E7%BD%91%E7%BB%9C-%E5%B0%8F%E7%9A%84%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%8F%82%E6%95%B0%E6%9B%B4%E5%B0%91/</guid><description>我们知道 2个3x3卷积核堆叠后等价于一个5x5卷积核, 以此为例我们来比较一下两者的参数大小. 卷积层参数大小的计算
假设输入通道数为 $C_{in}$, 输出通道数为 $C_{out}$
对于 $5\times5$ 卷积, 参数数量为: $$5\times5\times C_{in} \times C_{out}$$</description></item><item><title>方差的性质</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/%E6%96%B9%E5%B7%AE%E7%9A%84%E6%80%A7%E8%B4%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/%E6%96%B9%E5%B7%AE%E7%9A%84%E6%80%A7%E8%B4%A8/</guid><description>$$\begin{aligned} \operatorname{Var}(X) &amp;amp;=\mathrm{E}\left[X^{2}-2 X \mathrm{E}+(\mathrm{E})^{2}\right]\ &amp;amp;=\mathrm{E}\left[X^{2}\right]-2 \mathrm{E}\mathrm{E}+(\mathrm{E})^{2}\ &amp;amp;=\mathrm{E}\left[X^{2}\right]-(\mathrm{E})^{2} \end{aligned}$$ 上述的表示式可记为&amp;quot;平方的期望减掉期望的平方&amp;quot;。</description></item><item><title>滑动窗口协议信道利用率分析</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E5%8D%8F%E8%AE%AE%E4%BF%A1%E9%81%93%E5%88%A9%E7%94%A8%E7%8E%87%E5%88%86%E6%9E%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E5%8D%8F%E8%AE%AE%E4%BF%A1%E9%81%93%E5%88%A9%E7%94%A8%E7%8E%87%E5%88%86%E6%9E%90/</guid><description>滑动窗口协议信道利用率分析 Link Utilization of Sliding Window Protocols 1953184 傅驰原 2022-04-25 Tags: #ComputerNetwork #Course
我们假设某一帧出错的概率相等且独立, 并且都为 $P$ 1Bit滑动窗口 这其实是一种窗口大小为 $1$, 帧序号位数也为 $1$ 的特殊回退 $N$ 协议.</description></item><item><title>灰度图像与宇宙中的所有原子</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F%E4%B8%8E%E5%AE%87%E5%AE%99%E4%B8%AD%E7%9A%84%E6%89%80%E6%9C%89%E5%8E%9F%E5%AD%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F%E4%B8%8E%E5%AE%87%E5%AE%99%E4%B8%AD%E7%9A%84%E6%89%80%E6%9C%89%E5%8E%9F%E5%AD%90/</guid><description>考虑$28×28$的灰度图像。 如果每个像素可以取$256$个灰度值中的一个， 则有$256^{784}$个可能的图像。 这意味着指甲大小的低分辨率灰度图像的数量比宇宙中的原子1要多得多。2 反过来想, 小小的$28×28$的灰度图像中, 一定有很多很多图像整个宇宙中都不存在, 因为要是一个图像只用一个原子就能表达的话3, 宇宙中的所有原子都表达不完这些图片. $10^{80}$&amp;#160;&amp;#x21a9;&amp;#xfe0e;
https://zh-v2.</description></item><item><title>经典模型</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/</guid><description> LeNet AlexNet VGG NiN GoogLeNet(Inception) ResNet DenseNet RNN LSTM GRU Encoder-Decoder BERT</description></item><item><title>酉矩阵 Unitary Matrix</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/%E9%85%89%E7%9F%A9%E9%98%B5-Unitary-Matrix/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/%E9%85%89%E7%9F%A9%E9%98%B5-Unitary-Matrix/</guid><description>共轭转置是其自身的逆的矩阵:
$$U^{-1}=U^{*}$$
是 n×n 复数方块矩阵，满足： $$U^{}U=UU^{}=I_{n}$$
其中 $U^*$ 是 $U$ 的共轭转置，$I_n$ 是 n×n 单位矩阵。
酉矩阵是实数上的正交矩阵，在复数的推广 $$U^{T}U=UU^{T}=I_{n}$$
性质 酉矩阵代表的酉变换不改变向量的长度与夹角 不改变长度是正交所带来的 不改变夹角是什么带来的？ Unitary Transformations - YouTube</description></item><item><title>随机变量函数的期望</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E5%87%BD%E6%95%B0%E7%9A%84%E6%9C%9F%E6%9C%9B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E5%87%BD%E6%95%B0%E7%9A%84%E6%9C%9F%E6%9C%9B/</guid><description>一般的说，一个随机变量的函数的期望值并不等于这个随机变量的期望值的函数。 $$ \mathrm{E}(g(X))=\int_{\Omega} g(x) f(x) \mathrm{d} x \neq g(\mathrm{E}(X)) $$</description></item></channel></rss>