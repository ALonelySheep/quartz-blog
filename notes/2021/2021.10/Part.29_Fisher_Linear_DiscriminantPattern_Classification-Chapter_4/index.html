<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Fisher Linear Discriminant 2021-10-28 Tags: #MachineLearning #PatternClassification #Course #DimensionalityReduction
 通过降维进行分类, 降到一维即为线性判别. 其实, 线性判别分析 (LDA)就是对Fisher线性判别的归纳.1   Motivation  Curse of Dimensionality - 模型的表现随着维度的增加而变坏, 而且根据设计者的3维直觉, 无法很好的解决高维度的问题."><title>Part.29_Fisher_Linear_Discriminant(Pattern_Classification-Chapter_4)</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel=icon type=image/png sizes=16x16 href=favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=favicon-32x32.png><link rel=manifest href=site.webmanifest><link href=https://alonelysheep.github.io/quartz-blog/styles.7153093e4d1bbb584a28469cadfa3f88.min.css rel=stylesheet><link href=https://alonelysheep.github.io/quartz-blog/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://alonelysheep.github.io/quartz-blog/js/darkmode.753e6a835409aa87fdd04fabb270d592.min.js></script>
<script src=https://alonelysheep.github.io/quartz-blog/js/util.9825137f5e7825e8553c68ce39ac9e44.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://alonelysheep.github.io/quartz-blog/js/popover.9b72b70bd35617d0635e9d15463662b2.min.js></script>
<script src=https://alonelysheep.github.io/quartz-blog/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script>
<script src=https://alonelysheep.github.io/quartz-blog/js/clipboard.c20857734e53a3fb733b7443879efa61.min.js></script>
<script src=https://alonelysheep.github.io/quartz-blog/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const BASE_URL="https://alonelysheep.github.io/quartz-blog/",fetchData=Promise.all([fetch("https://alonelysheep.github.io/quartz-blog/indices/linkIndex.9bca8d05cc43a46971935a4cdd691fc0.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://alonelysheep.github.io/quartz-blog/indices/contentIndex.12dbac1a6d1bc75dd4456a366c1a399a.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://alonelysheep.github.io/quartz-blog",!0,!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!0;drawGraph("https://alonelysheep.github.io/quartz-blog",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:2,opacityScale:3,repelForce:.8,scale:.5}:{centerForce:1,depth:2,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:1.5,scale:1.2})}},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'’':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/alonelysheep.github.io\/quartz-blog\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-XYFD95KB4J"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-XYFD95KB4J",{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://alonelysheep.github.io/quartz-blog/js/full-text-search.24827f874defbbc6d529926cbfcfb493.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://alonelysheep.github.io/quartz-blog/>Cyan's Blog</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Part.29_Fisher_Linear_Discriminant(Pattern_Classification-Chapter_4)</h1><p class=meta>Last updated
Oct 28, 2021
<a href=https://github.com/alonelysheep/quartz-blog/tree/hugo/content/notes/2021/2021.10/Part.29_Fisher_Linear_Discriminant%28Pattern_Classification-Chapter_4%29.md rel=noopener>Edit Source</a></p><ul class=tags><li><a href=https://alonelysheep.github.io/quartz-blog/tags/all/>All</a></li><li><a href=https://alonelysheep.github.io/quartz-blog/tags/MachineLearning/>Machine learning</a></li><li><a href=https://alonelysheep.github.io/quartz-blog/tags/PatternClassification/>Pattern classification</a></li><li><a href=https://alonelysheep.github.io/quartz-blog/tags/Course/>Course</a></li><li><a href=https://alonelysheep.github.io/quartz-blog/tags/DimensionalityReduction/>Dimensionality reduction</a></li></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#motivation>Motivation</a></li><li><a href=#interlude---linear-transformation--dot-product>Interlude - Linear Transformation & Dot Product</a></li><li><a href=#interlude---covariance-and-covariance-matrix>Interlude - Covariance and Covariance Matrix</a></li><li><a href=#intuition>Intuition</a></li><li><a href=#详细推导>详细推导</a><ol><li><a href=#构造准则函数>构造准则函数</a></li><li><a href=#最大化准则函数>==最大化==准则函数</a></li></ol></li><li><a href=#可以将这个方法推广到多维的情况>可以将这个方法推广到多维的情况</a></li></ol></nav></details></aside><a href=#fisher-linear-discriminant><h1 id=fisher-linear-discriminant><span class=hanchor arialabel=Anchor># </span>Fisher Linear Discriminant</h1></a><div align=right>2021-10-28</div><p>Tags: #MachineLearning #PatternClassification #Course
#DimensionalityReduction</p><ul><li><strong>通过降维进行分类</strong>, 降到一维即为线性判别.</li><li>其实, <strong>线性判别分析</strong> (<strong>LDA</strong>)就是对<strong>Fisher线性判别</strong>的归纳.<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></li></ul><p><img src=https://alonelysheep.github.io/quartz-blog//notes/2021/2021.10/assets/img_2022-10-15-65.png width=auto alt></p><a href=#motivation><h2 id=motivation><span class=hanchor arialabel=Anchor># </span>Motivation</h2></a><ul><li>Curse of Dimensionality - 模型的表现随着维度的增加而变坏, 而且根据设计者的3维直觉, 无法很好的解决高维度的问题.</li><li>所以一个很直观的方法便是减少问题的维度, Fisher的方法便是将多维样本直接映射到一维的一种方法.</li><li>直接映射到一维是否太粗暴? 2维, 3维可以吗?<ul><li>的确, even if the samples formed well-separated, compact clusters in d-space, projection onto an arbitrary line will usually produce a confused mixture of samples from all of the classes, and thus poor recognition performance. <strong>However</strong>, 一维的问题是十分简单的, by moving the line around, we might be able to find an orientation for which the projected samples are well separated. This is exactly the goal of classical discriminant analysis. 二维, 三维也是可以的, 我们后面会谈到对于Fisher方法的多维推广.</li></ul></li></ul><a href=#interlude---linear-transformation--dot-product><h2 id=interlude---linear-transformation--dot-product><span class=hanchor arialabel=Anchor># </span>Interlude - Linear Transformation & Dot Product</h2></a><p><a href=/quartz-blog/notes/2021/2021.10/Dot_Product_and_Linear_Transformation-%E5%90%91%E9%87%8F%E5%86%85%E7%A7%AF%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2/ rel=noopener class=internal-link data-src=/quartz-blog/notes/2021/2021.10/Dot_Product_and_Linear_Transformation-%E5%90%91%E9%87%8F%E5%86%85%E7%A7%AF%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2/>Dot_Product_and_Linear_Transformation-向量内积与线性变换</a></p><a href=#interlude---covariance-and-covariance-matrix><h2 id=interlude---covariance-and-covariance-matrix><span class=hanchor arialabel=Anchor># </span>Interlude - Covariance and Covariance Matrix</h2></a><p><a href=/quartz-blog/notes/2021/2021.10/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5_Covariance_Matrix/ rel=noopener class=internal-link data-src=/quartz-blog/notes/2021/2021.10/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5_Covariance_Matrix/>协方差矩阵_Covariance_Matrix</a></p><a href=#intuition><h2 id=intuition><span class=hanchor arialabel=Anchor># </span>Intuition</h2></a><p><a href=https://sthalles.github.io/fisher-linear-discriminant/ rel=noopener>https://sthalles.github.io/fisher-linear-discriminant/</a></p><p>如果我们直接投影到<strong>样本均值的连线</strong>的方向的话, 可以看到将会有很多的重合:
<img src=https://alonelysheep.github.io/quartz-blog//notes/2021/2021.10/assets/img_2022-10-15-66.png width=300 alt="fisher-ld generator network|300">
<img src=https://alonelysheep.github.io/quartz-blog//notes/2021/2021.10/assets/img_2022-10-15-67.png width=300 alt="fisher-ld generator network|300"></p><p>Fisher的方法基于以下直觉:</p><ul><li>我们要使投影后的结果:<ol><li>不同类间隔得越开越好 (类间方差最大)</li><li>相同类内聚集的越紧密越好 (类内方差最小)</li></ol></li></ul><p><img src=https://alonelysheep.github.io/quartz-blog//notes/2021/2021.10/assets/img_2022-10-15-68.png width=auto alt="fisher-ld generator network">
所以我们这样构造准则函数(Criterion Function):</p><p><img src=https://alonelysheep.github.io/quartz-blog//notes/2021/2021.10/assets/img_2022-10-15-69.png width=500 alt="fisher-ld generator network|500">
我们需要找到使$J(w)$取得最大值的$w$, 即找到最优的投影方向.</p><a href=#详细推导><h2 id=详细推导><span class=hanchor arialabel=Anchor># </span>详细推导</h2></a><a href=#构造准则函数><h3 id=构造准则函数><span class=hanchor arialabel=Anchor># </span>构造准则函数</h3></a><ul><li><p>我们这样计算投影:
$$y=\mathbf{w}^{t} \mathbf{x_p}$$
上面的式子将样本点$\mathbf{x_p}$投影到$\mathbf{w}$方向的一条直线上</p></li><li><p>我们这样表示类别$i$样本的均值:
$$\mathbf{m}<em>{i}=\frac{1}{n</em>{i}} \sum_{\mathbf{x} \in \mathcal{D}_{i}} \mathbf{x}$$
其中$n_i$是该类别样本的个数</p></li><li><p>我们这样计算投影后的样本均值:
$$\begin{align}
\tilde{m}<em>{i}&=\frac{1}{n</em>{i}} \sum_{y \in \mathcal{Y}<em>{i}} y \\ &=\frac{1}{n</em>{i}} \sum_{\mathbf{x} \in \mathcal{D}<em>{i}} \mathbf{w}^{t} \mathbf{x}\\ &=\mathbf{w}^{t} \mathbf{m}</em>{i}\end{align}$$
可以发现, 投影后的均值 其实就是 均值$\mathbf{m}_{i}$的投影</p></li><li><p>所以我们可以这样衡量投影后的直线上面不同类间均值的距离:
$$\left|\tilde{m}<em>{1}-\tilde{m}</em>{2}\right|
=\left|\mathbf{w}^{t}\left(\mathbf{m}<em>{1}-\mathbf{m}</em>{2}\right)\right|$$</p></li><li><p>为了衡量投影后样本的分散程度, 我们定义 &ldquo;类内散度&rdquo;
$$\tilde{s}<em>{i}^{2}=
\sum</em>{y \in \mathcal{Y}<em>{i}}\left(y-\tilde{m}</em>{i}\right)^{2}$$
直观看来, 就是投影后样本与均值距离的平方</p></li><li><p>然后我们就可以根据直觉, 给出Fisher准则函数如下:
$$J(\mathbf{w})=
\frac{\left|\tilde{m}<em>{1}-\tilde{m}</em>{2}\right|^{2}}{\tilde{s}<em>{1}^{2}+\tilde{s}</em>{2}^{2}}$$
分子是类间的方差(越大越好), 分母是类内的方差(越小越好)</p></li></ul><a href=#最大化准则函数><h3 id=最大化准则函数><span class=hanchor arialabel=Anchor># </span>==最大化==准则函数</h3></a><a href=#将w提出来><h4 id=将w提出来><span class=hanchor arialabel=Anchor># </span>将w提出来</h4></a><ul><li><p>$J(\mathbf{w})$并不是与$\mathbf{w}$直接相关的, 所以先进如下变换:</p></li><li><p>我们先定义Scatter Matrix $\mathbf{S_i, S_w}$:
$$\mathbf{S}<em>{i}=
\sum</em>{\mathbf{x} \in \mathcal{D}<em>{i}}
\left(\mathbf{x}-\mathbf{m}</em>{i}\right)\left(\mathbf{x}-\mathbf{m}<em>{i}\right)^{t}$$
$$\mathbf{S</em>{w}} = \mathbf{S_1+S_2}$$</p></li><li><p>所以, 类内散度可以变为:
$$\begin{aligned}
\tilde{s}<em>{i}^{2} &=\sum</em>{\mathbf{x} \in \mathcal{D}<em>{i}}\left(\mathbf{w}^{t} \mathbf{x}-\mathbf{w}^{t} \mathbf{m}</em>{i}\right)^{2} \\ &=\sum_{\mathbf{x} \in \mathcal{D}<em>{i}}
\mathbf{w}^{t}\left(\mathbf{x}-\mathbf{m}</em>{i}\right)
\left(\mathbf{w}^{t}\left(\mathbf{x}-\mathbf{m}<em>{i}\right)\right)^T \\ &=\sum</em>{\mathbf{x} \in \mathcal{D}<em>{i}}
\mathbf{w}^{t}\left(\mathbf{x}-\mathbf{m}</em>{i}\right)\left(\mathbf{x}-\mathbf{m}<em>{i}\right)^{t} \mathbf{w} \\ &=\mathbf{w}^{t} \mathbf{S}</em>{i} \mathbf{w}
\end{aligned}$$</p></li><li><p>然后
$$\tilde{s}<em>{1}^{2}+\tilde{s}</em>{2}^{2}=\mathbf{w}^{t} \mathbf{S}_{W} \mathbf{w}$$
这样, 我们将分母里面的$\mathbf{w}$提取到了外面</p></li><li><p>对于分子, 我们可以有相似的操作:
$$\begin{aligned}
\left(\tilde{m}<em>{1}-\tilde{m}</em>{2}\right)^{2} &=\left(\mathbf{w}^{t} \mathbf{m}<em>{1}-\mathbf{w}^{t} \mathbf{m}</em>{2}\right)^{2} \\ &=\mathbf{w}^{t}\left(\mathbf{m}<em>{1}-\mathbf{m}</em>{2}\right)\left(\mathbf{m}<em>{1}-\mathbf{m}</em>{2}\right)^{t} \mathbf{w} \\ &=\mathbf{w}^{t} \mathbf{S}<em>{B} \mathbf{w}
\end{aligned}$$
观察里面的中间部分, 我们定义:
$$\mathbf{S}</em>{B}=\left(\mathbf{m}<em>{1}-\mathbf{m}</em>{2}\right)\left(\mathbf{m}<em>{1}-\mathbf{m}</em>{2}\right)^{t}$$</p></li><li><p>所以Criterion Function 变为了:
$$J(\mathbf{w})=\frac{\mathbf{w}^{t} \mathbf{S}<em>{B} \mathbf{w}}{\mathbf{w}^{t} \mathbf{S}</em>{W} \mathbf{w}}$$</p><p>这一表达式在数学物理中被称作广义Rayleigh 商(generalized Rayleigh quotient)</p></li></ul><a href=#关于两个矩阵><h4 id=关于两个矩阵><span class=hanchor arialabel=Anchor># </span>关于两个矩阵</h4></a><p>We call $S_{W}$ the within-class scatter matrix. It is proportional to the sample covariance matrix for the pooled $d$-dimensional data. It is symmetric and positive semi-definite, and is usually non-singular if $n>d$.
Likewise, $\mathbf{S}<em>{B}$ is called the between class scatter matrix. It is also symmetric and positive semi-definite, but because it is the outer product of two vectors, its rank is at most one. In particular, for any $\mathrm{w}$, $\mathbf{S}</em>{B} \mathbf{w}$ is in the direction of $\mathbf{m}<em>{1}-\mathbf{m}</em>{2}$, and $\mathbf{S}_{B}$ is quite singular.</p><a href=#解mathbfw><h4 id=解mathbfw><span class=hanchor arialabel=Anchor># </span>解$\mathbf{w}$</h4></a><p>解$\mathbf{w}$需要用到拉格朗日乘子法:
思路:
用拉格朗日乘子法得到以下条件
$$\mathbf{S}<em>{B} \mathbf{w}=\lambda \mathbf{S}</em>{W} \mathbf{w}$$
If $\mathbf{S}<em>{W}$ is non-singular we can obtain a conventional eigenvalue problem by writing
$$\mathbf{S}</em>{W}^{-1} \mathbf{S}<em>{B} \mathbf{w}=\lambda \mathbf{w}$$
In our particular case, it is unnecessary to solve for the eigenvalues and eigenvectors of $\mathbf{S}</em>{W}^{-1} \mathbf{S}_{B}$ due to the fact that $\mathbf{S_B w}$ is always in the direction of $m_1 −m_2$. Since the scale factor for $\mathbf{w}$ is immaterial, we can immediately write the solution for the $\mathbf{w}$ that optimizes $J(·)$:</p><p>$$\mathbf{w}=\mathbf{S}<em>{W}^{-1}\left(\mathbf{m}</em>{1}-\mathbf{m}_{2}\right)$$</p><p>详细过程</p><ul><li><p>Ref 模式识别(第三版) - 张学工, Page 64
<img src=https://alonelysheep.github.io/quartz-blog//notes/2021/2021.10/assets/img_2022-10-15-70.png width=auto alt></p></li><li><p>Ref 机器学习 周志华
<img src=https://alonelysheep.github.io/quartz-blog//notes/2021/2021.10/assets/img_2022-10-15-71.png width=auto alt>
<img src=https://alonelysheep.github.io/quartz-blog//notes/2021/2021.10/assets/img_2022-10-15-72.png width=auto alt></p></li><li><p>Ref 南瓜书
<img src=https://alonelysheep.github.io/quartz-blog//notes/2021/2021.10/assets/img_2022-10-15-73.png width=auto alt></p></li></ul><a href=#可以将这个方法推广到多维的情况><h2 id=可以将这个方法推广到多维的情况><span class=hanchor arialabel=Anchor># </span>可以将这个方法推广到多维的情况</h2></a><p>推广到高维:<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>
我们需要改变以下地方:</p><ul><li><img src=https://alonelysheep.github.io/quartz-blog//notes/2021/2021.10/assets/img_2022-10-15-74.png width=500 alt="fisher-id samples|500"></li><li>类内 Scatter Matrix, $S_W$直观的来说, 即从两个类的 $S_1+S_2$ 变成多个类 $S_i$ 的和</li><li>对于$S_B$, 变成了 &ldquo;每个类相对于全局平均的差&rdquo; 的加权和, 这里和只有两个类的情况并不是完全一致的, 具体参见 Duda 模式分类, page49</li></ul><p>若将 W 视为一个投影矩阵，则多分类 LDA 将样本投影到 N-1 维空间，N-1 通常远小子数据原有的属性数. 于是，可通过这个投影来减小样本点的维数，且投影过程中使用了类别信息, 因此 LDA 也常被视为一种经典的监督降维技术<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup></p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p><a href=https://en.wikipedia.org/wiki/Linear_discriminant_analysis rel=noopener>https://en.wikipedia.org/wiki/Linear_discriminant_analysis</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p><a href=https://sthalles.github.io/fisher-linear-discriminant/ rel=noopener>https://sthalles.github.io/fisher-linear-discriminant/</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3 role=doc-endnote><p>周志华 机器学习&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://alonelysheep.github.io/quartz-blog/js/graph.abd4bc2af3869a96524d7d23b76152c7.js></script></div></div><div id=contact_buttons><footer><p>Made by Cyan Fu using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2023</p><ul><li><a href=https://alonelysheep.github.io/quartz-blog/>Home</a></li><li><a href=https://github.com/ALonelySheep>Github</a></li><li><a href=https://space.bilibili.com/506700150>Bilibili</a></li></ul></footer></div></div></body></html>