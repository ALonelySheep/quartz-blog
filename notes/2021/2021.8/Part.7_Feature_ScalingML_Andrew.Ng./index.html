<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Feature Scaling 2021-08-06 Tags: #MachineLearning #FeatureEngineering
 1
深入阅读的链接: https://sebastianraschka.com/Articles/2014_about_feature_scaling.html
When to Use  在梯度下降的时候, 缩放数据可以让梯度变化更平滑     If an algorithm uses gradient descent, then the difference in ranges of features will cause different step sizes for each feature."><title>Part.7_Feature_Scaling(ML_Andrew.Ng.)</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel=icon type=image/png sizes=16x16 href=favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=favicon-32x32.png><link rel=manifest href=site.webmanifest><link href=https://alonelysheep.github.io/quartz-blog/styles.7153093e4d1bbb584a28469cadfa3f88.min.css rel=stylesheet><link href=https://alonelysheep.github.io/quartz-blog/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://alonelysheep.github.io/quartz-blog/js/darkmode.753e6a835409aa87fdd04fabb270d592.min.js></script>
<script src=https://alonelysheep.github.io/quartz-blog/js/util.9825137f5e7825e8553c68ce39ac9e44.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://alonelysheep.github.io/quartz-blog/js/popover.9b72b70bd35617d0635e9d15463662b2.min.js></script>
<script src=https://alonelysheep.github.io/quartz-blog/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script>
<script src=https://alonelysheep.github.io/quartz-blog/js/clipboard.c20857734e53a3fb733b7443879efa61.min.js></script>
<script src=https://alonelysheep.github.io/quartz-blog/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const BASE_URL="https://alonelysheep.github.io/quartz-blog/",fetchData=Promise.all([fetch("https://alonelysheep.github.io/quartz-blog/indices/linkIndex.190e435b47e393c9e783a55d2cb44c20.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://alonelysheep.github.io/quartz-blog/indices/contentIndex.65e4872c420f6c5e0d39e9b0645b8b93.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://alonelysheep.github.io/quartz-blog",!0,!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!0;drawGraph("https://alonelysheep.github.io/quartz-blog",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:2,opacityScale:3,repelForce:.8,scale:.5}:{centerForce:1,depth:2,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:1.5,scale:1.2})}},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'’':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/alonelysheep.github.io\/quartz-blog\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-XYFD95KB4J"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-XYFD95KB4J",{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://alonelysheep.github.io/quartz-blog/js/full-text-search.24827f874defbbc6d529926cbfcfb493.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://alonelysheep.github.io/quartz-blog/>Cyan's Blog</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Part.7_Feature_Scaling(ML_Andrew.Ng.)</h1><p class=meta>Last updated
Aug 6, 2021
<a href=https://github.com/alonelysheep/quartz-blog/tree/hugo/content/notes/2021/2021.8/Part.7_Feature_Scaling%28ML_Andrew.Ng.%29.md rel=noopener>Edit Source</a></p><ul class=tags><li><a href=https://alonelysheep.github.io/quartz-blog/tags/all/>All</a></li><li><a href=https://alonelysheep.github.io/quartz-blog/tags/MachineLearning/>Machine learning</a></li><li><a href=https://alonelysheep.github.io/quartz-blog/tags/FeatureEngineering/>Feature engineering</a></li><li><a href=https://alonelysheep.github.io/quartz-blog/tags/todo/>Todo</a></li></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#when-to-use>When to Use</a></li><li><a href=#normalization-归一化>Normalization 归一化</a></li><li><a href=#standardization-标准化>Standardization 标准化</a><ol><li><a href=#如何选择1>如何选择</a></li></ol></li><li><a href=#feature-scaling--regression>Feature Scaling & Regression</a></li><li><a href=#dont-confuse-regularization-normalization--standardization>Don&rsquo;t Confuse Regularization Normalization & Standardization</a></li></ol></nav></details></aside><a href=#feature-scaling><h1 id=feature-scaling><span class=hanchor arialabel=Anchor># </span>Feature Scaling</h1></a><div align=right>2021-08-06</div><p>Tags: #MachineLearning #FeatureEngineering</p><p><img src=https://alonelysheep.github.io/quartz-blog//notes/2021/2021.7/assets/img_2022-10-15-15.png width=auto alt><sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><p>深入阅读的链接:
<a href=https://sebastianraschka.com/Articles/2014_about_feature_scaling.html rel=noopener>https://sebastianraschka.com/Articles/2014_about_feature_scaling.html</a></p><a href=#when-to-use><h2 id=when-to-use><span class=hanchor arialabel=Anchor># </span>When to Use</h2></a><ul><li>在梯度下降的时候, 缩放数据可以让梯度变化更平滑</li><li><img src=https://alonelysheep.github.io/quartz-blog//notes/2021/2021.7/assets/img_2022-10-15-16.png width=auto alt></li></ul><blockquote><p>If an algorithm uses gradient descent, then the difference in ranges of features will cause different step sizes for each feature. To ensure that the gradient descent moves smoothly towards the minima and that the steps for gradient descent are updated at the same rate for all the features, we scale the data before feeding it to the model. Having features on a similar scale will help the gradient descent converge more quickly towards the minima.</p><p>Specifically, in the case of Neural Networks Algorithms, feature scaling benefits optimization by:</p><ul><li>It makes the training faster</li><li>It prevents the optimization from getting stuck in local optima</li><li>It gives a better error surface shape</li><li>Weight decay and Bayes optimization can be done more conveniently</li></ul></blockquote><ul><li>在以距离为基础的算法里面, 放缩数据可以让数据分布更均匀</li></ul><blockquote><p>Distance-based algorithms like KNN, K-means, and SVM are most affected by the range of features. This is because behind the scenes they are using distances between data points to determine their similarity and hence perform the task at hand. Therefore, we scale our data before employing a distance-based algorithm so that all the features contribute equally to the result.</p></blockquote><ul><li>在主成分分析里面, 放缩数据可以让凸显出数据的"变化", (一个数量级很大的数据变一点点&#187;一个数量级很小的数据变化几倍)</li></ul><blockquote><p>In 
<a href=https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html rel=noopener>PCA</a> we are interested in the components that maximize the variance. If one component (e.g. age) varies less than another (e.g. salary) because of their respective scales, PCA might determine that the direction of maximal variance more closely corresponds with the ‘salary’ axis, if those features are not scaled. As a change in the age of one year can be considered much more important than the change in salary of one euro, this is clearly incorrect.</p></blockquote><a href=#normalization-归一化><h2 id=normalization-归一化><span class=hanchor arialabel=Anchor># </span>Normalization 归一化</h2></a><p><img src=https://alonelysheep.github.io/quartz-blog//notes/2021/2021.7/assets/img_2022-10-15-17.png width=auto alt></p><p>$$x^\prime= \frac{x-x_{min}}{x_{max}-x_{min}}$$</p><ul><li><p>可以自己调控数据分布的范围, 比如你想让数据分布在$[a,b]$范围内, 公式变为:
$$x^{\prime}=a+\left(\frac{x-\min (x)}{\max (x)-\min (x)}\right)(b-a)$$</p></li><li><p>归一化对离群值十分敏感</p></li><li><p>会缩小很大的数据, 会改变数量级</p></li></ul><a href=#standardization-标准化><h2 id=standardization-标准化><span class=hanchor arialabel=Anchor># </span>Standardization 标准化</h2></a><p><img src=https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Normal_Distribution_PDF.svg/1920px-Normal_Distribution_PDF.svg.png width=500 alt="Probability density function for the Normal distribtion|500"></p><ul><li>拿正态分布的数据做例子: 就相当于把其他颜色的曲线都变成红色的那条标准正态曲线</li></ul><p>$$x^\prime= \frac{x-\mu}{\sigma}$$
$\mu$是均值, $\sigma$是标准差(方差的平方根)</p><ul><li><p>对离群值不是那么敏感</p></li><li><p>标准化也改变数量级, 会减去均值</p></li></ul><a href=#如何选择1><h3 id=如何选择1><span class=hanchor arialabel=Anchor># </span>如何选择<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></h3></a><p><strong>Normalization</strong> 在数据不符合正态分布的时候比较适用, 像KNN这种对数据分布没有要求的模型更适用于归一化</p><p>在神经网络里面常常要求数据分布在0-1之间, 这时候归一化必不可少; 另一个例子是图像处理的时候常常会把数据缩小到一个范围(比如0-255), 在这时标准化更加适用.</p><p><strong>Standardization</strong> 在数据满足正态分布的时候更加适用, 并且在放缩的时候没有范围限制, (不像归一化可以明确的规定一个范围$[a,b]$)</p><p>在聚类中, 标准化在比较不同特征的相似性的时候很好用(why? #todo), 另一个例子是PCA的时候常常用标准化来突出数据分布的差异度, 而不是用归一化把最大的变成一. ^375f2a</p><ul><li>总之:<ul><li>Standardization 适用于<strong>正态分布</strong>的数据,</li><li>Normalization 适用于<strong>非正态分布</strong>的数据</li></ul></li><li>Normalization里面离群值对数据的影响显著</li><li>不知道怎么办就都试试, 比较哪一个效果最好</li></ul><a href=#feature-scaling--regression><h2 id=feature-scaling--regression><span class=hanchor arialabel=Anchor># </span>Feature Scaling & Regression</h2></a><p>在多项式回归里面, 数据放缩很重要, 因为级数增长很快</p><a href=#dont-confuse-regularization-normalization--standardization><h2 id=dont-confuse-regularization-normalization--standardization><span class=hanchor arialabel=Anchor># </span>Don&rsquo;t Confuse Regularization Normalization & Standardization</h2></a><ul><li><strong>Regularization:</strong> 正则化</li><li><strong>Normalization:</strong> 归一化</li><li><strong>Standardization:</strong> 标准化</li></ul><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p><a href=https://sebastianraschka.com/Articles/2014_about_feature_scaling.html rel=noopener>https://sebastianraschka.com/Articles/2014_about_feature_scaling.html</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p><a href=https://www.atoti.io/when-to-perform-a-feature-scaling/ rel=noopener>https://www.atoti.io/when-to-perform-a-feature-scaling/</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://alonelysheep.github.io/quartz-blog/js/graph.abd4bc2af3869a96524d7d23b76152c7.js></script></div></div><div id=contact_buttons><footer><p>Made by Cyan Fu using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2023</p><ul><li><a href=https://alonelysheep.github.io/quartz-blog/>Home</a></li><li><a href=https://github.com/ALonelySheep>Github</a></li><li><a href=https://space.bilibili.com/506700150>Bilibili</a></li></ul></footer></div></div></body></html>