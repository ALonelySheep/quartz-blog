<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Batch Normalization 2022-03-05 Tags: #BatchNormalization #Normalization #DeepLearning #Regularization
  批量归一化是一种加速收敛的方法.
  批量归一化作用于每一个mini-Batch, 先将这个Batch归一化, 然后再做一个统一的偏移与拉伸.
 最后这个偏移和拉伸的量是一个可以学习的超参数    对于全连接层, BN作用于每一个特征"><title>D2L-44-Batch_Normalization-批量归一化</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel=icon type=image/png sizes=16x16 href=favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=favicon-32x32.png><link rel=manifest href=site.webmanifest><link href=https://alonelysheep.github.io/quartz-blog/styles.7153093e4d1bbb584a28469cadfa3f88.min.css rel=stylesheet><link href=https://alonelysheep.github.io/quartz-blog/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://alonelysheep.github.io/quartz-blog/js/darkmode.753e6a835409aa87fdd04fabb270d592.min.js></script>
<script src=https://alonelysheep.github.io/quartz-blog/js/util.9825137f5e7825e8553c68ce39ac9e44.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://alonelysheep.github.io/quartz-blog/js/popover.9b72b70bd35617d0635e9d15463662b2.min.js></script>
<script src=https://alonelysheep.github.io/quartz-blog/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script>
<script src=https://alonelysheep.github.io/quartz-blog/js/clipboard.c20857734e53a3fb733b7443879efa61.min.js></script>
<script src=https://alonelysheep.github.io/quartz-blog/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const BASE_URL="https://alonelysheep.github.io/quartz-blog/",fetchData=Promise.all([fetch("https://alonelysheep.github.io/quartz-blog/indices/linkIndex.190e435b47e393c9e783a55d2cb44c20.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://alonelysheep.github.io/quartz-blog/indices/contentIndex.65e4872c420f6c5e0d39e9b0645b8b93.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://alonelysheep.github.io/quartz-blog",!0,!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!0;drawGraph("https://alonelysheep.github.io/quartz-blog",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:2,opacityScale:3,repelForce:.8,scale:.5}:{centerForce:1,depth:2,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:1.5,scale:1.2})}},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'’':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/alonelysheep.github.io\/quartz-blog\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-XYFD95KB4J"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-XYFD95KB4J",{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://alonelysheep.github.io/quartz-blog/js/full-text-search.24827f874defbbc6d529926cbfcfb493.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://alonelysheep.github.io/quartz-blog/>Cyan's Blog</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>D2L-44-Batch_Normalization-批量归一化</h1><p class=meta>Last updated
Mar 5, 2022
<a href=https://github.com/alonelysheep/quartz-blog/tree/hugo/content/notes/2022/2022.3/D2L-44-Batch_Normalization-%e6%89%b9%e9%87%8f%e5%bd%92%e4%b8%80%e5%8c%96.md rel=noopener>Edit Source</a></p><ul class=tags><li><a href=https://alonelysheep.github.io/quartz-blog/tags/all/>All</a></li><li><a href=https://alonelysheep.github.io/quartz-blog/tags/BatchNormalization/>Batch normalization</a></li><li><a href=https://alonelysheep.github.io/quartz-blog/tags/Normalization/>Normalization</a></li><li><a href=https://alonelysheep.github.io/quartz-blog/tags/DeepLearning/>Deep learning</a></li><li><a href=https://alonelysheep.github.io/quartz-blog/tags/Regularization/>Regularization</a></li></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#motivation>Motivation</a></li><li><a href=#batch-normalization-layer>Batch Normalization Layer</a><ol><li><a href=#全连接层>全连接层</a></li><li><a href=#卷积层>卷积层</a></li></ol></li><li><a href=#规范化定义>规范化定义</a></li><li><a href=#其他相似的normalization>其他相似的Normalization</a></li><li><a href=#预测与训练的不同>预测与训练的不同</a></li></ol></nav></details></aside><a href=#batch-normalization><h1 id=batch-normalization><span class=hanchor arialabel=Anchor># </span>Batch Normalization</h1></a><div align=right>2022-03-05</div><p>Tags: #BatchNormalization #Normalization #DeepLearning #Regularization</p><ul><li><p>批量归一化是一种<strong>加速收敛</strong>的方法.</p></li><li><p>批量归一化作用于每一个mini-Batch, 先将这个Batch归一化, 然后再做一个统一的偏移与拉伸.</p><ul><li>最后这个偏移和拉伸的量是一个可以学习的超参数</li></ul></li><li><p>对于全连接层, BN作用于每一个特征</p></li><li><p>对于卷积层, BN作用于每一个通道.</p></li></ul><a href=#motivation><h2 id=motivation><span class=hanchor arialabel=Anchor># </span>Motivation</h2></a><ul><li><p>虽然BatchNorm效果很好, 但其实Batch Normalization效果好的原因并不明朗, 下面的原因也都是事后的推测.</p></li><li><p>原作者认为Batch Norm能够减少网络内部的"协变量偏移", 也就是说网络内部的特征分布可能发生变化. BatchNorm能够减弱这种变化</p></li><li><p>但是且不说作者误用了"协变量偏移"这个词, 一些研究表明Batch Norm并没有改变网络内部的变量分布, 而是使损失函数更加平滑了.</p></li><li><p>从权重稳定性的角度则可以这样理解:
<img src=https://alonelysheep.github.io/quartz-blog//notes/2022/2022.3/assets/img_2022-10-15-1.png width=auto alt></p></li></ul><a href=#batch-normalization-layer><h2 id=batch-normalization-layer><span class=hanchor arialabel=Anchor># </span>Batch Normalization Layer</h2></a><p>BN层可以作用在全连接层或者卷积层的输出之前和输入之后, 并且在激活函数之前.</p><a href=#全连接层><h3 id=全连接层><span class=hanchor arialabel=Anchor># </span>全连接层</h3></a><ul><li>对于全连接层, BN作用于每一个特征
$$\mathbf{h}=\phi(\mathbf{B N}(\mathbf{W} \mathbf{x}+\mathbf{b}))$$</li></ul><a href=#卷积层><h3 id=卷积层><span class=hanchor arialabel=Anchor># </span>卷积层</h3></a><ul><li><p>对于卷积层, BN作用于每一个通道.</p></li><li><p>也就是说, 假设每一个Batch有 $n$ 个样本 $X_1, X_2, \cdots, X_n$, 这些样本经过卷积之后得到 $n$ 个输出 $O_1, O_2, \cdots, O_n$, 每一个输出有 $c$ 个通道 $O_i^{(1)}, O_i^{(2)}, \cdots, O_i^{(c)}$, 通道的大小为 $h\times w$</p></li><li><p>BatchNorm将每一个输出 $O_i$ 里面的第 $k$ 个通道 $O_i^{(k)}$ 取出来, 全部展开拉成一条向量(Flatten), 里面一共有 $n\times h\times w$ 个元素 , 然后计算这个长条所有元素的平均值 $\mu_k$ 和方差 $\sigma_k$.</p><ul><li>接下来用均值 $\mu_k$ 和方差 $\sigma_k$ 对每一个元素进行归一化.</li><li>进一步, 利用参数 $\beta_k$ 和 $\gamma_k$ 对所有元素进行拉伸和平移.</li><li>最后将所有元素拼成原来的形状, 塞回原来的位置<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></li></ul><p><img src=https://alonelysheep.github.io/quartz-blog//notes/2022/2022.3/assets/img_2022-10-15-2.png width=auto alt>
<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></p></li><li><p>每一个通道对应一对 $\beta_k$ 和 $\gamma_k$ , $\beta_k$ 和 $\gamma_k$ 都是可以学习的参数.</p></li></ul><p><img src=https://alonelysheep.github.io/quartz-blog//notes/2022/2022.3/assets/Batchnorm.svg width=auto alt=Batchnorm></p><a href=#规范化定义><h2 id=规范化定义><span class=hanchor arialabel=Anchor># </span>规范化定义</h2></a><ul><li><p>从形式上来说, 用 $\mathbf{x} \in \mathcal{B}$ 表示一个来自小批量 $\mathcal{B}$ 的输入，批量规范化BN对 $\mathbf{x}$ 的作用可以表示为:
$$\operatorname{BN}(\mathbf{x})=\gamma \odot \frac{\mathbf{x}-\hat{\boldsymbol{\mu}}<em>{\mathcal{B}}}{\hat{\boldsymbol{\sigma}}</em>{\mathcal{B}}}+\boldsymbol{\beta}$$</p><ul><li>上式中 $\hat{\boldsymbol{\mu}}<em>{\mathcal{B}}$ 是小批量 $\mathcal{B}$ 的样本均值, $\hat{\boldsymbol{\sigma}}</em>{\mathcal{B}}$ 是小批量 $\mathcal{B}$ 的样本标准差。</li><li>应用标准化后, 生成的小批量的平均值为 0 和单位方差为 1 。</li><li>将输出进行这样的标准化是一个比较强的约束, 这并不总是合理的, 因此我们通常包含 拉伸参数（scale） $\gamma$ 和偏移参数（shift） $\boldsymbol{\beta}$, 它们的形状与 $\mathbf{x}$ 相同。</li><li>请注意, $\gamma$ 和 $\boldsymbol{\beta}$ 是需要与其他模型参数一起学习的参数。</li></ul></li><li><p>请注意，如果我们尝试使用大小为1的小批量应用批量规范化，我们将无法学到任何东西。 这是因为在减去均值之后，每个隐藏单元将为0。 所以，只有使用足够大的小批量，批量规范化这种方法才是有效且稳定的。 请注意，批量大小的选择在有BN时比没有BN时更重要。</p></li><li><p>每一个批量的均值 $\hat{\boldsymbol{\mu}}<em>{\mathcal{B}}$ 和方差 $\hat{\boldsymbol{\sigma}}</em>{\mathcal{B}}$ 的计算如下所示:
$$\begin{aligned}
\hat{\boldsymbol{\mu}}<em>{\mathcal{B}} &=\frac{1}{|\mathcal{B}|} \sum</em>{\mathbf{x} \in \mathcal{B}} \mathbf{x} \\ \hat{\boldsymbol{\sigma}}<em>{\mathcal{B}}^{2} &=\frac{1}{|\mathcal{B}|} \sum</em>{\mathbf{x} \in \mathcal{B}}\left(\mathbf{x}-\hat{\boldsymbol{\mu}}_{\mathcal{B}}\right)^{2}+\epsilon
\end{aligned}$$</p><ul><li>请注意, 我们在方差估计值中添加一个小的常量 $\epsilon>0$, 以确保即使方差很小, 我们也永远不会除以零。</li><li>估计值 $\hat{\boldsymbol{\mu}}<em>{\mathcal{B}}$ 和 $\hat{\boldsymbol{\sigma}}</em>{\mathcal{B}}$ 与Batch的随机性密切相关, 也就是说, 在对输入进行归一化的同时其实是会引入噪声的, 。你可能会认为这种噪声是一个问题, 而事实上它是有益的。<ul><li>This turns out to be a recurring theme in deep learning. For reasons that are not yet well-characterized theoretically, various sources of noise in optimization often lead to faster training and less overfitting: this variation appears to act as a form of regularization. In some preliminary research,
<a href=https://d2l.ai/chapter_references/zreferences.html#teye-azizpour-smith-2018 rel=noopener>Teye et al., 2018</a> and
<a href=https://d2l.ai/chapter_references/zreferences.html#luo-wang-shao-ea-2018 rel=noopener>Luo et al., 2018</a> relate the properties of batch normalization to Bayesian priors and penalties respectively. In particular, this sheds some light on the puzzle of why batch normalization works best for moderate minibatches sizes in the 50∼100 range.<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup></li></ul></li></ul></li></ul><a href=#其他相似的normalization><h2 id=其他相似的normalization><span class=hanchor arialabel=Anchor># </span>其他相似的Normalization</h2></a><ul><li>这里面N是样本的个数, D是特征数</li></ul><p><img src=https://alonelysheep.github.io/quartz-blog//notes/2022/2022.3/assets/Stanford%20CS231n%20BN.pdf width=auto alt="Stanford CS231n BN"></p><a href=#预测与训练的不同><h2 id=预测与训练的不同><span class=hanchor arialabel=Anchor># </span>预测与训练的不同</h2></a><ul><li>训练的时候是以Batch进行的, 而预测的时候我们通常输入的是单张图片, 那我们怎么计算均值和方差呢?<ul><li>我们使用训练样本整体的均值和方差来代替</li></ul></li></ul><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>只是为了解释的清楚, 实际计算的时候并不是我说的这样的&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p>这个图居然是Drawio画的, 可惜原作者的链接没了
<a href=https://stackoverflow.com/questions/65613694/calculation-of-mean-and-variance-in-batch-normalization-in-convolutional-neural rel=noopener>calculation of mean and variance in batch normalization in convolutional neural network - Stack Overflow</a>
<img src=https://alonelysheep.github.io/quartz-blog//notes/2022/2022.3/assets/BatchNormFull.png width=auto alt>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3 role=doc-endnote><p><a href="https://d2l.ai/chapter_convolutional-modern/batch-norm.html?highlight=scaling-issue#training-deep-networks" rel=noopener>7.5. Batch Normalization — Dive into Deep Learning 0.17.2 documentation</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/quartz-blog/notes/2022/2022.4/D2L-74-Transformer/ data-ctx=Batch_Normalization-批量归一化 data-src=/notes/2022/2022.4/D2L-74-Transformer class=internal-link>D2L-74-Transformer</a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://alonelysheep.github.io/quartz-blog/js/graph.abd4bc2af3869a96524d7d23b76152c7.js></script></div></div><div id=contact_buttons><footer><p>Made by Cyan Fu using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2023</p><ul><li><a href=https://alonelysheep.github.io/quartz-blog/>Home</a></li><li><a href=https://github.com/ALonelySheep>Github</a></li><li><a href=https://space.bilibili.com/506700150>Bilibili</a></li></ul></footer></div></div></body></html>