<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Logit: a confusing term 2022-02-11 Tags: #Math #DeepLearning #SoftmaxRegression
Ref: machine learning - What is the meaning of the word logits in TensorFlow?"><title>Logit</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel=icon type=image/png sizes=16x16 href=favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=favicon-32x32.png><link rel=manifest href=site.webmanifest><link href=https://alonelysheep.github.io/quartz-blog/styles.7153093e4d1bbb584a28469cadfa3f88.min.css rel=stylesheet><link href=https://alonelysheep.github.io/quartz-blog/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://alonelysheep.github.io/quartz-blog/js/darkmode.753e6a835409aa87fdd04fabb270d592.min.js></script>
<script src=https://alonelysheep.github.io/quartz-blog/js/util.9825137f5e7825e8553c68ce39ac9e44.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://alonelysheep.github.io/quartz-blog/js/popover.9b72b70bd35617d0635e9d15463662b2.min.js></script>
<script src=https://alonelysheep.github.io/quartz-blog/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script>
<script src=https://alonelysheep.github.io/quartz-blog/js/clipboard.c20857734e53a3fb733b7443879efa61.min.js></script>
<script src=https://alonelysheep.github.io/quartz-blog/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const BASE_URL="https://alonelysheep.github.io/quartz-blog/",fetchData=Promise.all([fetch("https://alonelysheep.github.io/quartz-blog/indices/linkIndex.04fdb58f9cd6a736acd4876488f75bec.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://alonelysheep.github.io/quartz-blog/indices/contentIndex.2bc57387ea62cc8bc170a20081146a64.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://alonelysheep.github.io/quartz-blog",!0,!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!0;drawGraph("https://alonelysheep.github.io/quartz-blog",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:.3,scale:.5}:{centerForce:1,depth:2,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:1.5,scale:1.2})}},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'’':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/alonelysheep.github.io\/quartz-blog\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-XYFD95KB4J"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-XYFD95KB4J",{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://alonelysheep.github.io/quartz-blog/js/full-text-search.24827f874defbbc6d529926cbfcfb493.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://alonelysheep.github.io/quartz-blog/>Cyan's Blog</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Logit</h1><p class=meta>Last updated
Feb 11, 2022
<a href=https://github.com/alonelysheep/quartz-blog/tree/hugo/content/notes/2022/2022.2/Logit.md rel=noopener>Edit Source</a></p><ul class=tags><li><a href=https://alonelysheep.github.io/quartz-blog/tags/all/>All</a></li><li><a href=https://alonelysheep.github.io/quartz-blog/tags/Math/>Math</a></li><li><a href=https://alonelysheep.github.io/quartz-blog/tags/DeepLearning/>Deep learning</a></li><li><a href=https://alonelysheep.github.io/quartz-blog/tags/SoftmaxRegression/>Softmax regression</a></li></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#理解1>理解</a></li><li><a href=#in-math>In Math</a></li><li><a href=#in-ml>In ML</a></li><li><a href=#historical-context>Historical Context</a></li><li><a href=#the-confusion>The Confusion</a></li></ol></nav></details></aside><a href=#logit-a-confusing-term><h1 id=logit-a-confusing-term><span class=hanchor arialabel=Anchor># </span>Logit: a confusing term</h1></a><div align=right>2022-02-11</div><p>Tags: #Math #DeepLearning #SoftmaxRegression</p><p>Ref:
<a href=https://stackoverflow.com/questions/41455101/what-is-the-meaning-of-the-word-logits-in-tensorflow rel=noopener>machine learning - What is the meaning of the word logits in TensorFlow? - Stack Overflow</a></p><ul><li>Logits is an overloaded term which can mean many different things:</li></ul><a href=#理解1><h2 id=理解1><span class=hanchor arialabel=Anchor># </span>理解<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></h2></a><ul><li>我们可以把Logit理解成"对数几率/概率", 也就是概率的对数: $\log p_i$.<ul><li>Logit输入到Softmax之后还原成原来的概率: $p_i$ : $$\frac{\exp(\log p_i)}{\sum_K \exp(\log p_i)}=p_i$$</li></ul></li></ul><a href=#in-math><h2 id=in-math><span class=hanchor arialabel=Anchor># </span>In Math</h2></a><ul><li><p><strong>In Math</strong>,
<a href=https://en.wikipedia.org/wiki/Logit rel=noopener>Logit</a> is a function that maps probabilities (<code>[0, 1]</code>) to R (<code>(-inf, inf)</code>)</p><p><img src=https://i.stack.imgur.com/zto5q.png width=auto alt="enter image description here"></p><ul><li>Probability of 0.5 corresponds to a logit of 0. Negative logit correspond to probabilities less than 0.5, positive to > 0.5.</li></ul></li></ul><a href=#in-ml><h2 id=in-ml><span class=hanchor arialabel=Anchor># </span>In ML</h2></a><ul><li><p><strong>For Tensorflow</strong>: It&rsquo;s a name that it is thought to imply that this Tensor is the quantity that is being mapped to probabilities by the Softmax.</p></li><li><p><strong>Logits Layer</strong>: In context of deep learning the
<a href=https://www.tensorflow.org/tutorials/estimators/cnn#logits_layer rel=noopener>logits layer</a> means the layer that feeds in to Softmax (or other such normalization).</p><ul><li>The output of the softmax are the probabilities for the classification task and its input is <strong>logits layer</strong>. The logits layer typically produces values from -infinity to +infinity and the softmax layer transforms it to values from 0 to 1.</li></ul></li></ul><a href=#historical-context><h2 id=historical-context><span class=hanchor arialabel=Anchor># </span>Historical Context</h2></a><p>Where does this term comes from?</p><ul><li><p>In 1930s and 40s, several people were trying to adapt linear regression to the problem of predicting probabilities. However linear regression produces output from -infinity to +infinity while for probabilities our desired output is 0 to 1.</p></li><li><p>One way to do this is by somehow mapping the probabilities 0 to 1 to -infinity to +infinity and then use linear regression as usual. One such mapping is cumulative normal distribution that was used by Chester Ittner Bliss in 1934 and he called this &ldquo;probit&rdquo; model, short for &ldquo;probability unit&rdquo;.</p></li><li><p>However this function is computationally expensive while lacking some of the desirable properties for multi-class classification. In 1944 Joseph Berkson used the function <code>log(p/(1-p))</code> to do this mapping and called it logit, short for &ldquo;logistic unit&rdquo;. The term logistic regression derived from this as well.</p></li></ul><a href=#the-confusion><h2 id=the-confusion><span class=hanchor arialabel=Anchor># </span>The Confusion</h2></a><p>Unfortunately the term logits is abused in deep learning.</p><ul><li>From pure mathematical perspective logit is a <em>function</em> that performs above mapping. In deep learning people started calling the layer &ldquo;logits layer&rdquo; that feeds in to logit function. Then people started calling the output <em>values</em> of this layer &ldquo;logit&rdquo; creating the confusion with logit <em>the function</em>.</li></ul><p><strong>TensorFlow Code</strong></p><ul><li><p>Unfortunately TensorFlow code further adds in to confusion by names like <code>tf.nn.softmax_cross_entropy_with_logits</code>.</p></li><li><p>What does logits mean here? It just means the input of the function is supposed to be the output of last neuron layer as described above. The <code>_with_logits</code> suffix is
<a href=https://github.com/tensorflow/tensorflow/issues/6531 rel=noopener>redundant, confusing and pointless</a>.</p></li><li><p>Functions should be named without regards to such very specific contexts because they are simply mathematical operations that can be performed on values derived from many other domains. In fact TensorFlow has another similar function <code>sparse_softmax_cross_entropy</code> where they fortunately forgot to add <code>_with_logits</code> suffix creating inconsistency and adding in to confusion.</p></li><li><p>PyTorch on the other hand simply names its function without these kind of suffixes.</p></li></ul><p><strong>Reference</strong></p><ul><li><a href=http://www.columbia.edu/~so33/SusDev/Lecture_9.pdf rel=noopener>Logit/Probit lecture slides</a></li><li><a href=https://en.wikipedia.org/wiki/Logit rel=noopener>Wikipedia article</a></li></ul><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p><a href=https://zh-v2.d2l.ai/chapter_convolutional-modern/nin.html#id3 rel=noopener>7.3. 网络中的网络（NiN） — 动手学深度学习 2.0.0-beta0 documentation</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/quartz-blog/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss/ data-ctx=Logit data-src=/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss class=internal-link>D2L-14-Cross Entropy as Loss</a></li><li><a href=/quartz-blog/notes/2022/2022.3/D2L-42-NiN/ data-ctx=Logit data-src=/notes/2022/2022.3/D2L-42-NiN class=internal-link>D2L-42-NiN</a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://alonelysheep.github.io/quartz-blog/js/graph.abd4bc2af3869a96524d7d23b76152c7.js></script></div></div><div id=contact_buttons><footer><p>Made by Cyan Fu using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2022</p><ul><li><a href=https://alonelysheep.github.io/quartz-blog/>Home</a></li><li><a href=https://github.com/ALonelySheep>Github</a></li><li><a href=https://space.bilibili.com/506700150>Bilibili</a></li></ul></footer></div></div></body></html>