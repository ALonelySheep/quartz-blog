---
title: Cyan's Blog
---
Welcome!🎉 I'm Cyan Fu, a CS undergrad at Tongji University.

I write about anything and everything here, and my main interest lies in the intersection of AI and 3D vision. I hope you find something new here!

## How about reading something about ...
### Math
- 一个抽象而有趣的引理： [Johnson Lindenstrauss Lemma](notes/2021/2021.12/Johnson%20Lindenstrauss%20Lemma%20-%20Publish%20Version.md)
- [从二项分布到泊松分布再到指数分布-From Binomial Distribution to Poisson Distribution to Exponential Distribution](notes/2022/2022.5/从二项分布到泊松分布再到指数分布-From%20Binomial%20Distribution%20to%20Poisson%20Distribution%20to%20Exponential%20Distribution.md)
- [为什么方差的分母常常是n-1](notes/2021/2021.10/为什么方差的分母常常是n-1.md)
- [酉矩阵为什么叫酉矩阵](notes/2021/2021.11/酉矩阵为什么叫酉矩阵.md)
- 如何对矩阵求导？
	- [矩阵的求导](notes/2021/2021.8/矩阵的求导.md)
	- [D2L-4-矩阵求导](notes/2022/2022.1/D2L-4-矩阵求导.md)
- [Diffie-Hellman问题](notes/2021/2021.6/Diffie-Hellman问题.md)
- [Dummy_Variables](notes/2022/2022.1/Dummy_Variables.md)
- [KL_Divergence-KL散度](notes/2022/2022.2/KL_Divergence-KL散度.md)
- [Gilbert Strang 深入浅出机器学习的矩阵知识](notes/2022/2022.10/Gilbert%20Strang%20深入浅出机器学习的矩阵知识.md)
- [Understanding Bayes' Theorem](notes/2021/2021.12/Understanding%20Bayes'%20Theorem.Md)
### Machine Learning
- [What on earth is Logit](notes/2022/2022.2/Logit.md)
- [为什么Softmax回归不用MSE](notes/2022/2022.2/为什么Softmax回归不用MSE.md)
- [Kernel Regression](notes/2022/2022.4/D2L-64-Kernel%20Regression.md)
- [Norm in Regularization - Intuition](notes/2022/2022.2/Norm%20in%20Regularization%20-%20Intuition.md)
- [Fisher_Linear_Discriminant(Pattern_Classification-Chapter_4)](notes/2021/2021.10/Part.29_Fisher_Linear_Discriminant(Pattern_Classification-Chapter_4).Md)
- [Cross_Entropy-交叉熵](notes/2022/2022.2/Cross_Entropy-交叉熵.md)

### Deep Learning
- [Convolution-卷积，数学与ML含义](notes/2022/2022.2/D2L-32-Convolution-卷积.md)
- [1x1卷积层有什么用](notes/2022/2022.2/D2L-36-1x1卷积层.md)
- 智能版加权平均：Attention
	- [Kernel Regression and Attention](notes/2022/2022.4/D2L-66-Kernel%20Regression%20and%20Attention.md)
	- [Attention Scoring Function](notes/2022/2022.4/D2L-67-Attention%20Scoring%20Function.md)
	- [图解Additive Attention](notes/2022/2022.4/D2L-68-Additive%20Attention.md)
	- [Scaled Dot-Product Attention](notes/2022/2022.4/D2L-69-Scaled%20Dot-Product%20Attention.md)
	- [Seq2Seq with Attention - Bahdanau Attention](notes/2022/2022.4/D2L-70-Seq2Seq%20with%20Attention%20-%20Bahdanau%20Attention.md)
	- [图解Multi-Head_Attention](notes/2022/2022.4/D2L-71-Multi-Head_Attention.md)
	- [图解Self-Attention](notes/2022/2022.4/D2L-72-Self-Attention.md)
- [智能版加权平均 is All You Need: 图解Transformer](notes/2022/2022.4/D2L-74-Transformer.md)
- [端到端学习-End_to_End_Learning-E2E](notes/2022/2022.5/端到端学习-End_to_End_Learning-E2E.md)
- [好的预测模型的特质](notes/2022/2022.2/好的预测模型的特质.md)
- [What_is_a_tensor](notes/2022/2022.1/D2L-1-What_is_a_tensor.md)
- [自动求导](notes/2022/2022.1/D2L-7-自动求导.md)
	- [为什么反向传播比前向传播更高效](notes/2022/2022.1/为什么反向传播比前向传播更高效.md)
- [MLP-多层感知机](notes/2022/2022.2/D2L-17-MLP-多层感知机.md)
- [权重衰减](notes/2022/2022.2/D2L-22-权重衰减.md)
- [数值稳定性](notes/2022/2022.2/D2L-24-数值稳定性.md)
	- [D2L-25-让训练更加稳定-Xavier初始化](notes/2022/2022.2/D2L-25-让训练更加稳定-Xavier初始化.md)
- [经典模型](notes/2022/2022.10/经典模型.md)
### English
[有效地背诵GRE单词](notes/2022/2022.7/有效地背诵GRE单词.md)

### Paul Graham
[如何努力工作_Paul_Graham](notes/2021/2021.8/如何努力工作_Paul_Graham.md)

## Blog as Network
