---
title: "凸优化与线性回归问题"
tags:
- all
- MachineLearning
- ConvexOptimization
- Math
date: "2021-08-02"
---
# Gradient Descent & Convex Optimization / 凸优化

<div align="right"> 2021-08-02</div>

Tags: #MachineLearning #ConvexOptimization #Math 

在[这里(和下面的引用里面)](notes/2021/2021.8/Part.5_Gradient_Descent(ML_Andrew.Ng.).md), 我们特殊的线性规划的损失函数一定是一个凸函数, 那么在其他情况下, 线性规划还是凸函数吗, 线性规划问题会陷入局部最优的问题中去吗?

> Note that, while gradient descent can be susceptible to local minima in general, the optimization problem we have posed here for linear regression has only one global, and no other local, optima; thus gradient descent always converges (assuming the learning rate α is not too large) to the global minimum. Indeed, J is a convex quadratic function. Here is an example of gradient descent as it is run to minimize a quadratic function.
> 
> ![](notes/2021/2021.8/assets/img_2022-10-15.png)
> 
> The ellipses shown above are the contours of a quadratic function.[^1]
	
- 凸优化问题与机器学习有着很密切的联系, 需要进一步了解


[^1]: https://www.coursera.org/learn/machine-learning/supplement/U90DX/gradient-descent-for-linear-regression