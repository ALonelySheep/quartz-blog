{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j058DilsrWsI",
        "origin_pos": 2,
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsjxDu5DrWsS",
        "origin_pos": 24
      },
      "source": [
        "## Loss Function\n",
        "\n",
        "At each time step, the decoder\n",
        "predicts a probability distribution for the output tokens.\n",
        "Similar to language modeling,\n",
        "we can apply softmax to obtain the distribution\n",
        "and calculate the cross-entropy loss for optimization.\n",
        "Recall :numref:`sec_machine_translation`\n",
        "that the special padding tokens\n",
        "are appended to the end of sequences\n",
        "so sequences of varying lengths\n",
        "can be efficiently loaded\n",
        "in minibatches of the same shape.\n",
        "However,\n",
        "prediction of padding tokens\n",
        "should be excluded from loss calculations.\n",
        "\n",
        "To this end,\n",
        "we can use the following\n",
        "`sequence_mask` function\n",
        "to [**mask irrelevant entries with zero values**]\n",
        "so later\n",
        "multiplication of any irrelevant prediction\n",
        "with zero equals to zero.\n",
        "For example,\n",
        "if the valid length of two sequences\n",
        "excluding padding tokens\n",
        "are one and two, respectively,\n",
        "the remaining entries after\n",
        "the first one\n",
        "and the first two entries are cleared to zeros.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PlcccE9krWsS",
        "origin_pos": 26,
        "outputId": "ba5c94f7-ac94-4659-ca57-686b7efb2bec",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1, 0, 0],\n",
              "        [4, 5, 0]])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@save\n",
        "def sequence_mask(X, valid_len, value=0):\n",
        "    \"\"\"Mask irrelevant entries in sequences.\"\"\"\n",
        "    maxlen = X.size(1)\n",
        "    mask = torch.arange((maxlen), dtype=torch.float32,\n",
        "                        device=X.device)[None, :] < valid_len[:, None]\n",
        "    X[~mask] = value\n",
        "    return X\n",
        "\n",
        "X = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "sequence_mask(X, torch.tensor([1, 2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X[None,:]\n",
            "torch.Size([1, 3])\n",
            "tensor([[1, 2, 3]])\n",
            "X[:, None]\n",
            "torch.Size([3, 1])\n",
            "tensor([[1],\n",
            "        [2],\n",
            "        [3]])\n",
            "X[None,:]<X[:, None]\n",
            "tensor([[False, False, False],\n",
            "        [ True, False, False],\n",
            "        [ True,  True, False]])\n",
            "tensor([[ True,  True,  True],\n",
            "        [False,  True,  True],\n",
            "        [False, False,  True]])\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "tensor([[0., 0., 0.],\n",
            "        [1., 0., 0.],\n",
            "        [1., 1., 0.]])\n",
            "tensor([1, 2, 3, 5, 6, 9])\n",
            "tensor([[0, 0, 0],\n",
            "        [4, 0, 0],\n",
            "        [7, 8, 0]])\n"
          ]
        }
      ],
      "source": [
        "X = torch.tensor([1,2,3])\n",
        "print(\"X[None,:]\")\n",
        "print(X[None,:].shape)\n",
        "print(X[None,:])\n",
        "print(\"X[:, None]\")\n",
        "print(X[:, None].shape)\n",
        "print(X[:, None])\n",
        "print(\"X[None,:]<X[:, None]\")\n",
        "print(X[None,:]<X[:, None])\n",
        "print(~(X[None,:]<X[:, None]))\n",
        "# 小于会触发Broadcasting, 生成一个3x3的tensor\n",
        "Y = torch.ones(3,3)\n",
        "print(Y)\n",
        "Y[~(X[None,:]<X[:, None])]=0;\n",
        "print(Y)\n",
        "\n",
        "Z = torch.arange(1,10).reshape(-1,3)\n",
        "print(Z[~(X[None,:]<X[:, None])]) #直接索引返回的是所有满足条件的元素\n",
        "Z[~(X[None,:]<X[:, None])]=0 # 也可以利用mask直接赋值\n",
        "print(Z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYSpUaLlrWsY",
        "origin_pos": 28
      },
      "source": [
        "(**We can also mask all the entries across the last\n",
        "few axes.**)\n",
        "If you like, you may even specify\n",
        "to replace such entries with a non-zero value.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ih51biPrrWsZ",
        "origin_pos": 30,
        "outputId": "2ce35232-9b5b-482e-d663-4318b4398f48",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 1.,  1.,  1.,  1.],\n",
              "         [-1., -1., -1., -1.],\n",
              "         [-1., -1., -1., -1.]],\n",
              "\n",
              "        [[ 1.,  1.,  1.,  1.],\n",
              "         [ 1.,  1.,  1.,  1.],\n",
              "         [-1., -1., -1., -1.]]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = torch.ones(2, 3, 4)\n",
        "sequence_mask(X, torch.tensor([1, 2]), value=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WBPguVPrWsZ",
        "origin_pos": 32
      },
      "source": [
        "Now we can [**extend the softmax cross-entropy loss\n",
        "to allow the masking of irrelevant predictions.**]\n",
        "Initially,\n",
        "masks for all the predicted tokens are set to one.\n",
        "Once the valid length is given,\n",
        "the mask corresponding to any padding token\n",
        "will be cleared to zero.\n",
        "In the end,\n",
        "the loss for all the tokens\n",
        "will be multipled by the mask to filter out\n",
        "irrelevant predictions of padding tokens in the loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "rmChimwurWsZ",
        "origin_pos": 34,
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [],
      "source": [
        "#@save\n",
        "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
        "    \"\"\"The softmax cross-entropy loss with masks.\"\"\"\n",
        "    # `pred` shape: (`batch_size`, `num_steps`, `vocab_size`)\n",
        "    # `label` shape: (`batch_size`, `num_steps`)\n",
        "    # `valid_len` shape: (`batch_size`,)\n",
        "    def forward(self, pred, label, valid_len):\n",
        "        weights = torch.ones_like(label)\n",
        "        weights = sequence_mask(weights, valid_len)\n",
        "        self.reduction='none'\n",
        "        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(\n",
        "            pred.permute(0, 2, 1), label)\n",
        "        print(unweighted_loss.shape)\n",
        "        print(unweighted_loss)\n",
        "        weighted_loss = (unweighted_loss * weights).mean(dim=1)\n",
        "        print(unweighted_loss * weights)\n",
        "        print(weighted_loss.shape)\n",
        "        print(weighted_loss)\n",
        "        return weighted_loss\n",
        "        # return unweighted_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_dA_bZbrWsa",
        "origin_pos": 36
      },
      "source": [
        "For [**a sanity check**], we can create three identical sequences.\n",
        "Then we can\n",
        "specify that the valid lengths of these sequences\n",
        "are 4, 2, and 0, respectively.\n",
        "As a result,\n",
        "the loss of the first sequence\n",
        "should be twice as large as that of the second sequence,\n",
        "while the third sequence should have a zero loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "F5M7II1yrWsa",
        "origin_pos": 38,
        "outputId": "ee97c25b-6157-4897-c0d6-741526861f91",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 4])\n",
            "tensor([[2.3026, 2.3026, 2.3026, 2.3026],\n",
            "        [2.3026, 2.3026, 2.3026, 2.3026],\n",
            "        [2.3026, 2.3026, 2.3026, 2.3026]])\n",
            "tensor([[2.3026, 2.3026, 2.3026, 2.3026],\n",
            "        [2.3026, 2.3026, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000]])\n",
            "torch.Size([3])\n",
            "tensor([2.3026, 1.1513, 0.0000])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([2.3026, 1.1513, 0.0000])"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss = MaskedSoftmaxCELoss()\n",
        "loss(torch.ones(3, 4, 10), torch.ones((3, 4), dtype=torch.long),\n",
        "     torch.tensor([4, 2, 0]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "seq2seq.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
