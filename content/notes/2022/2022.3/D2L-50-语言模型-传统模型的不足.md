---
title: "D2L-50-语言模型-传统模型的不足"
tags:
- all
- LanguageModel
date: "2022-03-08"
---
# 语言模型

<div align="right"> 2022-03-08</div>

Tags: #LanguageModel

## 传统模型
- 语言模型的输出是一个文本序列: $$x_{t},\space x_{t-1},\space \ldots\space ,\space x_{1}$$
	- 为了生成有意义的序列, 我们希望模拟语料库里面的语句, 生成概率 $P\left(x_{1}, x_{2}, \ldots, x_{T}\right)$ 最高的语句.

![](notes/2022/2022.3/D2L-48-序列模型-Sequence_Models.md#^973ecf)

- 一个例子是 $$\begin{aligned}&P(deep, learning, is, fun)=\\&P(deep) P(learning\mid deep ) P( is  \mid  deep, learning ) P( fun  \mid  deep, learning, is )\end{aligned}$$

- 为了估计这个句子的概率, 我们需要计算里面的每一个部分, 一个想法是用频率代替概率: $$\hat{P}(\text { learning } \mid \text { deep })=\frac{n(\text { deep }, \text { learning })}{n(\text { deep })}$$
- 但是: 
> 不幸的是，由于连续单词对“deep learning”的出现频率要低得多， 所以估计这类单词正确的概率要困难得多。 特别是对于一些不常见的单词组合，要想找到足够的出现次数来获得准确的估计可能都不容易。 而对于三个或者更多的单词组合，情况会变得更糟。 许多合理的三个单词组合可能是存在的，但是在数据集中却找不到。 除非我们提供某种解决方案，来将这些单词组合指定为非零计数， 否则将无法在语言模型中使用它们。 如果数据集很小，或者单词非常罕见，那么这类单词出现一次的机会可能都找不到。[^1]

### 拉普拉斯平滑
- 一种常见的策略是执行 **拉普拉斯平滑**（Laplace smoothing）， 具体方法是在所有计数中添加一个小常量。 用 $n$ 表示训练集中的单词总数，用 $m$ 表示唯一单词的数量。 此解决方案有助于处理单元素问题，例如通过：
$$\begin{aligned}
\hat{P}(x) &=\frac{n(x)+\epsilon_{1} / m}{n+\epsilon_{1}} \\
\hat{P}\left(x^{\prime} \mid x\right) &=\frac{n\left(x, x^{\prime}\right)+\epsilon_{2} \hat{P}\left(x^{\prime}\right)}{n(x)+\epsilon_{2}} \\
\hat{P}\left(x^{\prime \prime} \mid x, x^{\prime}\right) &=\frac{n\left(x, x^{\prime}, x^{\prime \prime}\right)+\epsilon_{3} \hat{P}\left(x^{\prime \prime}\right)}{n\left(x, x^{\prime}\right)+\epsilon_{3}} .
\end{aligned}$$
- 其中，$ϵ_1$, $ϵ_2$ 和 $ϵ_3$ 是超参数。 以 $ϵ_1$ 为例：当 $ϵ_1=0$ 时，不应用平滑； 当 $ϵ_1$ 接近正无穷大时，$\hat P(x)$ 接近均匀概率分布 $1/m$。 上面的公式是 [Wood et al., 2011](https://zh-v2.d2l.ai/chapter_references/zreferences.html#wood-gasthaus-archambeau-ea-2011 ) 的一个相当原始的变形。[^2]

#### 缺点
- 然而，这样的模型很容易失效，原因如下： 
	- 首先，我们需要存储所有的计数； 
	- 其次，这完全忽略了单词的意思。 例如，“猫”（cat）和“猫科动物”（feline）可能出现在相关的上下文中， 但是想根据上下文调整这类模型其实是相当困难的。 
	- 最后，长单词序列大部分是没出现过的， 因此一个模型如果只是简单地统计先前“看到”的单词序列频率， 那么模型面对这种问题肯定是表现不佳的。


[^1]: [8.3. 语言模型和数据集 — 动手学深度学习 2.0.0-beta0 documentation](https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/language-models-and-dataset.html)
[^2]: [8.3. 语言模型和数据集 — 学习语言模型](https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/language-models-and-dataset.html#id2)