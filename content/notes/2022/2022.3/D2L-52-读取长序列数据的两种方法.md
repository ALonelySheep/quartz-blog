---
title: "D2L-52-读取长序列数据的两种方法"
tags:
- all
- SequentialData
- DataPreprocessing
date: "2022-03-08"
---
# 读取长序列数据的两种方法

<div align="right"> 2022-03-08</div>

Tags:  #SequentialData #DataPreprocessing

- 尽管序列数据本质上是连续的, 我们在处理数据的时候也希望将其分为小批量, 方便模型读取。
- 设我们将使用神经网络来训练语言模型， 模型中的网络一次处理具有预定义长度 （例如n个时间步）的一个小批量序列。
![](notes/2022/2022.3/assets/timemachine-5gram.svg)
- 我们应该从上图中选择哪一个序列呢？ 事实上，他们都一样好。 
	- 然而，如果我们只选择一个偏移量， 那么用于训练网络的、所有可能的子序列的覆盖范围将是有限的。 
	- 因此，我们可以从随机偏移量开始划分序列， 以同时获得覆盖性（coverage）和随机性（randomness）。 下面，我们将描述如何实现随机采样（random sampling）和 顺序分区（sequential partitioning）策略。

## 随机采样
下图中 `Batchsize=2`, 也就是说每个Batch里面有两个序列, 一个红色的, 一个蓝色的.

![随机选取](notes/2022/2022.3/assets/随机选取.svg)
- 每个样本都是在原始的长序列上任意捕获的子序列。 
- 在迭代过程中，来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻。 
	- 注意, 即使采用了随机采样, 我们也同样覆盖了整个文本序列.
- 对于语言建模，目标是基于到目前为止我们看到的词元来预测下一个词元， 因此标签是移位了一个词元的原始序列。


## 顺序分区

![顺序选取](notes/2022/2022.3/assets/顺序选取.svg)
- 两个相邻的小批量中的子序列在原始序列上也是相邻的