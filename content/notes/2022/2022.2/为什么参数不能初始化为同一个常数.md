---
title: "为什么参数不能初始化为同一个常数"
tags:
- all
- DeepLearning
date: "2022-02-19"
---
# 深度学习: 参数化所固有的对称性

<div align="right"> 2022-02-19</div>

Tags: #DeepLearning 

- [4.8.1.3. 打破对称性](https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html#id5 "Permalink to this headline")

> - 对于一个多层感知机, 假设隐藏层只有两个单元, 输出层只有一个输出单元。 想象一下，如果我们将隐藏层的所有参数初始化为 $W^{(1)}=c$， $c$ 为常量，会发生什么？ 
> - 在这种情况下，在前向传播期间，两个隐藏单元采用相同的输入和参数， 产生相同的激活，该激活被送到输出单元。 在反向传播期间，根据参数 $W^{(1)}$ 对输出单元进行微分， 得到一个梯度，其元素都取相同的值。 因此，在一次梯度下降（例如，小批量随机梯度下降）之后， $W^{(1)}$ 的所有元素仍然有相同的值。 而每一次迭代永远都不会打破对称性，这也意味着隐藏层的行为就好像只有一个单元, 我们永远也无法利用多层网络强大的表达能力。 
> - 请注意，虽然小批量随机梯度下降不会打破这种对称性，但暂退法 [(Dropout-丢弃法)](notes/2022/2022.2/D2L-23-Dropout-丢弃法.md) 正则化可以。