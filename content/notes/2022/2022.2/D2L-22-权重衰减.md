---
title: "D2L-22-权重衰减"
tags:
- all
- Regularization
- DeepLearning
date: "2022-02-12"
---
# 权重衰减

<div align="right"> 2022-02-12</div>

Tags: #Regularization #DeepLearning 

- **权重衰减**就是利用 $\ell_{2}$ 范数进行 [正则化](notes/2022/2022.2/Regularization-正则化.md), 避免过拟合
	- 权重衰减是通过减小目标参数(weights)的大小来实现正则化的, 这也是其名称的由来.
	- 参数的范数代表了一种有用的简单性度量。[^5]

> Links:
> -  [Part.18_Regularization_Intuition(ML_Andrew.Ng.)](notes/2021/2021.9/Part.18_Regularization_Intuition(ML_Andrew.Ng.).md)
> -  [Part.19_Regularized_Linear_Regression(ML_Andrew.Ng.)](notes/2021/2021.9/Part.19_Regularized_Linear_Regression(ML_Andrew.Ng.).md)

## Intuition
![Norm in Regularization - Intuition](notes/2022/2022.2/Norm%20in%20Regularization%20-%20Intuition.md)

---
- **这项技术通过函数与零的距离来衡量函数的复杂度**， 因为在所有函数 $f$ 中，函数 $f=0$ （所有输入都得到值 $0$） 在某种意义上是最简单的。 但是我们应该如何精确地测量一个函数和零之间的距离呢？ 没有一个正确的答案。 事实上，函数分析和巴拿赫空间理论的研究，都在致力于回答这个问题。[^1]

## 推导[^2]
对于以下损失函数: 
$$L(\mathbf{w}, b)=\frac{1}{n} \sum_{i=1}^{n} \frac{1}{2}\left(\mathbf{w}^{\top} \mathbf{x}^{(i)}+b-y^{(i)}\right)^{2}$$

- 为了惩罚权重向量的大小， 我们必须以某种方式在损失函数中添加$‖\mathbf w‖^2$， 但是模型应该如何平衡这个新的额外惩罚的损失？ 实际上，我们通过 **正则化常数** $λ$ 来描述这种权衡， 这是一个非负超参数，我们使用验证数据拟合：

	$$L(\mathbf{w}, b)+\frac{\lambda}{2}\|\mathbf{w}\|^{2}$$

	- 其中 $1/2$ 是为了求导后公式的简洁.

- 你可能会想知道为什么我们使用平方范数 $\mathbf{\|w\|}_2^2$ 而不是标准范数 $\mathbf{\|w\|}_2$ （即欧几里得距离）？ 
	- 我们这样做是为了便于计算。 通过平方 $L_2$ 范数，我们去掉平方根，留下权重向量每个分量的平方和。 这使得惩罚的导数很容易计算：导数的和等于和的导数。
	
- $L_2$正则化回归的小批量随机梯度下降更新如下式：

$$\mathbf{w} \leftarrow(1-\eta \lambda) \mathbf{w}-\frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)}\left(\mathbf{w}^{\top} \mathbf{x}^{(i)}+b-y^{(i)}\right)$$

- 我们根据估计值与观测值之间的差异来更新$\mathbf w$。 然而，我们同时也在试图将 $\mathbf w$ 的大小缩小到零。 **这就是为什么这种方法有时被称为 _权重衰减_** 。 我们仅考虑惩罚项，优化算法在训练的每一步_衰减_ 权重。 
	- 与特征选择相比，权重衰减为我们提供了一种**连续的**机制来调整函数的复杂度。 较小的 $λ$ 值对应较少约束的 $\mathbf w$， 而较大的 $λ$ 值对 $\mathbf w$ 的约束更大。

- 是否对相应的偏置 $b^2$ 进行惩罚在不同的实践中会有所不同， 在神经网络的不同层中也会有所不同。 **通常，网络输出层的偏置项不会被正则化**。

## 两种限制方式的等价性
![](notes/2022/2022.2/assets/Pasted%20image%2020220214194009.png)
![](notes/2022/2022.2/assets/Pasted%20image%2020220214194022.png)

## Tikhonov Regularization
- L2 正则化还有一个名字叫"**吉洪诺夫正则化**"[^3]

	- 1995 年，克里斯托弗·毕晓普[^4]证明了: 具有输入噪声的训练等价于 Tikhonov 正则化 [Bishop, 1995](https://zh-v2.d2l.ai/chapter_references/zreferences.html#bishop-1995)。 ^c88d1b

## 实现
- 优化算法里面的 `weight_decay` 就是实现的权重衰减
	- 在深度学习里面权重衰减是整合到了优化函数里面的, 所以对于任意损失我们都可以进行衰减.
	- 例: PyTorch里面的随机梯度下降:  [SGD — PyTorch 1.10 documentation](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)
		- ![](notes/2022/2022.2/assets/Pasted%20image%2020220302193549.png)
- weight_decay相对于Dropout应用面更广[^6]

[^1]: [4.5. 权重衰减 — 动手学深度学习 2.0.0-beta0 documentation](https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/weight-decay.html#id2)
[^2]: [4.5. 权重衰减 — 动手学深度学习 2.0.0-beta0 documentation](https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/weight-decay.html#id2)
[^3]: [吉洪诺夫正则化 - 维基百科，自由的百科全书](https://zh.wikipedia.org/zh-hans/%E5%90%89%E6%B4%AA%E8%AF%BA%E5%A4%AB%E6%AD%A3%E5%88%99%E5%8C%96) English:  [Tikhonov regularization - Wikipedia](https://en.wikipedia.org/wiki/Tikhonov_regularization)
[^4]: 就是写 PRML 那本书 的 Bishop
[^5]: [4.6. 暂退法（Dropout） — 动手学深度学习 2.0.0-beta0 documentation](https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/dropout.html#id2)
[^6]: [13 丢弃法【动手学深度学习v2】_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1Y5411c7aY?p=3&t=1196.3)