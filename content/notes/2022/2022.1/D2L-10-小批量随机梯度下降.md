---
title: "D2L-10-小批量随机梯度下降"
tags:
- all
- MachineLearning
- GradientDescent
- DeepLearning
- Optimization
date: "2022-02-02"
---
# 小批量随机梯度下降(Mini-Batch)是深度学习默认的求解方法

<div align="right"> 2022-02-02</div>

Tags: #MachineLearning #GradientDescent #DeepLearning #Optimization 

- [Different_Gradient_Descent_Methods](notes/2021/2021.8/Different_Gradient_Descent_Methods.md)

注意有两个点: 小批量(Mini-Batch), 随机(Stochastic) 梯度下降
其中: 
- 小批量是因为在整个数据集上面训练一次又慢又贵
	- 同时小批量还能从多个相似的数据点中选一个代表来计算, 节约了计算资源
	- 但是样本不能太小, 太小的样本不适合用GPU并行计算
- 随机是选取小样本的方法: 随机选取 