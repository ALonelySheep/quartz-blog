{"/":{"title":"Cyan's Blog","content":"","lastmodified":"2022-10-15T14:06:29.026497453Z","tags":null},"/notes/2021/2021.10/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%96%B9%E5%B7%AE%E7%9A%84%E5%88%86%E6%AF%8D%E5%B8%B8%E5%B8%B8%E6%98%AFn-1":{"title":"为什么方差的分母常常是n-1","content":"# 为什么方差的分母常常是$n-1$?\n\n\u003cdiv align=\"right\"\u003e 2021-10-29\u003c/div\u003e\n\nTags: #Math/Statistics #Variance\n\n- 按照定义, 方差的分母的确应该是$n$\n\n- 但是因为我们用样本的均值$\\overline X$代替了数学期望$\\mu$, 而这个$\\overline X$是有误差的, $\\frac 1 n \\rightarrow \\frac 1 {(n-1)}$是为了消除这个误差.\n\n## 详细解释\n\u003e 前提： $X_i$ 相互独立\n\n按照定义, 方差的公式是:\n$$\\sigma^{2}=\\mathbb E\\left(\\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}-\\mu \\right)^{2}\\right)$$\n其中, $\\mu$是随机变量的数学期望.\n\n- $$\\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}-\\mu \\right)^{2}$$是方差的无偏估计\n\n但是, 实际工作中, 样本的数学期望常常难以得到 ,所以我们用样本的均值来代替数学期望:\n$$\\mu \\rightarrow \\overline X = \\frac{1}{n} \\sum_{i=1}^{n} X_{i}$$\n\n这会不会带来误差呢? 当然会!\n\n- **如果直接采用$\\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)^{2}$  作为估计，那么你会倾向于低估方差!** (变成偏低的有偏估计)\n\n这是因为:\n$$\\begin{aligned}\n\\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)^{2} \u0026=\\frac{1}{n} \\sum_{i=1}^{n}\\left[\\left(X_{i}-\\mu\\right)+(\\mu-\\bar{X})\\right]^{2} \\\\\n\u0026=\\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}-\\mu\\right)^{2}+\\frac{2}{n} \\sum_{i=1}^{n}\\left(X_{i}-\\mu\\right)(\\mu-\\bar{X})+\\frac{1}{n} \\sum_{i=1}^{n}(\\mu-\\bar{X})^{2} \\\\\n\u0026=\\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}-\\mu\\right)^{2}+2(\\bar{X}-\\mu)(\\mu-\\bar{X})+(\\mu-\\bar{X})^{2} \\\\\n\u0026=\\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}-\\mu\\right)^{2}-(\\mu-\\bar{X})^{2}\n\\end{aligned}$$\n\n换言之，除非正好 $X=\\mu$, 否则我们一定有\n$$\\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)^{2}\u003c\\sigma^2$$\n\n而 （证明见[Prove that $E (\\overline{X} - \\mu)^2 = \\frac{1}{n}\\sigma^2$](https://math.stackexchange.com/questions/1363505/prove-that-e-overlinex-mu2-frac1n-sigma2)）：\n$$\\mathrm{E}\\left[(\\bar{X}-\\mu)^{2}\\right]=\\frac{1}{n} \\sigma^{2}$$\n\n所以:\n\n$$E\\left[\\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)^{2}\\right]=\\sigma^{2}-\\frac{1}{n} \\sigma^{2}=\\frac{n-1}{n} \\sigma^{2}$$\n\n为了调整, 我们乘上$\\frac n {n-1}$\n\n公式也随之变为:\n$$\n\\frac n {n-1} \\times \\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)^{2} = \\frac{1}{n-1} \\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)^{2} \n$$\n\n\n\n%%\n下面这个说法好像有点问题, 问题的关键是直接代换会使方差的估计变成有偏估计\n\n我们看以下例子:\n![](notes/2021/2021.10/assets/img_2022-10-15.gif)\n图中, 绿色散点是样本. 对于可能的真实分布, 我们取不同的$\\mu$, 可以看到, $\\mu$与$\\overline X$的距离越大, 方差的误差也越大%%\n\n\n\n\n\n## Source\n- https://www.zhihu.com/question/20099757\n- https://baike.baidu.com/item/%E6%96%B9%E5%B7%AE/3108412\n\n\n\n\n\n","lastmodified":"2022-10-15T14:06:29.082498057Z","tags":null},"/notes/2021/2021.10/%E4%BD%8F%E6%88%BF%E8%87%AA%E6%9C%89%E7%8E%87-House_Ownership_Rate":{"title":"住房自有率-House_Ownership_Rate","content":"# 住房自有率 - House Ownership Rate\n\n\u003cdiv align=\"right\"\u003e 2021-11-06\u003c/div\u003e\n\nTags: #HousingMarket #Index\n\n- 即拥有住房的人口占总人口的比率\n- 不同国家的统计方式不同\n\n[List of countries by home ownership rate](https://en.wikipedia.org/wiki/List_of_countries_by_home_ownership_rate#cite_note-4)\n\n- 中国的住房自有率为89%(2018)\n- 老牌发达经济体如美国、英国，住房自有率稳定在60%左右。德国、香港更低，不到50%。\n\n![](notes/2021/2021.10/assets/img_2022-10-15.png)\n![](notes/2021/2021.10/assets/img_2022-10-15-1.png)\n[^3]\n- 我国的住房自有率接近90%，**主要是计算口径不同导致的虚高**。国际上，住房自有率计算方法是居住在自有产权的家庭户数/全部家庭户数，而我国则是按照自有（私有）住宅建筑面积/实有住宅建筑面积计算，实为住房私有率。两者有两个区别，一是国外是按照户数，而我国是根据建筑面积，二是后者将本地无房、但在外地有房的也纳入计算。考虑到这两个因素，我国的住房自有率应该处于较合理的水平。[^2]\n\n## About Housing Market\n#### Why is the housing market important to the economy?[^1]\nThe housing market is closely linked to consumer spending. When house prices go up, homeowners become better off and feel more confident. Some people will borrow more against the value of their home, either to spend on goods and services, renovate their house, supplement their pension, or pay off other debt.\n\nWhen house prices go down, homeowners risk that their house will be worth less than their outstanding mortgage.  People are therefore more likely to cut down on spending and hold off from making personal investments.\n\n\n\n[^1]: https://www.bankofengland.co.uk/knowledgebank/how-does-the-housing-market-affect-the-economy\n[^2]: http://finance.sina.cn/zl/2019-04-25/zl-ihvhiewr8156465.d.html?from=wap\n[^3]: 住房自有率与经济发展水平——基于中国31 个省和地区的经验分析余秋梅1，2，孙伟增3，4，郑思齐3，4","lastmodified":"2022-10-15T14:06:29.082498057Z","tags":null},"/notes/2021/2021.10/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5_Covariance_Matrix":{"title":"协方差矩阵_Covariance_Matrix","content":"# Covariance Matrix\n\n\u003cdiv align=\"right\"\u003e 2021-10-29\u003c/div\u003e\n\nTags: #Matrix #Math/Statistics \n\n\nhttps://janakiev.com/blog/covariance-matrix/\n\n\n## Variance, Covariance \n\n- **Variance** measures the variation of a single random variable (like height of a person in a population)\n\t$$\\sigma_{x}^{2}=\\mathbb E \\left(\\frac{1}{n-1} \\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}\\right)$$\n\tLink: [为什么方差的分母常常是n-1](notes/2021/2021.10/为什么方差的分母常常是n-1.md)\n\t\n- Whereas **[covariance](notes/2021/2021.12/Covariance-协方差.md)** is a measure of **how much two random variables vary together** (like the height of a person and the weight of a person in a population)\n\t$$\\sigma(x, y)=\\mathbb E\\left(\\frac{1}{n-1} \\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)\\right)$$\n\n所以方差也可以看作一个随机变量自己与自己的协方差:\n$$\\sigma_{x}^{2} = \\frac{1}{n-1} \\sum_{i=1}^{n}(x_{i}-\\bar{x})(x_{i}-\\bar{x}) = \\sigma(x, x)$$\n\n## 协方差矩阵\n[Wikipedia](https://zh.wikipedia.org/zh-hans/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5):\n\n假设$X$是以$n$个随机变量组成的列向量，\n\n$\\mathbf{X} = \\begin{bmatrix} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{bmatrix}$\n\n并且$\\mu_i$是$X_i$的期望值，即, $\\mu_i = \\mathrm{E}(X_i)$。协方差矩阵的第$(i,j)$項（第$(i,j)$項是一个协方差）被定义为如下形式：\n\n$$\\Sigma_{ij}\n= \\mathrm{cov}(X_i, X_j) = \\mathrm{E}\\begin{bmatrix}\n(X_i - \\mu_i) (X_j - \\mu_j)\n\\end{bmatrix}$$\n\n而协方差矩阵为：\n\n$$\\Sigma=\\mathrm{E}\n\\left[\n \\left(\n \\mathbf{X} - \\mathrm{E}[\\mathbf{X}]\n \\right)\n \\left(\n \\mathbf{X} - \\mathrm{E}[\\mathbf{X}]\n \\right)^{\\rm T}\n\\right]$$\n\n\n$$=\n\\begin{bmatrix}\n \\mathrm{E}[(X_1 - \\mu_1)(X_1 - \\mu_1)] \u0026 \\mathrm{E}[(X_1 - \\mu_1)(X_2 - \\mu_2)] \u0026 \\cdots \u0026 \\mathrm{E}[(X_1 - \\mu_1)(X_n - \\mu_n)] \\\\ \\\\\n \\mathrm{E}[(X_2 - \\mu_2)(X_1 - \\mu_1)] \u0026 \\mathrm{E}[(X_2 - \\mu_2)(X_2 - \\mu_2)] \u0026 \\cdots \u0026 \\mathrm{E}[(X_2 - \\mu_2)(X_n - \\mu_n)] \\\\ \\\\\n \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\\\\n \\mathrm{E}[(X_n - \\mu_n)(X_1 - \\mu_1)] \u0026 \\mathrm{E}[(X_n - \\mu_n)(X_2 - \\mu_2)] \u0026 \\cdots \u0026 \\mathrm{E}[(X_n - \\mu_n)(X_n - \\mu_n)]\n\\end{bmatrix}$$\n\n矩阵中的第$(i,j)$个元素是$X_i$与$X_j$的协方差\n\n## 进一步\n\n矩阵的奇异值分解可以将数据还原为普通的形式, 这在LDA等许多算法中都有应用\n进一步可以阅读以下文章:\nhttps://janakiev.com/blog/covariance-matrix/","lastmodified":"2022-10-15T14:06:29.082498057Z","tags":null},"/notes/2021/2021.10/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0-Lagrange_Multiplier":{"title":"拉格朗日乘数-Lagrange_Multiplier","content":"# 拉格朗日乘数\n\n\u003cdiv align=\"right\"\u003e 2021-10-29\u003c/div\u003e\n\nTags: #Math #Optimization \n\n\n\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Lagrange_multiplier.png/300px-Lagrange_multiplier.png)\n\n## Intuition\n$$\\mathcal{L}(x, y, \\lambda)=f(x, y)-\\lambda g(x, y)$$\n在一个三维曲面($f(x,y)$)上面画了一条曲线($g(x,y)$), 求这条曲线上面的最低点.\n\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/b/bf/LagrangeMultipliers2D.svg/300px-LagrangeMultipliers2D.svg.png)\n\n## Explanation\n\n中英文的维基百科已经解释的十分直观与清晰了:\n\n\n- [Chinese](https://zh.wikipedia.org/zh-hans/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0)\n- [English](https://en.wikipedia.org/wiki/Lagrange_multiplier)\n\n","lastmodified":"2022-10-15T14:06:29.082498057Z","tags":null},"/notes/2021/2021.10/%E6%A2%AF%E5%BA%A6%E7%9A%84%E6%96%B9%E5%90%91%E6%98%AF%E5%93%AA%E4%B8%AA%E6%96%B9%E5%90%91-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%A2%AF%E5%BA%A6%E6%98%AF%E5%87%BD%E6%95%B0%E5%A2%9E%E9%95%BF%E6%9C%80%E5%BF%AB%E7%9A%84%E6%96%B9%E5%90%91":{"title":"梯度的方向是哪个方向-为什么梯度是函数增长最快的方向","content":"# 为什么梯度是函数增长最快的方向\n\n\u003cdiv align=\"right\"\u003e 2021-10-14\u003c/div\u003e\n\nTags: #Math \n\n看这篇文章可快速回顾高数:\nhttps://zhuanlan.zhihu.com/p/38525412\n\n## Key Idea\n- 对于任意方向一个增量 $\\Delta z=f\\left(x_{0}+t \\cos \\alpha, y_{0}+t \\sin \\alpha\\right)-f\\left(x_{0}, y_{0}\\right)$ \n- 函数沿此方向的变化率为:\n$$\n\\lim _{t \\rightarrow 0^{+}} \\frac{f\\left(x_{0}+t \\cos \\alpha, y_{0}+t \\sin \\alpha\\right)-f\\left(x_{0}, y_{0}\\right)}{t}=f_{x}\\left(x_{0}, y_{0}\\right) \\cos \\alpha+f_{y}\\left(x_{0}, y_{0}\\right) \\sin \\alpha\n$$\n\n- 由于上式可以看成两个向量的内积, 即以下两个向量:\n $\\mathbf{g}=\\left(f_{x}\\left(x_{0}, y_{0}\\right), f_{y}\\left(x_{0}, y_{0}\\right)\\right)$ \n $\\mathbf{e}_{l}=(\\cos \\alpha, \\sin \\alpha)$\n则:\n$f_{x}\\left(x_{0}, y_{0}\\right) \\cos \\alpha+f_{y}\\left(x_{0}, y_{0}\\right) \\sin \\alpha=\\mathbf{g} \\cdot \\mathbf{e}_{l}=\\left|\\mathbf{g} \\| \\mathbf{e}_{l}\\right| \\cos \\theta=\\textcolor[RGB]{203,77,73}{|\\mathbf{g}| \\cos \\theta}$\n\n其中， $\\theta$ 为 $\\mathbf{g}$ 和 $\\mathbf{e}_{l}$ 的夹角。所以根据夹角:\n- 当 $\\theta=0$ 时, 即 $\\mathbf{e}_{l}$ 和 $\\mathbf{g}$ 方向相同时，函数变化率最大，且在点 $\\left(x_{0}, y_{0}\\right)$ 处呈上升趋势;\n- 当 $\\theta=\\pi$ 时，即 $\\mathbf{e}_{l}$ 和 $\\mathrm{g}$ 方向相反时，函数变化率最大，且在点 $\\left(x_{0}, y_{0}\\right)$ 处呈下降趋势;\n\n而梯度的定义是:\n\n设二元函数 $z=f(x, y)$ 在平面区域D上具有一阶连续偏导数，则对于每一个点P $(x, y)$ 都可定出一个向量 $\\left\\{\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}\\right\\}=f_{x}(x, y) \\bar{i}+f_{y}(x, y) \\bar{j}$,该函数就称为函数 $z=f(x, y)$ 在点 $P(x, y)$ 的梯度, 记作$grad\\ f (x, y)$ 或 $\\nabla f(x, y)$,即有:\n$$grad\\ f (x, y)=\\nabla f(x, y)=\\left\\{\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}\\right\\}=f_{x}(x, y) \\vec{i}+f_{y}(x, y) \\vec{j}$$\n其中 $\\nabla=\\frac{\\partial}{\\partial x} \\vec{i}+\\frac{\\partial}{\\partial y} \\vec{j}$ 称为二维的) 向量微算子或$Nabla$算子, $\\nabla f=\\frac{\\partial f}{\\partial x} \\vec{i}+\\frac{\\partial f}{\\partial y} \\vec{j}$ 。\n\n所以梯度的方向就是向量$\\mathbf{g}$的方向, 方向导数在梯度方向取得最大值, 该最大值为梯度的模:\n$$|\\operatorname{grad} f(x, y)|=\\sqrt{\\left(\\frac{\\partial f}{\\partial x}\\right)^{2}+\\left(\\frac{\\partial f}{\\partial y}\\right)^{2}}$$","lastmodified":"2022-10-15T14:06:29.082498057Z","tags":null},"/notes/2021/2021.10/CMU15-445_3_Lecture_Note":{"title":"CMU15-445_3_Lecture_Note","content":"# 03 - Database Storage I\n\n\u003cdiv align=\"right\"\u003e 2021-10-20\u003c/div\u003e\n\nTags: #Database\n\n## Outline\nDifferent Layers of the whole system, One layer at a time, from bottom to top.\n![](notes/2021/2021.10/assets/img_2022-10-15-2.png)\n\n## Disk-Oriented Architecture\n- Not in Memory\n- Move data between non-volatile to volatile storage\n\t![](notes/2021/2021.10/assets/img_2022-10-15-3.png)\n\t- 注意寻址方式也在变, 下面的是 Block-Addressable, 上面的是Byte-Addressable\n\t- 下面的是Sequential的 上面的是Non-Sequential的\n\t- 我们关心下面的三层和上面的第一层(这门课), 因为下面的实在是太慢了, 先考虑下面的\n\t\t![](notes/2021/2021.10/assets/img_2022-10-15-4.png)\n\t- First hand material: Non-volatile Memory\n\t- 直观的理解不同层面的读取速度:\n\t\t![](notes/2021/2021.10/assets/img_2022-10-15-5.png)\n\t- **Goal:** Allow the DBMS to manage databases that exceed the amount of memory available. (Create an illusion that all the data is stored in memory )\n\nHow it works:\n![](notes/2021/2021.10/assets/img_2022-10-15-6.png)\nSyllabus: \n![](notes/2021/2021.10/assets/img_2022-10-15-7.png)\n\n## Why not use the OS?\n### Virtual Memory?\nThe OS only sees a bunch of reads and writes, the DBMS (almost) always wants to control things itself and can do a better job at it.\n\n**mmap**: the way virtual memory works:\n![](notes/2021/2021.10/assets/img_2022-10-15-8.png)\n\n**Problem #1:  How the DBMS represents the database in files on disk:**\n\n(Later) Problem #2: How the DBMS manages its memory and move data back-and-forth from disk. \n\n## Problem #1\n### File Storage\nThe DBMS stores a database as one or more files on disk, In the meanwhile,  The OS doesn't know anything about the contents of these files.\n\n#### Who do the work?\n**The storage manager**\n→ responsible for maintaining a database's files.\n→ Some do their own scheduling for reads and writes to improve spatial and temporal locality of pages.\n\n#### How?\nIt organizes the files as **a collection of pages**.\n\nA page is **a fixed-size block of data.**\n→ Most systems do NOT mix page types.\n→ Some systems require a page to be **self-contained.**\n\nEach page is given **a unique identifier.**\n→ The DBMS uses **an indirection layer** to map page ids to physical locations.\n\nThere are three different notions of \"pages\" in a DBMS:\n→ Hardware Page (usually 4KB)\n→ OS Page (usually 4KB)\n→ Database Page (512B-16KB)\n\n- 硬件层保证了4KB的原子性, (比如有一次写入操作失败了, 那么失败的范围一定是4KB最小单位的)\n\n#### Page Storage Architecture\n- Different DBMSs manage pages in files on disk in different ways.\n\t**→ Heap File Organization** \n\t→ Sequential / Sorted File Organization\n\t→ Hashing File Organization\n\n- At this point in the hierarchy we don't need to know anything about what is inside of the pages.\n\n##### Heap File Organization\n- an **unordered** collection of pages where tuples that are stored in **random order**.\n- Need **meta-data** to keep track of what pages exist and which ones have free space.\n- Two ways to represent a heap file:\n\t→ Linked List\n\t→ Page Directory\n\n###### Linked List\n![](notes/2021/2021.10/assets/img_2022-10-15-9.png)\n\n###### Page Directory\n![](notes/2021/2021.10/assets/img_2022-10-15-10.png) \n\n### Page Layout\n ![](notes/2021/2021.10/assets/img_2022-10-15-11.png)\n- We have a page header to store meta-data.\n#### How to organize the data stored inside of the page?\nAssume we only store tuples;\n\n**Two approaches:**\n\t→ Tuple-oriented\n\t→ Log-structured\n\t\n##### Tuple-oriented\n- It's a bad idea to store all the tuples linearly:\n![](notes/2021/2021.10/assets/img_2022-10-15-12.png)\n\n###### Slotted Pages\n- Instead, the most common layout scheme is called **slotted pages**.\n\t![](notes/2021/2021.10/assets/img_2022-10-15-13.png)\n- The slot array maps \"slots\" to the tuples' starting position offsets.\n- 增长方向:\n![](notes/2021/2021.10/assets/img_2022-10-15-14.png)\n\n- If you delete a slot in the middle, some DBMS clean the empty gap while others don't.\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/1D81vXw2T_w?start=3609\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n![](notes/2021/2021.10/assets/img_2022-10-15-15.png)\n\n##### Log Structured File Organization\n- Instead of storing tuples in pages, the DBMS only stores **log records**.\n\tE.g. Insertion Deletion Updates etc.\n\t![](notes/2021/2021.10/assets/img_2022-10-15-16.png)\n\n- To read a record, the DBMS scans the log backwards and \"recreates\" the tuple to find what it needs. \t\n\t![](notes/2021/2021.10/assets/img_2022-10-15-17.png)\n\n- Build indexes to allow it to jump to locations in the log.\n\t![](notes/2021/2021.10/assets/img_2022-10-15-18.png)\n\n- Periodically (周期性地) compact the log.\n\t![](notes/2021/2021.10/assets/img_2022-10-15-19.png)\t\n\n\n\n\n### Tuple Layout\n- A tuple is essentially **a sequence of bytes**.\n\t- It's the job of the DBMS to interpret those bytes into attribute types and values.\n#### Tuple header\n![](notes/2021/2021.10/assets/img_2022-10-15-20.png)\n\t→ Visibility info (concurrency control)\n\t→ Bit Map for NULL values.\n- We do not need to store meta-data about the schema.\n\n#### Tuple Data\n![](notes/2021/2021.10/assets/img_2022-10-15-21.png)\n- Stored in the order that you specify them when you create the table.\n- This is done for software engineering reasons.\n\n#### Denormalized Tuple Dataq\n![](notes/2021/2021.10/assets/img_2022-10-15-22.png)\n\n![](notes/2021/2021.10/assets/img_2022-10-15-23.png)\n\n![](notes/2021/2021.10/assets/img_2022-10-15-24.png)\n\n#### Record IDs\nIn order to keep track of individual tuples, each tuple is assigned **a unique record identifier**.\n\t→ Most common: `page_id + offset/slot`\n\t→ Can also contain file location info.\n\nAn application cannot rely on these ids to mean anything.\n\u003e It changes!\n\n\n\n\n\n","lastmodified":"2022-10-15T14:06:29.026497453Z","tags":null},"/notes/2021/2021.10/Call_StackStack_Frame%E7%9A%84%E6%9E%84%E6%88%90":{"title":"Call_Stack(Stack_Frame)的构成","content":"# Call Stack\n\n\u003cdiv align=\"right\"\u003e 2021-10-07\u003c/div\u003e\n\nTags: #Stack #OperatingSystem #Assembly\n\n两个非常好的视频:\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/jyRQpRHSYNY\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Gfmq2vGhWbw\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n","lastmodified":"2022-10-15T14:06:29.026497453Z","tags":null},"/notes/2021/2021.10/Compiler-2_Bottom-Up_Parsing-%E8%87%AA%E5%BA%95%E5%90%91%E4%B8%8A%E5%88%86%E6%9E%90":{"title":"Compiler-2_Bottom-Up_Parsing-自底向上分析","content":"# Bottom-Up Parsing\n\n\u003cdiv align=\"right\"\u003e 2021-10-30\u003c/div\u003e\n\nTags: #Compiler #Course \n\n- 自底向上分析是一种语法分析方法, 它从语法树的下边缘(即一堆终结符)开始, 逐步向上构建这个句子的推导过程\n\n- 一般来说, 自底向上分析比自上而下分析要更强大, 同时也更复杂.\n\n## Shift-reduce parsing[^1]\n- 移位-规约分析(Shift-Reduce Parsing)是自底向上分析的主流方法\n![](https://upload.wikimedia.org/wikipedia/en/thumb/0/0e/Shift-Reduce_Parse_Steps_Numbered.svg/265px-Shift-Reduce_Parse_Steps_Numbered.svg.png)\n\n- 有许多不同的移位规约分析方法: 比如: [\"算符优先分析\"](notes/2021/2021.10/Compiler-3_算符优先分析.md)和\"LR分析\"都属于移位规约分析.\n- This is a good illustration\n![](notes/2021/2021.10/assets/img_2022-10-15-25.png)[^2]\n\n- Operator Precedence grammar could be either ambiguous or unambiguous.[^2]\n### 课堂概念: 短语 直接短语 句柄 规范规约\n![](notes/2021/2021.10/assets/img_2022-10-15-26.png)\n规范规约: 每步都替换句柄的规约\n![](notes/2021/2021.10/assets/img_2022-10-15-27.png)\n句柄就是 **\"那个可以直接规约的东西\"**\n\n### stack-based shift-reduce parsing\n\n- 像table-driven predictive parser一样(比如LL(1)分析里面的那个表), 有的自底向上分析器使用栈来跟踪分析的位置, 使用分析表来决定接下来做什么.\n\n\n\n\n\n\n\n\n[^1]: https://en.wikipedia.org/wiki/Shift-reduce_parser\n[^2]: https://www.geeksforgeeks.org/role-of-operator-precedence-parser/","lastmodified":"2022-10-15T14:06:29.026497453Z","tags":null},"/notes/2021/2021.10/Compiler-3_%E7%AE%97%E7%AC%A6%E4%BC%98%E5%85%88%E5%88%86%E6%9E%90":{"title":"Compiler-3_算符优先分析","content":"# Operator-precedence grammar\n\n\u003cdiv align=\"right\"\u003e 2021-10-30\u003c/div\u003e\n\nTags: #Compiler #Course #FormalLanguage \n\n## 算符优先文法\n- 算符优先文法(OPG)是一种**有特殊性质的上下文无关文法**(CFG)\n- 它的特殊性质表现为:\n\t- 产生式右部不能为空 (即没有 $P\\rightarrow \\varepsilon$)\n\t- 产生式右边不能有两个连续的非终结符 (即没有 $P\\rightarrow \\cdots AB\\cdots$)\n- 上述规则使得我们可以定义终结符之间的\"优先级\"(Precedence), ([为什么?](#算符优先文法的核心特征))\n\n- 算符优先分析不是规范规约, 它的每一步不一定替换句柄\n\n- 在书里面的定义中, **算符文法**是不含两个连续非终结符的文法, **算符优先文法**则是终结符之间最多只有一种优先关系的**算符文法**\n\n\u003e ## Main differences with respect to LR parsers\n\u003e  - There is **no explicit state** associated to the parser (and thus no state  pushed on the stack)\n\u003e  - The decision of whether to shift or reduce is taken **based solely on the symbol on the top of the stack and the next input symbol** (and stored in a shift-reduce table)\n\u003e  - In case of reduction, the handle is the longest sequence at the top of stack matching the RHS of a rule\n\n### 算符优先文法的核心特征\n有几个概念我感到很难理解:\n- 为什么算符优先文法要叫\"算符优先\"文法? 如果是因为它定义了运算符之间的优先级, 那么:\n- 运算符的优先级怎样帮助我们进行语法分析?\n- 为什么\"产生式没有两个连续的终结符\"便可以定义运算符之间的优先级? 如果有了两个连续的终结符又会怎样干扰算符优先分析的正确进行?\n\n\u003e - 算符优先分析法是**仿效四则运算的计算过程而构造的一种语法分析方法**。算符优先分析法的关键是比较两个相继出现的终结符的优先级而决定应采取的动作。[^2]\n\u003e - \"仿效四则运算的计算过程\" - 何以见得?\n- 总之, 算符优先分析的核心概念还是像一团难以捉摸的雾\n\n- 仿照四则运算可否这样理解:\n![](notes/2021/2021.10/assets/img_2022-10-15-28.png)\n\n- Oberlin University的PPT[^5]为我们提供了一种新的思路:\n\n我们首先定义\"括号文法(parenthesis grammar)\":  \n- a) The right hand side of every rule is enclosed in parentheses.\n- b) Parentheses occur nowhere else.\n- c) No two rules have the same right hand side.\n![](notes/2021/2021.10/assets/img_2022-10-15-29.png)\n- **The parentheses make the prime phrases disjoint**. The handle is always the leftmost prime phrase.\n\n那么我们可以这样理解优先级文法: \n- Def. A simple **precedence grammar** is one **in which we can insert symbols \"\u003c\", \"=\", and \"\u003e\" to produce a language** (treating \"\u003c\" and \"\u003e\" as parentheses) that can be parsed like a parenthesized grammar.\n\n一个\"优先级文法\"产生的句子是一串嵌套的括号.\n一个素短语便是一对括号, 我们不断消除最左边的素短语, 暴露下面的(栈里面的)素短语\n嵌套的\"越深\"的括号\"堆的越高\", 我们parse的过程便是不断让这个山变矮的过程.\n![](notes/2021/2021.10/assets/img_2022-10-15-30.png)\n- 理解\"句子括号\":\nWe always start with \"\u003c\" and the first token on the stack, and at EOF push \"\u003e\". We should end with the Start symbol on the stack.\n\n- 理解算符文法: \n关于为什么没有两个连续非终结符, 便可以称之为\"算符文法\",我实在找不到更详细的资料了, 下面那篇论文里面的这段话似乎是其最初的定义:\n![](notes/2021/2021.10/assets/img_2022-10-15-31.png)\n注意这个性质; 如果产生式里面没有两个连续的非终结符, 那么任意句型里面也不可能有两个连续的非终结符:\n- 证明:![](notes/2021/2021.10/assets/img_2022-10-15-32.png)\n我猜测因为因为算术表达式里面都是算子和算符交替出现, 所以会有\"算符文法\"这个名字.\n\n综上, 结合\"算符\"文法和\"优先级\"文法, 便有了\"算符优先文法\"\n\n### History\nRobert W. Floyd. 1963. Syntactic Analysis and Operator Precedence. _J. ACM_ 10, 3 (July 1963), 316–333. DOI:https://doi.org/10.1145/321172.321179\n\n没有细致的考证过出处, 只是谷歌搜索到的, 但是根据文章内容猜测应当是出处.\n\n在文章里面, 我理解到了以下概念:\n- 理解\"**素短语**\":\n\t- A prime phrase is a phrase which contains at least one terminal character, but no prime phrase other than itself.\n\t- 素短语里面的\"素\", 指代的是Prime, 也就是素数的\"素\", 应当理解为\"最基本的, 最原初的\", 因为素数是\"最小不可分的数\"(算术基本定理[^3])\n- 理解课本里面的\"句子括号: #\"[^4]\n\t- 对于 $BxC$, 其中B, C为终结符\n\t\t![](notes/2021/2021.10/assets/img_2022-10-15-33.png)\n\t- 像上面这样记忆其实比\"把#当作一个终结符\"简单\n\n## 算符优先级\n**a ⋖ b** This means a “yields precedence to” b.  \n**a ⋗ b** This means a “takes precedence over” b.  \n**a ≐ b** This means a “has same precedence as” b.\n\n### 怎么确定优先级\n\n- **a等于b** \n\t当且仅当 文法G中含有形如$P→ \\cdots ab\\cdots$ 或 $P→\\cdots aQb\\cdots$的产生式；\n- **a小于b** \n\t当且仅当 G中含有形如$P→\\cdots aR\\cdots$ 的产生式，而$R  \\overset{+}{\\Rightarrow}b\\cdots$ 或$R\\overset{+}{\\Rightarrow}Qb\\cdots$ \n\t即语法树中, a在b的左上面\n- **a大于b** \n\t当且仅当 G中含有形如$P→\\cdots Rb\\cdots$ 的产生式，而$R\\overset{+}{\\Rightarrow}\\cdots a$或$R\\overset{+}{\\Rightarrow}\\cdots aQ$\n\t即语法树中, a在b的左下面\n\t\n\t在算符优先文法里面, 任意两个终结符 **至多满足一种关系**\n\n## 分析步骤\n![](notes/2021/2021.10/assets/img_2022-10-15-34.png)\n前三步都是在进行准备工作, 即构造后面要用到的算符优先表, \n同时, 确定优先级的过程也是验证这个文法是不是算符优先文法的过程\n\n### 检查是否有ε-产生式\n\n### FIRSTVT(NT), LASTVT(NT)\n- 构造对象: 所有非终结符\n- 定义:\n$$\\begin{aligned}\n\u0026\\operatorname{FIRSTVT}(\\mathrm{P})=\\left\\{\\mathrm{a} \\mid \\mathrm{P} \\stackrel{+}{\\Rightarrow} \\mathrm{a} \\cdots \\text { 或 }{\\mathrm{P}} \\stackrel{+}{\\Rightarrow} \\mathrm{Qa}\\cdots , \\mathrm{a} \\in \\mathrm{V}_{\\mathrm{T}} \\text { 而 } \\mathrm{Q} \\in \\mathrm{V}_{\\mathrm{N}}\\right\\} \\\\\n\u0026\\mathrm{LASTVT}(\\mathrm{P})=\\left\\{\\mathrm{a} \\mid \\mathrm{P} \\stackrel{+}{\\Rightarrow} \\cdots \\mathrm{a} \\text { 或 } \\mathrm{P} \\stackrel{+}{\\Rightarrow} \\cdots \\mathrm{aQ}, \\mathrm{a} \\in \\mathrm{V}_{\\mathrm{T}} \\text { 而 } \\mathrm{Q} \\in \\mathrm{V}_{\\mathrm{N}}\\right\\}\n\\end{aligned}$$\n- FIRSTVT: 非终结符的所有第一个(FIRST)终结符(VT)\n- LASTVT: 非终结符的所有最后一个(LAST)终结符(VT)\n\n#### 如何构造 FIRSTVT(NT), LASTVT(NT)\n- 递归构造\n\n##### FIRSTVT(B)\n- 首先所有产生式$B→a\\cdots$,  或者  $B→Ca\\cdots$, 有 $a\\in FIRSTVT(B)$\n- 然后对于所有的$B→Ca\\cdots$,  有$FIRSTVT(B) = FIRSTVT(C)\\cup FIRSTVT(B)$\n\n这样的话可能会套很多层, 书上采用的方法是用一个栈:\n![](notes/2021/2021.10/assets/img_2022-10-15-35.png)\n\n- 首先, 将直接能够看出来的元素在表中标记, 并且入栈.\n![](notes/2021/2021.10/assets/img_2022-10-15-36.png)\n- 然后, 将栈顶元素$(C, d)$出栈, 如果C是某一个产生式的第一个非终结符(比如$B→Ca\\cdots$), 那么将$d$加入到$FIRSTVT(B)$里面.\n- 标记$(B, d)$, 同时$(B, d)$入栈\n- 重复, 直到栈空\n\n理解FIRSTVT(B)\n![500](notes/2021/2021.10/assets/img_2022-10-15-37.png)\n理解这种\"栈中元素对\"的更新方式\n![500](notes/2021/2021.10/assets/img_2022-10-15-38.png)\n\n###### LASTVT(B)\n类似的\n- 首先所有产生式$B→\\cdots a$,  或者  $B→\\cdots aC$, 有 $a\\in LASTVT(B)$\n- 然后对于所有的$B→\\cdots aC$,  有$LASTVT(B) = LASTVT(C)\\cup LASTVT(B)$\n\n### 构造优先级表\n\u003e 注意区分: 这个表是构造FIRSTVT 和 LASTVT 的, 不是优先级表\n\u003e ![300](notes/2021/2021.10/assets/img_2022-10-15-39.png)\n\n然后我们根据FIRSTVT 和 LASTVT 来构造优先级表\n![500](notes/2021/2021.10/assets/img_2022-10-15-40.png)\n![600](notes/2021/2021.10/assets/img_2022-10-15-41.png)\n构造出来长这样\n![](notes/2021/2021.10/assets/img_2022-10-15-42.png)\n\n#### 特别注意!\n算符优先级不满足交换律, 所以$a\\lessdot b\\nRightarrow b\\gtrdot a$   \n**所以这个表格并不是反对称矩阵!!**\n要分清横纵轴, 在课本和课件里面, 都是\n![300](notes/2021/2021.10/assets/img_2022-10-15-43.png)\n\n\u003e 课本里我们引入#符号表示句子的括号\n\u003e 即一开始 在文法中添加E→#E# , E为开始符号, \n\u003e 容易推出: \n\u003e - \\# ⋖ FIRSTVT(E)\n\u003e - LASTVT(E) ⋗ \\# \n\u003e - \\# ≐ \\# \n%%\u003e ![400](notes/2021/2021.10/assets/img_2022-10-15-44.png)%%\n\u003e ![](notes/2021/2021.10/assets/img_2022-10-15-45.png)\n\u003e 课件里面没有写,  但是在分析具体的句型的时候需要记住\n\n\n### 最左素短语—算符优先分析中的可归约串\n- 素短语\n至少含有一个终结符且除它自身之外不含有任何更小的素短语。\n(语法分析树里面至少含有一个终结符的最小子树)\n- 最左素短语\n处于句型最左边的那个素短语。\n![600](notes/2021/2021.10/assets/img_2022-10-15-46.png)\n在算符优先句型里面, 句型一定是以下格式:\n$\\#N_1\\ a_1\\ N_2\\ a_2\\ \\cdots N_n\\ a_n\\ N_{n+1} \\#$ \n其中: ${a}_{{i}} \\in {V}_{{T}}, {N}_{{i}} \\in {V}_{{N}}$ (非终结符可有可无, 但是一定不会挨着)\n\n句型是这种形式是算符文法的定义造成的, 算符文法不允许出现两个连续的非终结符.\n\n一个例子:\n![500](notes/2021/2021.10/assets/img_2022-10-15-47.png)\n直观上这样理解:\n![500](notes/2021/2021.10/assets/img_2022-10-15-48.png)\n### Start Parsing!\n![算符优先算法 解析](notes/2021/2021.10/assets/img_2022-10-15-49.png)\n一个具体的例子:\n![Operator_Precedence_Parse](notes/2021/2021.10/assets/Operator_Precedence_Parse.pdf)\n\n\n需要注意的是:\n- 算符优先分析在规约这一步上面, 得到的是\"某个非终结符N\", 这个非终结符是不重要的, 这在下图中可以清晰的看出:\n- ![](notes/2021/2021.10/assets/Pasted%20image%2020211030172247.png)\n- 上图同时说明了 算符优先分析不等价于规范归约，未必是严格的最左归约(从树里面可以看出省略了一些步骤, 即跳过了所有单非产生式所对应的归约步骤), 所以归约速度快，但容易误判(因为忽略非终结符在归约过程中的作用，存在某种危险性，可能导致把本来不是句子的输入串误认为是句子)\n- 这也同时说明了算符优先分析没有\"状态(State)\"一说, 即没有状态入栈, 推导过程完全凭借栈顶终结符与下一个终结符\n\n易错:\n- 优先级关系是每一步都有的\n![400](notes/2021/2021.10/assets/Pasted%20image%2020211030211829.png)\n- 我的:\n- ![400](notes/2021/2021.10/assets/Pasted%20image%2020211030211857.png)\n\n### 优先函数\n实际应用中, 考虑到存储优先表的开销太大, 我们常常用优先函数代替优先表:\n![](notes/2021/2021.10/assets/Pasted%20image%2020211030193845.png)\n\n函数f 称为入栈优先函数， g 称为比较优先函数\n\n- 若 $\\theta_{1}⋖\\theta_{2}\\quad$ 则 $\\quad f(\\theta_1)\u003cg(\\theta_2)$\n- 若 $\\theta_{1}≐\\theta_{2}\\quad$ 则 $\\quad f(\\theta_{1})=g(\\theta_2)$\n- 若 $\\theta_{1}⋗\\theta_{2}\\quad$ 则 $\\quad f(\\theta_{1})\u003eg(\\theta_{2})$\n\n注意: \n- 不是每一个优先表都有对应的优先函数\n- 原来优先表为空的项(不存在优先关系的终结符对),  转化为优先函数以后, 与自然数相对应，变成可以比较的。所以要进行一些特殊的判断\n- 优先函数不唯一，只要存在一对，必存在无穷对优先函数。\n\n![400](notes/2021/2021.10/assets/Pasted%20image%2020211030194053.png)\n\n#### 怎么画\n\n- 如果a 的优先级高于或等于b，则从 $f_a$ 至 $g_b$ 画一条有向边\n- 如果a 的优先级低于或等于b，则从 $g_b$ 至 $f_a$ 画一条有向边\n- 注意相等的话要画来回两条边\n- 每个结点赋予一个数, 该数等于从该结点出发可达结点（包括出发结点本身在内）的个数.\n- 因为可能有的表没有优先函数, 所以还要检查是否有矛盾: 是否有矛盾的优先级, 即: 是否有环?\n- An Illustration:\n- ![](notes/2021/2021.10/assets/Pasted%20image%2020211105185333.png)[^1]\n\n可以证明：\n若$a≐b$, 则$f(a)=g(b)$; 若$a⋖ b$, 则$f(a)\u003cg(b)$; 若$a⋗b$, 则$f(a)\u003eg(b)$\n\n\n\n\n---\n课件上完全没有讲出错处理\n\n\n[^1]: https://www.geeksforgeeks.org/role-of-operator-precedence-parser/\n[^2]: https://moyangsensei.github.io/2019/05/20/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%EF%BC%9A%E7%AE%97%E7%AC%A6%E4%BC%98%E5%85%88%E5%88%86%E6%9E%90/\n[^3]: 不要混淆了\"算术基本定理\"与\"代数基本定理\"哦\n[^4]: 我觉得, 这是一个很奇怪的称呼, 完全没有必要又造一个专有名词出来\n[^5]: https://www.cs.oberlin.edu/~bob/cs331/Class%20Notes/February/February%2017/Precedence%20Grammars.pdf","lastmodified":"2022-10-15T14:06:29.026497453Z","tags":null},"/notes/2021/2021.10/Compiler-4-1_%E4%BB%80%E4%B9%88%E6%98%AF-LR-%E5%88%86%E6%9E%90":{"title":"Compiler-4-1_什么是 LR 分析","content":"## 什么是 LR 分析\n- LR 分析方法是一种自底向上的分析方法: 它从终结符开始, 从左到右, 逐步寻找句柄进行归约. 从这个角度看, LR 分析相当于一种高效的句柄查找方法.\n\n- LR 分析比 LL 分析更强大.\n\u003e LL (k) 分析技术的一个弱点是，它在仅仅看到右部的前 k 个单词时就必须预测要使用的是哪一个产生式。另一种更有效的分析方法是 LR (k) 分析，它可以将这种判断推迟至己看到与正在考虑的这个产生式的整个右部对应的输入单词以后（多于 K 个单词） 。[^1]\n\n- 字面上来说, LR (k) 分析代表: Left-to-right parse, (Reversed) Right-most derivation, k-token look ahead\n\n- LR 分析是一种表驱动的移进归约分析方法 (Table-driven \u0026 shift-reduce), 所以在进行分析的时候我们将语法规则编码到一个表里面, 根据表来选择不同的移进归约操作.\n\t- 分析表示意图:\n\t\t![](notes/2021/2021.10/assets/img_2022-10-15-50.png)\n\t\t上面这个表实际上是一个 DFA 的编码表:\n\t\t![](notes/2021/2021.10/assets/img_2022-10-15-51.png)\n\t\tLR 分析器将\"栈顶句柄状态\"转化为 (抽象为) 对应的\"DFA 状态\". 在分析时, 有时读入终结符, 执行\"Shift\"操作; 有时读入非终结符, 执行\"Goto\"操作; 有时从栈里面弹出句柄, 执行\"Reduce\"操作. LR 分析将栈与 DFA 结合, 一起进行分析.\n\t\t- **Shift**: Push Terminal to the stack, shift to the next state in DFA \n\t\t- **Goto**: Push Non-terminal to the stack, go to that corresponding state in DFA\n\t\t- **Reduce**: Pop the handle out of stack, also pop the corresponding states out of stack. Intuitively, this means \"retreat\" to the state before the handle. (This may pass through multiple states) \n\t- 要注意我们分析的 LR 语言是 CFG 语言的子类, DFA 等价于正则语言, 是 LR 语言的一个子类. 所以我们需要结合栈来增强 DFA 的表现力.\n\t\n\t- 分析操作示意图:\n\t\t![400](notes/2021/2021.10/assets/img_2022-10-15-52.png)\n\n下面我们将从按照 $LR(0) \\rightarrow SLR \\rightarrow LR(1) \\rightarrow LALR(1)$ 的顺序来梳理 LR 文法的思想.[^2]\n\n### 语法分析: 文法层次结构\n![500](notes/2021/2021.10/assets/img_2022-10-15-53.png)[^1]\n\n\n[^1]: 虎书第三章\n[^2]: 这也是虎书的顺序, 我感觉虎书适合第二遍看, 因为它写的十分精炼, 在第一遍学习掌握了大概方法以后, 第二遍看虎书可以快速抓住核心思想. (把书写这么精炼又清晰真的好厉害了)","lastmodified":"2022-10-15T14:06:29.026497453Z","tags":null},"/notes/2021/2021.10/Compiler-4-2_LR0_Parse":{"title":"Compiler-4-2_LR(0)_Parse","content":"## LR (0)  Parse\n- 在分析 LR (0) 语法时, 我们只需要观察栈顶的元素, 根据栈顶元素来决定如何移进/归约. 这也是 LR (0) 里面\"0\"的含义 - 0 look ahead.\n\n### LR (0) 项目 - LR (0) Item\n- LR 分析表的构造以\"LR Item\"为基础, \"LR Item\"可以理解为\"在寻找句柄的过程中的一种推断\".\n- 一个 LR (0) 项目由两部分组成: 一个产生式和一个指示栈顶位置的句点:\n\t$$E\\rightarrow \\alpha\\cdot \\beta$$ \n\t- 比如: $E\\rightarrow \\alpha\\cdot \\beta$ 代表: 我们已经识别到了 $\\alpha$ ($\\alpha$ 已经在栈里面了), 如果后面接下来的部分是 $\\beta$, 那么就可以确定是句柄 $E\\rightarrow \\alpha\\beta$ 了\n\t- 即, 每一个项都代表着\"句柄的一种可能性\"\n\n- 通常, 在一个项目集合里面有很多不同的项目: \n\t- 比如: 下图高亮部分是一个项目集合\n\t\t![](notes/2021/2021.10/assets/img_2022-10-15-54.png)\n\t\t- 同一个集合里的项目看起来很不同, 但是他们\"抽象的共同点\"让我们将其聚合为 DFA 里面的一个等价类, 后面我们将叙述怎样寻找这种\"共同点\".\n\t\t- Intuitively, 一个状态里面有许多不同的项的意思是: 基于现在栈里面的情况, 这些句柄是可能出现的.\n\n### 构建转化规则: DFA\n这涉及到两个操作\n- $\\mathrm{Closure(一个项目集合)}\\ \\Rightarrow$ 求闭包, 即求出一个封闭的 LR 项目集合, 十分类似于将 NFA 转化为 DFA 里面的 $\\varepsilon-$ 闭包 \n- $\\mathrm{Goto(一个项目集合, 某个符号)}\\ \\Rightarrow$ 看看当前状态输入 (吃掉) 某个符号后, 会转移到哪个状态\n\n\u003e 这里可以联系形式语言与自动机的知识: 构成 DFA 有两种不同的思路: 我们可以先根据产生式构造一个 LR 项的 NFA, 然后再转化为 DFA[^3]; 或者我们也可以直接构造 DFA. \n\u003e 不过 LR 分析的时候我们通常都采用后一种方法: 即从初始状态出发, 不断地根据所有可能的输入求闭包, 生成新的状态集, 直到状态数不变. \n\n下面我们详细解释这两种操作:\n\n#### Closure (a set of items) \n这是算法:\n![400](notes/2021/2021.10/assets/img_2022-10-15-55.png)\n\n直观的来说, 这通常会让这个项 \n$$\\mathrm{S} \\rightarrow . \\mathrm{L} \\alpha$$\n变成这个集合:\n$$\\begin{aligned}\n\u0026\\mathrm{S} \\rightarrow . \\mathrm{L} \\alpha \\\\\n\u0026\\mathrm{L} \\rightarrow .(\\mathrm{L}) \\\\\n\u0026\\mathrm{L} \\rightarrow . \\mathrm{\\beta}\n\\end{aligned}$$\n可是, 为什么下面这两个新加进来的项会和上面的项拥有相同的地位呢?\n\n我们从闭包的角度来考虑这个问题:[^4]\n- 对于这样一个转换, 可能有两种情况:\n\t![](notes/2021/2021.10/assets/img_2022-10-15-56.png)\n\t- 如果 X 是一个终结符, 那么这个转移表示将这个终结符压入栈顶, 句点前移.\n\t- 如果 X 是一个非终结符, 情况则变得复杂起来, 因为我们分析的句子肯定是一个终结符串, 不会出现非终结符. \n\t\t进一步考虑, 如果出现了非终结符 X 入栈, 那么代表着前面一定出现了对于产生式 $X\\rightarrow \\beta$ 的归约, 这个归约将终结符串 $\\beta$ 换成了非终结符 $X$. 要出现这样的归约, 需要首先识别出可以产生 X 的句柄 $\\beta$. 我们将这种思想表示为下图:\n\t\t![](notes/2021/2021.10/assets/img_2022-10-15-57.png)\n\t\t求 $\\mathrm{\\varepsilon-CLOSURE}$ 的时候就将两个项合并到同一个状态里面去了\n\n所以我们上面的例子可以这样理解:\n![CLOSURE](notes/2021/2021.10/assets/CLOSURE.svg)\n\n#### Goto (a set, a character) \n这是算法:\n![300](notes/2021/2021.10/assets/Pasted%20image%2020211106165501.png)\n即对于每一个 LR 项目, 尝试向后移动一个符号 (包括终结符和非终结符), 然后再取这个转移的闭包.\n\n#### 构建 DFA\n说明了两个基本操作, 接下来便是从初始状态出发, 不断\"转移, 取闭包, 转移, 取闭包......\"直到整个 DFA 没有变化. 这和 NFA 转 DFA 时候的思想很相似.\n\n![590](notes/2021/2021.10/assets/Pasted%20image%2020211106165953.png)\n\n- 这时, 我们通常会得到一个类似于下图的 DFA:\n\t![](notes/2021/2021.10/assets/Pasted%20image%2020211106170041.png)\n\t- 里面包含了 Shift 和 Goto 的所有信息, 但是还没有说明需要 Reduce 的时候该怎么办 (构造转移表的时候会解决这个问题). \n\n- 在分析的时候, 使用 DFA 比使用转移表直观多了.\n\t\n### 构建转移表 DFA → ACTION \u0026 GOTO tables\n- 对于 DFA 里面的每一条边:\n\t- 如果这个转移是终结符, 那么在 ACTION 表里面填上 **\"Shift \u003c目标状态\u003e\"**\n\t- 如果这个转移是非终结符, 那么在 GOTO 表里面填上 **\"Goto \u003c目标状态\u003e\"**\n- 对于包含形如 $A \\rightarrow \\gamma\\  \\textbf.$ 的状态 (即这个状态包含一个 LR 项, 这个 LR 项成功识别了一个句柄), 在 ACTION 表对应位置写上 **\"Reduce \u003c产生式序号\u003e\"**\n- 对于包含 S′ → S.$ 项的状态, 我们在 ACTION 表填入 **\"Accept\"**, 表示分析成功. (成功识别了第一个产生式, 即产生初始符号的产生式: S′ → S.$ )\n\n![500](notes/2021/2021.10/assets/Pasted%20image%2020211106171144.png)\n\n### Start Parsing!\n- 我们先向符号栈里面推入符号$, 表示句子的末尾, 向状态栈里面推入状态 0, 表示初始状态.\n- 然后我们开始逐个读入符号, 根据表 (DFA) 来进行**Shift**\n- 对于**Reduce**, 我们需要 pop 符号栈里面的句柄和状态栈里面对应的状态, (在 DFA 里面回退到句柄之前的状态), 然后再压入产生式右侧的非终结符, 进行**Goto**操作.\n- 一直在状态之间转移, 直到\n\t- 遇到 Accept → 分析成功, \n\t- 或者, 遇到表中为空的项 (DFA 里面不存在的边) → 存在语法错误\n\n\u003e 相比维护符号栈和状态栈两个栈, 我们也可以只维护一个栈, 将状态和对应的符号都压到这个栈里面, 只是需要注意这样 reduce 的时候需要 pop 两倍于 RHS (产生式右侧) 长度的元素.\n\n- 以上便是 LR (0) 文法的分析过程.\n\n### LR (0) 文法的不足\n\n- 与此同时, LR (0) 文法也有一些不足之处:\n\n- LR (0) 文法的表现能力相对较弱, 几乎所有“真正的”文法都不是 LR ( 0 ) 的.[^5]\n- 所以, 很多文法里面, LR (0) 的分析表里面会出现移进-归约冲突 (Shift Reduce Conflict) 或者归约-归约冲突 (Reduce-Reduce Conflict)\n\n#### 移进-归约冲突\n- LR (0) 分析表的每一行要么为 Shift 或空, 要么全部为某一个归约, 也就是: DFA 的每一个状态要是有完全识别句柄的项(Reduce)就不能有终结符指出去 (不能 Shift 出去)\n![](notes/2021/2021.10/assets/LR(0)移进规约冲突.svg)\n要是有一个地方又有 Reduce 又有 Shift, 就是一个移进归约冲突.\n\n#### 归约-归约冲突\n同理, 要是有个状态里面有两个不同的完全识别的句柄: $P\\rightarrow\\alpha\\ \\cdot$ 与 $Q\\rightarrow\\beta\\ \\cdot$ 那么就出现了归约-归约冲突.\n\n\n- 注意只可能出现 Shift-Reduce Conflict, 不可能出现 Goto-Reduce Conflict.\n\t- 这是因为: \n\t\t1. 假设现在是状态 a, 如果下一个读入的是非终结符 N, 那么之前一定有一个归约 $N\\rightarrow \\alpha$ 使得 DFA 回退到了状态 a. \n\t\t2. 假设现在状态 a 有一个 Goto-Reduce Conflict, 那么意味着现在栈顶一定又有一个完全识别的句柄 $\\beta$, 构成一个归约 $P\\rightarrow\\beta$. \n\t\t3. 这意味着在归约 $N\\rightarrow \\alpha$ 之前, 栈里面的状态是:  $\\ \\cdots\\beta \\alpha栈顶$.\n\t\t4. 但是这是不可能的, 我们构造 DFA 的方式决定了 $\\beta$ 一定一出现就会被归约掉, 不可能被完整的放到栈里面去.\n\n\n[^3]: 见CCP\u0026P Page155\n[^4]: 见CCP\u0026P 5.2.2节\n[^5]: 见 CCP\u0026P 5.2.3 节","lastmodified":"2022-10-15T14:06:29.026497453Z","tags":null},"/notes/2021/2021.10/Compiler-4-3_SLR_parse":{"title":"Compiler-4-3_SLR_parse","content":"## SLR parse\n\n- 在 LR (0) 里面, Reduce 完全占据了一个状态, 这造成了极大的浪费:\n\t- 回忆 LL 分析里面的 FOLLOW 集合: FOLLOW (A) = {可以立即跟在 A 后面的所有终结符}\n\t- 一个状态需要 Reduce 代表着出现了一个完全识别句柄的项: $P\\rightarrow\\alpha\\ \\cdot$, 但是仔细想想: 我们完全可以再向后多看一个符号, 只有在下一个符号属于 FOLLOW (P) 的时候, 我们才应该规约, \n\t\t![p后面要有Follow(p)](notes/2021/2021.10/assets/img_2022-10-15-58.png)\n\t\t- 如上图所示, 要是 b 不在 FOLLOW (P) 里面的话, α就不应该是 $P\\rightarrow\\alpha$ 的句柄了, 应该继续 shift, 读进 b, 寻找句柄 $Q\\rightarrow\\alpha\\ b\\cdots$\n\nSLR (1) 分析法就是利用 Look ahead, 看看下一个符号和规约产生式的非终结符是不是一致的, 只有在一致的时候才进行规约, 消除了部分的冲突.\n\n### DFA 的构造\nSLR (k) 分析在 DFA 的构造上和 LR (0) 分析法完全一致.\n\n### 分析表的构造\nSLR (1) 在构造分析表的时候与 LR (0) 的唯一不同就是 Reduce 的填入方式:\n- LR (0) 观察每一个 set 里面的项, 只要出现完全识别句柄的项 $P\\rightarrow\\alpha\\ \\cdot$ , 就将这个状态全部标成 Reduce $P\\rightarrow\\alpha$\n- SLR (1) 还要观察 Follow (P), 只在属于 Follow (P) 的终结符下面填入 Reduce\n\n对比:\n- 文法:\n\t![400](notes/2021/2021.10/assets/img_2022-10-15-59.png)\n- SLR (1) 分析表:\n\t![300](notes/2021/2021.10/assets/img_2022-10-15-60.png)\n- LR (0) 分析表:\n\t![400](notes/2021/2021.10/assets/img_2022-10-15-61.png)\n\n\n### SLR (k) - k\u003e1\nSLR (k) 顾名思义就是在填表的时候向后观察 k 个符号, 怎样观察呢?\n- 除了在 Reduce 的时候观察, 还需要在 Shift 之前观察\n\n\u003e #### First Follow 集合的推广\n\u003e $First(A)$ 是 A 推出的所有终结符串里面的第一个终结符\n\u003e $First_k(A)$ 即 A 推出的所有终结符串里面的前 k 个终结符 (所有长度为 k 的前缀)\n\u003e \n\u003e 同理, $Follow_k(A)$ 即 A 推出的所有终结符串里面的后 k 个终结符 (所有长度为 k 的后缀)\n\n- **Shift:** \n\t对于 $A→α.Xβ$ (其中 X 是一个符号, $β$ 是产生式剩下的部分)\n\t现在输入串是 $Xw\\cdots$, 其中 $|Xw|=k$. \n\t如果 $Xw\\in First_k(Xβ)$ 那么就将 $X$ 读入, 将相应的 Shift 项填入表中.\n\t换句话说, 如果 $Xw\\in First_k(Xβ)$, 说明现在输入的前 k 个都可能被这个产生式推出, 所以我们读入 $X$.\n- **Reduce:**\n\t对于 $P→α.$ \n\t现在输入串是 $w\\cdots$, 其中 $w$ 是前 k 个终结符.\n\t如果 $w\\in Follow_k(P)$, 即输入串接下来的前 k 个字符可以立即跟在 P 后面, 那么将 Reduce $P→α$ 填入表中相应位置\n\nSLR (k) 比 SLR (1) 更强大, 但是也比 SLR (1) 复杂得多. 在实际应用中, 我们通常采用后面介绍的 LALR 分析方法.\n\n### SLR 分析方法的不足\n- SLR 的不足在于: 它在分析的时候利用了 Look ahead, 但是在构建 DFA 的时候却没有考虑到 Look Head.\n- 考虑下面这个 SLR (1) 的例子: \n\t\t$$\\begin{aligned}\n\t\u0026S \\rightarrow \\mathbf{i} \\boldsymbol{d} \\mid V:=E \\\\\n\t\u0026V \\rightarrow \\mathbf{i} \\boldsymbol{d} \\\\\n\t\u0026E \\rightarrow V \\mid \\boldsymbol{n}\n\t\\end{aligned}$$\n\t- 其中 S 代表赋值语句 (Assign-Statement), V 代表变量 (Variable), E 代表表达式 (Expression)\n\t- 1. 构造状态 0:\n\t\t$$\\begin{aligned}\n\t\u0026S^{\\prime} \\rightarrow . S \\\\\n\t\u0026S \\rightarrow . id \\\\\n\t\u0026S \\rightarrow . V:=E \\\\\n\t\u0026V \\rightarrow . id\n\t\\end{aligned}$$\n\t\t2. 状态 0 输入终结符 $id$,  shift 得到状态 1:\n\t\t\t$$\\begin{aligned}\n\t\t\u0026S \\rightarrow id . \\\\\n\t\t\u0026V \\rightarrow id . \n\t\t\\end{aligned}$$\n\t\t3. $Follow (S)=\\{\\$\\}, Follow (V)=\\{\\$, :=\\}$, 所以这两个项都会在 $符号处产生一个规约, 这是一个 Reduce-Reduce 冲突.\n\t- 但是这个冲突实际上是不存在的, **在读入 $:=$ 之前**, $V \\rightarrow id .$ 不应该在前瞻符号为 $ 的时候进行规约:\n\t- ![200](notes/2021/2021.10/assets/img_2022-10-15-62.png)\n\nLR (1) 分析将前瞻符号的判断整合到 DFA 的构造过程中去, 解决了这个问题.","lastmodified":"2022-10-15T14:06:29.026497453Z","tags":null},"/notes/2021/2021.10/Compiler-4-4_LR1_%E5%88%86%E6%9E%90":{"title":"Compiler-4-4_LR(1)_分析","content":"## LR (1) 分析\nLR (1) 分析在一开始构建 DFA 的时候便考虑到了前瞻符号, 使得其在结构上更为强大.\n\n### LR (1) 项\n在 LR (0) 项的基础上, LR (1) 项添加了前瞻符号:\n$$A\\rightarrow \\alpha\\cdot \\beta, a $$\n- 其中 $a$ 是前瞻符号\n\n### LR(1)分析: 构造DFA\n- 构造DFA的目标和之前一样: 寻找LR(1)项的规范集, 并且构造这些集合之间的转换关系.\n\n- 尽管我们在实际构造的时候往往直接构造DFA, 但是从NFA的角度来观察构造过程有助于我们理解LR(1)的本质特征:\n\n- 对于LR(1), 状态转换在构造时最大的不同是: $\\varepsilon$-转移需要考虑**前瞻符号的变化**.\n\n#### LR(1): NFA的状态转移\n- 我们先考虑非空的转移:\n\t$$[A\\rightarrow \\alpha\\cdot X\\beta,\\ a]\\overset{shift(X)}\\longrightarrow[A\\rightarrow \\alpha X\\cdot\\beta,\\ a]$$ (X是终结符或非终结符) \n\t- 在上面, 项$[A\\rightarrow \\alpha\\cdot X\\beta,\\ a]$读入了符号$X$(将$X$压入符号栈顶), 转移到了新状态$[A\\rightarrow \\alpha X\\cdot\\beta,\\ a]$, **前瞻符号不变**.\n\n注意到除了不变的前瞻符号a, 这和LR(0)里面几乎相同.\n\n- 接下来考虑$\\varepsilon$-转移, 即构造等价集CLosure(A)的时候涉及到的转移:\n\t$$[A\\rightarrow \\alpha\\cdot B\\gamma,\\ a]\\overset{\\varepsilon}\n\t\\longrightarrow\n\t[B\\rightarrow \\cdot\\beta,\\ b_i\\ ]$$  \n\t其中, $b_i\\in First(\\gamma a)$, 所以这其实是多个$\\varepsilon$-转移:\n\t![LR(1) Transition Closure](notes/2021/2021.10/assets/LR(1)%20Transition%20Closure.svg)\n%%$$\\begin{align}[A\\rightarrow \\alpha\\cdot B\\gamma,\\ a]\u0026\\overset{\\varepsilon}\\longrightarrow\t[B\\rightarrow \\cdot\\beta,\\ b_1\\ ]\\\\[A\\rightarrow \\alpha\\cdot B\\gamma,\\ a]\u0026\\overset{\\varepsilon}\\longrightarrow\t[B\\rightarrow \\cdot\\beta,\\ b_2\\ ]\\\\\u0026\\quad\\vdots\\\\[A\\rightarrow \\alpha\\cdot B\\gamma,\\ a]\u0026\\overset{\\varepsilon}\\longrightarrow\t[B\\rightarrow \\cdot\\beta,\\ b_n\\ ]\\end{align}$$%%\n- 与LR(0)不同, 我们在构造空转移的时候还需要考虑前瞻符号的变化, 即$b_i$可以是什么符号:\n\t- 首先, 因为有产生式$A\\rightarrow \\alpha B\\gamma$, 所以$b_i\\in First(\\gamma)$, 又如果$\\gamma$是可为空的, 那么$b_i$可能是$a$. 综上, $b_i\\in First(\\gamma a)$\n\n\n- 与SLR(1)不同, 我们在考虑前瞻符号的时候还考虑了前一个LR(1)项 (里面的 $\\gamma$ 和 $a$ ), 而不是将整个$Follow(B)$写上去.\n\t- 我们考虑$First(\\gamma a)$ 与$Follow(B)$ 的关系: \n\t\t首先$A\\rightarrow \\alpha B\\gamma$ , 所以$First(\\gamma)\\subset Follow(B)$. 如果$\\gamma$是可为空的, 那么有$Follow(A)\\subset Follow(B)$ , 又 $a\\in Follow(A)$. 所以$First(\\gamma a)\\subseteq Follow(B)$. \n- 注意: 因为$B$还可能出现在其他产生式里面, 所以$First(\\gamma a)$很有可能是$Follow(B)$的真子集, 这正是LR(1)文法的强大之处.\n\t\t*(The power of the general LR(1) method lies in the fact that the set $First(\\gamma a)$ may be a proper subset of $Follow(B)$. [^6])*\n\n##### 再论SLR(1)\nSLR(1)分析法虽然在构造DFA的时候没有考虑前瞻符号, 但是我们可以这样类比LR(1):\n- 令$Follow(B)=\\{b_j, c_i\\mid b_j\\in First(\\gamma a), c_i\\notin First(\\gamma a)\\}$, 则我们构造的项目集既包括$[B\\rightarrow \\cdot\\beta,\\ b_j\\ ]$, 又包括 $[B\\rightarrow \\cdot\\beta,\\ c_i\\ ]$, 如下图左边所示:\n\n\t![SLR(1) Transition States](notes/2021/2021.10/assets/SLR(1)%20Transition%20States.svg)\n\n- 在上图中, 假设$\\beta$是一个终结符,  填表的时候在$Follow(B)$对应的地方都填上Reduce, 其实就相当于将图中右边所有的项都当成要规约的项.\n\n- 相比之下, 如果我们仅仅包括$First(\\gamma a)$里面的项, 则相当于去除上图里面所有黄色的项. 基于$[A\\rightarrow\\alpha\\cdot B\\gamma,\\ a]$这个前提, 这些黄色的项实际上是不可能被规约的, 可以见得, LR(1)分析有着\"更精准的句柄识别能力\", 相比SLR(1)文法, LR(1)文法有着更强的表现力. \n\n\t![LR(1) Transition States](notes/2021/2021.10/assets/LR(1)%20Transition%20States.svg)\n\n### 构造DFA\n我们还是从初始状态出发, 逐步构建闭包, 直到整个图形不再变化.\n\n- 初始状态:\n\t$$[S^\\prime\\rightarrow\\cdot S, \\$]$$\n\t其中$代表句子的结束.\n\n- 下面是取闭包的操作与转移操作的算法:\n![](notes/2021/2021.10/assets/Pasted%20image%2020211109222126.png)[^7]\n\n- 下面是 Reduce 的算法, 我们只在这个项前瞻符号对应的地方填上 Reduce, 也就是 $First (\\gamma a)$ 指代的地方.\n![400](notes/2021/2021.10/assets/Pasted%20image%2020211109222451.png)\n\n### 开始分析!\n- 吃掉终结符(Shift)或者非终结符(Goto)都只需要将符号压栈, 移动到拥有对应项的状态: \n $$[A\\rightarrow \\alpha\\cdot X\\beta,\\ a]\\overset{shift(X)}\\longrightarrow[A\\rightarrow \\alpha X\\cdot\\beta,\\ a]$$\n - Reduce 操作需要前瞻一个符号. 比如现在的状态里面有 $[A\\rightarrow\\alpha\\ ., a\\ ]$ 如果输入串里面下一个符号是 $a$, 才进行 Reduce 操作.\n\n - 遇到 $[S^′ → S., \\$]$, 说明分析成功.\n\n- 如果当前表项为空, 则报错 \n\n\n\n[^6]: CCP\u0026P 5.4节\n[^7]: 这个图是虎书里面的算法, 所以符号和前面稍有不同, 请注意 ","lastmodified":"2022-10-15T14:06:29.026497453Z","tags":null},"/notes/2021/2021.10/Compiler-4-5_LALR1":{"title":"Compiler-4-5_LALR(1)","content":"## LALR (1)\n- LR (1) 的分析表通常十分巨大, 为了在节省空间的同时保留 LR (1) 大部分的优点, 我们常常采用 LALR (1) 分析方法, LALR 代表 Look-Ahead LR.\n\n- LR (1) 分析的 DFA 里面常常有两个状态的产生式完全相同, 只有前瞻符号不同. LALR 分析就是将这样的状态合并为一个状态.\n\n- 在合并状态以后, LALR 的 DFA 和 LR (0) 的 DFA 拓扑结构上是一样的, 唯一不同的是每个状态里面都是 LR (1) 项, 包含了前瞻符号.\n\n- 我们可以从 LR (1) 的 DFA 构造 LALR 的 DFA, 但是这样需要事先构造 LR (1) 庞大的表. 所以我们常常使用一种称为 \"前瞻传播 (Look-ahead Propagation) \" 的方法来从 LR (0) 的 DFA 构造 LALR 的 DFA. \n\t- Look-ahead Propagation 很简单, 请参见【CCP\u0026P】Compiler Construction Principles and Practice by Kenneth C. Louden 对应的部分\n\n- 所有合理的程序设计语言都有一个 LALR (1) 文法，并且存在着许多对 LALR (1) 文法有效的语法分析器生成器工具。由于这一原因， LALR (1) 文法已变成程序设计语言和自动语法分析器生成器的标准。[^8]\n\n- Yacc 就是一个 LALR (1) 分析器的自动生成器 \n\n\n\n\n[^8]: 虎书 3.3.5","lastmodified":"2022-10-15T14:06:29.026497453Z","tags":null},"/notes/2021/2021.10/Compiler-4_LR%E5%88%86%E6%9E%90":{"title":"Compiler-4_LR分析","content":"# LR 分析\n\n\u003cdiv align=\"right\"\u003e 2021-11-05\u003c/div\u003e\n\nTags: #Compiler #Course \n\n- 在自底向上语法分析中, 如何**寻找句柄**是关键问题\n\n[Compiler-4-1_什么是 LR 分析](notes/2021/2021.10/Compiler-4-1_什么是%20LR%20分析.md)\n\n[Compiler-4-2_LR(0)_Parse](notes/2021/2021.10/Compiler-4-2_LR(0)_Parse.md)\n\n[Compiler-4-3_SLR_parse](notes/2021/2021.10/Compiler-4-3_SLR_parse.md)\n\n[Compiler-4-4_LR(1)_分析](notes/2021/2021.10/Compiler-4-4_LR(1)_分析.md)\n\n[Compiler-4-5_LALR(1)](notes/2021/2021.10/Compiler-4-5_LALR(1).md)\n\n\n## 总结\n- 我们从 LR (0) 分析方法开始, 了解了移位归约分析方法的基本要点, 进一步将\"前瞻\"的思想加入到自底向上分析的过程中去, 最终构建了更加强大的 SLR (1) 分析器和 LR (1) 分析器.\n\n- 相比 SLR (1) 分析器, LR (1) 分析将前瞻的思想融合到了 DFA 等价类的构建中去, 最大程度上地利用了栈内的已知信息, 避免了无效的归约, 减少了冲突, 有着最强大的表达能力.\n\n- LALR (1) 是对 LR (1) 的合理简化, 它在保留 LR (1) 的大部分能力的同时极大地减小了复杂度.\n\n- 编译器是对\"计算\"的等价变换, 那么语法分析这一步其实相当于\"重构/解释\"思维过程.\n- 编译器将更抽象形式的计算解释为计算机能够理解的低级形成, 而这让我们能够在更高层面上去思考解决问题的方法, 用更精炼的语言来表达解决这个问题所需要的计算步骤\n- 词法, 语法分析就是在根据规则重构一个句子的组成(重构语法树). 我们拿到一个问题, 不断思考, 修改 最终得到一个完整的程序, 那么语法分析就是重现我们\"推导这个程序的过程\", 即解决问题的过程, 思维的过程.\n---\n后面的部分笔记还没有, 先把思考写在这里:\n\n- 语义分析其实还有检查错误地功能, 那么编译器其实还有一个功能是负责\"清洗\", 检查错误\n- 其实\"解释成更低级的运算\"这一步大部分是中间代码/目标代码生成这一步在做\n\n- 编译器的主要责任/职责: 解释(计算步骤), 检查(代码错误), 优化(代码性能)\n\n\n\n","lastmodified":"2022-10-15T14:06:29.026497453Z","tags":null},"/notes/2021/2021.10/Dot_Product_and_Linear_Transformation-%E5%90%91%E9%87%8F%E5%86%85%E7%A7%AF%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2":{"title":"Dot_Product_and_Linear_Transformation-向量内积与线性变换","content":"# 内积与线性变换\n\n\u003cdiv align=\"right\"\u003e 2021-10-29\u003c/div\u003e\n\nTags: #Math/LinearAlgebra #LinearTransformation #DotProduct #Vector\n\n- 向量内积即多维空间到一维空间的线性变换. \n\t- 将第二个向量变换到第一个向量的方向上去\n\t- 如果第一个向量的长度为1, 那么就是第二个向量到第一个向量方向上的投影变换\n## Highlights\n![](notes/2021/2021.10/assets/img_2022-10-15-63.png)\n![](notes/2021/2021.10/assets/img_2022-10-15-1.gif)\n![](notes/2021/2021.10/assets/img_2022-10-15-2.gif)\n![](notes/2021/2021.10/assets/img_2022-10-15-3.gif)\n\n## 视频\n\n\u003ciframe width=\"800\" height=\"480\" src=\"https://www.youtube.com/embed/LyGKycYT2v0\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n","lastmodified":"2022-10-15T14:06:29.026497453Z","tags":null},"/notes/2021/2021.10/Odyssey-%E5%A5%A5%E5%BE%B7%E8%B5%9B":{"title":"Odyssey-奥德赛","content":"# Odyssey - 奥德赛\n\n\u003cdiv align=\"right\"\u003e 2021-11-06\u003c/div\u003e\n\nTags: #Greek #Poems #Homer #Literature\n\n《奥德赛》（古希腊语：Ὀδύσσεια，转写：Odýsseia，英语：Odyssey）是古希腊最重要的两部史诗之一（另一部是《伊利亚特》）。《奥德赛》延续了《伊利亚特》的故事情节，由盲诗人荷马所作。这部史诗是西方文学的奠基之作，是除《伊利亚特》外现存最古老的西方文学作品。\n\nIt follows the Greek hero Odysseus, king of Ithaca, and his journey home after the Trojan War. After the war itself, which lasted ten years, his journey lasted for ten additional years, during which time he encountered many perils and all his crew mates were killed. In his absence, Odysseus was assumed dead, and his wife Penelope and son Telemachus had to contend with a group of unruly suitors who were competing for Penelope's hand in marriage. \n\n","lastmodified":"2022-10-15T14:06:29.026497453Z","tags":null},"/notes/2021/2021.10/Part.28_Cost_Function-Neural_NetworkML_Andrew.Ng.":{"title":"Part.28_Cost_Function-Neural_Network(ML_Andrew.Ng.)","content":"# Cost Function - Neural Network\n\n\u003cdiv align=\"right\"\u003e 2021-10-09\u003c/div\u003e\n\nTags: #MachineLearning #NeuralNetwork #CostFunction \n\n## Basic Concepts\n![](notes/2021/2021.10/assets/img_2022-10-15-64.png)\n$$\\left\\{\\left(x^{(1)}, y^{(1)}\\right),\n\\left(x^{(2)}, y^{(2)}\\right), \\ldots,\n\\left(x^{(m)}, y^{(m)}\\right)\\right\\}$$\n\n- $m$: Number of **Training Samples** - 训练样本数\n- $L$: Total Number of **Layers** in the network - 网络层数\n- $s_{l}$ =no. of **units** (not counting bias unit) in  layer - 每一层激活单元数（不包括常数）\n\n## Cost Function: Representation\n神经网络用来分类的时候，它的损失函数可以通过对Logistic Regression的损失函数稍加改造来得到：\n\n### 回顾Cost Function of Logistic Regression (With Regularization)\n![正则化以后的损失函数](notes/2021/2021.9/Part.20_Regularized_Logistic_Regression(ML_Andrew.Ng.).md#正则化以后的损失函数)\n\n### Intuition of the relation\n- 回顾前面我们提到过的神经网络与Logistic回归的联系：\n![与Logistic Regression的联系](notes/2021/2021.9/Part.23_Forward_Propagation-Neural_Network(ML_Andrew.Ng.).md#与Logistic%20Regression的联系)\n\n- 在Output Layer, \n\n![](notes/2021/2021.10/assets/Pasted%20image%2020211009210215.png)\n\n\n","lastmodified":"2022-10-15T14:06:29.026497453Z","tags":null},"/notes/2021/2021.10/Part.29_Fisher_Linear_DiscriminantPattern_Classification-Chapter_4":{"title":"Part.29_Fisher_Linear_Discriminant(Pattern_Classification-Chapter_4)","content":"# Fisher Linear Discriminant\n\n\u003cdiv align=\"right\"\u003e 2021-10-28\u003c/div\u003e\n\nTags: #MachineLearning  #PatternClassification #Course \n#DimensionalityReduction \n\n- **通过降维进行分类**, 降到一维即为线性判别.\n- 其实, **线性判别分析** (**LDA**)就是对**Fisher线性判别**的归纳.[^1]\n\n![](notes/2021/2021.10/assets/img_2022-10-15-65.png)\n\n## Motivation\n- Curse of Dimensionality - 模型的表现随着维度的增加而变坏, 而且根据设计者的3维直觉, 无法很好的解决高维度的问题.\n- 所以一个很直观的方法便是减少问题的维度, Fisher的方法便是将多维样本直接映射到一维的一种方法.\n- 直接映射到一维是否太粗暴? 2维, 3维可以吗?\n\t- 的确, even if the samples formed well-separated, compact clusters in d-space, projection onto an arbitrary line will usually produce a confused mixture of samples from all of the classes, and thus poor recognition performance. **However**, 一维的问题是十分简单的, by moving the line around, we might be able to find an orientation for which the projected samples are well separated. This is exactly the goal of classical discriminant analysis. 二维, 三维也是可以的, 我们后面会谈到对于Fisher方法的多维推广.\n\n\n## Interlude - Linear Transformation \u0026 Dot Product\n[Dot_Product_and_Linear_Transformation-向量内积与线性变换](notes/2021/2021.10/Dot_Product_and_Linear_Transformation-向量内积与线性变换.md)\n\n\n## Interlude - Covariance and Covariance Matrix\n[协方差矩阵_Covariance_Matrix](notes/2021/2021.10/协方差矩阵_Covariance_Matrix.md)\n\n## Intuition\nhttps://sthalles.github.io/fisher-linear-discriminant/\n\n如果我们直接投影到**样本均值的连线**的方向的话, 可以看到将会有很多的重合:\n![fisher-ld generator network|300](notes/2021/2021.10/assets/img_2022-10-15-66.png) ![fisher-ld generator network|300](notes/2021/2021.10/assets/img_2022-10-15-67.png)\n\n\nFisher的方法基于以下直觉:\n\n- 我们要使投影后的结果: \n\t1. 不同类间隔得越开越好 (类间方差最大)\n\t2. 相同类内聚集的越紧密越好 (类内方差最小)\n\n![fisher-ld generator network](notes/2021/2021.10/assets/img_2022-10-15-68.png)\n所以我们这样构造准则函数(Criterion Function):\n\n![fisher-ld generator network|500](notes/2021/2021.10/assets/img_2022-10-15-69.png)\n我们需要找到使$J(w)$取得最大值的$w$, 即找到最优的投影方向.\n\n## 详细推导\n\n### 构造准则函数\n- 我们这样计算投影:\n\t$$y=\\mathbf{w}^{t} \\mathbf{x_p}$$\n\t上面的式子将样本点$\\mathbf{x_p}$投影到$\\mathbf{w}$方向的一条直线上\n- 我们这样表示类别$i$样本的均值:\n\t$$\\mathbf{m}_{i}=\\frac{1}{n_{i}} \\sum_{\\mathbf{x} \\in \\mathcal{D}_{i}} \\mathbf{x}$$\n\t其中$n_i$是该类别样本的个数\n- 我们这样计算投影后的样本均值:\n\t$$\\begin{align}\n\t\\tilde{m}_{i}\u0026=\\frac{1}{n_{i}} \\sum_{y \\in \\mathcal{Y}_{i}} y \\\\\n\t\u0026=\\frac{1}{n_{i}} \\sum_{\\mathbf{x} \\in \\mathcal{D}_{i}} \\mathbf{w}^{t} \\mathbf{x}\\\\\n\t\u0026=\\mathbf{w}^{t} \\mathbf{m}_{i}\\end{align}$$\n\t可以发现, 投影后的均值 其实就是 均值$\\mathbf{m}_{i}$的投影\n- 所以我们可以这样衡量投影后的直线上面不同类间均值的距离:\n\t$$\\left|\\tilde{m}_{1}-\\tilde{m}_{2}\\right|\n\t=\\left|\\mathbf{w}^{t}\\left(\\mathbf{m}_{1}-\\mathbf{m}_{2}\\right)\\right|$$\n\t\n- 为了衡量投影后样本的分散程度, 我们定义 \"类内散度\"\n\t$$\\tilde{s}_{i}^{2}=\n\t\\sum_{y \\in \\mathcal{Y}_{i}}\\left(y-\\tilde{m}_{i}\\right)^{2}$$\n\t直观看来, 就是投影后样本与均值距离的平方\n\t\n- 然后我们就可以根据直觉, 给出Fisher准则函数如下:\n\t$$J(\\mathbf{w})=\n\t\\frac{\\left|\\tilde{m}_{1}-\\tilde{m}_{2}\\right|^{2}}{\\tilde{s}_{1}^{2}+\\tilde{s}_{2}^{2}}$$\n\t分子是类间的方差(越大越好), 分母是类内的方差(越小越好)\n\n### ==最大化==准则函数\n\n#### 将w提出来\n- $J(\\mathbf{w})$并不是与$\\mathbf{w}$直接相关的, 所以先进如下变换:\n\n- 我们先定义Scatter Matrix $\\mathbf{S_i, S_w}$:\n\t$$\\mathbf{S}_{i}=\n\t\\sum_{\\mathbf{x} \\in \\mathcal{D}_{i}}\n\t\\left(\\mathbf{x}-\\mathbf{m}_{i}\\right)\\left(\\mathbf{x}-\\mathbf{m}_{i}\\right)^{t}$$\n\t$$\\mathbf{S_{w}} = \\mathbf{S_1+S_2}$$\n\n- 所以, 类内散度可以变为:\n\t$$\\begin{aligned}\n\t\\tilde{s}_{i}^{2} \u0026=\\sum_{\\mathbf{x} \\in \\mathcal{D}_{i}}\\left(\\mathbf{w}^{t} \\mathbf{x}-\\mathbf{w}^{t} \\mathbf{m}_{i}\\right)^{2} \\\\\n\t\u0026=\\sum_{\\mathbf{x} \\in \\mathcal{D}_{i}}\n\t\\mathbf{w}^{t}\\left(\\mathbf{x}-\\mathbf{m}_{i}\\right)\n\t\\left(\\mathbf{w}^{t}\\left(\\mathbf{x}-\\mathbf{m}_{i}\\right)\\right)^T \\\\\n\t\u0026=\\sum_{\\mathbf{x} \\in \\mathcal{D}_{i}}\n\t\\mathbf{w}^{t}\\left(\\mathbf{x}-\\mathbf{m}_{i}\\right)\\left(\\mathbf{x}-\\mathbf{m}_{i}\\right)^{t} \\mathbf{w} \\\\\n\t\u0026=\\mathbf{w}^{t} \\mathbf{S}_{i} \\mathbf{w}\n\t\\end{aligned}$$\n\n- 然后\n\t$$\\tilde{s}_{1}^{2}+\\tilde{s}_{2}^{2}=\\mathbf{w}^{t} \\mathbf{S}_{W} \\mathbf{w}$$\n\t这样, 我们将分母里面的$\\mathbf{w}$提取到了外面\n- 对于分子, 我们可以有相似的操作:\n$$\\begin{aligned}\n\\left(\\tilde{m}_{1}-\\tilde{m}_{2}\\right)^{2} \u0026=\\left(\\mathbf{w}^{t} \\mathbf{m}_{1}-\\mathbf{w}^{t} \\mathbf{m}_{2}\\right)^{2} \\\\\n\u0026=\\mathbf{w}^{t}\\left(\\mathbf{m}_{1}-\\mathbf{m}_{2}\\right)\\left(\\mathbf{m}_{1}-\\mathbf{m}_{2}\\right)^{t} \\mathbf{w} \\\\\n\u0026=\\mathbf{w}^{t} \\mathbf{S}_{B} \\mathbf{w}\n\\end{aligned}$$\n观察里面的中间部分, 我们定义:\n$$\\mathbf{S}_{B}=\\left(\\mathbf{m}_{1}-\\mathbf{m}_{2}\\right)\\left(\\mathbf{m}_{1}-\\mathbf{m}_{2}\\right)^{t}$$\n\n- 所以Criterion Function 变为了:\n\t$$J(\\mathbf{w})=\\frac{\\mathbf{w}^{t} \\mathbf{S}_{B} \\mathbf{w}}{\\mathbf{w}^{t} \\mathbf{S}_{W} \\mathbf{w}}$$\n\t\n\t这一表达式在数学物理中被称作广义Rayleigh 商(generalized Rayleigh quotient)\n\n#### 关于两个矩阵\nWe call $S_{W}$ the within-class scatter matrix. It is proportional to the sample covariance matrix for the pooled $d$-dimensional data. It is symmetric and positive semi-definite, and is usually non-singular if $n\u003ed$. \nLikewise, $\\mathbf{S}_{B}$ is called the between class scatter matrix. It is also symmetric and positive semi-definite, but because it is the outer product of two vectors, its rank is at most one. In particular, for any $\\mathrm{w}$, $\\mathbf{S}_{B} \\mathbf{w}$ is in the direction of $\\mathbf{m}_{1}-\\mathbf{m}_{2}$, and $\\mathbf{S}_{B}$ is quite singular.\n\n#### 解$\\mathbf{w}$\n解$\\mathbf{w}$需要用到拉格朗日乘子法:\n思路:\n用拉格朗日乘子法得到以下条件\n$$\\mathbf{S}_{B} \\mathbf{w}=\\lambda \\mathbf{S}_{W} \\mathbf{w}$$\nIf $\\mathbf{S}_{W}$ is non-singular we can obtain a conventional eigenvalue problem by writing\n$$\\mathbf{S}_{W}^{-1} \\mathbf{S}_{B} \\mathbf{w}=\\lambda \\mathbf{w}$$\nIn our particular case, it is unnecessary to solve for the eigenvalues and eigenvectors of $\\mathbf{S}_{W}^{-1} \\mathbf{S}_{B}$ due to the fact that $\\mathbf{S_B w}$ is always in the direction of $m_1 −m_2$. Since the scale factor for $\\mathbf{w}$ is immaterial, we can immediately write the solution for the $\\mathbf{w}$ that optimizes $J(·)$:\n\n$$\\mathbf{w}=\\mathbf{S}_{W}^{-1}\\left(\\mathbf{m}_{1}-\\mathbf{m}_{2}\\right)$$\n\n\n详细过程\n- Ref 模式识别(第三版) - 张学工, Page 64\n![](notes/2021/2021.10/assets/img_2022-10-15-70.png)\n\n- Ref 机器学习 周志华\n![](notes/2021/2021.10/assets/img_2022-10-15-71.png)\n![](notes/2021/2021.10/assets/img_2022-10-15-72.png)\n- Ref 南瓜书\n![](notes/2021/2021.10/assets/img_2022-10-15-73.png)\n\n## 可以将这个方法推广到多维的情况\n推广到高维:[^2]\n我们需要改变以下地方:\n- ![fisher-id samples|500](notes/2021/2021.10/assets/img_2022-10-15-74.png)\n- 类内 Scatter Matrix, $S_W$直观的来说, 即从两个类的 $S_1+S_2$ 变成多个类 $S_i$ 的和\n- 对于$S_B$, 变成了 \"每个类相对于全局平均的差\" 的加权和, 这里和只有两个类的情况并不是完全一致的, 具体参见 Duda 模式分类, page49\n\n若将 W 视为一个投影矩阵，则多分类 LDA 将样本投影到 N-1 维空间，N-1 通常远小子数据原有的属性数. 于是，可通过这个投影来减小样本点的维数，且投影过程中使用了类别信息, 因此 LDA 也常被视为一种经典的监督降维技术[^3]\n\n\n[^1]: https://en.wikipedia.org/wiki/Linear_discriminant_analysis\n[^2]: https://sthalles.github.io/fisher-linear-discriminant/\n[^3]: 周志华 机器学习","lastmodified":"2022-10-15T14:06:29.026497453Z","tags":null},"/notes/2021/2021.10/ad_hoc-Latin_Phrase":{"title":"ad_hoc-Latin_Phrase","content":"# Ad hoc\n\n\u003cdiv align=\"right\"\u003e 2021-11-04\u003c/div\u003e\n\nTags: #Latin #English \n\n`adjective`\t\nuk/ˌæd ˈhɒk/    us/ˌæd ˈhɑːk/\n\n- made or happening only for a particular purpose or need, not planned before it happens\n\t特别的；专门的；临时安排的\n\t- an ad hoc committee/meeting \n\t\t特别委员会／会议\n\t- We deal with problems on an ad hoc basis (= as they happen). \n\t\t我们应对问题的方式是只要出现就随时解决。\n\n---\n**Ad hoc** is a Latin phrase meaning literally '**to this**'. In English, it typically signifies a solution for a specific purpose, problem, or task rather than a generalized solution adaptable to collateral instances. (Compare with a priori.)\n\nCommon examples are ad hoc committees and commissions created at the national or international level for a specific task. In other fields the term could refer to, for example, a military unit created under special circumstances (see task force), a tailor-made suit, a handcrafted network protocol (e.g., ad hoc network), a temporary banding together of geographically-linked franchise locations (of a given national brand) to issue advertising coupons, or a purpose-specific equation. \n\n","lastmodified":"2022-10-15T14:06:29.026497453Z","tags":null},"/notes/2021/2021.11/%E5%85%B3%E4%BA%8E%E7%89%B9%E5%BE%81%E5%80%BC%E7%9A%84%E4%B8%80%E4%B8%AA%E7%BB%93%E8%AE%BA":{"title":"关于特征值的一个结论","content":"**If $A$ is $m$ by $n$ and $B$ is $n$ by $m$, then $AB$ and $BA$ have the same nonzero eigenvalues.**\n\n**证明:**\n\n![](notes/2021/2021.11/assets/Pasted%20image%2020211116205055.png)\n\n\n\n\n\n\n\n**推论:** \n$A^TA$和$AA^T$的非零特征值相同\n\n","lastmodified":"2022-10-15T14:06:29.13849866Z","tags":null},"/notes/2021/2021.11/%E5%85%B3%E4%BA%8E%E7%A7%A9%E7%9A%84%E4%B8%80%E4%B8%AA%E7%BB%93%E8%AE%BA":{"title":"关于秩的一个结论","content":"$Rank(A)=Rank(A^TA)=Rank(AA^T)$\n\n**证明:**\n\n一方面:\n$Ax=0\\Rightarrow A^TAx=0$\n\n另一方面:\n$A^TAx=0\\Rightarrow x\\cdot0=x\\cdot A^TAx=0$\n\n$x\\cdot A^TAx=0\\Rightarrow x^TA^TAx=0$\n\n$x^TA^TAx=0\\Rightarrow (Ax)^TAx=0 \\Rightarrow ||Ax||^2=0\\Rightarrow Ax=0$\n\n所以$A$与$A^TA$核空间相同，所以秩相等","lastmodified":"2022-10-15T14:06:29.13849866Z","tags":null},"/notes/2021/2021.11/%E5%86%85%E7%A7%AF%E5%92%8C%E7%9B%B8%E5%85%B3%E6%80%A7%E7%9A%84%E8%81%94%E7%B3%BB-DotInner_Product__Correlation":{"title":"内积和相关性的联系-Dot(Inner)_Product_\u0026_Correlation","content":"# Inner Product \u0026 Correlation\n\n\u003cdiv align=\"right\"\u003e 2021-11-16\u003c/div\u003e\n\nTags: #Math #InnerProduct\n\nSource: [David Joyce's answer to Is there any relation between 'correlation of two signals' and 'dot product of two vectors'? - Quora](https://qr.ae/pGmGkx)\n\n## Original Version\n\n### Q: Is there any relation between 'correlation of two signals' and 'dot product of two vectors'?\n\n**Answer:**\nYes, there is a connection between correlation and dot products (also called inner products).\n\nConsider the vector space of real-valued random variables. These random variables don't have to be independent, so they may have a covariance\n\n$$X \\cdot Y=\\operatorname{Cov}(X, Y)=E\\left(\\left(X-\\mu_{X}\\right)\\left(Y-\\mu_{Y}\\right)\\right)$$\n\nCovariance is bilinear, so it can be used to define an inner product on the vector space of random variables making it an inner product space.\n\nWith an inner product, you can define the norm of a vector (also called the length of the vector by as the square root of the inner product with itself. So $\\|X\\|=\\sqrt{X \\cdot X}$ is defined as the square root of $\\operatorname{Cov}(X, X)=E\\left(\\left(X-\\mu_{X}\\right)^{2}\\right)$. This norm is the standard deviation $\\sigma_{X}$ of $X$.\n\nThe correlation of two random variables is defined by\n$$\n\\rho_{X Y}=\\frac{\\operatorname{Cov}(X, Y)}{\\sigma_{X} \\sigma_{Y}}\n$$\n\nThat is precisely the definition of the the cosine of the angle $θ$ between two vectors\n\n$$\n\\cos \\theta=\\frac{X \\cdot Y}{\\|X\\|\\|Y\\|}\n$$\n\nIn summary, covariance is an inner product, standard deviations are norms, and correlations are cosines of angles.\n\n\n## Translation - 翻译\n\n### 问题: \"相关度\"和\"内积\"这两个概念之间有什么联系吗? \n\n答: 的确, 相关度和内积(也叫做点积)之间有着一定的联系.\n\n考虑一个由实数随机变量组成的向量空间. 这些随机变量之间不一定是相互独立的, 它们的协方差由下面这个式子给出:\n\n$$X \\cdot Y=\\operatorname{Cov}(X, Y)=E\\left(\\left(X-\\mu_{X}\\right)\\left(Y-\\mu_{Y}\\right)\\right)$$\n\n协方差是一个双线性函数, 所以它能够在实数随机变量组成的向量空间里面定义一个内积, 使其成为内积空间.\n\n有了内积空间, 我们便能定义一个向量的范数(也就是向量的长度, 即一个向量与其自身内积的平方根): $\\|X\\|=\\sqrt{X \\cdot X}$ . 而这个定义下的内积也是协方差 $\\operatorname{Cov}(X, X)=E\\left(\\left(X-\\mu_{X}\\right)^{2}\\right)$ 的平方根. 即随机变量 $X$ 的标准差 $\\sigma_{X}$.\n\n两个随机变量之间的相关度的定义如下:\n$$\n\\rho_{X Y}=\\frac{\\operatorname{Cov}(X, Y)}{\\sigma_{X} \\sigma_{Y}}\n$$\n而这正是两个变量之间夹角 $\\theta$ 的余弦的定义:\n$$\n\\cos \\theta=\\frac{X \\cdot Y}{\\|X\\|\\|Y\\|}\n$$\n总之, 协方差等价于内积, 标准差等价于范数, 而相关度等价于夹角的余弦值. (余弦相似度)","lastmodified":"2022-10-15T14:06:29.13849866Z","tags":null},"/notes/2021/2021.11/%E7%90%86%E8%A7%A3%E7%9B%B8%E4%BC%BC%E7%9F%A9%E9%98%B5":{"title":"理解相似矩阵","content":"# 理解相似矩阵\n\n\u003cdiv align=\"right\"\u003e 2021-11-14\u003c/div\u003e\n\nTags: #Math/LinearAlgebra \n\n\n- 设 $A,B$ 都是 $n$ 阶矩阵，若有可逆矩阵 $P$ , 使得 $B=P^{-1}AP$ , 则称$B$是$A$的相似矩阵。\n\n- 相似矩阵是同一个线性变换在**不同基向量**下的不同矩阵表示.\n\n\t- $P$是**基变换矩阵(Base Change Matrix)**, 它是一个不改变空间维数的可逆线性变换, 其目的是改变当前线性空间的基底: $(\\ \\vec i',\\  \\vec j'\\ )\\rightarrow (\\ \\vec i,\\ \\vec j\\ )$, 也可以理解为进行坐标换算, 但是不改变空间里面的实际位置\n\n所以图里面右边的\n$$v'\\rightarrow Bv'$$\n等价于\n- 先变换基底到$V_1$, 得到位置相同但是坐标不同的向量$v$\n$$v'\\rightarrow Pv'$$\n- 然后进行$V_1$下面等价的线性变换$A$, 得到$V_1$下的结果$Av$\n$$v'\\rightarrow Pv'\\rightarrow APv'$$\n- 最后再把基底换回来, 得到$V_2$里面的结果\n$$v'\\rightarrow Pv'\\rightarrow APv'\\rightarrow P^{-1}APv'$$\n\n我们有: \n$$P^{-1}AP =  B$$\n\n\n![](notes/2021/2021.11/assets/img_2022-10-15.png)[^1]\n\n\n\n[^1]: [如何理解相似矩阵？ - 知乎](https://zhuanlan.zhihu.com/p/31003468)","lastmodified":"2022-10-15T14:06:29.13849866Z","tags":null},"/notes/2021/2021.11/%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%98-%E5%85%B3%E4%BA%8E%E7%BB%B4%E5%BA%A6%E7%9A%84%E6%96%B0%E8%A7%86%E8%A7%92":{"title":"矩阵相乘-关于维度的新视角","content":"# 关于矩阵相乘维度关系的新视角\n\n\u003cdiv align=\"right\"\u003e 2021-11-16\u003c/div\u003e\n\nTags: #Math/LinearAlgebra \n\n\n![矩阵相乘 维度1](notes/2021/2021.11/assets/矩阵相乘%20维度1.svg)\n\n![矩阵相乘 维度2](notes/2021/2021.11/assets/矩阵相乘%20维度2.svg)\n\n![矩阵相乘 维度3](notes/2021/2021.11/assets/矩阵相乘%20维度3.svg)\n\n![矩阵相乘 维度4](notes/2021/2021.11/assets/矩阵相乘%20维度4.svg)!","lastmodified":"2022-10-15T14:06:29.13849866Z","tags":null},"/notes/2021/2021.11/%E9%85%89%E7%9F%A9%E9%98%B5%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%AB%E9%85%89%E7%9F%A9%E9%98%B5":{"title":"酉矩阵为什么叫酉矩阵","content":"# 酉矩阵为什么叫酉矩阵\n\n\u003cdiv align=\"right\"\u003e 2021-11-16\u003c/div\u003e\n\nTags: #Math/LinearAlgebra #English \n\n- 酉矩阵里面的\"**酉**\"其实是字母$U$的音译\n\n（又译作幺正矩阵，英语：unitary matrix）\n\n\n### unitary\nadjective\n`uk` /ˈjuː.nɪ.tər.i/ `us` /ˈjuː.nɪ.ter.i/\n- of a system of local government in the UK in which official power is given to one organization that deals with all matters in a local area instead of to several organizations that each deal with only a few matters\n（英国地方政府）集权制的，单一自治体的\n\n- Wales will be divided into 21 unitary authorities instead of eight counties and 37 districts. 威尔士将被划分为21个单一自治体，以取代8郡、37个地区的行政区划。\n\n\n","lastmodified":"2022-10-15T14:06:29.13849866Z","tags":null},"/notes/2021/2021.11/%E9%85%89%E7%9F%A9%E9%98%B5-Unitary-Matrix":{"title":"酉矩阵 Unitary Matrix","content":"\n共轭转置是其自身的逆的矩阵:\n\n $$U^{-1}=U^{*}$$\n\n-  是 n×n 复数方块矩阵，满足：\n\n$$U^{*}U=UU^{*}=I_{n}$$\n\n其中 $U^*$ 是 $U$ 的共轭转置，$I_n$ 是 n×n 单位矩阵。\n\n酉矩阵是**实数上的正交矩阵，在复数的推广**\n$$U^{T}U=UU^{T}=I_{n}$$\n\n## 性质\n- 酉矩阵代表的酉变换不改变向量的**长度**与**夹角**\n\t- 不改变长度是正交所带来的\n\t- 不改变夹角是什么带来的？\n[Unitary Transformations - YouTube](https://www.youtube.com/watch?v=46Hpy4FiGls\u0026list=PLMrJAkhIeNNSVjnsviglFoY2nXildDCcv\u0026index=10)\n","lastmodified":"2022-10-15T14:06:29.13849866Z","tags":null},"/notes/2021/2021.11/A-Fancy-Example-of-SVD":{"title":"A Fancy Example of SVD","content":"==Example==\n\nConsider the $4 × 5$ matrix\n\n  \n\n$$\\mathbf{M} = \\begin{bmatrix}\n\n 1 \u0026 0 \u0026 0 \u0026 0 \u0026 2 \\\\\n\n 0 \u0026 0 \u0026 3 \u0026 0 \u0026 0 \\\\\n\n 0 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \\\\\n\n 0 \u0026 2 \u0026 0 \u0026 0 \u0026 0\n\n \\end{bmatrix}\n\n$$\n\n  \n\nA singular value decomposition of this matrix is given by $UΣV^⁎$\n\n  \n\n$$\\begin{align}\n\n \\mathbf{U} \u0026= \\begin{bmatrix}\n\n \\color{Green}0 \u0026 \\color{Blue}-1 \u0026 \\color{Cyan}0 \u0026 \\color{Emerald}0 \\\\\n\n \\color{Green}-1 \u0026 \\color{Blue}0 \u0026 \\color{Cyan}0 \u0026 \\color{Emerald}0 \\\\\n\n \\color{Green}0 \u0026 \\color{Blue}0 \u0026 \\color{Cyan}0 \u0026 \\color{Emerald}-1 \\\\\n\n \\color{Green}0 \u0026 \\color{Blue}0 \u0026 \\color{Cyan}-1 \u0026 \\color{Emerald}0\n\n \\end{bmatrix} \\\\[6pt]\n\n  \n\n \\boldsymbol{\\Sigma} \u0026= \\begin{bmatrix}\n\n 3 \u0026 0 \u0026 0 \u0026 0 \u0026 \\color{Gray}\\mathit{0} \\\\\n\n 0 \u0026 \\sqrt{5} \u0026 0 \u0026 0 \u0026 \\color{Gray}\\mathit{0} \\\\\n\n 0 \u0026 0 \u0026 2 \u0026 0 \u0026 \\color{Gray}\\mathit{0} \\\\\n\n 0 \u0026 0 \u0026 0 \u0026 \\color{Red}\\mathbf{0} \u0026 \\color{Gray}\\mathit{0}\n\n \\end{bmatrix} \\\\[6pt]\n\n  \n\n \\mathbf{V}^* \u0026= \\begin{bmatrix}\n\n \\color{Violet}0 \u0026 \\color{Violet}0 \u0026 \\color{Violet}-1 \u0026 \\color{Violet}0 \u0026\\color{Violet}0 \\\\\n\n \\color{Plum}-\\sqrt{0.2}\u0026 \\color{Plum}0 \u0026 \\color{Plum}0 \u0026 \\color{Plum}0 \u0026\\color{Plum}-\\sqrt{0.8} \\\\\n\n \\color{Magenta}0 \u0026 \\color{Magenta}-1 \u0026 \\color{Magenta}0 \u0026 \\color{Magenta}0 \u0026\\color{Magenta}0 \\\\\n\n \\color{Orchid}0 \u0026 \\color{Orchid}0 \u0026 \\color{Orchid}0 \u0026 \\color{Orchid}1 \u0026\\color{Orchid}0 \\\\\n\n \\color{Purple} - \\sqrt{0.8} \u0026 \\color{Purple}0 \u0026 \\color{Purple}0 \u0026 \\color{Purple}0 \u0026 \\color{Purple}\\sqrt{0.2}\n\n \\end{bmatrix}\n\n\\end{align}$$\n\n  \n\nThe scaling matrix $\\mathbf{\\Sigma}$ is zero outside of the diagonal (grey italics) and one diagonal element is zero (red bold). Furthermore, because the matrices $U$ and $V^T$ are unitary matrix|unitary, multiplying by their respective conjugate transposes yields identity matrix|identity matrices, as shown below.  In this case, because $U$ and $V^T$ are real valued, each is an orthogonal matrix.\n\n  \n\n$$\\begin{align}\n\n \\mathbf{U} \\mathbf{U}^* \u0026=\n\n \\begin{bmatrix}\n\n 1 \u0026 0 \u0026 0 \u0026 0 \\\\\n\n 0 \u0026 1 \u0026 0 \u0026 0 \\\\\n\n 0 \u0026 0 \u0026 1 \u0026 0 \\\\\n\n 0 \u0026 0 \u0026 0 \u0026 1\n\n \\end{bmatrix} = \\mathbf{I}_4 \\\\[6pt]\n\n \\mathbf{V} \\mathbf{V}^* \u0026=\n\n \\begin{bmatrix}\n\n 1 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \\\\\n\n 0 \u0026 1 \u0026 0 \u0026 0 \u0026 0 \\\\\n\n 0 \u0026 0 \u0026 1 \u0026 0 \u0026 0 \\\\\n\n 0 \u0026 0 \u0026 0 \u0026 1 \u0026 0 \\\\\n\n 0 \u0026 0 \u0026 0 \u0026 0 \u0026 1\n\n \\end{bmatrix} = \\mathbf{I}_5\n\n\\end{align}$$\n\n  \n\nThis particular singular value decomposition is not unique.  Choosing $\\mathbf V$ such that\n\n$$\\mathbf{V}^* = \\begin{bmatrix}\n\n \\color{Violet}0 \u0026 \\color{Violet}1 \u0026 \\color{Violet}0 \u0026 \\color{Violet}0 \u0026 \\color{Violet}0 \\\\\n\n \\color{Plum}0 \u0026 \\color{Plum}0 \u0026 \\color{Plum}1 \u0026 \\color{Plum}0 \u0026 \\color{Plum}0 \\\\\n\n \\color{Magenta}\\sqrt{0.2} \u0026 \\color{Magenta}0 \u0026 \\color{Magenta}0 \u0026 \\color{Magenta}0 \u0026 \\color{Magenta}\\sqrt{0.8} \\\\\n\n \\color{Orchid}\\sqrt{0.4} \u0026 \\color{Orchid}0 \u0026 \\color{Orchid}0 \u0026 \\color{Orchid}\\sqrt{0.5} \u0026 \\color{Orchid}-\\sqrt{0.1} \\\\\n\n \\color{Purple}-\\sqrt{0.4} \u0026 \\color{Purple}0 \u0026 \\color{Purple}0 \u0026 \\color{Purple}\\sqrt{0.5} \u0026 \\color{Purple}\\sqrt{0.1}\n\n\\end{bmatrix}$$\n\n  \n\nis also a valid singular value decomposition.","lastmodified":"2022-10-15T14:06:29.082498057Z","tags":null},"/notes/2021/2021.11/Balance_of_trade-%E5%87%80%E5%87%BA%E5%8F%A3-%E8%B4%B8%E6%98%93%E4%BD%99%E9%A2%9D":{"title":"Balance_of_trade-净出口-贸易余额","content":"# Balance of trade\n\n\u003cdiv align=\"right\"\u003e 2021-11-11\u003c/div\u003e\n\nTags: #Economy\n\n**净出口**，或称**贸易余额**，是指一国在一定时间内的出口总值与其进口总值的差额。\n- $\\text{净出口} = Sum(\\text{出口}) - Sum(\\text{进口})$\n\t- 净出口为正值时，称为贸易黑字、贸易顺差、贸易盈余或出超。\n\n\t- 净出口为负值时，称为贸易赤字、贸易逆差或入超。\n\n## 美国净出口的变化\n![](notes/2021/2021.11/assets/img_2022-10-15-1.png)\n- 从图中可见, 美国长时间处于贸易逆差状态, 即:\n\t- 销往美国的商品总值\u003e美国出口的贸易总值\n- 在2008经济危机期间, 美国贸易逆差减小很多\n- 近年来逐渐回落\n\n## 全球净出口余额状态\n### 1980年－2008年平均\n![Cumulative_Current_Account_Balance](notes/2021/2021.11/assets/img_2022-10-15-2.png)\n- 中美差异明显","lastmodified":"2022-10-15T14:06:29.082498057Z","tags":null},"/notes/2021/2021.11/Invertable-word":{"title":"Invertable-word","content":"# Invertable\n\n\u003cdiv align=\"right\"\u003e 2021-11-18\u003c/div\u003e\n\nTags: #English \n\n- 我突然意识到 \"**Invertable**\" 其实是 \"**invert**\" + \"**able**\", 即可以求逆的, 你可以求它的 \"Inverse\"\n- 它的词根并不是 **in**\n\n\n\n","lastmodified":"2022-10-15T14:06:29.082498057Z","tags":null},"/notes/2021/2021.11/LU%E5%88%86%E8%A7%A3%E7%9A%84%E4%BE%8B%E5%AD%90":{"title":"LU分解的例子","content":"将一个简单的$3×3$矩阵A进行LU分解：\n$$ A=\n        \\begin{bmatrix}\n           1 \u0026 2 \u0026 3 \\\\\n           2 \u0026 5 \u0026 7 \\\\\n           3 \u0026 5 \u0026 3 \\\\\n        \\end{bmatrix}\n$$\n\n先将矩阵第一列元素中$a_{11}$以下的所有元素变为0，即\n$$ L_{1}A=\n        \\begin{bmatrix}\n           1 \u0026 0 \u0026 0 \\\\\n          -2 \u0026 1 \u0026 0 \\\\\n          -3 \u0026 0 \u0026 1 \\\\\n        \\end{bmatrix} \\times\n        \\begin{bmatrix}\n           1 \u0026 2 \u0026 3 \\\\\n           2 \u0026 5 \u0026 7 \\\\\n           3 \u0026 5 \u0026 3 \\\\\n        \\end{bmatrix}  =\n        \\begin{bmatrix}\n           1 \u0026 2 \u0026 3 \\\\\n           0 \u0026 1 \u0026 1 \\\\\n           0 \u0026 -1 \u0026 -6 \\\\\n        \\end{bmatrix}\n$$\n再将矩阵第二列元素中$a_{22}$以下的所有元素变为0，即\n$$ L_{2}(L_{1}A)=\n        \\begin{bmatrix}\n           1 \u0026 0 \u0026 0 \\\\\n           0 \u0026 1 \u0026 0 \\\\\n           0 \u0026 1 \u0026 1 \\\\\n        \\end{bmatrix} \\times\n        \\begin{bmatrix}\n           1 \u0026 2 \u0026 3 \\\\\n           0 \u0026 1 \u0026 1 \\\\\n           0 \u0026 -1 \u0026 -6 \\\\\n        \\end{bmatrix}  =\n        \\begin{bmatrix}\n           1 \u0026 2 \u0026 3 \\\\\n           0 \u0026 1 \u0026 1 \\\\\n           0 \u0026 0 \u0026 -5 \\\\\n        \\end{bmatrix} =U\n$$\n然后我们将 $L_1, L_2$ 移到等号的右边, 就得到了所有步骤的总和$L$:\n\n$$L= L_{1}^{-1}L_{2}^{-1}=\n        \\begin{bmatrix}\n           1 \u0026 0 \u0026 0 \\\\\n           2 \u0026 1 \u0026 0 \\\\\n           3 \u0026 0 \u0026 1 \\\\\n        \\end{bmatrix} \\times\n        \\begin{bmatrix}\n           1 \u0026 0 \u0026 0 \\\\\n           0 \u0026 1 \u0026 0 \\\\\n           0 \u0026 -1 \u0026 1 \\\\\n        \\end{bmatrix} =\n        \\begin{bmatrix}\n           1 \u0026 0 \u0026 0 \\\\\n           2 \u0026 1 \u0026 0 \\\\\n           3 \u0026 -1 \u0026 1 \\\\\n        \\end{bmatrix} $$\n\t\t\nRef: [LU分解 - 维基百科，自由的百科全书](https://zh.wikipedia.org/zh-hans/LU%E5%88%86%E8%A7%A3)","lastmodified":"2022-10-15T14:06:29.082498057Z","tags":null},"/notes/2021/2021.11/MIT-18.065-Matrix-Methods-in-Data-Analysis-Signal-Processing-and-Machine-Learning":{"title":"MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning,","content":"# MIT 18.065 - Matrix Methods in Data Analysis, Signal Processing, and Machine Learning,\n\n\u003cdiv align=\"right\"\u003e 2021-11-12\u003c/div\u003e\n\nTags: #Matrix #Math/LinearAlgebra #Math \n\n- 学习这门课的主要动力是线性代数的知识在大二的一年内已经有所遗忘了, 并且在学习机器学习期间常常设计线性代数与矩阵的相关知识, 所以想要有针对性地深入学习与复习一下.\n\t- 也想体验一下Gilbert Strang的课堂\n \n \u003e (Gilbert Strang, during class)\n \u003e \"This is what I want to say the most, and I say it to every class I teach near the start of the semester. My feeling about my job is to teach you things, or to join with you in learning things, as has happened today. It's not to grade you.\"\n\n## Resources\n- [MIT 18.065, Spring 2018 - YouTube](https://www.youtube.com/playlist?list=PLUl4u3cNGP63oMNUHXqIUcrkS2PivhN3k)\n\n### Road Map\n![](notes/2021/2021.11/assets/img_2022-10-15-3.png)[^1]\n\n\n## Syllabus\n[MIT OpenCourseWare](https://ocw.mit.edu/courses/mathematics/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/readings/)\n\n\n[^1]:[Relationship](https://ocw.mit.edu/courses/mathematics/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/18-065s18_big2.jpg) among linear algebra, probability and statistics, optimization, and deep learning. Courtesy of Jonathan Harmon. ","lastmodified":"2022-10-15T14:06:29.082498057Z","tags":null},"/notes/2021/2021.11/MIT_18.065-Part_1-A_Column_Space_Perspective":{"title":"MIT_18.065-Part_1-A_Column_Space_Perspective","content":"# The Column Space of A Contains All Vectors Ax \n\n\u003cdiv align=\"right\"\u003e 2021-11-12\u003c/div\u003e\n\nTags: #Math/LinearAlgebra \n\nVideo Link: [1. The Column Space of A Contains All Vectors Ax - YouTube](https://www.youtube.com/watch?v=YiqIkSHSmyc\u0026list=PLUl4u3cNGP63oMNUHXqIUcrkS2PivhN3k\u0026index=3)\n\n- This video reviewed some of the basic ideas of Linear algebra, especially \"the right way to think of a matrix\".\n\n## 理解Ax - 列空间\n$$Ax$$\n![](notes/2021/2021.11/assets/img_2022-10-15-4.png)\n看待矩阵与向量乘积的两种方式:\n- A的行向量与x的dot product - Not Intuitive\n- A的列向量的线性组合 - Intuitive\n\n进一步, Ax可以表示Col(A), 即A的列空间.\n\n- Rank(A)就是A里面独立的列向量个数\n\n这可以带来一种有趣的分解形式:\n\n![](notes/2021/2021.11/assets/img_2022-10-15-5.png)\n\n$$A=CR$$\n\n$$A_{3\\times 3}=C_{3\\times 2}R_{2\\times 3}$$\n\n- 在矩阵C里面列都是独立的, 且构成Col(A)的基向量, 在矩阵R里面行也都是独立的, 也构成Row(A)的基向量. 这解释了一个重要的事实: \n\t- 矩阵的行秩等于列秩\n\n- The big factorization for data science is the \"SVD\" of A-when the first factor C\nhas r orthogonal columns and the second factor R has r orthogonal rows.\n\n- Actually R is a famous matrix in linear algebra:\nR = rref(A) = row-reduced echelon form of A (without zero rows).\n(A的行化简最简阶梯型)[^1]\n\n- $A=CR$的一个更一般的形式:\n\t- ![](notes/2021/2021.11/assets/img_2022-10-15-6.png)[^2]\n\n\n## 理解AB \n$$A_{m\\times n}B_{n\\times p}= M_{m\\times p}$$\n![](notes/2021/2021.11/assets/img_2022-10-15-7.png)\n\n我们用A的列向量乘上B的行向量, 得到一个矩阵, 这个矩阵构成了\"the perfect building blocks for every matrix.\"\n\n两个矩阵的积就是n个这样矩阵的加和.\n\n每一个矩阵的方向都是A里面一个列向量的方向, 即上面u的方向\n\n每一个矩阵的秩都为1, 所以AB即n个秩为一的矩阵的加和:\n![](notes/2021/2021.11/assets/img_2022-10-15-8.png)\n\n对比一下我们原来的思考方式, 两种方法分别对应向量的内积与外积:\n![](notes/2021/2021.11/assets/img_2022-10-15-9.png)\n\n\n\n[^1]: 一个听起来熟悉又陌生的名字哈哈哈\n[^2]: Linear Algebra and Learning from Data by Gilbert Strang, ","lastmodified":"2022-10-15T14:06:29.082498057Z","tags":null},"/notes/2021/2021.11/MIT_18.065-Part_10-SVD_in_Action":{"title":"MIT_18.065-Part_10-SVD_in_Action","content":"# SVD in Action\n\n\u003cdiv align=\"right\"\u003e 2021-11-17\u003c/div\u003e\n\nTags: #SVD #Math/LinearAlgebra \n\n\n\n## Economy SVD\n在被分解的矩阵A特别\"瘦高\"的时候(m\u003e\u003en), 我们可以只取$U$的前n列, 因为后面的\"重要性\"不大.\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/xy3QyyhiuY4?start=246\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n## Application\n\n### Digital Watermark\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/QQ8vxj-9OfQ?start=554\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n\n## Hands-on Tips\n### Plot how the information varies\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/QQ8vxj-9OfQ?start=610\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n\n\n### SVD Method of Snapshots\n- A different way to compute SVD if the data is so large that you can't store it into memory at once.\n\n[SVD Method of Snapshots - YouTube](https://youtu.be/rs63fnUWJkk)\n\n\n### Relation with Fourier\nSVD is kind of a data-driven generation of Fourier Transform/\n(如何理解？)\n\n傅里叶变换矩阵就是一个酉矩阵， SVD里面的U也是一个酉矩阵\n[Unitary Transformations - YouTube](https://www.youtube.com/watch?v=46Hpy4FiGls\u0026list=PLMrJAkhIeNNSVjnsviglFoY2nXildDCcv\u0026index=10)\n","lastmodified":"2022-10-15T14:06:29.082498057Z","tags":null},"/notes/2021/2021.11/MIT_18.065-Part_11-SVD__Linear_System":{"title":"MIT_18.065-Part_11-SVD_\u0026_Linear_System","content":"# SVD \u0026 Linear System\n\n\u003cdiv align=\"right\"\u003e 2021-11-18\u003c/div\u003e\n\nTags: #LinearRegression #SVD #Math/LinearAlgebra \n\n## $Ax=b$\n对于这个线性约束方程组: $$Ax=b$$\n\n只有在A可逆的方阵的时候, 才有唯一解: $$x=A^{-1}b$$\n\n而在A为其他形状的时候, 常常不能够简单的利用$A^{-1}$来求解这个方程组\n- Under-determined: (不定方程) \n\t- 这时我们没有足够的约束来限制x, x常常有无穷解\n\t- 换一个看法, 这可以看作因为$Row(A)$没有填满$R^n$, 所以我们可以在每一个解里面加上一部分核空间里面的向量$x_{kernel}$, 同时不影响方程的成立: \n\t- $$\\begin{aligned}\u0026A(x+x_{kernel})=b \\\\\\Rightarrow \u0026Ax+Ax_{kernel}=b\\\\\\Rightarrow \u0026Ax+0=b\\end{aligned}$$\n\n![](notes/2021/2021.11/assets/img_2022-10-15-40.png)\n- Over-determined: (超定方程)\n\t- 在这个情况下, 我们有太多限制来限制x, 所以有可能出现矛盾, 导致x没有解.\n\n![](notes/2021/2021.11/assets/img_2022-10-15-41.png)\n\n## Pseudo-Inverse \u0026 SVD\n- 对于长方形的情况, 我们可以定义矩阵A的\"伪逆\"(Pseudo-Inverse): $A^+$. 这样我们可以近似地求解不理想情况下的线性方程组\n\n- 我们定义$A^+$如下\n\t- 由SVD有:\n\t$$A=U\\Sigma V^T$$\n\t$$\\begin{aligned}\u0026\\quad \\quad Ax=b \\\\\n\t\u0026\\Rightarrow\\quad U\\Sigma V^Tx=b\\\\\n\t\u0026\\Rightarrow\\quad x=V\\Sigma^{-1}U^Tb\\end{aligned}$$\n\t- $$A^+=V\\Sigma^{-1}U^T$$\n\n这样, 我们就通过SVD得到了广义逆\n\n在$A$是可逆方阵的时候, 广义逆等于$A^{-1}$ :\n$$A^{-1}=(U\\Sigma V^T)^{-1}={(V^T)}^{-1}\\Sigma^{-1}U^{-1}=V\\Sigma^{-1}U^T$$\n\n- 这个广义逆也称为[Moore–Penrose inverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse#cite_note-Penrose1956-25)\n\n## Using Pseudo-Inverse to Solve $Ax=b$\n\n$$x=A^+b$$\n$$x=V\\Sigma^{-1}U^Tb$$\n- 在不定方程的情况下, 这个解是无穷多个解里面$l_2$ norm最小的\n![](notes/2021/2021.11/assets/img_2022-10-15-42.png)\n\n- 在超定方程的情况下, 这个解是最小二乘解, 即Least Square解, 是b向量对A的列空间上面的投影.\n![](notes/2021/2021.11/assets/img_2022-10-15-43.png)\n\n我们得到的近似解$\\hat x$到底是什么呢? 我们可以再计算$A\\hat x$ :\n$$\\begin{aligned}A\\hat x\u0026=AA^+b\\\\\n\u0026=(U\\Sigma V^T)(V\\Sigma^{-1}U^T)b\\\\\n\u0026=UU^Tb\\end{aligned}$$\n\n**注意**: 如果我们的U是Economy SVD里面的$\\hat U$, 即只取有效的前r列的U, 那么$UU^T\\neq I$.\n\n\nSource: [Linear Systems of Equations, Least Squares Regression, Pseudoinverse - YouTube](https://www.youtube.com/watch?v=PjeOmOz9jSY\u0026list=PLMrJAkhIeNNSVjnsviglFoY2nXildDCcv\u0026index=13)","lastmodified":"2022-10-15T14:06:29.082498057Z","tags":null},"/notes/2021/2021.11/MIT_18.065-Part_2-Matrix_Factorization":{"title":"MIT_18.065-Part_2-Matrix_Factorization","content":"# 矩阵分解\n\n\u003cdiv align=\"right\"\u003e 2021-11-12\u003c/div\u003e\n\nTags: #Matrix #Math/LinearAlgebra #Math \n\n![](notes/2021/2021.11/assets/img_2022-10-15-10.png)\n- 我们看待矩阵乘积的新方式有助于我们理解数据科学里面对矩阵的各种分解. 我们常常需要发掘一个矩阵$A$里面隐藏的信息, 而通过将$A$分解为$CR$, 我们可以观察A里面最基本的组成部分: 秩为1的矩阵: $col_k(C)\\ row_k(R)$\n- ![](notes/2021/2021.11/assets/img_2022-10-15-11.png)\n\n\n下面列举重要的分解, 在详细论述后将补充相应细节\n\n## 五个重要的矩阵分解\n![](notes/2021/2021.11/assets/img_2022-10-15-12.png)\n\n### $LU$分解 : $A=L U$\n- $A=L U$ 可以理解为矩阵的化简(elimination). \n- $L$ 代表 Lower Triangular, 是一个下三角矩阵, 同理, $U$代表Upper Triangular, 是一个上三角矩阵.\n- LU分解可以被视为高斯消去法的矩阵形式。$L$ 是高斯消元法的过程, 而$U$是高斯消元法的结果. 看下面这个例子:\n- ![LU分解的例子](notes/2021/2021.11/LU分解的例子.md)\n\n### 正交分解 : $A=Q R$\n- 通过将矩阵 $A$ 的列$\\boldsymbol{a}_{1}$ 到 $\\boldsymbol{a}_{n}$\"正交化\" (orthogonalize), 我们就得到了正交分解$A=Q R$\n\n- 其中:\n\t- 矩阵 $Q$ 的列相互正交 (orthonormal),  如果列向量长度为一, 还有:  $Q^{\\mathrm{T}} Q=I$ , \n\t- $R$ 是一个上三角矩阵(可能不是方阵).\n- 正交化常用的方法是 \"Gram-Schmidt\" 方法.\n\n![](notes/2021/2021.11/assets/Pasted%20image%2020211112210720.png)\n\n### $S=Q \\Lambda Q^{\\mathrm{T}}$\n- $S=Q \\Lambda Q^{\\mathrm{T}}$ comes from the eigenvalues $\\lambda_{1}, \\ldots, \\lambda_{n}$ of a symmetric matrix $S=S^{\\mathrm{T}}$ Eigenvalues on the diagonal of $\\Lambda$. Orthonormal eigenvectors in the columns of $Q$.\n\n- S是对称矩阵, Q是S的正交特征列向量, $\\Lambda$是特征值组成的对角矩阵.\n- [MIT_18.065-Part_3-A_Different_Perspectvie_of_Matrix_Multiplication-An_Example](notes/2021/2021.11/MIT_18.065-Part_3-A_Different_Perspectvie_of_Matrix_Multiplication-An_Example.md)\n\n### $A=X \\Lambda X^{-1}$ \n- $A=X \\Lambda X^{-1}$ is diagonalization when $A$ is $n$ by $n$ with $n$ independent eigenvectors. Eigenvalues of $A$ on the diagonal of $\\Lambda$. Eigenvectors of $A$ in the columns of $X$.\n\n### $A=U \\Sigma V^{\\mathrm{T}}$ \n- $A=U \\Sigma V^{\\mathrm{T}}$ is the Singular Value Decomposition of any matrix $A$ (square or not). Singular values $\\sigma_{1}, \\ldots, \\sigma_{r}$ in $\\Sigma$. Orthonormal singular vectors in $U$ and $V$.","lastmodified":"2022-10-15T14:06:29.082498057Z","tags":null},"/notes/2021/2021.11/MIT_18.065-Part_3-A_Different_Perspectvie_of_Matrix_Multiplication-An_Example":{"title":"MIT_18.065-Part_3-A_Different_Perspectvie_of_Matrix_Multiplication-An_Example","content":"\n#Math/LinearAlgebra #Matrix #Math \n\n\n$$S=Q \\Lambda Q^{\\mathrm{T}}$$\n- 其中S是一个对称矩阵, $S=S^{\\mathrm{T}}$\n- Q的行向量是S的特征向量, 这些特征向量相互正交\n$$Q=\\left[\\begin{array}{ccc}\n\\mid \u0026 \u0026 \\mid \\\\\nq_{1} \u0026 \\ldots \u0026 q_{n} \\\\\n\\mid \u0026 \u0026 \\mid\n\\end{array}\\right]$$\n- $\\Lambda$是对角矩阵, 由S的特征值组成\n\n- $S=(Q \\Lambda) (Q^{T})$ , 所以这个矩阵由$Q\\Lambda$的列向量组成:\n- $$Q\\Lambda=\\left[\\begin{array}{ccc}\\mid \u0026 \u0026 \\mid \\\\\nq_{1} \u0026 \\ldots \u0026 q_{n} \\\\\n\\mid \u0026 \u0026 \\mid\\end{array}\\right]\\left[\\begin{array}{ccc}\n\\lambda_1 \u0026 \u0026  \\\\\u0026 \\ddots \u0026  \\\\ \u0026 \u0026 \\lambda_n\n\\end{array}\\right]=\\left[\\begin{array}{ccc}\\mid \u0026 \u0026 \\mid \\\\\n\\lambda_1q_{1} \u0026 \\ldots \u0026 \\lambda_nq_{n} \\\\\n\\mid \u0026 \u0026 \\mid\\end{array}\\right]$$\n\n---\n$$S=(Q \\Lambda) Q^{\\mathrm{T}}=\n\\lambda_{1} \\boldsymbol{q}_{1}\\boldsymbol{q}_{1}^{\\mathrm{T}}+\n\\lambda_{2} \\boldsymbol{q}_{2}\\boldsymbol{q}_{2}^{\\mathrm{T}}+\n\\cdots+\n\\lambda_{n} \\boldsymbol{q}_{n} \\boldsymbol{q}_{n}^{\\mathrm{T}}$$\n---\n\n- 这样, S就被拆分成了秩为一的小矩阵的加和, 其中每一个小矩阵$\\lambda_{i} \\boldsymbol{q}_{i}\\boldsymbol{q}_{i}^{\\mathrm{T}}$也是对称矩阵, 因为这个小矩阵是特征向量$\\boldsymbol{q}_{i}$自己与自己的外积$\\boldsymbol{q}_{i}\\boldsymbol{q}_{i}^{\\mathrm{T}}$的$\\lambda_i$倍\n\n- 验证: 如果我们计算$Sq_1$:\n\t- 因为q相互正交, 所以: $\\boldsymbol{q}_{2}^{\\mathrm{T}}\\boldsymbol{q}_{1}=0$\n\t- 所以:\n\t$$\\lambda_{2} \\boldsymbol{q}_{2}\\boldsymbol{q}_{2}^{\\mathrm{T}}\\boldsymbol{q}_{1}=0$$\n- 又因为q是单位向量, 所以: $\\boldsymbol{q}_{1}^{\\mathrm{T}}\\boldsymbol{q}_{1}=1$\n\t$$S\\boldsymbol{q}_{1}=\n\\lambda_{1}\\boldsymbol{q}_{1}\\boldsymbol{q}_{1}^{\\mathrm{T}}\\boldsymbol{q}_{1}=\\lambda_{1}\\boldsymbol{q}_{1}$$\n\t符合特征向量的定义.\n\n","lastmodified":"2022-10-15T14:06:29.082498057Z","tags":null},"/notes/2021/2021.11/MIT_18.065-Part_4-LU_Factorization":{"title":"MIT_18.065-Part_4-LU_Factorization","content":"# LU 分解\n\n\u003cdiv align=\"right\"\u003e 2021-11-12\u003c/div\u003e\n\nTags: #Math/LinearAlgebra #Math #Matrix \n\n- $A=L U$\n- Key Idea: **怎样从A = Sum of Rank 1 Matrices的角度来理解这个分解?**\n\n- 简单的概念回顾:\n\t- [LU 分解 $A=LU$](notes/2021/2021.11/MIT_18.065-Part_2-Matrix_Factorization.md#LU%20分解%20A%20L%20U)\n\n## 看待Ax=B的两个角度\n### Row Perspective\n![](notes/2021/2021.11/assets/img_2022-10-15-13.png)\n- 几个约束方程的公共解, 即超平面在空间里面共同的交点\n\n### Column Perspective\n![](notes/2021/2021.11/assets/img_2022-10-15-14.png)\n- 利用基向量来表达目标向量\n\n- 对于二维的问题Row Perspective 看起来还行, 但是对于多维的情形, Column Perspective更加直观(想象好几个超平面的交点是很困难的!)\n\n## 回顾A=LU的分解步骤\n![LU分解的例子](notes/2021/2021.11/LU分解的例子.md)\n- 从上面的例子里面我们能够得到一种直觉: 求LU分解的过程就是每次解决一列, 直到结果完全变成三角矩阵.\n- **Intuition:**\n\t![](notes/2021/2021.11/assets/Pasted%20image%2020211113010753.png)\n\n## The \"Sum of Rank 1 Matrices\" Perspective\n- How is the original A related to the final matrix U ?\n\n$$\\boldsymbol{A}=\\boldsymbol{\\ell}_{\\mathbf{1}} \\boldsymbol{u}_{\\mathbf{1}}^{*}+\\boldsymbol{\\ell}_{\\mathbf{2}} \\boldsymbol{u}_{\\mathbf{2}}^{*}+\\boldsymbol{\\ell}_{\\mathbf{3}} \\boldsymbol{u}_{\\mathbf{3}}^{*}+\\boldsymbol{\\ell}_{\\mathbf{4}} \\boldsymbol{u}_{\\mathbf{4}}^{*}=\\left[\\begin{array}{cccc}\n1 \u0026 0 \u0026 0 \u0026 0 \\\\\n\\ell_{21} \u0026 1 \u0026 0 \u0026 0 \\\\\n\\ell_{31} \u0026 \\ell_{32} \u0026 1 \u0026 0 \\\\\n\\ell_{41} \u0026 \\ell_{42} \u0026 \\ell_{43} \u0026 1\n\\end{array}\\right]\\left[\\begin{array}{l}\n\\text { pivot row 1 } \\\\\n\\text { pivot row 2 } \\\\\n\\text { pivot row 3 } \\\\\n\\text { pivot row 4 }\n\\end{array}\\right]=\\boldsymbol{L} \\boldsymbol{U}$$\n\n每一个Rank为1的矩阵都\"处理(peel off)\"它对应的一行一列, 所有的矩阵加起来就构成了A:\n![](notes/2021/2021.11/assets/Pasted%20image%2020211113011439.png)\n![](notes/2021/2021.11/assets/Pasted%20image%2020211113011500.png)\n\n- Intuition:\n![300](notes/2021/2021.11/assets/Pasted%20image%2020211113012248.png)\n方向好像画错了, 但是You Get the Idea.\n\n### With Row Exchange\n上面的分解都是不带Row Exchange的, 但是有时候Pivot是0, 我们就需要交换一下行, 用对应位置非零的行来作为Pivot:\n![](notes/2021/2021.11/assets/Pasted%20image%2020211113012555.png)\n\n这是就变成了PLU分解, [维基百科上面有更详细的叙述](https://zh.wikipedia.org/wiki/LU%E5%88%86%E8%A7%A3)\n\n\n\n","lastmodified":"2022-10-15T14:06:29.082498057Z","tags":null},"/notes/2021/2021.11/MIT_18.065-Part_5-Four_Subspaces":{"title":"MIT_18.065-Part_5-Four_Subspaces","content":"# 4 Subspaces\n\n\u003cdiv align=\"right\"\u003e 2021-11-13\u003c/div\u003e\n\nTags: #Math/LinearAlgebra \n\n## What are they?\nThe Four Subspaces are:\n\n- $A$的列空间 $Col(A)$\n- $A$的行空间 $Row(A)=Col(A^T)$\n- $A$的核(零空间) $N(A) \\Rightarrow Ax=0$\n- $A^T$的核 $N(A^T) \\Rightarrow A^Ty=0$\n\n为了与课本一致, 我们将$Col(A)$简称为$C(A)$\n\n## What do they represent?\n怎样理解这四个子空间呢?\n\n对于$A_{m\\times n}$, 假设$Rank(A)=r$\n- 从线性映射的角度来看, $A_{m\\times n}$是从$\\mathbf{R^n}$到$\\mathbf{R^m}$的一个映射\n\n- $C(A)$是A的列空间, 即列向量张成的子空间\n\t- 每个列向量是m维的, 所以$C(A)$属于 $\\mathbf{R^m}$\n\t- $dim(C(A))=r$\n\n\n- $C(A^T)$是A的行空间, 即行向量张成的子空间\n\t- 每个行向量是n维的, 所以$C(A^T)$属于 $\\mathbf{R^n}$\n\t- $dim(C(A))=r$\n\n- $N(A)$是$Ax=0$的解空间, 属于映射前的空间$\\mathbf{R^n}$. 与行向量在同一个维度的空间下.\n\t- $dim(N(A))=n-r$\n\t\t![](notes/2021/2021.11/assets/img_2022-10-15-15.png)[^1]\n\n![400](notes/2021/2021.11/assets/img_2022-10-15-16.png)\n\n-  $N(A^T)$是$A^Ty=0$的解空间, 属于映射后的空间$\\mathbf{R^m}$. 与列向量在同一个维度的空间下.\n\t- $dim(N(A^T))=m-r$\n\n\n## How do they relate?\n- A great video:\n\t[Visualizing the Four Fundamental Spaces - YouTube](https://www.youtube.com/watch?v=ZdlraR_7cMA)\n![](notes/2021/2021.11/assets/img_2022-10-15-17.png)\n- \"A的每一行与x的内积都为0\", 所以$N(A)\\perp Row(A)\\Rightarrow N(A)\\perp C(A^T)$\n- 同理, $C(A)\\perp N(A^T)$\n![](notes/2021/2021.11/assets/img_2022-10-15-18.png)\n\n- 两个子空间在$\\mathbf{R^n}$, 两个子空间在$\\mathbf{R^m}$, 同一个维度的两个子空间相互垂直, 一起填满整个空间\n\n- 不同维度的空间通过一个线性映射联系起来, $A_{m\\times n}$将$\\mathbf{R^n}$映射到$\\mathbf{R^m}$\n![500](notes/2021/2021.11/assets/img_2022-10-15-19.png)\n\n- 每一个$\\mathbf{R^n}$里面的向量$\\mathbf{R^n}$都有一部分在$Row(A)$里面, 有一部分在$Null(A)$里面. 映射过去以后, 便都在ColA里面了.\n\t- x全部在$Null(A)$里面的时候, 映射过去就是0向量\n\t- 映射的过程其实可以看成两部分:\n\t\t![](notes/2021/2021.11/assets/img_2022-10-15-20.png)\n\t\tRow Space里面那部分被映射到Col Space, 而Null Space里面那部分则被映射到0.\n\n\n---\n\u003e 线性代数有好多看待问题的角度, 对于不同的问题, 不同角度来看是很不一样的.\n\u003e 比如我们有线性变换的角度, 坐标的角度, 内积外积的角度, 矩阵相乘的角度......\n\u003e 对于每一个问题, 从多个角度去想一想\n\n[^1]: C.lay, 线性代数及其应用, Chapter 4","lastmodified":"2022-10-15T14:06:29.082498057Z","tags":null},"/notes/2021/2021.11/MIT_18.065-Part_6-Orthonormal-Columns-in-Q-Give-QQ-I":{"title":"MIT_18.065-Part_6-Orthonormal Columns in Q Give Q'Q = I","content":"# Orthonormal Columns in Q Give Q'Q = I\n\n\u003cdiv align=\"right\"\u003e 2021-11-13\u003c/div\u003e\n\nTags: #Math/LinearAlgebra \n\n\u003e orthogonal: 正交的\n\u003e orthonormal: **Ortho**(gonal) + **normal**, 即又正交又是单位向量, 长度为1.\n\n## Orthogonal Vectors\n内积的向量化表示:  $\\boldsymbol{x}^{\\mathrm{T}} \\boldsymbol{y}=x_{1} y_{1}+\\cdots+x_{n} y_{n}=0$\n\n需要注意的是, 如果是复数相连的内积, 我们需要使用共轭复数(Conjugate)组成的向量来计算内积: $\\overline{\\boldsymbol{x}}^{\\mathrm{T}} \\boldsymbol{y}=\\bar{x}_{1} y_{1}+\\cdots+\\bar{x}_{n} y_{n}=0$\n\n### 与勾股定理的联系\n$$\\begin{aligned}||x-y||^2\u0026=(x-y)^T(x-y)\\\\\u0026=x^Tx+y^Ty-x^Ty-y^Tx\\\\\u0026=||x||^2+||y||^2-2||x||\\ ||y||cos\\theta\\end{aligned}$$\n\n- x y正交的时候 $cos\\theta=0$, \n\n## Orthogonal Matrix\n有了向量相互正交的表示, 我们可以很容易的推广到矩阵:\n\n- 如果一个矩阵的行向量和向量x正交, 那么有$Ax=0$, 这正是前面的核空间$Null(A)$\n\t![](notes/2021/2021.11/assets/img_2022-10-15-21.png)\n\t\n- 如果矩阵Q的列是单位正交的(Orthonormal), 那么有:\n\t![](notes/2021/2021.11/assets/img_2022-10-15-22.png)\n\t- 注意这个矩阵不一定是方阵, 它可以是一个又高又瘦的矩阵\n\n- 矩阵Q有Orthonormal的列向量, 那么Q代表的线性变换不改变向量的长度, 这一点很有用. \n\t- 证明: ![](notes/2021/2021.11/assets/img_2022-10-15-23.png)\n\t- Computations with Q never overflow!\n\n- 如果Q是一个方阵的话, $Q^{\\mathrm{T}} Q=I$ 说明 $Q^{\\mathrm{T}}=Q^{-1}$, 这也说明 $QQ^{\\mathrm{T}}=I$\n\t- 但是要是Q是一个又高又瘦的矩阵 (m\u003en), 那么因为m个列向量不可能是正交的(因为正交向量不可能多于矩阵的秩), 所以大小为$m\\times m$的矩阵$QQ^{\\mathrm{T}}\\neq I$\n\n- \"Orthogonal matrices\" are square with orthonormal columns: $Q^{\\mathrm{T}}=Q^{-1}$\n\t- 所以\"Orthogonal matrices\"其实叫\"Orthonormal matrices\"更合适\n\n$$Orthogonal\\ matrix: Q^{\\mathrm{T}} Q=QQ^{\\mathrm{T}}=I_{n\\times n}$$\n\n## 重要矩阵列举\n\n### Rotation Matrix \u0026 Reflection Matrix\n$$Q_{\\text {rotate }}=\\left[\\begin{array}{rr}\n\\cos \\theta \u0026 -\\sin \\theta \\\\\n\\sin \\theta \u0026 \\cos \\theta\n\\end{array}\\right]=\\text { rotation through an angle } \\theta$$\n\n$$Q_{\\text {reflect }}=\\left[\\begin{array}{rr}\n\\cos \\theta \u0026 \\sin \\theta \\\\\n\\sin \\theta \u0026 -\\cos \\theta\n\\end{array}\\right]=\\text { reflection across the } \\frac{\\theta}{2}\\text { line. }$$\n![](notes/2021/2021.11/assets/img_2022-10-15-24.png)\n\n- 并且正规矩阵的积也是正规矩阵:\n\t$$Q_{1} Q_{2} \\text { is orthogonal: } \\quad\\left(Q_{1} Q_{2}\\right)^{\\mathrm{T}}\\left(Q_{1} Q_{2}\\right)=Q_{2}^{\\mathrm{T}} Q_{1}^{\\mathrm{T}} Q_{1} Q_{2}=Q_{2}^{\\mathrm{T}} Q_{2}=I$$\n\t\n- Rotation $\\times$ rotation = rotation. \n- Reflection $\\times$ reflection = rotation. \n- Rotation$\\times$ reflection = reflection\n\t\n### Hadamard matrices\n![](notes/2021/2021.11/assets/img_2022-10-15-25.png)\n\n\n### Householder Reflections\n- Why It's Called Reflection? Watch the vid below:\n[Householder transformations, part 1 - YouTube](https://www.youtube.com/watch?v=6TIVIw4B5VA)\n$$H_{n}=I-2 u u^{\\mathrm{T}}$$\n- H是一个矩阵, 这个矩阵表示这样的一个线性变换: \n\t- **求向量x与某个超平面的对称向量**\n\t- 哪个超平面? 单位法向量为$u$的超平面\n\n- $H_n$是一个对称矩阵, 有:\n\t$$\\boldsymbol{H}^{\\mathrm{T}} \\boldsymbol{H}=\n\\boldsymbol{H}^{\\mathbf{2}}=\n\\left(I-2 \\boldsymbol{u} \\boldsymbol{u}^{\\mathrm{T}}\\right)\n\\left(I-2 \\boldsymbol{u} \\boldsymbol{u}^{\\mathrm{T}}\\right)=\nI-4 \\boldsymbol{u} \\boldsymbol{u}^{\\mathrm{T}}+\n4\\boldsymbol{u}\\boldsymbol{u}^{\\mathrm{T}}\\boldsymbol{u}\\boldsymbol{u}^{\\mathrm{T}}=\\boldsymbol{I}$$\n\n- 如果我们选这一个单位向量: $u = (1, 1, ... , 1)/ \\sqrt n$\n\t- 那么\n\t\t$$H_n=I-2 u u^{\\mathrm{T}}=I-\\frac 2 n \\mathrm{ones(n,n)}$$\n\n两个例子:\n$$\\boldsymbol{H}_{3}=I-\\frac{2}{3} \\text { ones }=\\frac{1}{3}\\left[\\begin{array}{rrr}\n1 \u0026 -2 \u0026 -2 \\\\\n-2 \u0026 1 \u0026 -2 \\\\\n-2 \u0026 -2 \u0026 1\n\\end{array}\\right] \\quad \\boldsymbol{H}_{4}=I-\\frac{2}{4} \\text { ones }=\\frac{1}{2}\\left[\\begin{array}{rrrr}\n\\mathbf{1} \u0026 -1 \u0026 -1 \u0026 -1 \\\\\n-1 \u0026 \\mathbf{1} \u0026 -1 \u0026 -1 \\\\\n-1 \u0026 -1 \u0026 \\mathbf{1} \u0026 -1 \\\\\n-1 \u0026 -1 \u0026 -1 \u0026 \\mathbf{1}\n\\end{array}\\right]$$\n\n- 关于特征值这一点还不是很理解:\nThe \"eigenvalues\" of H are -1 (once) and +1 (n- 1 times). All reflection matrices have eigenvalues -1 and 1.\n\n\n### Haar wavelets\n![](notes/2021/2021.11/assets/img_2022-10-15-26.png)\nn=8\n$$\\left[\\begin{array}{ccc}1\u00261\u00261\u0026\u00261\u0026\u0026\u0026\\\\ 1\u00261\u00261\u0026\u0026-1\u0026\u0026\u0026\\\\ 1\u00261\u0026-1\u0026\u0026\u00261\u0026\u0026\\\\ 1\u00261\u0026-1\u0026\u0026\u0026-1\u0026\u0026\\\\ 1\u0026-1\u0026\u00261\u0026\u0026\u00261\u0026\\\\ 1\u0026-1\u0026\u00261\u0026\u0026\u0026-1\u0026\\\\ 1\u0026-1\u0026\u0026-1\u0026\u0026\u0026\u00261\\\\ 1\u0026-1\u0026\u0026-1\u0026\u0026\u0026\u0026-1\\end{array}\\right]$$\n\n## Eigenvectors of $S=S^T$ \u0026 $Q^TQ=I$\n- **The Eigenvectors of a Symmetric Matrix and an Orthogonal Matrix is Orthogonal.**\n- 一个例子是下面的矩阵P的特征向量构成了 4 by 4 Fourier matrix F.\n$$P=\\left[\\begin{array}{llll}\n0 \u0026 1 \u0026 0 \u0026 0 \\\\\n0 \u0026 0 \u0026 1 \u0026 0 \\\\\n0 \u0026 0 \u0026 0 \u0026 1 \\\\\n1 \u0026 0 \u0026 0 \u0026 0\n\\end{array}\\right]$$\n$$F=\\left[\\begin{array}{ccrr}\n1 \u0026 1 \u0026 1 \u0026 1 \\\\\n1 \u0026 i \u0026 i_2 \u0026 i^3 \\\\\n1 \u0026 i^{2} \u0026 i_4 \u0026 i^6 \\\\\n1 \u0026 i^{3} \u0026 i^6 \u0026 i^9\n\\end{array}\\right]$$\n下面的矩阵Q有orthonormal的列向量:\n$$Q=\\frac{F}{2}=\\frac{1}{2}\\left[\\begin{array}{ccrr}\n1 \u0026 1 \u0026 1 \u0026 1 \\\\\n1 \u0026 i \u0026 -1 \u0026 -i \\\\\n1 \u0026 i^{2} \u0026 1 \u0026 -1 \\\\\n1 \u0026 i^{3} \u0026 -1 \u0026 i\n\\end{array}\\right]$$\n\n注意复数的内积需要取共轭:\n验证, 对于F, 第二列和第四列的内积:\n$[1,-i,i^2,-i^3]\\left[\\begin{array}{ccrr}1 \\\\i^3 \\\\i^6 \\\\ i^9\\end{array}\\right]=1-1+1-1=0$\n\n\n## 每一个向量空间$R^n$都有一组正交基\n- 这可以由Gram-Schmidt方法得到\n\n- 奇异值分解可以找到矩阵$A$的$Row\\ Space$的一组正交基: $u_1\\cdots u_r$, 矩阵$A$的$Column\\ Space$的一组正交基: $v_1\\cdots v_r$, 其中r是A的秩. \n\t- 这个两个正交基特殊的地方在于它们由矩阵A联系起来:\n\t\n$$\\text { Singular vectors } \\quad A \\boldsymbol{v}_{1}=\\sigma_{1} \\boldsymbol{u}_{1} \\quad A \\boldsymbol{v}_{2}=\\sigma_{2} \\boldsymbol{u}_{2} \\quad \\cdots \\quad A \\boldsymbol{v}_{r}=\\sigma_{r} \\boldsymbol{u}_{r}$$\n\n- For the bases from the SVD, multiplying by A takes an orthogonal basis of v's to an orthogonal basis of u's.\n\n\n## 投影矩阵\n![](notes/2021/2021.11/assets/img_2022-10-15-27.png)\n![](notes/2021/2021.11/assets/img_2022-10-15-28.png)\n![](notes/2021/2021.11/assets/img_2022-10-15-29.png)\n\n$$Px=QQ^{\\mathrm{T}}x=Q(Q^{\\mathrm{T}}x)=$$\n$$Col(Q)(Inner\\ product\\ of\\ Row(Q)\\ and\\ x)$$\n就是先计算x与Q里面各个正交基底的内积, 得到在这个正交基底下的\"坐标\", 然后再用Col(Q)表示出来.\n\n\n","lastmodified":"2022-10-15T14:06:29.082498057Z","tags":null},"/notes/2021/2021.11/MIT_18.065-Part_7-Eigenvalues-and-Eigenvectors":{"title":"MIT_18.065-Part_7-Eigenvalues and Eigenvectors","content":"# Eigenvalues and Eigenvectors\n\n\u003cdiv align=\"right\"\u003e 2021-11-14\u003c/div\u003e\n\nTags: #Math/LinearAlgebra #Math/LinearAlgebra/Eigenvalue \n\n## 特征值的一些性质\n![](notes/2021/2021.11/assets/img_2022-10-15-30.png)\n\n\n## 相似矩阵\n\n[[notes/2021/2021.11/理解相似矩阵]]\n\n### 相似矩阵有相同的特征值\n$$P^{-1}AP =  B$$\n假设矩阵$B$有特征值$\\lambda$:\n$$Bx=\\lambda x$$\n则\n$$\\begin{aligned}P^{-1}APx\u0026=\\lambda x \\\\\n\u0026\\Rightarrow \\\\ APx\u0026=P\\lambda x \\\\\n\u0026\\Rightarrow \\\\ A(Px)\u0026=\\lambda (Px)\\end{aligned}$$\n\nA也有特征值$\\lambda$, 对应特征向量$Px$\n- 而且$Px$刚好是 把B里面的特征向量x 通过基变换矩阵P 转换到A的坐标下.\n\n\n## Symmetric Matrix: $S=S^T$\n\n### S have Real Eigenvalues\nSee the document below:\n[Symmetric matrices have real eigenvalues](notes/2021/2021.11/Symmetric%20matrices%20have%20real%20eigenvalues.pdf)\n\n\n### S have Orthogonal Eigenvectors\n- This is a good video, 视频的开头同时也说明了Eigenvalue可能遇到的\"Defective Case\"\n[Eigenvectors of Symmetric Matrices Are Orthogonal - YouTube](https://www.youtube.com/watch?v=gJhlkEBZsfI)\n\n- 首先, 对于实对称矩阵S, 有$S=S^T$\n- 对于不同的两个特征值$\\lambda_1, \\lambda_2$, 有\n$$\\begin{aligned}Sx\u0026=\\lambda_1 x\\\\Sy\u0026=\\lambda_2 y\\end{aligned}$$\n- 我们做下面的变形, 将两边都变成内积:\n$$\\begin{aligned}y^TSx\u0026=\\lambda_1y^T x\\\\\nx^TSy\u0026=\\lambda_2x^T y\\end{aligned}$$\n- 对于第一个式子的左边, 因为是$y^T$与$Sx$的内积, 是一个数, 其转置还是自己:\n$$(y^TSx)^T=x^TS^Ty=x^TSy=\\text{第二个式子的左边}$$\n- 两式相减, 所以有:\n$$0=(\\lambda_1-\\lambda_2)x^T y=(\\lambda_1-\\lambda_2)x\\cdot y$$\n- 因为假设特征值是不相同的, 所以x与y的内积为0, 所以任意两个特征向量相互垂直.\n\n注意: 有的特征空间可能是多维的, 但是在这个特征空间里面也可以找到一个正交的基, 并且其他特征空间里面的特征向量是和这个特征空间垂直的, 自然也和这个正交的基垂直.\n\n---\n- **综上:** 实对称矩阵是一个很特殊的矩阵, 它只有实特征值, 并且特征向量都是相互正交的.\n\t- 这是一种找正交矩阵的很方便的方法\n\n## 对角化矩阵\n![](notes/2021/2021.11/assets/img_2022-10-15-31.png)\n\n对于对称矩阵, 则更为特殊:\n$$S=Q\\Lambda Q^{-1}$$\n因为对称矩阵的特征向量都是正交的, 我们有$Q^T=Q^{-1}$:\n$$S=Q\\Lambda Q^{T}$$\n- $S=Q\\Lambda Q^{T}$也被称为\"Spectral Theorem\"(谱定理) ^a919e0\n\n每一个实对称矩阵由两部分组成: 相互正交的特征向量组成的Q和实特征值组成的对角矩阵$\\Lambda$","lastmodified":"2022-10-15T14:06:29.082498057Z","tags":null},"/notes/2021/2021.11/MIT_18.065-Part_8-Positive-Definite-and-Semidefinite-Matrices":{"title":"MIT_18.065-Part_8-Positive Definite and Semidefinite Matrices","content":"# Positive Definite and Semi-definite Matrices\n\n\u003cdiv align=\"right\"\u003e 2021-11-14\u003c/div\u003e\n\nTags: #Math/LinearAlgebra\n\n- Positive Definite Matrices are the Best of the Symmetric Matrices.\n\n## 五个判别条件\n- 同时也是正定矩阵的重要性质:\n![](notes/2021/2021.11/assets/img_2022-10-15-32.png)\n- Positive Eigenvalues\n\t- 所有的特征值都是正数\n- Energy $x^TSx\u003e0$,  $\\forall x\\neq 0$\n\t- 有正的\"能量\", 这点后面会详述\n- $S=A^TA$, A has Independent Columns\n\t- S可以被分解为一个矩阵的转置与自己的乘积\n- All leading Determinants \u003e 0\n\t- ![](notes/2021/2021.11/assets/img_2022-10-15-33.png)\n- All Pivots in Elimination \u003e 0\n\t- ![](notes/2021/2021.11/assets/img_2022-10-15-34.png)\n\n### Energy\n(在视频里面没有找到详细的定义, 网上也没有相关的资料, 应当是一个直观的概念)\n- \"能量\"在这里体现为一种\"二次(quadratic)\"的概念, 比如在动能, 势能等定义里面, 都有二次项的存在.\n\n- x里面的能量通过计算$x^TSx$来得出, 类似于内积, 但是中间多了一个$S$\n\n以正定矩阵$$\\left[\\begin{array}{ll}\n2 \u0026 4 \\\\\n4 \u0026 9\n\\end{array}\\right]$$为例:\n\n$$[x, y]\\left[\\begin{array}{ll}\n2 \u0026 4 \\\\ 4 \u0026 9\n\\end{array}\\right]\\left[\\begin{array}{l}\nx \\\\ y \\end{array}\\right]=f(x, y)$$\n$$\\begin{align}\u0026=2x^2+9y^2+4xy+4yx\\\\\u0026=2x^2+9y^2+8xy\\\\\u0026=2(x+2y)^2+y^2\u003e0\n\\end{align}$$\n- 为什么能量大于零, 这个矩阵就是正定的呢?\n首先, 对于特征向量:\n$$\\text { If } S x=\\lambda x \\text { then } x^{\\mathrm{T}} S x=\\lambda x^{\\mathrm{T}} x \\text {. So } \\lambda\u003e0 \\text { leads to } x^{\\mathrm{T}} S x\u003e0 \\text {. }$$\n- 而在正规矩阵里面特征向量可以表示任意向量x, (因为它们构成一组正交基底):\n$$\\begin{aligned}\n\\boldsymbol{x}^{\\mathrm{T}} S \\boldsymbol{x} \u0026=\\left(c_{1} \\boldsymbol{x}_{1}^{\\mathrm{T}}+\\cdots+c_{n} \\boldsymbol{x}_{n}^{\\mathrm{T}}\\right) S\\left(c_{1} \\boldsymbol{x}_{1}+\\cdots+c_{n} \\boldsymbol{x}_{n}\\right) \\\\\n\u0026=\\left(c_{1} \\boldsymbol{x}_{1}^{\\mathrm{T}}+\\cdots+c_{n} \\boldsymbol{x}_{n}^{\\mathrm{T}}\\right)\\left(c_{1} \\lambda_{1} \\boldsymbol{x}_{1}+\\cdots+c_{n} \\lambda_{n} \\boldsymbol{x}_{n}\\right) \\\\\n\u0026=c_{1}^{2} \\lambda_{1} \\boldsymbol{x}_{1}^{\\mathrm{T}} \\boldsymbol{x}_{1}+\\cdots+c_{n}^{2} \\lambda_{n} \\boldsymbol{x}_{n}^{\\mathrm{T}} \\boldsymbol{x}_{n}\u003e\\mathbf{0} \\text { if every } \\boldsymbol{\\lambda}_{i}\u003e\\mathbf{0}\n\\end{aligned}$$\n可以看到正定矩阵里面的对角线元素对应二次项, 而其他元素对应交叉项.\n这个函数的图像如下图所示:\n![](notes/2021/2021.11/assets/img_2022-10-15-35.png)\n\n而这与机器学习里面的损失函数有着密切的联系: 最小化损失 \u003c=\u003e 最小化能量\n[Cost_Function_Intuition](notes/2021/2021.8/Part.4_Cost_Function_Intuition(ML_Andrew.Ng.).md)\n- 如果一个函数是严格凸的, 那么它的二阶导数矩阵在每一点都是正定的.\n- 对于极小值:\n![](notes/2021/2021.11/assets/img_2022-10-15-36.png)\n\n![](notes/2021/2021.11/assets/img_2022-10-15-37.png)\n如果矩阵S有负的特征值, 那么f的图像会在0以下. 在S是负定的时候(所有特征值都是负的), 与正定的时候相反, 函数会有最大值. 函数有的特征值大于零有的小于零, 那么它会有\"鞍点\", A saddle point matrix is \"indefinite\".\n![](notes/2021/2021.11/assets/img_2022-10-15-38.png)\n![](notes/2021/2021.11/assets/img_2022-10-15-39.png)\n\n\n## 半正定矩阵\n- Semi-definite allows energy / eigenvalues / determinants / pivots of S to be zero.\n\n- 第三点里面也允许不独立的列\n\n见视频里面的这段:\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/xsP-S7yKaRA?controls=0\u0026amp;start=2504\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n## $S=A^TA$\n$S=A^TA$容易知道是一个对称矩阵, 在这个情况下:\n\n$$Energy=x^TSx=x^TA^TAx = (Ax)^TAx= ||Ax||^2\\geq0$$\n\n- 所以S一定是正定或者半正定的\n\n当A有线性相关的列的时候, Ax可以=0, 这个时候S为半正定矩阵.\n\n\n\n\n","lastmodified":"2022-10-15T14:06:29.082498057Z","tags":null},"/notes/2021/2021.11/MIT_18.065-Part_9-Singular-Value-Decomposition-SVD":{"title":"MIT_18.065-Part_9-Singular Value Decomposition-SVD","content":"# Singular Value Decomposition\n\n\u003cdiv align=\"right\"\u003e 2021-11-14\u003c/div\u003e\n\nTags: #Math/LinearAlgebra #SVD\n\n![Singular-Value-Decomposition|500](notes/2021/2021.11/assets/Singular-Value-Decomposition.svg)[^2]\n- SVD将任意矩阵$A$分解成了三个部分:\n\t- $U$ 的列是相互正交的. -  Left Singular Vectors\n\t- $\\Sigma$ 的对角线上面是递减的奇异值. -  Singular Values\n\t- $V^T$ 的行是相互正交的. - Right Singular Vectors\n\n\n## Basic Concepts\n在最棒的情况下, 我们的矩阵是一个实对称矩阵$S$: 它有实特征值和正交的特征向量. 根据[谱定理](notes/2021/2021.11/MIT_18.065-Part_7-Eigenvalues%20and%20Eigenvectors.md#^a919e0), 我们能够将这个矩阵分解成以下形式:\n$$S=Q\\Lambda Q^T$$\n\n但是在很多情况下, 我们的特征空间并没有那么大, 所以我们需要将上述分解进行一些推广. SVD便是一种优美的推广形式:\n\n$$A=U\\Sigma V^T$$\n\n\n\n\n\n\n![Singular_value_decomposition](notes/2021/2021.11/assets/Singular_value_decomposition.gif)[^1]\n## SVD的两种形式\n- **完整形式的SVD长这样:**\n$$A_{m\\times n}=U_{m\\times m}\\Sigma_{m\\times n}V^T_{n\\times n}$$\n\n![](notes/2021/2021.11/assets/Pasted%20image%2020211116203007.png)\n\n这样的话有许多\"没用的部分\": $\\Sigma$里面有很多零, 并且U, V里面有许多零空间里面的向量(后面会解释).\n![400](notes/2021/2021.11/assets/Pasted%20image%2020211114211808.png)[^3]\n\n- **精简版的SVD长这样(The Reduced Form):**\n\n$$A_{m\\times n}=U_{m\\times r}\\Sigma_{r\\times r}V^T_{r\\times n}$$\n其中$r$是矩阵$A$的秩\n![](notes/2021/2021.11/assets/Pasted%20image%2020211116203445.png)\n这是$\\Sigma$是一个对角矩阵了, 并且$V, U$只包含\"有用的特征向量\"了\n\n## Proof\n### 预备部分\n[关于特征值的一个结论](notes/2021/2021.11/关于特征值的一个结论.md)\n[关于秩的一个结论](notes/2021/2021.11/关于秩的一个结论.md)\n[$S=A^TA$至少是半正定的, 所以它的特征值一定是非负的](notes/2021/2021.11/MIT_18.065-Part_8-Positive%20Definite%20and%20Semidefinite%20Matrices.md#S%20A%20TA)\n\n### 正式开始\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/CpD9XlTu3ys?start=182\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n[^4]\n\n证明和矩阵$A^TA$与$AA^T$有着紧密的联系:\n\n- [容易知道](notes/2021/2021.11/MIT_18.065-Part_8-Positive%20Definite%20and%20Semidefinite%20Matrices.md#S%20A%20TA)$A^TA$与$AA^T$都是对称矩阵:\n\t- $A^TA$是$Col(A)$相互乘, $AA^T$是$Row(A)$相互乘\n\n根据对称矩阵的[谱定理](notes/2021/2021.11/MIT_18.065-Part_7-Eigenvalues%20and%20Eigenvectors.md#^a919e0)这个良好的性质, 我们可以得到下面的这两个分解:\n$$\\begin{aligned}\n \u0026A^\\mathrm{T} A= V \\Sigma_1 V^{\\mathrm{T}} \\\\\n \u0026A A^\\mathrm{T}=U \\Sigma_2 U^{\\mathrm{T}}\n\\end{aligned}$$\n- 其中$V,U$均为正交矩阵, 为$A^\\mathrm{T} A$和$AA^\\mathrm{T}$的特征向量. \n\n如果我们假设SVD这个分解是成立的, 那么我们可以得到如下分解:\n$$\\begin{aligned}\n\u0026A^\\mathrm{T} A=(V \\Sigma^\\mathrm{T} U^\\mathrm{T})(U \\Sigma V^\\mathrm{T}) = V \\Sigma^2 V^\\mathrm{T} \\\\\n\u0026A A^\\mathrm{T}=(U \\Sigma V^\\mathrm{T})(V \\Sigma^\\mathrm{T} U^\\mathrm{T})=U \\Sigma^2 U^\\mathrm{T}\n\\end{aligned}$$\n可以猜想, 上下两个等式的$V,U,\\Sigma$ 都是对应的, 后面我们将会证明这是成立的, 即:\n- SVD分解$A=U\\Sigma V^T$里面的\n\t- $V^T$的行向量是$A^\\mathrm{T} A$的特征向量, 它们orthonormal\n\t- $U$的列向量是$AA^\\mathrm{T}$的特征向量, 它们orthonormal\n\t- $\\sigma^2_1$ 到 $\\sigma^2_r$ 是 $A^TA$ 与 $AA^T$ 的非零特征值, 后面会详细叙述.\n\n我们下一步将证明$A$将这两组正交向量$U,V$一一联系起来, 即:\n$$A v_{k}=\\sigma_{k} u_{k}$$\n\n观察我们上面的猜想, 我们假设$\\sigma_{k}=\\sqrt{\\lambda_{k}}$, 选择$A^TA$单位正交的一组特征向量$v_1, \\cdots v_r$\n- 因为$v_k$是$A^\\mathrm{T} A$的特征向量, 所以有\n$$A^{\\mathbf{T}} A v_{k}=\\sigma_{k}^{2} v_{k}$$\n- 根据证明的目标$A v_{k}=\\sigma_{k} u_{k}$, 我们构造\n$$u_{k}=\\frac{A v_{k}}{\\sigma_{k}}$$\n- 下面我们只需要证明$u_k$也是$A A^\\mathrm{T}$单位正交的特征向量即可:\n\t- 证明是特征向量:\n\t$$AA^\\mathbf{T}\\boldsymbol{u}_{k}=\n\tAA^{\\mathrm{T}}\\left(\\frac{A\\boldsymbol{v}_{k}}{\\sigma_{k}}\\right)=\n\tA\\left(\\frac{A^\\mathrm{T}A \\boldsymbol{v}_{k}}{\\sigma_{k}}\\right)=\n\tA\\frac{\\sigma_{k}^{2} \\boldsymbol{v}_{k}}{\\sigma_{k}}=\n\t\\sigma_{k}^{2} \\boldsymbol{u}_{k}=\\lambda_k\\boldsymbol{u}_{k}$$\n\t- 证明单位正交:\n\t\t$$\\boldsymbol{u}_{j}^{\\mathrm{T}} \\boldsymbol{u}_{k}=\\left(\\frac{A \\boldsymbol{v}_{j}}{\\sigma_{j}}\\right)^{\\mathrm{T}}\\left(\\frac{A \\boldsymbol{v}_{k}}{\\sigma_{k}}\\right)=\\frac{\\boldsymbol{v}_{j}^{\\mathrm{T}}\\left(A^{\\mathrm{T}} A \\boldsymbol{v}_{k}\\right)}{\\sigma_{j} \\sigma_{k}}=\\frac{\\sigma_{k}}{\\sigma_{j}} \\boldsymbol{v}_{j}^{\\mathrm{T}} \\boldsymbol{v}_{k}= \\begin{cases}1 \u0026 \\text { if } j=k \\\\ 0 \u0026 \\text { if } j \\neq k\\end{cases}$$\n\n如果我们证明的是缩减形式的SVD, 那么证明已经结束了, 如果证明的是一般形式的SVD, 那么需要将矩阵$U, V$补全:\n\n- 对于$V_{n\\times n}$我们可以从$Null(A)$里面选择相互正交的$n-r$个向量$v_{r+1}, \\cdots v_n$来补足正交向量\n- 对于$U_{m\\times m}$我们可以从$Null(A^T)$里面选择相互正交的$m-r$个向量$u_{r+1}, \\cdots u_m$来补足正交向量\n\n因为核空间$Null(A)$和$Row(A)$相互正交, $\\{v_1, \\cdots v_r\\}\\subset Row(A)$, $Null(A^T)$和$Col(A)$相互正交, $\\{u_1, \\cdots u_r\\}\\subset Col(A)$, 所以补足后的正交向量依然相互正交.\n\n这样, 我们就证明了SVD.\n\n\n## 不同的视角\n\n### 视角一\n- $AV=U\\Sigma$说明了, A将一个正交基底$V$映射到$U$, 而$U$依然是正交的\n\n![](notes/2021/2021.11/assets/Pasted%20image%2020211115195835.png)\n\n这个视频的开头部分演示了这个视角:\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/CpD9XlTu3ys\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n### 视角二\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/CpD9XlTu3ys?start=320\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n\n### 视角三\n\n\u003e The eigenvectors give $AX = XA$. But $AV = U\\Sigma$: needs two sets of singular vectors.\n\n- Right Singular vectors in $V$ contains orthonormal eigenvectors of $A^T A$\n\t- 因为$A^T A$在A的Row Space里面, V包含了Row(A)的信息\n- Left Singular vectors in $U$ contains orthonormal eigenvectors of $AA^T$\n\t- 因为$AA^T$在A的Column Space里面, U包含了Col(A)的信息\n- $\\sigma_1^2$ to $\\sigma_r^2$ are the nonzero eigenvalues of both $A^T A$ and $AA^T$\n\t - 奇异值包含的信息则是不同特征向量的\"重要性\"\n\n\n### 视角 3.5\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/WmDnaoY2Ivs?start=246\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n[^5]\n上面这个视频的重点是:$AA^T$, $A^TA$是有实际含义的: **Correlation Matrix**\n- U和V是两个相关矩阵的特征值\n\n\n\n\n### 视角四\n- $U$: \"Eigen-Faces\"\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/nbBvuuNVfco?start=277\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n- $V$: \"Eigen-Time\"/\"Eigen-Composition\"\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/nbBvuuNVfco?start=550\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n### 视角五\n**Sum of Rank 1 Matrices**: Different Importance \n\n - Why is the SVD so important for this subject and this book? Like the other factorizations A = LU and A = QR and $S = Q\\Lambda Q^T$, it separates the matrix into rank one pieces.\n- A special property of the SVD is that those pieces come **in order of importance**.\n![](notes/2021/2021.11/assets/Pasted%20image%2020211116214859.png)\n- The first piece of $\\sigma_1 u_1 v_1^T$ is the closest rank one matrix to $A$. More than that is true: The sum of the first k pieces is best possible for rank k:\n$$A_{k}=\\sigma_{1} u_{1} v_{1}^{\\mathrm{T}}+\\cdots+\\sigma_{k} u_{k} v_{k}^{\\mathrm{T}}$$\n\n\u003e ![](notes/2021/2021.11/assets/Pasted%20image%2020211116215418.png)\n\n下面这个演示也是这个视角, 这是PCA里面的主要视角\n[SVD Intuition](notes/2021/2021.11/SVD%20Intuition.md)\n\n\n\n\n## Example\n\n[[notes/2021/2021.11/A Fancy Example of SVD]]\n\n\n## More Illustrations\n[SVD Intuition](notes/2021/2021.11/SVD%20Intuition.md)\n- 关于SVD网上有很多很棒的讲解与演示, 这是一个很有趣也很重要的主题.\n\n\n\n\n\n\n\n\n[^1]: [Singular value - Wikipedia](https://en.wikipedia.org/wiki/Singular_value)\n[^2]: [Singular value decomposition - Wikipedia](https://en.wikipedia.org/wiki/Singular_value_decomposition)\n[^3]: [Podcast: Gilbert Strang's Feeling about Singular Value Decomposition - YouTube](https://www.youtube.com/watch?v=YPe5OP7Clv4)\n[^4]: What is the Singular Value Decomposition? - YouTube: https://youtu.be/CpD9XlTu3ys?t=182\n[^5]: Singular Value Decomposition (SVD): Dominant Correlations - YouTube: https://www.youtube.com/watch?v=WmDnaoY2Ivs\u0026list=PLMrJAkhIeNNSVjnsviglFoY2nXildDCcv\u0026index=4","lastmodified":"2022-10-15T14:06:29.082498057Z","tags":null},"/notes/2021/2021.11/OS-11_%E4%B8%AD%E6%96%AD":{"title":"OS-11_中断","content":"# 中断\n\n\u003cdiv align=\"right\"\u003e 2021-11-28\u003c/div\u003e\n\nTags: #OperatingSystem \n\n- 我觉得我对UNIX进程管理一直迷迷糊糊的原因就是还是对中断与上下文切换迷迷糊糊, 学习的顺序没有理清.\n\n- 我们来重新梳理一下:\n\n## 硬件\n- UNIX v6++ 建立在x86平台上面, 下面的叙述都是基于 x86 的\n\n### 中断控制器\n- 中断需要硬件支持, 即中断控制器, 用于处理**外设中断**\n- 中断控制器相当于CPU的秘书, 将各种外设发送来的中断请求汇总(考虑优先级), 一个一个地传个CPU进行处理\n\n\t- 在MIPS架构里面, 中断处理器对应协处理器0, 即CP0\n\t\t[CP0_CoProcessor0_in_CPU](../../../1_Project/2021.6_CPU/CP0_CoProcessor0_in_CPU.md)\n\t- i386的中断控制器由两个串联的`8259A`芯片构成:\n\t![400](notes/2021/2021.11/assets/img_2022-10-15-44.png)\n\n- 中断控制器保存当前进程的优先级, 以此判断是否应该发送新的中断请求, 这个优先级存放在中断控制器的\"处理机优先级寄存器\"里面\n\n\t- MIPS 里面 对应Status寄存器: Status register\n\n\t- 在i386里面倒是没有单独的处理及优先级寄存器, 而是用CS寄存器(代码段寄存器, 在CPU里面)的最后两位代表当前进程的优先级(Current Privilege Level, CPL)\n\n### 中断控制器如何与CPU交互?\n![](notes/2021/2021.11/assets/img_2022-10-15-45.png)\n\n注意: \n- CPU需要回送Accept信号(ACK)\n- CPU需要回送中断完成的信号(EOI, End of Interrupt)\n- 中断控制器也在向硬件回送ACK信号\n\n\n## 软件\n### 中断描述符 / 中断门\n中断门存放对中断的描述数据: \n![](notes/2021/2021.11/assets/img_2022-10-15-46.png)\n\n- 数据存在哪里?\n\t- Offset Segment Selector\n- 一些标志位\n\t- DPL: Descriptor Privilege Level 描述符优先级\n\t- P: 段是否在内存中\n\t- 等等\n\n#### How it works[^1]\n- CPU 在中断发生时将 Segment Selector 装入CS 寄存器、Offset 装入EIP 寄存器，这样就跳转到了相应的中断入口程序。\n- 由于中断门和陷入门中selector 字段的最后两位为00, 所以一旦将 selector 装入CS后，无论中断发生前处理器处千何种运行状态，执行中断处理程序时，处理器均在核心态运行。\n\t- 前面提到过, i386中, CS寄存器最后两位作为处理机状态字\n\n\n#### Interrupt Gate In UNIX v6++\n```c\n//src\\include\\IDT.h\n\n/* 定义了IDT中每一个门描述符的格式 */\nstruct GateDescriptor\n{\n\tunsigned short\tm_Low16BitsOffset;\t\t/*OFFSET的低16位*/\n\tunsigned short\tm_SegmentSelector;\t\t/*段选择子*/\n\tunsigned char\tm_Reserved : 5;\t\t\t/*保留区域，长5个bit*/\n\tunsigned char\tm_Zero : 3;\t\t\t\t/*全零区域*/\n\tunsigned char\tm_Type : 4;\t\t\t/*描述符类型.  0xE为中断门  0xF为陷入门*/\n\tunsigned char\tm_System : 1;\t\t/*1：系统描述符  0：代码、数据段描述符*/\n\tunsigned char\tm_DPL : 2;\t\t\t\t\t/*描述符访问优先级*/\n\tunsigned char\tm_SegmentPresent : 1;\t\t/*存在标志位*/\n\tunsigned short\tm_High16BitsOffset;\t\t\t/*OFFSET的高16位*/\n}__attribute__((packed));\n\n```\n### IDT 中断描述符表\nIDT: Interrupt Descriptor Table, 里面存放了所有的中断, 异常, 系统调用描述符\n\n- 系统启动后， IDT 常驻内存，位置不变，它的基地址和长度登记在 **CPU 内**的IDTR 寄存器中。\n\n#### IDTR \n![400](notes/2021/2021.11/assets/img_2022-10-15-47.png)\n\n- IDTR 长 48 位: 高 32 位保存IDT的**基地址**，低 16 位保存 IDT 的**长度**\n\n```cpp\n//IDT.h\nstruct IDTR\n{\n\tunsigned short\tm_Limit;\t\t/* IDT的限长 */\n\tunsigned int\tm_BaseAddress;\t/* IDT的起始地址(线性地址) */\n}__attribute__((packed));\n```\n\n\n\n#### IDT in UNIX v6++\n\n![](notes/2021/2021.11/assets/img_2022-10-15-48.png)\n##### 初始化\n- IDT对象是Machine类里面的私有成员:\n```cpp\n//Machine.h\nclass Machine\n{\n...\npublic:\n\tvoid LoadIDT();\t\t/* 把建立好的IDT表的基地址和长度加载进IDTR寄存器 */\n\tvoid InitIDT();\n\tIDT\u0026 GetIDT();\t\t\t\t\t\t/* 获取当前正在使用的IDT */\nprivate:\n\tIDT* m_IDT;   /* 这里实例化了IDT */\n...\n```\n\n- 在系统启动的时候, 实例化Machine对象, 同时调用`InitIDT`将IDT表加载进内存, 调用`LoadIDT`将IDT基地址和长度加载进IDTR寄存器\n```cpp\n//main.cpp\nextern \"C\" int main0(void)\n{\n\tMachine \u0026machine = Machine::Instance();\n...\n\t//init idt\n\tmachine.InitIDT();\n\tmachine.LoadIDT();\n...\n```\n\n##### 如何初始化IDT表\n在IDT类里面只实现了\"设置单个描述符\"的功能:\n```c\n//IDT.cpp\nvoid IDT::SetInterruptGate(int number, unsigned int handler)\n{\n\tthis-\u003em_Descriptor[number].m_Low16BitsOffset\t= handler;\n\tthis-\u003em_Descriptor[number].m_High16BitsOffset\t= handler\u003e\u003e16;\n\tthis-\u003em_Descriptor[number].m_SegmentSelector\t= 0x8;\n\tthis-\u003em_Descriptor[number].m_Reserved\t= 0;\n\tthis-\u003em_Descriptor[number].m_Zero\t\t= 0;\n\tthis-\u003em_Descriptor[number].m_System\t\t= 0;\n\tthis-\u003em_Descriptor[number].m_Type\t\t= 0xE;\t//中断门，清IF位\n\tthis-\u003em_Descriptor[number].m_DPL\t\t= 0x3;\n\tthis-\u003em_Descriptor[number].m_SegmentPresent\t= 1;\n}\nvoid IDT::SetTrapGate(int number, unsigned int handler)\n{\n...\t//其他的和上面一样\n\tthis-\u003em_Descriptor[number].m_Type\t\t= 0xF;\t//陷入门，不清IF位\n...\t\n}\n```\n\n- `InitIDT`: IDT表的初始化是在Machine类里面实现的:\n```c\n//machine.cpp\nvoid Machine::InitIDT()\n{\n\tthis-\u003em_IDT = \u0026g_IDT;\n\tfor (int i = 0; i \u003c= 255; i++)\n\t{\n\tif (i \u003c 32)\n\tthis-\u003eGetIDT().SetTrapGate(i, (unsigned long)IDT :: DefaultExceptionHandler);\n\telse\n\tthis-\u003eGetIDT().SetInterruptGate(i, (unsigned long)IDT :: DefaultInterruptHandler);\n\t}\n\t/* 初始化INT 0 - 31号异常 */\n\tthis-\u003eGetIDT().SetTrapGate(0, (unsigned long)Exception::DivideErrorEntrance);\n\t...Sililar Code....\n\tthis-\u003eGetIDT().SetTrapGate(19, (unsigned long) Exception::SIMDExceptionEntrance);\n\tthis-\u003eGetIDT().SetInterruptGate(0x20, (unsigned long) Time::TimeInterruptEntrance);\n\tthis-\u003eGetIDT().SetInterruptGate(0x21, (unsigned long) KeyboardInterrupt::KeyboardInterruptEntrance);\n\tthis-\u003eGetIDT().SetInterruptGate(0x2E, (unsigned long) DiskInterrupt::DiskInterruptEntrance);\n\t/* 0x80号中断向量作为系统调用，设置系统调用对应的陷入门 */\n\tthis-\u003eGetIDT().SetTrapGate(0x80, (unsigned long)SystemCall::SystemCallEntrance);\n}\n```\n我们单独看其中一项: \n```c\nthis-\u003eGetIDT().SetInterruptGate(0x21, (unsigned long) KeyboardInterrupt :: KeyboardInterruptEntrance);\n```\n可以看到, 这是键盘中断, 中断号是0x21, 中断入口程序是`KeyboardInterruptEntrance`:\n![](notes/2021/2021.11/assets/img_2022-10-15-49.png)\n后面会介绍一个中断的完整流程.\n\n- `LoadIDT`: 将长度与起始地址存入IDTR\n```c\nvoid Machine::LoadIDT()\n{\n\tIDTR idtr;\n\tGetIDT().FormIDTR(idtr);\n\tX86Assembly::LIDT((unsigned short *)(\u0026idtr));\n}\n```\n\n### 中断处理\n我们有了一个完整的IDT表, 现在来看看一个中断的具体流程是什么.\n\n#### 中断隐指令[^2]\n- CPU在接收到中断信号之后, 首先需要使用IDT表, 读取里面的信息. 这个过程是中断隐指令自动完成的.\n- 中断隐指令其实并不是指令, 而是由硬件(CPU)自动完成的.\n- 中断隐指令完成了从\"CPU接收到中断信号\"到\"进入中断入口程序\"这一段的转换, 在UNIX v6++里面没有这一段的代码, 因为这是CPU硬件完成的内容, 不是操作系统的工作\n\n##### 中断隐指令的过程\n![400](notes/2021/2021.11/assets/img_2022-10-15-50.png)\n1. 查询IDT, 获得相应中断源的中断门\n2. 关中断\n3. 现场保护\n\t- 将当前 EFLAGS 、CS 和 EIP 寄存器的值依次压入栈（ 如果中断前，进程运行在用户态，还需压入 SS 和 ESP 的值）\n\t- 对于会产生出错码的异常，除了将基本的EFLAGS 、CS 、EIP 压入堆栈之外，还会在后面压入一个ErrorCode \n\t- 中断隐指令保存的现场称为**硬件现场**\n4. 装入中断描述符\n\t- 将取得的中断门中的Segment Selector 装入CS, Offset 装入EIP。之后，CPU 将转入执行各种中断入口程序\n\n![](notes/2021/2021.11/assets/img_2022-10-15-51.png)\n\n### 中断入口程序 Interrupt service routines (ISR)\n\n![](notes/2021/2021.11/assets/img_2022-10-15-52.png)\n中断入口程序负责以下功能:\n```\n1. SaveContext();\t\t\t\t/*保存现场*/\n2. SwitchToKernel();\t\t\t/*切换至核心态*/\n3. CallHandler(Class, Handler); /*调用中断、异常处理子程序*/\n(这里可能有例行调度)\n4. RestoreContext(); \t\t\t/*恢复现场*/\n5. Leave(); \t\t\t\t\t/*手工撤销栈帧*/\n6. InterruptReturn();  \t\t\t/*中断返回*/\n```\n\n- 每个子程序的具体作用参见学校讲义, 上面比较详细\n- 下面是自己补充的说明:\n- 其中第三步`CallHandler(Class, Handler);`是不同中断区别最大的地方, 不同的中断根据IDT表有不同的中断处理子程序.\n\n![](notes/2021/2021.11/assets/img_2022-10-15-53.png)\n#### 比较不同的中断入口程序\n- 代码: \n[比较不同的中断入口程序](notes/2021/2021.11/assets/比较不同的中断入口程序.cpp)\n- 外设中断需要在中断处理子程序返回后发送EOI信号, 这是最标准的中断流程.\n- 时钟中断的特别之处在中断处理子程序Clock里面:\n\t- 时钟中断发送EOI在中断处理子程序里面, 因为时钟中断有一些很费时间的工作, 不能等到这些都完成了才继续接受中断\n\t\t![](notes/2021/2021.11/assets/img_2022-10-15-54.png)\n- 系统调用的特别之处只在中断处理子程序Trap里面, 因为系统调用不涉及外设, 所以不需要发送EOI\n- 异常的中断入口程序最为特别\n\t- 首先异常没有例行调度\n\t- 其次因为异常的中断隐指令可能会压入ErrorCode, 所以在恢复现场的时候需要跳过ErrorCode.\n\t- ~~还有不知道为什么,~~ Unix v6++里面的异常的入口程序都是用宏定义的\n\t\t- Unix v6++里面用宏定义是为了一次性定义大量的入口程序, 这样比较简洁.\n\n#### 中断处理时的中断保护\n- 中断的优先级处理是完全由中断控制器决定的, CPU不考虑这个问题\n\n- CPU只有一个IF标志位, IF标志位在EFLAGS寄存器里面, 它控制了CPU是否接受新的中断\n\t- `IF==1`, 表示接受中断(中断开), `IF==0`, 表示不接受中断(中断关)\n\t- CLI(CLear Interrupt)将标志位清零, 表示关中断\n\t- STI(SeT Interrupt)将标志位置一, 表示开中断[^4]\n\n- 重要性: CPU的标志位IF\u003e中断控制器的请求\n\n- 所以如果一个中断A后跟着更高优先级的中断B, 而中断A还没有开中断, 那么CPU不会立即响应中断B. \n\t- 中断的嵌套要在原子操作(不可分割的操作)以外进行, 即中断A完成原子操作以后, 开中断(STI), 再立即响应中断B.\n\n- 其实中断还分为maskable和Non-maskable的中断, IF标志位只对Maskable的标志位有效, Non-Maskable的中断一般是不可挽回的硬件错误, 要求立即响应.[^3]\n\n#### 中断嵌套的一些问题\n- 现在关于中断嵌套有一些矛盾的说法:\n\n##### The ideal way\n![](https://bbs-img.huaweicloud.com/data/forums/attachment/forum/202107/15/1636074zw5kme065cb89hp.png)\n1) 关中断。CPU响应中断后，首先要保护程序的现场状态，在保护现场的过程中，CPU 不应响应更高级中断源的中断请求。否则，若现场保存不完整，在中断服务程序结束后，也就不能正确地恢复并继续执行现行程序。\n\n2) 保存断点。为保证中断服务程序执行完毕后能正确地返回到原来的程序，必须将原来的程序的断点（即程序计数器PC）保存起来。\n\n3) 中断服务程序寻址。其实质是取出中断服务程序的入口地址送入程序计数器PC。\n\n4) 保存现场和屏蔽字。进入中断服务程序后，首先要保存现场，现场信息一般是指程序状态字寄存器PSWR和某些通用寄存器的内容。\n\n5) 开中断。允许更高级中断请求得到响应。\n\n6) 执行中断服务程序。这是中断请求的目的。\n\n7) 关中断。保证在恢复现场和屏蔽字时不被中断。\n\n8) 恢复现场和屏蔽字。将现场和屏蔽字恢复到原来的状态。  \n\n9) 开中断、中断返回。中断服务程序的最后一条指令通常是一条中断返回指令，使其返回到原程序的断点处，以便继续执行原程序。  \n\n - 其中，1~3步是在CPU进入中断周期后，由硬件自动（中断隐指令）完成的; 4~9步由中断服务程序完成。\n - 恢复现场是指在中断返回前，必须将寄存器的内容恢复到中断处理前的状态，这部分工作由中断服务程序完成。\n - 中断返回由中断服务程序的最后一条中断返回指令完成。[^5]\n\n##### Intel Manual[^6]\n在Intel Manual里面说明了以下几点:\n- STI CLI在仅在CPL\u003c=IOPL的时候才能使用:\n\t\u003e IOPL是EFLAGS里面的两个保护位, 用于保护IO\n\t\u003e ![](notes/2021/2021.11/assets/img_2022-10-15-55.png) \n\n- IF 中断标志位会被以下汇编指令影响:\n\t- PUSHF, POPF, 将EFLAGS寄存器的值存入/弹出栈\n\t- 进程切换会加载EFLAGS寄存器, 所以可能改变IF\n\t- IRET指令执行和中断隐指令相反的内容, 包括弹出并恢复保存的EFLAGS寄存器, 所以也可能改变IF\n\t- 当Interrupt是通过**Interrupt Gate**实现的时候, **IF标志位会被自动清零(关闭maskable中断)**\n\t\t- 与此对应的是, 如果一个中断是通过**Trap Gate**实现的时候, IF标志位不会被自动清零\n\n- 上面这一点是Interrupt Gate和Trap Gate的唯一区别: \n\u003e - The only difference between an interrupt gate and a trap gate is the way the processor handles the **IF flag** in the EFLAGS register. \n\u003e - When accessing an exception- or interrupt-handling procedure through an **interrupt gate**, the processor **clears the IF flag** to prevent other interrupts from interfering with the current interrupt handler. \n\u003e - A subsequent **IRET** instruction **restores the IF flag** to its value in the saved contents of the EFLAGS register on the stack. \n\u003e - Accessing a handler procedure through a **trap gate does not affect the IF flag**.\n\n在Intel手册里面没有提到中断嵌套的问题, 根据上面的叙述, 一个Interrupt Gate发起的中断不会被其他中断抢占.\n\n##### Linux Assembly Language Programming[^7] -  by Bob Neveln\n- 8086的设计使我们能够用一种非常简单的策略来避免中断相互冲突导致的死锁与栈溢出. 我们可以在运行中断入口程序(ISR)之前自动屏蔽中断(IF == 0), 在ISR运行结束以后再结束屏蔽 (恢复IF原来的值). \n- 这样, 对于两个连续的中断A, B, 中断控制器只需要一直向CPU发送中断B, 直到中断B被接受.\n- 但是这种策略的效率较低, 较为缓慢的磁盘中断会阻挡其他更为重要的中断.\n- 中断嵌套可以让高优先级的中断interrupt低优先级的中断, 要实现中断的嵌套, CPU和中断控制器都需要额外的工作:\n\t- ISR需要在一开始利用STI将CPU的IF位置为1, 开启中断\n\t- 中断控制器需要进行中断的优先级控制, 只向CPU转发比当前中断优先级更高的中断\n\t- CPU需要在一个中断结束的时候向中断控制器发送EOI指令, 允许转发相同或更低优先级的中断.\n\n##### Interrupts - Stonely Brook University Slide[^8]\nInterrupt - Slide(@nim04interruptsStonely)\n- 中断不应该被屏蔽, 所以Trap Gate进入的时候不应该关闭IF\n\n##### todo\n- 看Lions里面原来的是怎么写的\n- 加了STI试试\n\t- 加了还是可以正常运行, 至少键盘中断是这样\n- EOI在中断子程序之前?\n\n\n#### 中断处理子程序结束以后\n\n\n\n- 值得注意的是, EOI\n\n\n一个完整的例子;\n[Basic x86 interrupts | There is no magic here](https://alex.dzyoba.com/blog/os-interrupts/#x86%20interrupts)\n\n\n[^1]: 本笔记里面Unix v6++相关的内容均为同济大学操作系统课程相关资料\n[^2]: 找了半天也没有找到中断隐指令的英文对应词, 许多资料都只是把它当作了 Interrupt service routines (ISR)之前的一步, 没有具体说明.\n[^3]: [Non-maskable interrupt - Wikipedia](https://en.wikipedia.org/wiki/Non-maskable_interrupt)\n[^4]: [Interrupt flag - Wikipedia](https://en.wikipedia.org/wiki/Interrupt_flag)\n[^5]: [我们常说的中断和异常到底是什么_AltlasClub_HERO联盟](https://developer.huaweicloud.com/hero/thread-140036-1-1.html) 这里所说的中断服务程序即前文中的中断入口程序, 也即ISR: Interrupt Service Routine\n[^6]: 64-ia-32-architectures-software-developer-vol-3a-part-1-manual \n[^7]: Linux Assembly Language Programming (Open Source Technology Series) by Bob Neveln\n[^8]: - Nima Honarmand","lastmodified":"2022-10-15T14:06:29.082498057Z","tags":null},"/notes/2021/2021.11/Part.30_Dimensionality_ReductionML_Andrew.Ng.":{"title":"Part.30_Dimensionality_Reduction(ML_Andrew.Ng.)","content":"# Dimensionality Reduction - 降维\n\n\u003cdiv align=\"right\"\u003e 2021-11-11\u003c/div\u003e\n\nTags: #MachineLearning #DimensionalityReduction\n\n## Motivation\n![](notes/2021/2021.11/assets/img_2022-10-15-56.png)\n### Data Compression\n将数据维数减少后, 可以节约存储数据的空间\n\n### Visualization\n高维数据降维后才能可视化, 而可视化有助于我们理解高维数据的内在含义.\n\n## Principal Component Analysis\n[[notes/2021/2021.11/Part.31_Principal_Component_Analysis(ML_Andrew.Ng.)]]\n","lastmodified":"2022-10-15T14:06:29.082498057Z","tags":null},"/notes/2021/2021.11/Part.31_Principal_Component_AnalysisML_Andrew.Ng.":{"title":"Part.31_Principal_Component_Analysis(ML_Andrew.Ng.)","content":"# Principal Component Analysis - 主成分分析\n\n\u003cdiv align=\"right\"\u003e 2021-11-11\u003c/div\u003e\n\nTags: #MachineLearning #DimensionalityReduction #PCA\n\n## 基本步骤\n### Step 0 - Data Preprocessing\n- [Normalization 归一化](notes/2021/2021.8/Part.7_Feature_Scaling(ML_Andrew.Ng.).md#Normalization%20归一化)\n- [Standardization 标准化](notes/2021/2021.8/Part.7_Feature_Scaling(ML_Andrew.Ng.).md#Standardization%20标准化)\n\n[PCA依赖于欧氏距离](notes/2021/2021.8/Part.7_Feature_Scaling(ML_Andrew.Ng.).md#^375f2a), 所以预处理数据可以让降维效果更好.\n\n### Step 1 - Compute the Covariance Matrix\n$$\\Sigma=\\frac{1}{m} \\sum_{i=1}^{n}\\left(x^{(i)}\\right)\\left(x^{(i)}\\right)^{T}$$\n\n### Step 2 - Compute Eigenvectors of Matrix $\\Sigma$\n- Using Singular Value Decomposition\n- `[U,S,V] = svd(Sigma);`\n- We need to use\n\t $$U=\\left[\\begin{array}{cccc}\n\\mid \u0026 \\mid \u0026 \u0026 \\mid \\\\\nu^{(1)} \u0026 u^{(2)} \u0026 \\ldots \u0026 u^{(n)} \\\\\n\\mid \u0026 \\mid \u0026 \u0026 \\mid\n\\end{array}\\right] \\in \\mathbb{R}^{n \\times n}$$\n\n### Step 3 - Mapping the Data\n- 取出$U$里面的前$k$个特征向量:\n \t$$\\left[\\begin{array}{cccc}\n\t\\mid \u0026 \\mid \u0026 \u0026 \\mid \\\\\n\tu^{(1)} \u0026 u^{(2)} \u0026 \\ldots \u0026 u^{(k)} \\\\\n\t\\mid \u0026 \\mid \u0026 \u0026 \\mid\n\t\\end{array}\\right] \\in \\mathbb{R}^{n \\times k}$$\n\t- 这就是我们的投影矩阵 \n\n- 单个数据点的投影:\n\t$$z^{(i)}=\\left[\\begin{array}{cccc}\n\t\\mid \u0026 \\mid \u0026 \u0026 \\mid \\\\\n\tu^{(1)} \u0026 u^{(2)} \u0026 \\ldots \u0026 u^{(k)} \\\\\n\t\\mid \u0026 \\mid \u0026 \u0026 \\mid\n\t\\end{array}\\right]^{T} x^{(i)}_{n \\times 1}=\\left[\\begin{array}{c}\n-\\left(u^{(1)}\\right)^T- \\\\\n\\vdots \\\\\n-\\left(u^{(k)}\\right)^T-\n\\end{array}\\right]_{k \\times n}x^{(i)}_{n \\times 1}$$\n\n\n\n\n\n","lastmodified":"2022-10-15T14:06:29.082498057Z","tags":null},"/notes/2021/2021.11/Quid-pro-quo-Latin-Phrase":{"title":"Quid pro quo - Latin Phrase","content":"# Quid pro quo \n\n\u003cdiv align=\"right\"\u003e 2021-11-11\u003c/div\u003e\n\nTags: #Latin #English \n\n- **Quid pro quo** ('what for what' in Latin) is a Latin phrase used in English to mean an exchange of goods or services, in which one transfer is contingent upon the other; \"a favor for a favor\". \n- Phrases with similar meanings include: \"give and take\", \"tit for tat\", \"you scratch my back, and I'll scratch yours\", and \"one hand washes the other\". \n\nSource: [Quid pro quo - Wikipedia](https://en.wikipedia.org/wiki/Quid_pro_quo)","lastmodified":"2022-10-15T14:06:29.082498057Z","tags":null},"/notes/2021/2021.11/Roll-Pitch-Yaw":{"title":"Roll-Pitch-Yaw","content":"# Roll Pitch \u0026 Yaw\n\n\u003cdiv align=\"right\"\u003e 2021-11-14\u003c/div\u003e\n\nTags: #English\n\n![400](notes/2021/2021.11/assets/img_2022-10-15-57.png)\n\n三维空间里面的三个旋转方向\n\n[Roll, Pitch, and Yaw | How Things Fly](https://howthingsfly.si.edu/flight-dynamics/roll-pitch-and-yaw)\n\n![pitch-roll-yaw_0](notes/2021/2021.11/assets/img_2022-10-15.gif)","lastmodified":"2022-10-15T14:06:29.082498057Z","tags":null},"/notes/2021/2021.11/SVD-Intuition":{"title":"SVD Intuition","content":"## Section 1\n![](notes/2021/2021.11/assets/img_2022-10-15-58.png)\n![](notes/2021/2021.11/assets/img_2022-10-15-59.png)\n![](notes/2021/2021.11/assets/img_2022-10-15-60.png)\n![](notes/2021/2021.11/assets/img_2022-10-15-61.png)\n![](notes/2021/2021.11/assets/img_2022-10-15-62.png)\n![](notes/2021/2021.11/assets/img_2022-10-15-63.png)\nSource:\n- [Singular Value Decomposition (SVD) and Image Compression - YouTube](https://www.youtube.com/watch?v=DG7YTlGnCEo\u0026t=693s)\n\nThe Notebook in the link below is a good demonstration of what each rank 1 matrices represents:\n[GitHub - singular_value_decomposition](https://github.com/luisguiserrano/singular_value_decomposition)\n![](notes/2021/2021.11/assets/img_2022-10-15-64.png)\n![](notes/2021/2021.11/assets/img_2022-10-15-65.png)\n\n\n## Section 2 \n[Podcast: Gilbert Strang's Feeling about Singular Value Decomposition - YouTube](https://www.youtube.com/watch?v=YPe5OP7Clv4)\n\n","lastmodified":"2022-10-15T14:06:29.082498057Z","tags":null},"/notes/2021/2021.11/et-al.":{"title":"et al.","content":"# et al.\n\n\u003cdiv align=\"right\"\u003e 2021-11-30\u003c/div\u003e\n\nTags: #Latin \n\n    And others; to complete a list, especially of persons, as authors of a published work.\n\n\n- From Latin, abbreviation of et aliī (“and others”)\n\n## Usage notes\n- Formally preferred by some over `etc.` for lists of people in all contexts, reserving etc. for lists of things (inanimate objects); the distinction is sometimes ignored in casual use, and the two abbreviations are used synonymously in many contexts for completing lists except in very careful or formal use. However, in lists of authors of a published work, et al. is still regularly used.","lastmodified":"2022-10-15T14:06:29.13849866Z","tags":null},"/notes/2021/2021.11/somebody-is-golden":{"title":"somebody is golden","content":"# somebody is golden\n\n\u003cdiv align=\"right\"\u003e 2021-11-13\u003c/div\u003e\n\nTags: #\n\n\n- `informal` \n- used to say that someone is in a very good situation and is likely to be successful \n\nIf the right editor looks at your article, you’re golden.\n\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/k095NdrHxY4?start=313\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e","lastmodified":"2022-10-15T14:06:29.13849866Z","tags":null},"/notes/2021/2021.12/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1-Parameter_Estimation":{"title":"参数估计-Parameter_Estimation","content":"# 参数估计\n\n\u003cdiv align=\"right\"\u003e 2021-12-25\u003c/div\u003e\n\nTags: #MachineLearning #ParameterEstimation #Math/Statistics \n\n- 在设计分类器或者进行回归预测的时候, 我们需要知道目标问题的概率分布情况. 但是通常我们能得到的数据只是一些特例(即训练样本). 为了对问题进行建模, 我们不仅需要确定合适的概率分布模型, 还需要根据训练样本确定模型里面的具体参数. 参数估计就是在模型已知的情况下得到最优参数的过程.\n\n\n- 对于贝叶斯分类器, 估计先验概率$P(\\omega_i)$通常不是很困难. 难点在于估计类条件概率密度$p(x|\\omega_i)$, 这是因为: \n\t- 在很多情况下, 我们没有足够的样本\n\t- 在表示特征的向量$x$维数较大的情况下, 计算复杂度很高.\n\n- 目前比较常用的参数估计方法有极大似然估计(Maximum LIkelihood Estimation)和贝叶斯估计(Bayesian Estimation).\n\n\t- [Maximum_Likelihood_Estimation-极大似然估计](notes/2021/2021.12/Maximum_Likelihood_Estimation-极大似然估计.md)\n\n\n\n","lastmodified":"2022-10-15T14:06:29.162498918Z","tags":null},"/notes/2021/2021.12/%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83-Mutivariate_Gaussian":{"title":"多元高斯分布-Mutivariate_Gaussian","content":"# Multivariate Gaussian\n\n\u003cdiv align=\"right\"\u003e 2021-12-24\u003c/div\u003e\n\nTags: #GaussianDistribution #Math/Probability \n\n[正态分布_高斯分布_Normal_Distribution-Gaussian_Distribution](notes/2021/2021.9/正态分布_高斯分布_Normal_Distribution-Gaussian_Distribution.md)\n\n$$p(x ; \\mu, \\Sigma)=\\frac{1}{(2 \\pi)^{n / 2}|\\Sigma|^{1 / 2}} \\exp \\left(-\\frac{1}{2}(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)\\right)$$\n\n- 其中$\\Sigma$是随机向量$X$的协方差矩阵, 可以证明: $\\Sigma$一定是对称的正定矩阵.\n\n---\n---\n\n关于多元高斯分布有很多很好的资料:\n- CS229的Lecture Note\n- [CS229: Machine Learning](https://cs229.stanford.edu/syllabus.html)\n- 矩阵求导的那个PDF里面也有很好的推导过程\n- 这两个链接:\n\t- [搞懂多维高斯分布的由来 - 知乎](https://zhuanlan.zhihu.com/p/39763207)\n\t- [多元高斯分布完全解析 - 知乎](https://zhuanlan.zhihu.com/p/58987388)\n\n","lastmodified":"2022-10-15T14:06:29.162498918Z","tags":null},"/notes/2021/2021.12/%E6%96%B9%E5%B7%AE%E7%9A%84%E6%80%A7%E8%B4%A8":{"title":"方差的性质","content":"\n\n- $$\\begin{aligned}\n\\operatorname{Var}(X)\n\u0026=\\mathrm{E}\\left[X^{2}-2 X \\mathrm{E}[X]+(\\mathrm{E}[X])^{2}\\right]\\\\\n\u0026=\\mathrm{E}\\left[X^{2}\\right]-2 \\mathrm{E}[X] \\mathrm{E}[X]+(\\mathrm{E}[X])^{2}\\\\\n\u0026=\\mathrm{E}\\left[X^{2}\\right]-(\\mathrm{E}[X])^{2}\n\\end{aligned}$$\n上述的表示式可记为\"平方的期望减掉期望的平方\"。","lastmodified":"2022-10-15T14:06:29.162498918Z","tags":null},"/notes/2021/2021.12/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E7%9A%84%E5%88%A4%E5%88%AB%E5%87%BD%E6%95%B0":{"title":"正态分布的判别函数","content":"# Discriminant Function of Gaussian\n\n\u003cdiv align=\"right\"\u003e 2021-12-24\u003c/div\u003e\n\nTags: #MachineLearning #DiscriminantFunction #GaussianDistribution \n\n\u003e 下面是学习Duda模式分类第二章做的简单的笔记, 有时间应该进一步梳理\n\n高斯分布的判别函数(贝叶斯分类器)的一个常见形式是把Bayes定理的分子取下来, 再取对数.\n\n即以下形式:\n$$g_{i}(\\mathbf{x})=\\ln p\\left(\\mathbf{x} \\mid \\omega_{i}\\right)+\\ln P\\left(\\omega_{i}\\right)$$\n\n如果后验概率$p\\left(\\mathrm{x} \\mid \\omega_{i}\\right) \\sim N\\left(\\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right)$, 那么将多元高斯分布的公式带进去可以得到:\n[多元高斯分布-Mutivariate_Gaussian](notes/2021/2021.12/多元高斯分布-Mutivariate_Gaussian.md)\n\n$$g_{i}(\\mathrm{x})=-\\frac{1}{2}\\left(\\mathrm{x}-\\mu_{i}\\right)^{t} \\Sigma_{i}^{-1}\\left(\\mathrm{x}-\\mu_{i}\\right)-\\frac{d}{2} \\ln 2 \\pi-\\frac{1}{2} \\ln \\left|\\Sigma_{i}\\right|+\\ln P\\left(\\omega_{i}\\right)$$\n\n后面书上分三个情况,由特殊到一般, 分别说明了\n- $\\Sigma_{i}=\\sigma^{2} \\mathbf{I}$ : 空间里面分布着大小相同的球球\n- $\\Sigma_{i}=\\Sigma$: 空间里面分布着大小相同的椭球\n- $\\Sigma=$任意\n\n三种情况下判别函数的样子与位置:\n前两种情况都是线性的, 但是可能不过中点(还要看先验概率)\n第三种情况判别函数是任意二次型\n\n分析的过程就是把式子展开, 分别看哪个部分是和i无关的.\n\n\n\n","lastmodified":"2022-10-15T14:06:29.162498918Z","tags":null},"/notes/2021/2021.12/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E5%87%BD%E6%95%B0%E7%9A%84%E6%9C%9F%E6%9C%9B":{"title":"随机变量函数的期望","content":"一般的说，一个随机变量的函数的期望值并不等于这个随机变量的期望值的函数。\n$$\n\\mathrm{E}(g(X))=\\int_{\\Omega} g(x) f(x) \\mathrm{d} x \\neq g(\\mathrm{E}(X))\n$$","lastmodified":"2022-10-15T14:06:29.162498918Z","tags":null},"/notes/2021/2021.12/Bayesian-Decision-Theory-Part1":{"title":"Bayesian Decision Theory - Part1","content":"# 贝叶斯决策论 - Part1\n\n\u003cdiv align=\"right\"\u003e 2021-12-21\u003c/div\u003e\n\nTags: #MachineLearning #Bayes \n\n- 贝叶斯决策其实就是把生活中我们基于直觉和常识的决策方法形式化了, 并加以进一步地推广.\n- 贝叶斯决策综合考量每种情况的概率和决策带来的代价.\n- 贝叶斯决策假设问题可以用概率分布的形式来刻画, 属于贝叶斯学派的一种方法.\n\n## Intro\n### A Sad Case\n- 假如圣诞老人打包了100盒糖果, 其中20盒是巧克力($\\omega_1$), 80盒是水果硬糖($\\omega_2$), 现在他从里面随机挑了一盒给小企鹅, 盒子的颜色和重量都一样, 那么小企鹅得到的是巧克力还是水果硬糖呢?\n\t- 如果我们猜对了, 就可以得到一盒糖果!\n- 因为我们只知道不同礼物的比例, 也就是$P(\\omega_1)=0.2, P(\\omega_2)=0.8$, 所以为了使正确的概率最大, 我们应该猜礼物是 $\\omega_2$.\n- 但是要是圣诞老人要给整个南极洲的企鹅每人一个礼物呢? 如果我们一直猜, 为了获得更多的糖果, 我们只能一直猜礼物是 $\\omega_2$ ! 这显然太傻了.\n\n### More Information\n- 幸运的是, 我们发现了规律, 圣诞老人会根据小企鹅的体重($x$)来发放不同的礼物, 越胖的小企鹅越不容易得到巧克力($\\omega_1$).\n- 假设我们给所有已经有了礼物的小企鹅都称了一遍体重, 并且偷偷看了它们的礼物是什么. 现在我们知道了\"机密\": 不同礼物小企鹅的体重分布情况$P(x\\mid \\omega_i)$\n- 我们又从官方手册上查到了小企鹅的体重分布情况$P(x)$.\n- 现在圣诞老人又来给小企鹅发糖果了! 但是现在我们可以先偷偷给小企鹅称体重, 再猜它们会得到什么礼物. 假设下一只小企鹅\"Pupu\"的体重为$x$:\n- [根据Bayes定理](notes/2021/2021.12/Understanding%20Bayes'%20Theorem.md):\n\t$$P(\\omega_i \\mid x)=\\frac{P(x\\mid \\omega_i) P(\\omega_i)}\n\t{P(x)}$$\n\t也就是:\n\t$$postprior=\\frac{likelihood\\times prior}{evidence}$$\n\t- 后验概率$postprior$就是我们综合所有信息后作出的决策: 如果$P(\\omega_1 \\mid x)\u003eP(\\omega_2 \\mid x)$, 那么说明Pupu更可能得到巧克力, 反之亦然\n\t- $likelihood$就是我们掌握的\"机密\": 礼物已知的体重分布关系$P(x\\mid \\omega_i)$.\n\t- $evidence$是我们的\"线索\": Pupu的体重.\n\t- $prior$就是圣诞老人告诉我们的礼物组成: 20%的巧克力, 80%的水果硬糖.\n\n- 可以看到, 我们现在能够更聪明地作出决策了! 我们通过小企鹅的体重$x$, 综合 prior 分布和 likelihood 分布得到了一个更聪明的分布: postprior 分布. 也就是说, 我们综合考虑了\"胖企鹅得不到巧克力\" 和 \"巧克力有多少\", 而不是像之前那样一直猜同一个东西. 这就是贝叶斯决策的主要思想.\n\n### 已知的信息不重要\n- 因为在决策的时候我们关心的只有$\\omega_1$还是$\\omega_2$的概率大.  所以可以把分母上的evidence: $P(x)$看作一个比例系数, 作用是让x所有取值下的$P(\\omega_i \\mid x)$加起来为 1. 而真正和决策对象$\\omega$有关的只是分子上的$P(x\\mid \\omega_i) P(\\omega_i)$\n\n## 加入风险\n- 下面我们用更形式化的方法来介绍贝叶斯决策的进一步推广.\n\n- 有的时候不同的选择的代价是不一样的, 我们可以定量地刻画代价的不同, 并在决策的时候进行综合考虑:\n\t- 我们令$\\lambda\\left(\\alpha_{i} \\mid \\omega_{j}\\right)$为实际类别为$\\omega_{j}$的时候采取行动$\\alpha_{i}$的代价, 通常当$i=j$的时候这个值较小.\n\t- 那么采取行动$\\alpha_{i}$的总体代价可以表示为:\n\t$$R\\left(\\alpha_{i} \\mid \\mathbf{x}\\right)=\\sum_{j=1}^{c} \\lambda\\left(\\alpha_{i} \\mid \\omega_{j}\\right) P\\left(\\omega_{j} \\mid \\mathbf{x}\\right)$$\n\t$R\\left(\\alpha_{i} \\mid \\mathbf{x}\\right)$也被称为条件风险.\n\t- 更进一步, 对于任意的$\\mathbf{x}$, 我们的决策规则可以抽象为一个函数$\\alpha(\\mathbf{x})$, 自动给出情况$\\mathbf{x}$下最优的决策方案$\\alpha_i$. 所以情况$\\mathbf{x}$下的\"加权风险\"可以表示为: \n\t\t$$R(\\alpha(\\mathbf{x}) \\mid \\mathbf{x}) p(\\mathbf{x})$$\n\t- 综合下来, 对于所有可能的$\\mathbf{x}$, 整个决策方案的风险为:\n\t\t$$R=\\int R(\\alpha(\\mathbf{x}) \\mid \\mathbf{x}) p(\\mathbf{x}) d \\mathbf{x}$$ \n\t\t这就是我们想要最小化的东西.\n\n- 最小的代价也叫贝叶斯风险(Bayes risk), 是整个决策方案所能够达到的最佳水平.\n\n\n### 例子: 二分类问题\n二分类问题两个决策的代价可以表示如下:\n$$\\begin{aligned}\n\u0026R\\left(\\alpha_{1} \\mid \\mathbf{x}\\right)=\\lambda_{11} P\\left(\\omega_{1} \\mid \\mathbf{x}\\right)+\\lambda_{12} P\\left(\\omega_{2} \\mid \\mathbf{x}\\right) \\\\\n\u0026R\\left(\\alpha_{2} \\mid \\mathbf{x}\\right)=\\lambda_{21} P\\left(\\omega_{1} \\mid \\mathbf{x}\\right)+\\lambda_{22} P\\left(\\omega_{2} \\mid \\mathbf{x}\\right)\n\\end{aligned}$$\n- 当然, 如果$R\\left(\\alpha_{1} \\mid \\mathbf{x}\\right)\u003cR\\left(\\alpha_{2} \\mid \\mathbf{x}\\right)$, 我们会选择$\\alpha_1$, 因为此时风险更小.\n- 换个形式, 也就是$$\\left(\\lambda_{21}-\\lambda_{11}\\right) P\\left(\\omega_{1} \\mid \\mathbf{x}\\right)\u003e\\left(\\lambda_{12}-\\lambda_{22}\\right) P\\left(\\omega_{2} \\mid \\mathbf{x}\\right)$$或者$$\\left(\\lambda_{21}-\\lambda_{11}\\right) p\\left(\\mathbf{x} \\mid \\omega_{1}\\right) P\\left(\\omega_{1}\\right)\u003e\\left(\\lambda_{12}-\\lambda_{22}\\right) p\\left(\\mathbf{x} \\mid \\omega_{2}\\right) P\\left(\\omega_{2}\\right)$$时选择方案1.\n- 我们也可以表示为分数的形式:$$\\frac{p\\left(\\mathbf{x} \\mid \\omega_{1}\\right)}{p\\left(\\mathbf{x} \\mid \\omega_{2}\\right)} \u003e\\frac{\\lambda_{12}-\\lambda_{22}}{\\lambda_{21}-\\lambda_{11}} \\frac{P\\left(\\omega_{2}\\right)}{P\\left(\\omega_{1}\\right)}$$这时决策指标$\\frac{p\\left(\\mathbf{x} \\mid \\omega_{1}\\right)}{p\\left(\\mathbf{x} \\mid \\omega_{2}\\right)}$为一个数$\\theta$, 称为likelihood ratio(似然比). 这时控制决策边界的重要参数:\n![](notes/2021/2021.12/assets/Pasted%20image%2020211221211244.png)\n在上面这个图中, 如果将$\\omega_2$误判为$\\omega_2$的代价更大, 即 $\\lambda_{12}\u003e\\lambda_{21}$ . 那么会导致$\\theta$上升到$\\theta_b$, 即$\\mathcal{R}_{1}$变小, 判断为$\\omega_1$的情况表少.\n\n### 误差率\n- 二分类情况下, 一次决策的误差率可以表示为$$P(\\text { error } \\mid x)= \\begin{cases}P\\left(\\omega_{1} \\mid x\\right) \u0026 \\text { if we decide } \\omega_{2} \\\\ P\\left(\\omega_{2} \\mid x\\right) \u0026 \\text { if we decide } \\omega_{1}\\end{cases}$$\n- 总的情况下, 平均误差率可以表示为:$$P(\\text { error })=\\int_{-\\infty}^{\\infty} P(\\text { error }, x) d x=\\int_{-\\infty}^{\\infty} P(\\text { error } \\mid x) p(x) d x$$\n\n- 为了使误差率最小, 决策方案为: \n\tDecide $\\omega_{1}$ if $P\\left(\\omega_{1} \\mid x\\right)\u003eP\\left(\\omega_{2} \\mid x\\right) ;$ otherwise decide $\\omega_{2}$, \n也就是:$$P(\\text { error } \\mid x)=\\min \\left[P\\left(\\omega_{1} \\mid x\\right), P\\left(\\omega_{2} \\mid x\\right)\\right]$$\n\n### 例子: 最小误差率分类\n- 在很多情况下, 我们会希望误判的次数最少, 即误差率最小. 我们可以这样构造损失函数来获得贝叶斯决策下的最小误差率:\n$$\\lambda\\left(\\alpha_{i} \\mid \\omega_{j}\\right)=\\left\\{\\begin{array}{ll}0 \u0026 i=j \\\\1 \u0026 i \\neq j\n\\end{array} \\quad i, j=1, \\ldots, c\\right.$$\n在判断正确时为0, 在其他错误情况下都为1, 故这个损失函数也称0-1损失函数.\n- 我们可以证明在0-1损失函数下有着最小误差率:\n\t- 此时条件损失为:\n\t\t$$\\begin{aligned}\n\tR\\left(\\alpha_{i} \\mid \\mathbf{x}\\right) \u0026=\\sum_{j=1}^{c} \\lambda\\left(\\alpha_{i} \\mid \\omega_{j}\\right) P\\left(\\omega_{j} \\mid \\mathbf{x}\\right) \\\\\n\t\u0026=\\sum_{j \\neq i} P\\left(\\omega_{j} \\mid \\mathbf{x}\\right) \\\\\n\t\u0026=1-P\\left(\\omega_{i} \\mid \\mathbf{x}\\right)\n\t\\end{aligned}$$\n\t- 为了获得最小的总代价, 我们需要让每一个条件代价都尽可能地小, 也就是让$P(\\omega_{i} \\mid \\mathbf{x})$尽可能大, 也就是选择$\\mathbf{x}$已知时出现概率最大的$\\omega_{i}$: $$P(\\text { error } \\mid x)=\\min \\left[P\\left(\\omega_{i} \\mid x\\right) \\right]$$\n\n## 先验概率未知: Minimax Criterion\n- 有时候先验概率是不确定的, 但是likelihood是确定的, 我们希望能够在这种情况下也能够有较好的表现.\n- prior是不确定的会导致最佳表现(Bayes Risk)是关于prior的一个函数, 后面我们会看到Bayes Risk $R$是关于$P(\\omega_i)$的一个线性函数, 我们希望得到这个线性函数在最坏情况下的最好表现(Mini-Max, \"最小的最大\")\n\n- 怎么寻找这个\"最好表现\"?\n\t- 首先, 我们固定Prior: $P(\\omega_i)$, 计算在$P(\\omega_i)$不同取值下, 贝叶斯决策的最好表现(Bayes Risk), 得到下图中的拱形曲线. \n\t\t(下图中Bayes Risk以最小误差为例, 所以纵坐标为$P(error)$)\n\t![](notes/2021/2021.12/assets/Pasted%20image%2020211221221529.png)可以看到, 如果先验概率不变, 在$P(\\omega_1)=0.6$时, 模型有最坏的表现.\n\t- 在这些所有最好表现里面, 如果$P(\\omega_1)$偏移了最佳情况下的取值, 那么模型误差率随着$P(\\omega_1)$线性变化, 这表现为图中的虚线. \n\t\t- 在虚线代表的\"最好\"表现上可能发展到的\"最坏\"结果在$P(\\omega_1)=1$时取得, 大约为0.34\n\t- 我们可以看到, 为了限制这个\"最好表现\"上可能发展到的\"最坏结果\", 原来在0.6取到的最差的Bayes Risk反而成为了最好的情况, 换句话说, $P(\\omega_1)=0.6$时模型表现没有那么好, 但是很稳妥.\n\t- 模型在这一点取得的Bayes Risk即$R_{mm}$, minimax risk.\n\n下面我们证明likelihood不变的情况下, 模型的表现随先验概率呈线性变化.\n- 总的风险可以表示为:$$\\begin{aligned}\nR \u0026=\\int_{\\mathcal{R}_{1}}\\left[\\lambda_{11} P\\left(\\omega_{1}\\right) p\\left(\\mathbf{x} \\mid \\omega_{1}\\right)+\\lambda_{12} P\\left(\\omega_{2}\\right) p\\left(\\mathbf{x} \\mid \\omega_{2}\\right)\\right] d \\mathbf{x} \\\\\n\u0026+\\int_{\\mathcal{R}_{2}}\\left[\\lambda_{21} P\\left(\\omega_{1}\\right) p\\left(\\mathbf{x} \\mid \\omega_{1}\\right)+\\lambda_{22} P\\left(\\omega_{2}\\right) p\\left(\\mathbf{x} \\mid \\omega_{2}\\right)\\right] d \\mathbf{x}\n\\end{aligned}$$\n- 利用恒等式 $P\\left(\\omega_{2}\\right)=1-P\\left(\\omega_{1}\\right)$ 和 $\\int_{\\mathcal{R}_{1}} p\\left(\\mathbf{x} \\mid \\omega_{1}\\right) d \\mathbf{x}=1-\\int_{\\mathcal{R}_{2}} p\\left(\\mathbf{x} \\mid \\omega_{1}\\right) d \\mathbf{x}$ 我们可以把上式重新表示为:\n$$\\begin{aligned}\nR\\left(P\\left(\\omega_{1}\\right)\\right) \u0026=\\overbrace{\\lambda_{22}+\\left(\\lambda_{12}-\\lambda_{22}\\right) \\int_{\\mathcal{R}_{1}} p\\left(\\mathbf{x} \\mid \\omega_{2}\\right) d \\mathbf{x}}^{=R_{m m}, \\operatorname{minimax} \\text { risk }} \\\\\n\u0026+P\\left(\\omega_{1}\\right)[\\underbrace{\\left[\\left(\\lambda_{11}-\\lambda_{22}\\right)-\\left(\\lambda_{21}-\\lambda_{11}\\right) \\int_{\\mathcal{R}_{2}} p\\left(\\mathbf{x} \\mid \\omega_{1}\\right) d \\mathbf{x}-\\left(\\lambda_{12}-\\lambda_{22}\\right) \\int_{\\mathcal{R}_{1}} p\\left(\\mathbf{x} \\mid \\omega_{2}\\right) d \\mathbf{x}\\right]}_{=0 \\text { for minimax solution }} .\n\\end{aligned}$$\n在后面那部分为0的时候, 极小化极大风险为:\n$$\\begin{aligned}\nR_{m m} \u0026=\\lambda_{22}+\\left(\\lambda_{12}-\\lambda_{22}\\right) \\int_{\\mathcal{R}_{1}} p\\left(\\mathbf{x} \\mid \\omega_{2}\\right) d \\mathbf{x} \\\\\n\u0026=\\lambda_{11}+\\left(\\lambda_{21}-\\lambda_{11}\\right) \\int_{\\mathcal{R}_{2}} p\\left(\\mathbf{x} \\mid \\omega_{1}\\right) d \\mathbf{x}\n\\end{aligned}$$\n\n\n## 对风险有约束: Neyman-Pearson准则\n有时候我们需要在某个约束条件下最小化总风险, 比如我们做某个决策的资源是一定的, 就有约束:$$\\int R\\left(\\alpha_{i} \\mid \\mathbf{x}\\right) d \\mathbf{x}\u003c\\text { constant for some particular } i .$$\n在这个情况下的贝叶斯决策需要满足Neyman-Pearson准则, 我们通常用多次调节决策边界的方法来达到目的.\n\n\n\n\n\n\n\n\n\n","lastmodified":"2022-10-15T14:06:29.13849866Z","tags":null},"/notes/2021/2021.12/Bayesian_EstimationInference%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1":{"title":"Bayesian_Estimation(Inference)贝叶斯估计","content":"# 贝叶斯估计\n\n\u003cdiv align=\"right\"\u003e 2021-12-27\u003c/div\u003e\n\nTags: #Bayes #MachineLearning \n\n\n贝叶斯估计 有点难, 还要进一步学习\n\n- 贝叶斯估计是一种参数估计方法, 不只局限于设计贝叶斯分类器\n- 贝叶斯估计的核心在于用新的样本来更新旧的Prior, 一起得到一个PostPrior的参数的概率分布, 合并的过程利用的是贝叶斯分布.\n- 贝叶斯估计和极大似然估计的最大不同就是贝叶斯估计的是参数可能的概率分布, 而不是一个确定的值. 通过对这个概率分布进行积分, 我们可以平均地得到所有情况下最可能出现的参数.\n\n\n\n这两篇文章写的很好: \n- [Conjugate Prior Explained. With examples \u0026 proofs | by Aerin Kim | Towards Data Science](https://towardsdatascience.com/conjugate-prior-explained-75957dc80bfb)\n- [Bayesian Inference — Intuition and Example | by Aerin Kim | Towards Data Science](https://towardsdatascience.com/bayesian-inference-intuition-and-example-148fd8fb95d6)\n\nDuda书上的逻辑是有点乱的, 在整理笔记的时候先看上面的两片文章, 然后看书上的过程, 然后再自己推一遍正态分布的情况.\n","lastmodified":"2022-10-15T14:06:29.13849866Z","tags":null},"/notes/2021/2021.12/Chernoff-Bounds":{"title":"Chernoff Bounds","content":"# Chernoff Bounds\n\n\u003cdiv align=\"right\"\u003e 2021-12-04\u003c/div\u003e\n\nTags: #Math/Statistics \n\n[probabilitycourse.com - Chernoff Bounds](https://www.probabilitycourse.com/chapter6/6_2_3_chernoff_bounds.php)\n\nzotero:@ChernoffBounds(zotero://select/items/@ChernoffBounds)\n\n\n- The generic Chernoff bound for a random variable $X$ is attained by applying Markov's inequality to $e^{tX}$. This gives a bound in terms of the [moment-generating function](https://en.wikipedia.org/wiki/Moment-generating_function \"Moment-generating function\") of $X$. For every $t ≥ 0$:\n\n$$\\operatorname{Pr}(X \\geq a)=\\operatorname{Pr}\\left(e^{t \\cdot X} \\geq e^{t \\cdot a}\\right) \\leq \\frac{E\\left[e^{t \\cdot X}\\right]}{e^{t \\cdot a}}$$\n\n- $E\\left[e^{t \\cdot X}\\right]$实际上就是moment-generating function: $M_{X}(s)$\n\n$$\\begin{array}{ll}\nP(X \\geq a) \\leq e^{-t a} M_{X}(t), \u0026 \\text { for all } t\u003e0 \\\\\nP(X \\leq a) \\leq e^{-t a} M_{X}(t), \u0026 \\text { for all } t\u003c0\n\\end{array}$$","lastmodified":"2022-10-15T14:06:29.13849866Z","tags":null},"/notes/2021/2021.12/Chi-Squared_Distribution-%E5%8D%A1%E6%96%B9%E5%88%86%E5%B8%83":{"title":"Chi-Squared_Distribution-卡方分布","content":"# Chi-Squared Distribution\n\n\u003cdiv align=\"right\"\u003e 2021-12-04\u003c/div\u003e\n\nTags: #Math/Statistics \n\n[A very Good Website](https://www.probabilitycourse.com/chapter8/8_3_3_confidence_intervals_for_norm_samples.php)\n[Chi-squared distribution - Wikipedia](https://en.wikipedia.org/wiki/Chi-squared_distribution)\n\n## 重要结论\n\n$Z_{1},Z_{2},\\cdots,Z_{n}$ are independent standard normal random variables,\n\n- **The Sum of independent Standard Normal Variables are still Normal:**\n$$X=Z_{1}+Z_{2}+\\cdots+Z_{n}$$\n\nLink: [正态分布_高斯分布_Normal_Distribution-Gaussian_Distribution](notes/2021/2021.9/正态分布_高斯分布_Normal_Distribution-Gaussian_Distribution.md)\n\n- 但是正态独立随机变量的**平方和**是卡方分布的:\n\n$$Y=Z_{1}^{2}+Z_{2}^{2}+\\cdots+Z_{n}^{2}$$\nthen $Y$ is said to have a chi-squared distribution with $n$ degrees of freedom shown by $$Y \\sim \\chi^{2}(n)$$\n\n- 卡方分布其实是一种特殊的Gamma分布: $$Y\\sim Gamma\\left(\\frac n2,\\frac1 2\\right)$$\n\n- 下面是卡方分布的图像随着自由度的变化:\n\n![The chi-Square distribution](notes/2021/2021.12/assets/The%20chi-Square%20distribution.svg)","lastmodified":"2022-10-15T14:06:29.13849866Z","tags":null},"/notes/2021/2021.12/Covariance-%E5%8D%8F%E6%96%B9%E5%B7%AE":{"title":"Covariance-协方差","content":"# Covariance\n\n\u003cdiv align=\"right\"\u003e 2021-12-11\u003c/div\u003e\n\nTags: #Math/Statistics \n\n\n 期望值分别为  $E(X)=\\mu$  与  $E(Y)=\\nu$ 的两个随机变量  X  与  Y  之间的协方差定义为: \n$$\\begin{aligned}\n\\operatorname{cov}(X,Y)\u0026=\\mathrm{E}((X-\\mu)(Y-\\nu))\\\\\n\u0026=\\mathrm{E}(X \\cdot Y-\\nu X-\\mu Y +\\mu\\nu)\\\\\n\u0026=\\mathrm{E}(X \\cdot Y)-\\nu \\mathrm{E}(X)-\\mu \\mathrm{E}(Y) +\\mu\\nu\\\\\n\u0026=\\mathrm{E}(X \\cdot Y)-\\mu\\nu-\\mu\\nu +\\mu\\nu\\\\\n\u0026=\\mathrm{E}(X \\cdot Y)-\\mu \\nu\n\\end{aligned}$$ \n\n或者也可以表示为:\n$$\\operatorname{cov}(X,Y)=\\mathrm{E}(X \\cdot Y)-\\mathrm{E}(X)\\mathrm{E}(Y)$$\n\n- 如果$X,Y$是相互独立的, 那么它们之间的协方差为$0$ :\n\t$$\\begin{aligned}\n\\operatorname{cov}(X,Y)\u0026=\\mathrm{E}(X \\cdot Y)-\\mu \\nu\\\\\n\u0026=\\mathrm{E}(X)\\mathrm{E}(Y)-\\mu \\nu\\\\\n\u0026=\\mu \\nu-\\mu \\nu\\\\\n\u0026=0\n\\end{aligned}$$ ","lastmodified":"2022-10-15T14:06:29.13849866Z","tags":null},"/notes/2021/2021.12/Cross-Validation":{"title":"Cross Validation","content":"# Cross Validation\n\n\u003cdiv align=\"right\"\u003e 2021-12-19\u003c/div\u003e\n\nTags: #MachineLearning \n\n- 交叉验证是一种评价模型好坏的方法.\n- 设置验证集的目的在于减少样本给模型带来的Bias, 即我们想要找到一个普遍适用的模型, 而不是只在训练集上表现很好的模型.\n\t- 交叉验证是一种增大验证集, 充分利用数据的方法.\n\n\n## 交叉验证的方法\n- [Cross-validation (statistics) - Wikipedia](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation)\n- [Machine Learning Fundamentals: Cross Validation - YouTube](https://www.youtube.com/watch?v=fSytzGwwBVw)\n\n简单的来说, 交叉验证会把训练集随机分成几个部分. 每次选一小部分当作测试集(Validation Set). 一共训练多个模型, 综合所有结果评价模型的好坏.\n\n## 交叉验证的用处\n- 交叉验证可以用来:\n\t- 对于同一个问题, 选出表现最好的模型\n\t- 对于同一个模型, 选出最好的参数.\n\n## 交叉验证的分类\n交叉验证大致可以分为两类: Exhaustive \u0026 Non-exhaustive\n- Exhaustive方法的训练次数很多, 每次只用几个样本来测试模型, 这种方法通常被称作 Leave-p-Out Cross Validation, 即每次只leave out p 个样本用来测试\n![LOOCV|350](notes/2021/2021.12/assets/img_2022-10-15.gif)\n- Non-exhaustive 方法的训练次数更少, 测试集更大, 比如K折交叉验证(K-Fold Cross Validation)就是把样本分成K个部分, 每次舍弃一个作为测试集, 一共训练K个模型.\n![KfoldCV|400](notes/2021/2021.12/assets/img_2022-10-15-1.gif)\n\n\n## 交叉验证会生成一堆模型吗?\n不会, The purpose of cross-validation is model checking, not model building.[^1] 在交叉验证之后, 我们会用所有的数据作为训练集来训练一个最终的模型.\n\n- 这个回答写的非常好:  [How to choose a predictive model after k-fold cross-validation? - Cross Validated](https://stats.stackexchange.com/a/52277/354372)\n\n[^1]: [How to choose a predictive model after k-fold cross-validation? - Cross Validated](https://stats.stackexchange.com/a/52277/354372)","lastmodified":"2022-10-15T14:06:29.13849866Z","tags":null},"/notes/2021/2021.12/Dilemmalemma":{"title":"Dilemma\u0026lemma","content":"# The relation between Dilemma and lemma\n\n\u003cdiv align=\"right\"\u003e 2021-12-10\u003c/div\u003e\n\nTags: #English #Etymology #Latin \n\n- di: two, 两个\n- lemma: premise 假定, 先决条件\n- 在两个(不好的)可能性中间做出抉择, 即进退两难.\n- a form of argument involving a choice between equally unfavorable alternatives\n\n![](notes/2021/2021.12/assets/img_2022-10-15-1.png)\n\n\u003e A form of argument in which it is shown that whoever maintains a certain proposition must accept one or other of two alternative conclusions, and that each of these involves the denial of the proposition in question. [Century Dictionary]\n\nLoosely, \"choice between two undesirable alternatives,\" from 1580s. It should be used only of situations where someone is forced to choose between two alternatives, both unfavorable to him (the alternatives are called the horns of a dilemma). But even logicians disagree on whether certain situations are dilemmas or mere syllogisms. Related: Dilemmatic.[^1]\n\n\n[^1]: [dilemma | Etymology, origin and meaning of dilemma by etymonline](https://www.etymonline.com/word/dilemma)\n","lastmodified":"2022-10-15T14:06:29.13849866Z","tags":null},"/notes/2021/2021.12/Johnson-Lindenstrauss-Lemma-Publish-Version":{"title":"Johnson Lindenstrauss Lemma - Publish Version","content":"# Johnson Lindenstrauss Lemma\n\n\u003cdiv align=\"right\"\u003e 2021-12-03\u003c/div\u003e\n\nTags: #MachineLearning #Math \n\n- 对于高维数据，我们能够在降维的过程中保留其大部分的几何特征，即使降维的幅度非常大。\n\n\u003e 这是徐亦达老师让我们学习的第一个主题\n\n![](notes/2021/2021.12/assets/Pasted%20image%2020211205113411.png)[^1]\n\n## Study Materials\n- MIT 6.854 Spring 2016 Lecture 5: Johnson Lindenstrauss Lemma and Extensions PDF(zotero://select/items/@mcnally2021RethinkingKeypoint)\n\t- [MIT 6.854 Spring 2016 Lecture 5: Johnson Lindenstrauss Lemma and Extensions - YouTube](https://youtu.be/Tw0J5Xv6xQw)\n\t- Course Website: [6.854/18.415 Advanced Algorithms, Spring 2016](http://people.csail.mit.edu/moitra/854.html)\n\n- Course notes on dimensionality reduction from [TTI](http://ttic.uchicago.edu/~gregory/courses/LargeScaleLearning/lectures/jl.pdf) and [UBC](http://www.cs.ubc.ca/~nickhar/W12/Lecture6Notes.pdf)\n\t- [UBC Course homepage](https://www.cs.ubc.ca/~nickhar/W12/) PDF(zotero://select/items/@prof.nickharvey2011UBCCPSC) \n\t\t**This is the most Clear Version to me**\n\t- [TTI Course homepage - CMSC 3590 - Large Scale Learning, Spring 2009](https://home.ttic.edu/~gregory/courses/LargeScaleLearning/)PDF(zotero://select/items/@shamkakade2009TTICMSC)\n\n- Wikipedia: \n\t- [Johnson–Lindenstrauss lemma - Wikipedia](https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma)\n\t- [约翰逊-林登斯特劳斯定理 - 维基百科，自由的百科全书](https://zh.wikipedia.org/zh-hans/%E7%BA%A6%E7%BF%B0%E9%80%8A-%E6%9E%97%E7%99%BB%E6%96%AF%E7%89%B9%E5%8A%B3%E6%96%AF%E5%AE%9A%E7%90%86)\n## Johnson Lindenstrauss Lemma\n - $x_{1}, \\cdots, x_{m} \\in \\mathbb{R}^{n}$ 是$n$维空间里面的任意$m$个点,   $\\epsilon\\in(0,1)$. \n - 则存在 $d$ 维空间里面的$m$个点 $y_{1}, \\ldots, y_{m} \\in \\mathbb{R}^{d}$, 其中  $d=O\\left(\\log (m) / \\epsilon^{2}\\right)$ , 使得:\n$$\n\\begin{array}{rcl}\n(1-\\epsilon)\\left\\|x_{j}\\right\\|  \\leq\u0026\n\\left\\|y_{j}\\right\\|  \u0026\\leq\n(1+\\epsilon)\\left\\|x_{j}\\right\\| \\qquad \\forall j \\\\\n(1-\\epsilon)\\left\\|x_{j}-x_{j^{\\prime}}\\right\\| \\leq\u0026\n\\left\\|y_{j}-y_{j^{\\prime}}\\right\\| \u0026\\leq\n(1+\\epsilon)\\left\\|x_{j}-x_{j^{\\prime}}\\right\\| \\qquad \\forall j, j^{\\prime} .\n\\end{array}\n$$\n\n- 并且我们能在多项式时间里面找到一个  $y_{j}:=L\\left(x_{j}\\right)$ 的线性变换 $L: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{d}$ 使得不等式成立的概率至少为 $1-2 / \\mathrm{m} .$\n\n- 后面我们可以看到这个线性变换可以是一个由许多正态分布的随机变量构成的随机矩阵$A \\in \\mathbb{R}^{k \\times d}$.\n\n## A few things to note\n- 这个引理可以从降维的角度来加速算法\n\t- 举个例子, 比如求最小生成树的Kruskal算法, 如果我们需要计算$H$维空间里面$n$个点最小生成树, 那么需要计算$C_n^2=O(n^2)$个距离, 每个距离的计算花费$O(H)$的时间($H$较大时), 那么总的时间复杂度是$O(n^2H)$\n\t- 根据JL引理, 我们可以降维的过程中保留其大部分的几何特征, 在降到$d$维后Kruskal算法的时间复杂度为:\n\t\t- 每一个降维的体现为一次矩阵运算$A_{d\\times H}\\vec v_{H\\times 1}$, 时间复杂度为$O(dH)$, $n$个点总的时间复杂度为$O(dHn)$\n\t\t- 每一个距离的计算花费$O(d)=O\\left(\\log (m) / \\epsilon^{2}\\right)$, 总的距离计算需要花费$O\\left(n^2d\\right)$的时间.\n\t\t- 综上,总的时间复杂度为$O(dHn+n^2d)$, 在$d\u003c\u003cH$的时候, 这个提升是很大的.\n\n- 即使下面证明使用的是线性变换, JL引理同样适用于非线性变换. JL Lemma is tight.\n- JL的证明是构造性(Constructive)的, 从证明里面可以看到, 虽然转换矩阵是一个随机矩阵, 但是求得这个矩阵的概率是很高的, 多试几次总会得到一个合适的矩阵.\n- [When to use the Johnson-Lindenstrauss lemma over SVD? - Theoretical Computer Science Stack Exchange](https://cstheory.stackexchange.com/questions/21487/when-to-use-the-johnson-lindenstrauss-lemma-over-svd)\n\u003e - The JL Lemma says essentially \"you give me the error you want, and I'll give you a low dimensional space that captures the distances upto that error\". It's also a **worst-case** pairwise guarantee: for **each pair of points**, etc etc\n\u003e - The SVD essentially promises \"you tell me what dimension you want to live in, and I'll give you the best possible embedding\", where \"best\" is defined as **on average**: the total error of true similarity versus projected similarity is minimum.\n\n\n## Proof\n\n证明的思路如下图所示:\n\n![](notes/2021/2021.12/assets/JL_Proof.svg)\n\n### 用Union Bound推出JL\n#### Norm Preservation Property\n- 我们先如下构造一个线性变换$f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{d}$\n\t\n\t- 构造$d$个$n$维的随机向量$r_i$, 每一个向量都由$n$个独立同分布的元素 $r_{i,j}$ 构成. 则$f$是下面这个矩阵 $F$ , 它将$n$维向量映射为$d$维向量:\n$$\nF=\\left[\n\\begin{array}{c}\n---r_{1}--- \\\\\t\n\\vdots \\\\\n---r_{d}---\\\\\n\\end{array}\n\\right]_{d\\times n}\n$$\n\n-  对于$n$维单位向量 $v \\in \\mathbb{R}^{n}$ , $\\|v\\|=1$ 和 $d=O\\left(\\log (m) / \\epsilon^{2}\\right)$, 我们可以证明以下结论: \n\n$$\\operatorname{Pr}\\left[1-\\epsilon \\leq \\frac{\\|f(v)\\|}{\\sqrt{d}} \\leq 1+\\epsilon\\right] \\geq 1-2 / m^{3} .$$\n\n- 前面引理里提到的线性变换即$L(v):=f(v) / \\sqrt{d}$, 因为$f$是线性的, 所以$L$也是线性的.\n\n- 因为线性变换有这个优良的性质: $f(ax)=af(x)$, 所以上式其实可以推出任意向量$l_2$Norm的不变性: 对于任意长度的 $v$, 有$\\frac v {\\|v\\|}$为单位向量, 带入上面的不等式:\n$$\\operatorname{Pr}[(1-\\epsilon)\\leq\\|L(\\frac v {\\|v\\|})\\| \\leq(1+\\epsilon)] \\geq 1-2 / m^{3}$$\n$$\\Rightarrow$$\n$$\\operatorname{Pr}[(1-\\epsilon)\\|v\\| \\leq\\|L(v)\\| \\leq(1+\\epsilon)\\|v\\|] \\geq 1-2 / m^{3}$$\n\n如果我们将结果应用到 $v=x_{j}$ 和任意的 $v=x_{j}-x_{j^{\\prime}}$ (对于 $j \\neq j^{\\prime}$ ). 因为一共有$(^m_2)=O(m^{2})$ 对向量, 所以进一步应用 union bound 可以得到: 任意一对向量不满足上面式子的概率最大为 $2 / \\mathrm{m}$, 也就是成功的的概率为$1-2 / \\mathrm{m}$.\n\n#### Union Bound\n\n[[notes/2021/2021.12/Union_Bound-布尔不等式-Boole's_inequality]]\n\nFor any events $A_{1}, A_{2}, \\ldots, A_{n}$, we have\n$$\nP\\left(\\bigcup_{i=1}^{n} A_{i}\\right) \\leq \\sum_{i=1}^{n} P\\left(A_{i}\\right)\n$$\n\n展开就是:\n\n$$\n\\mathbb{P}\\left(A_{1} \\bigcup A_{2} \\bigcup \\cdots\\right) \\leq \\mathbb{P}\\left(A_{1}\\right)+\\mathbb{P}\\left(A_{2}\\right)+\\cdots\n$$\n\n#### Using Union Bound\n\n- 这部分这个TTI的讲义讲的比较好(zotero://select/items/@shamkakade2009TTICMSC)\n\n注意现在有 $O\\left(m^{2}\\right)$ 对向量 $u, v .$ 根据 union bound,\n$$\n\\begin{aligned}\n\u0026 \\operatorname{Pr}\\left(\\exists u, v \\text { s.t. the following event fails: }(1-\\epsilon)\\|u-v\\| \\leq\\|L(u-v)\\| \\leq(1+\\epsilon)\\|u-v\\|\\right) \\\\\n\\leq \u0026 \\sum_{\\forall\nu, v} \\operatorname{Pr}\\left(\\text { s.t. the following event fails: }(1-\\epsilon)\\|u-v\\| \\leq\\|L(u-v)\\| \\leq(1+\\epsilon)\\|u-v\\|\\right) \\\\\n\\leq \u0026 \\space m^2\\times\\frac 2 {m^3} \\\\\n=\u0026 \\frac 2 {m}\n\\end{aligned}\n$$\n所以总的成功概率为$1-2 / \\mathrm{m}$.\n\n### Prove the Norm Preservation Property\n\n![](notes/2021/2021.12/assets/JL_Proof_2.svg)\n\n#### $N(0, \\sigma_X^2)+N(0, \\sigma_Y^2)=N(0,\\sigma_X^2+\\sigma_Y^2)$\n\n[正态分布_高斯分布_Normal_Distribution-Gaussian_Distribution](notes/2021/2021.9/正态分布_高斯分布_Normal_Distribution-Gaussian_Distribution.md)\n\n- 我们可以把这个性质进一步推广到$m$个独立的随机变量: $Y_{1}, \\ldots, Y_{m}$ 且 $Y_{i}$ 服从 $N(0,1)$. 对于标量 $\\sigma_{1}, \\ldots, \\sigma_{m}$, 有 $\\sum_{i} \\sigma_{i} Y_{i}$ 服从 $N\\left(0, \\sum_{i} \\sigma_{i}^{2}\\right)$.\n\n#### Chi-Squared Distribution - 卡方分布\n\n[[notes/2021/2021.12/Chi-Squared_Distribution-卡方分布]]\n\n正态独立随机变量的**平方和**是卡方分布的:\n\n$$Y=Z_{1}^{2}+Z_{2}^{2}+\\cdots+Z_{n}^{2}$$\nthen $Y$ is said to have a chi-squared distribution with $n$ degrees of freedom shown by $$Y \\sim \\chi^{2}(n)$$\n\n#### Prove The Norm Preservation Lemma\n\n$$\\operatorname{Pr}\\left[1-\\epsilon \\leq \\frac{\\|f(v)\\|}{\\sqrt{d}} \\leq 1+\\epsilon\\right] \\geq 1-\\frac2  {m^{3}}$$\n\n这个结论实际上是对称的, 我们只证明一半, 即只证明右边那半边的失败概率小于 $1/m^3$ :\n\n$$\\operatorname{Pr}\\left[\\frac{\\|f(v)\\|}{\\sqrt{d}}\\geq 1+\\epsilon\\right] \\leq \\frac1 {m^{3}}$$\n\n为了方便, 我们可以平方一下里面的部分:\n\n$$\\operatorname{Pr}\\left[\\|f(v)\\|^{2}\u003e(1+\\epsilon)^2 d\\right]\\leq1/m^3$$\n\n- 证明的第一步是证明 $\\|f(v)\\|^{2}$ 实际上服从自由度为 $d$ 的卡方分布:\n\n$$\\|f(v)\\|^{2}=\\|Fv\\|^2=\\sum_{i=1}^{d}\\left(r_{i}^{T} v\\right)^{2}$$\n\n其中 $r_{i}^{T} v$ 是第 $i$ 行与向量 $v$ 的点乘, 即转化后的向量$f(v)$里面的第$i$个元素. 我们令其为 $X_i$, 容易知道 $X_i$ 是 $n$ 个服从标准正态分布的随机变量 $r_i$ 的加权和: $v_1r_{i1}+v_2r_{i2}+\\cdots+v_nr_{in}$, 权值是 $v$ 的对应元素, 所以 $X_i\\sim N\\left(0, \\sum_{i} v_{i}^{2}\\right)=N(0,1)$. (因为 $\\sum_{i} v_{i}^{2}=\\|v\\|=1$)\n\n所以上面的式子可以表示为:$$\\|f(v)\\|^{2}=\\sum_{i=1}^{d} X_{i}^{2}$$\n\n容易见得$\\|f(v)\\|^{2}\\sim \\chi^{2}(d)$, $d$是自由度.\n\n- 证明的第二步是利用类似于Chernoff Bound的方法来对事件 ( $\\|f(v)\\|^{2}\u003e(1+\\epsilon)^2 d$ ) 发生的概率上限进行约束:\n\n我们先简化一下问题的表述:\n\n令 $Y=\\|f(v)\\|^2=\\sum_{i=1}^{d} X_{i}^{2}$ ,  令 $\\alpha=d(1+\\epsilon)^{2}$. 需要证明的命题可以表述为:$$\\operatorname{Pr}[Y\u003e\\alpha] \\leq \\frac1 {m^{3}} \\tag{1}$$\n\n我们下面将证明\n\n$$\\operatorname{Pr}[Y\u003e\\alpha] \\leq \\exp \\left(-(3 / 4) d \\epsilon^{2}\\right)\\tag{2}$$\n\n\u003e 在 $d=4 \\ln (m) / \\epsilon^{2}$ 的时候, $(1)$与$(2)$等价:\n\u003e $$\\begin{aligned}\\exp(\\frac{-3} 4 d \\epsilon^{2})\n\u0026=\\exp(\\frac{-3}4 \\epsilon^{2}\\frac{4\\ln(m)}{\\epsilon^{2}})\\\\\n\u0026=\\exp(-3\\ln(m))\\\\\n\u0026=\\exp(\\ln(m^{-3}))\\\\\n\u0026=\\frac1 {m^{3}}\n\\end{aligned}$$\n\n令 $t\\in [0,1/2)$, 像[Chernoff Bounds](notes/2021/2021.12/Chernoff%20Bounds.md)里面一样, 我们可以把不等式换到指数部分里面去, 然后再应用Markov不等式:\n\n$$\\operatorname{Pr}[Y\u003e\\alpha]=\\operatorname{Pr}\\left[e^{t Y}\u003ee^{t \\alpha}\\right] \\leq  \\frac {\\mathrm{E}\\left[e^{t Y}\\right]}{e^{t \\alpha}}\\tag{3}$$\n\n随后, 因为$Y=\\sum_{i=1}^{d} X_{i}^{2}$, 而 $X_i$ 是相互独立的, 所以求和符号可以换到求数学期望的外面去:\n\n$$\\mathrm{E}\\left[e^{t Y}\\right]=\\mathrm{E}\\left[\\exp \\left(t \\sum_{i=1}^{d} X_{i}^{2}\\right)\\right]=\\prod_{i=1}^{d} \\mathrm{E}\\left[\\exp \\left(t X_{i}^{2}\\right)\\right]\\tag{4}$$\n\n对于$\\mathrm{E}\\left[\\exp \\left(t X_{i}^{2}\\right)\\right]$, 结合[关于随机变量函数的期望](notes/2021/2021.12/随机变量函数的期望.md), 我们可以将这个期望按照定义展开:\n\n$$\\mathrm{E}(g(X))=\\int_{\\Omega} g(x) f(x) \\mathrm{d} x$$\n$$\\Rightarrow$$\n$$\\begin{aligned}\n\\mathrm{E}\\left[\\exp \\left(t X_{i}^{2}\\right)\\right]\n\u0026= \\int_{-\\infty}^{\\infty} \\exp \\left(t y^{2}\\right) f(y) d y\\qquad (其中f(y)是X的概率密度函数)\\\\\n\u0026=\\int_{-\\infty}^{\\infty} \\exp \\left(t y^{2}\\right) \\frac{1}{\\sqrt{2 \\pi}} \\exp \\left(-y^{2} / 2\\right) d y\\\\\n\u0026=\\frac{1}{\\sqrt{2 \\pi}} \\int_{-\\infty}^{\\infty} \\exp \\left(t y^{2}\\right) \\exp \\left(-y^{2} / 2\\right) d y\\\\\n\u0026=\\frac{1}{\\sqrt{2 \\pi}} \\int_{-\\infty}^{\\infty} \\exp \\left(-y^{2}\\left(\\frac{1}{2}-t\\right)\\right) d y\\end{aligned}$$\n\n观察上面的式子, 如果 $t=0$, 那么这个式子就是一个标准正态分布的概率密度函数在整个数轴上面的积分, 结果为$1$. 为了达到相似的效果, 我们可以通过换元得到一个类似的形式:\n\n令$z=y \\sqrt{1-2 t}$, 有:\n\n$$\\begin{aligned}\n\\mathrm{E}\\left[\\exp \\left(t X_{i}^{2}\\right)\\right] \n\u0026=\\frac{1}{\\sqrt{2 \\pi}} \\int_{-\\infty}^{\\infty} \\exp \\left(\\frac{-(y \\sqrt{1-2 t})^{2}}2\\right) d y \\\\\n\u0026=\\frac{1}{\\sqrt{2 \\pi} \\sqrt{1-2 t}} \\int_{-\\infty}^{\\infty} \\exp \\left(-z^{2} / 2\\right) d z \\\\\n\u0026=\\frac{1}{\\sqrt{1-2 t}} \n\\end{aligned}$$\n\n结合$(3), (4)$, 可以得到:\n$$\\begin{aligned}\n\\mathrm{E}\\left[e^{t Y}\\right]\u0026=\n\\prod_{i=1}^{d}\\mathrm{E}\\left[\\exp\\left(tX_i^{2}\\right) \\right]\\\\\n\u0026=(1-2 t)^{-d / 2}\n\\end{aligned}$$\n$$\\operatorname{Pr}[Y\u003e\\alpha] \\leq  \\frac {(1-2 t)^{-d / 2}}{e^{t \\alpha}}=e^{-t \\alpha}(1-2 t)^{-d / 2}$$\n\n接下来我们只需要选择一个合适的 $t$ 即可, 我们令 $t=(1-d / \\alpha) / 2$ \n\n$$\\operatorname{Pr}[Y\u003e\\alpha] \\leq e^{-t \\alpha}(1-2 t)^{-d / 2}=e^{(d-\\alpha) / 2}(d / \\alpha)^{-d / 2}$$\n\n现在我们带入$\\alpha=d(1+\\epsilon)^{2}$, 得到:\n$$\\exp \\left(\\frac{d}{2}\\left(1-(1+\\epsilon)^{2}\\right)-\\frac{d}{2} \\ln \\left(\\frac{1}{(1+\\epsilon)^{2}}\\right)\\right)\n=\\exp \\left(-d\\left(\\epsilon+\\epsilon^{2} / 2-\\ln (1+\\epsilon)\\right)\\right)$$\n\n%%注意括号里面的这部分: $\\epsilon+\\epsilon^{2} / 2-\\ln (1+\\epsilon)$%%\n\n我们的证明目标是 $-(3 / 4) d \\epsilon^{2}=-d\\frac3 4\\epsilon^{2}$, 所以我们只需要证明:\n$$\\begin{aligned}\n-d\\left(\\epsilon+\\epsilon^{2} / 2-\\ln (1+\\epsilon)\\right)\u0026\\leq-d\\frac3 4\\epsilon^{2}\\\\\n\\epsilon+\\epsilon^{2} / 2-\\ln (1+\\epsilon)\u0026\\geq\\frac3 4\\epsilon^{2}\\\\\n(\\epsilon+\\epsilon^{2} / 2)-\\frac3 4\\epsilon^{2}\u0026\\geq\\ln (1+\\epsilon)\\\\\n\\epsilon-\\epsilon^{2} / 4\u0026\\geq\\ln (1+\\epsilon)\\\\\n\\end{aligned}$$\n即可\n\n利用函数的凹凸性, 我们可以证明在$x \\in[0,1]$有 $\\ln (1+x) \\leq x-x^{2} / 4$\n\n\u003e 两者在0取值都是0, 一阶导数都是1, 二阶导数$\\ln (1+x)$是负的, $x-x^{2} / 4$是正的.\n\n综上:\n$$\\operatorname{Pr}[Y\u003e\\alpha] \\leq \\exp \\left(-d\\left(\\epsilon+\\epsilon^{2} / 2-\\left(\\epsilon-\\epsilon^{2} / 4\\right)\\right)\\right) \\leq \\exp \\left(-(3 / 4) d \\epsilon^{2}\\right)$$\n\n我们即证明了Norm Preservation Lemma, 从而证明了Johnson Lindenstrauss Lemma\n\n#### 卡方分布的Chernoff Bound\n有的证明, 比如 TTI (zotero://select/items/@shamkakade2009TTICMSC), MIT(zotero://select/items/@prof.ankurmoitra2016MIT854)讲义里面的证明, 都应用了Chi-Square Distribution的一个[Chernoff Bounds](notes/2021/2021.12/Chernoff%20Bounds.md): \n\n\u003e Lemma 4 (Chernoff bound for chi-square distributions).\n\u003e $$\n\u003e \\begin{aligned}\n\u003e \u0026\\mathbb{P}\\left[\\sum_{i=1}^{k} Y_{i}^{2}\u003e(1+\\epsilon) k\\right] \\leq e^{-\\frac{k}{4}\\left(\\epsilon^{2}-\\epsilon^{3}\\right)} \\\\\n\u003e \u0026\\mathbb{P}\\left[\\sum_{i=1}^{k} Y_{i}^{2}\u003c(1-\\epsilon) k\\right] \\leq e^{-\\frac{k}{4}\\left(\\epsilon^{2}-\\epsilon^{3}\\right)}\n\u003e \\end{aligned}\n\u003e $$\n\u003e Now, we may set $k=O\\left(\\frac{\\log (1 / \\delta)}{\\epsilon^{2}}\\right)$ and get $\\|A x\\|_{2}^{2} {\\sim\\over1\\pm\\epsilon}\\|x\\|_{2}^{2}$ with probability at least $1-\\delta$.\n\n但是Chernoff Bound的证明很复杂, 我觉得UBC课件里面的证明(即上面的证明)更好.\n\n\n[^1]: [The Johnson-Lindenstrauss Lemma. Why you don’t always need all of your… | by Haris Angelidakis | Cantor’s Paradise](https://www.cantorsparadise.com/the-johnson-lindenstrauss-lemma-3058a123c6c) This is actually from a Presentation slides of Laurent Jacques.","lastmodified":"2022-10-15T14:06:29.13849866Z","tags":null},"/notes/2021/2021.12/Markovs-and-Chebyshevs-Inequalities":{"title":"Markov's and Chebyshev's Inequalities","content":"# Markov and Chebyshev Inequalities\n\n\u003cdiv align=\"right\"\u003e 2021-12-04\u003c/div\u003e\n\nTags: #Math/Statistics \n\nFileLink(zotero://select/items/@InequalitiesMarkov) \n\n## Markov's Inequality\n $X$ 是一个非负的随机变量. 对于任意正实数 $a$ , 有\n$$\nP(X \\geq a) \\leq \\frac{E(X)}{a}\n$$\n\n### Proof\n- 证明里我们假设$X$是离散随机变量, 对于连续随机变量只需要将求和变成求积分即可.\n\n按照定义, $E(X)=\\sum_{x} x P(X=x)$. We'll split this sum into two pieces, depending on whether or not $x \\geq a$.\n$$\n\\begin{aligned}\nE(X) \u0026=\\sum_{x \\geq a} x P(X=x)+\\sum_{x\u003ca} x P(X=x) \\\\\n\u0026 \\geq \\sum_{x \\geq a} a P(X=x)+0 \\quad(\\text { since in the first sum we assume } x \\geq a) \\\\\n\u0026=a \\sum_{x \\geq a} P(X=x) \\\\\n\u0026=a P(X \\geq a)\n\\end{aligned}\n$$\n\n## Chebyshev's Inequality\nLet $X$ be any random variable with finite expected value and variance. Then for every positive real number a,\n$$\nP(|X-E(X)| \\geq a) \\leq \\frac{\\operatorname{Var}(X)}{a^{2}} .\n$$\n\n### Proof\nWe can prove it using Markov's Inequality:\n\nLet $Y=(X-E(X))^{2}$. Recall the definition of the variance of $X$:\n$$\\operatorname{Var}(X)=\\mathrm{E}\\left[(X-E(X))^{2}\\right]$$\n\nThen $Y$ is a non-negative valued random variable with expected value $E(Y)=\\operatorname{Var}(X)$. By Markov's inequality,\n$$\nP\\left(Y \\geq a^{2}\\right) \\leq \\frac{E(Y)}{a}=\\frac{\\operatorname{Var}(X)}{a^{2}} .\n$$\nBut notice that the event $Y \\geq a^{2}$ is the same as $|X-E(X)| \\geq a$, so we conclude that\n$$\nP(|X-E(X)| \\geq a) \\leq \\frac{\\operatorname{Var}(X)}{a^{2}}\n$$\n","lastmodified":"2022-10-15T14:06:29.13849866Z","tags":null},"/notes/2021/2021.12/Maximum_Likelihood_Estimation-%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1":{"title":"Maximum_Likelihood_Estimation-极大似然估计","content":"# 极大似然估计 MLE\n\n\u003cdiv align=\"right\"\u003e 2021-12-25\u003c/div\u003e\n\nTags: #MachineLearning #Math/Statistics \n\nLinks: [Likelihood_Function-似然函数](notes/2022/2022.2/Likelihood_Function-似然函数.md)\n\n假设样本 $X$ 服从已知的概率分布(比如正态分布)\n- **极大似然估计**就是要找一个参数 $\\hat\\theta$,  使似然函数 $\\mathcal{L}(\\theta \\mid X)$ 取得最大值$$i.e.\\quad \\hat{\\theta}=\\operatorname{argmax}_{\\theta \\in \\Theta} \\mathcal{L}(\\theta \\mid X)$$\n- 极大似然估计认为: 最佳的参数 $\\hat\\theta$ 最可能使取样结果为现在的 $x$, 也就是说, 概率$P(X=x\\mid \\theta)$最大:\n$$\\hat{\\theta}=\\operatorname{argmax}_{\\theta \\in \\Theta} P(X=x\\mid \\theta)$$\n\t\n\n","lastmodified":"2022-10-15T14:06:29.13849866Z","tags":null},"/notes/2021/2021.12/Moment-Generating-Function-MGF":{"title":"Moment Generating Function-MGF","content":"# Moment Generating Function - MGF\n\n\u003cdiv align=\"right\"\u003e 2021-12-04\u003c/div\u003e\n\nTags: #Math/Statistics \n\n\n- **This article covers it all.**\n[Moment Generating Function Explained | by Aerin Kim | Towards Data Science](https://towardsdatascience.com/moment-generating-function-explained-27821a739035)\n\n\n- The moments are the expected values of $X$, e.g., $E(X), E(X^2), E(X^3)$, … etc.\n\n- what is Moment Generating Function (MGF)?\n\tAs its name hints, MGF is literally the function that generates the moments — **E(X), E(X²), E(X³), … , E(X^n).**\n\t\n- $M_{X}(t)=\\mathbb{E}\\left(e^{t X}\\right)$\n\n- 从MGF得到相应的Moment只需要取导数\n![](notes/2021/2021.12/assets/img_2022-10-15.png)","lastmodified":"2022-10-15T14:06:29.13849866Z","tags":null},"/notes/2021/2021.12/OS-12-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86":{"title":"OS-12-内存管理","content":"# 内存管理\n\n\u003cdiv align=\"right\"\u003e 2021-12-11\u003c/div\u003e\n\nTags: #OperatingSystem/Memory\n\n程序运行不仅需要处理机, 还需要有地方来保存代码与数据, 这一节我们谈论计算机在运行时, 这些这些数据是如何管理的.\n\n## 目标\n- 电脑的内存总是有限的, 我们想要用有限的内存满足更多的程序的需要, 并且还想这些程序运行得越快越好.\n- 在容量上, 为了营造一个内存\"无限大\"的假象, 我们可以从\"开源\", \"节流\"两方面来入手: \n\t- 节流就是要减少不必要的内存浪费. 分段把程序分成大小不同的段, 避免了程序内部大段的空白. 分页则把程序分成大小相同的页, 在减少内部碎片的同时便于管理.\n\t\t- 分页管理进一步带来了页表过大的问题: 一个进程完整地址空间的页表本身也可能有大段的空白. 这催生了对分段和分页的结合: 段页法通过对页表进行分段来减少页表的大小. 当然, 我们也可以对页表再次进行分页, 用多级页表来解决这个问题.\n\t- 开源则是将不常用的内存数据暂时存储在磁盘, 来扩展内存的大小. 虚拟内存做到了这一点.\n- 在速度上, 以上管理内存的方法都增加了系统的复杂性, 地址变换等工作减慢了调用内存数据的速度. \n\t- 在分页里面, 我们可以增设Cache, 用TLB保存曾经使用过的地址变换结果, 减少内存访问次数. \n\t- 在虚拟内存里面, 我们可以设置高低水位线, 在后台主动地释放一些内存(到磁盘), 避免重要进程在运行时触发Page Fault, 造成时间的大量浪费.\n\n### 随之而来的问题\n分配问题与替换问题是在进行内存管理时需要处理的共性问题:\n\n- 分配问题就是对于有限资源(空闲内存)的管理与分配. \n\t- 不均匀地分配内存空间会带来外部碎片, Best Fit / Worst Fit / First Fit / Next Fit ... 就是一些匹配算法, 想要尽可能合理地分配空闲空间. \n\t- 时不时地整理内存空间, 进行空间归并也是一个很好的方法, 但是它的开销很大. \n\t- 除此以外, 我们还可以从数据结构的角度入手: 链表是管理空闲空间最简单的方法, 但像是\"Binary Buddy Allocator\", 平衡二叉树等方法可以增加查找空闲空间的速度, 增加系统的可拓展性, 也可以更好地管理内存.\n- 替换问题就是有限资源(内存, TLB)已经被塞满时, 该踢出哪个旧单元, 从而周转进新的单元.\n\t- 这类问题的一个典型场景就是某个元素可以在两个地方周转: 一个地方小而快, 另一个地方大而慢. (E.g. Register\u0026Memory, Cache(TLB)\u0026SlowerMemory, Memory\u0026Disk)\n\t- 替换算法有很多: FIFO, 随机方法, LRU(Least Recently Used), LFU(Least Frequently Used), 近似LRU等等\n\t- 在虚拟存储里面这是一个很重要的问题, 因为磁盘实在是太慢了, 置换算法的性能提升带来的收益是很可观的.\n\n- 多级的思想: 存储器层次结构, 应当阅读\u003c计算机组成与设计：硬件、软件接口\u003e第五章.\n- 程序在运行时的局部性原理是上面很多方法的基础, 局部性原理分为空间局部性和时间局部性.\n\n- 并且, 我们做的所有内存管理工作都要尽可能对程序(员)透明, 在运行程序的时候, 我们不想考虑太多令人头疼的内存管理问题\n\n\n\n","lastmodified":"2022-10-15T14:06:29.13849866Z","tags":null},"/notes/2021/2021.12/Ring-Hollow":{"title":"Ring Hollow","content":"# ring/sound hollow\n\n#English \n- If something someone says rings hollow, it does not sound true or sincere.\n- 显得虚假，听起来不诚恳\n","lastmodified":"2022-10-15T14:06:29.13849866Z","tags":null},"/notes/2021/2021.12/Sliver-Bullet":{"title":"Sliver Bullet","content":"# Silver bullet\n\n#English \n\n- a simple solution to a complicated problem\n- 银弹（指针对复杂问题的简单解决办法），良方，高招\n\nIn folklore, a bullet cast from silver is often one of the few weapons that are effective against a werewolf or witch. The term silver bullet is also a metaphor for a simple, seemingly magical, solution to a difficult problem: for example, penicillin circa 1930 was a \"silver bullet\" that allowed doctors to treat and successfully cure many bacterial infections. ","lastmodified":"2022-10-15T14:06:29.13849866Z","tags":null},"/notes/2021/2021.12/Understanding-Bayes-Theorem":{"title":"Understanding Bayes' Theorem","content":"# Understanding Bayes' Theorem\n\n\u003cdiv align=\"right\"\u003e 2021-12-19\u003c/div\u003e\n\nTags: #Math/Probability #Bayes\n\n- 单从形式上来说, Bayes定理是十分简单的. 但是如果我们结合实际问题与一点几何直觉, Bayes定理可以从两个独特的角度来直观理解:\n\n\n\t- **Update of Prior Beliefs \u0026 Change of Perspective**\n\n## Bayes' Theorem: Statement\n$$P(A \\mid B)=\\frac{P(B \\mid A) P(A)}{P(B)}$$\n- 其中$P(A \\mid B)$也称为A的**后验概率**(**posterior probability of A given B**), 因为我们已经知道了B.\n- 而$P(A)$称为A的**先验概率** (**prior probability or marginal probability**), 因为我们没有加任何条件.\n\nBayes定理还有不同的形式:\n- 我们可以把$P(B)$拆开:\n$$P(A \\mid B)=\\frac{P(B \\mid A) P(A)}{P(B \\mid A) P(A)+P(B \\mid \\neg A) P(\\neg A)}$$\n\n- 从条件概率(Conditional Probability)的角度来理解, 分子也可以替换为$P(A\\cap B)$:\n$$P(A \\mid B)=\\frac{P(A\\cap B)}{P(B)}$$\n这其实就是条件概率的定义.\n\n## Bayes' Theorem: Counter-intuitive Side\n这些科普视频都集中展现了Bayes定理的反直觉的一面:\n- [How To Update Your Beliefs Systematically - Bayes’ Theorem - YouTube](https://www.youtube.com/watch?v=R13BD8qKeTg)\n- [Bayes theorem, the geometry of changing beliefs - YouTube](https://www.youtube.com/watch?v=HZGCoVF3YvM)\n- [The medical test paradox, and redesigning Bayes' rule - YouTube](https://www.youtube.com/watch?v=lG4VkPoG3ko)\n\n[Wikipedia: 一个类似的例子: 吸毒者检测](https://zh.wikipedia.org/wiki/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86#%E5%90%B8%E6%AF%92%E8%80%85%E6%A3%80%E6%B5%8B)\n\nBayes定理为什么在上面的例子里面会给出反直觉的结论?\n因为即使误检率很低, 要是基数很大的话, 呈阳性的人里面也会有很大一部分是患病的人, 所以即使检出了阳性, 实际患病的概率还是很小的:\n![400](notes/2021/2021.12/assets/img_2022-10-15-2.png)\n要想提高检出正确阳性的概率, 就应该减小\"健康的人检出为阳性的概率(Specificity)\"\n\n\u003e 一个检测方法的正确率其实需要两个指标来衡量:\n\u003e - **敏感度(Sensitivity)**: 对于患病者, 有多大概率检出ta为患病的(True Positive Rate)\n\u003e - **明确度(Specificity)**: 对于健康的人, 有多大概率能正确得出ta为健康的(True Negative Rate)\n\n![400](notes/2021/2021.12/assets/img_2022-10-15-3.png)\n## Bayes' Theorem: Intuitive Geometry Representation\n\n- 首先, 我们需要注意其实\"条件概率的表示方法\"让贝叶斯定理没有那么直观了:\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/9wCnvr7Xw4E?start=699\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n### Geometry Representation:\n\n![500](notes/2021/2021.12/assets/img_2022-10-15-4.png)\n- 我们可以用上图来形象地说明贝叶斯定理的组成.\n\n- 为了得到上图, 首先我们将A的概率分布$P(A)$和$P(\\neg A)$表示到一个正方形里面:\n![250](notes/2021/2021.12/assets/img_2022-10-15-5.png)\n这个正方形边长为1, 满足$P(A)+P(\\neg A)=1$\n\n- 同样, 对于事件B的概率分布, 我们也可以表示为一个正方形:\n![250](notes/2021/2021.12/assets/img_2022-10-15-6.png)\n\n- 如果$A, B$是相互独立的, 那么它们的概率分布可以表示为:\n![250](notes/2021/2021.12/assets/img_2022-10-15-7.png)\n有$P(AB)=P(A)P(B)$\n\n- 但是如果$A,B$是相关的, 又该怎么表示它们的概率分布呢?\n\t- 若$A,B$是相关的, 说明如果我们知道了$A$, 那么$B$的概率分布会因为$A$的取值的不同而不同:\n\t\t\t![](notes/2021/2021.12/assets/img_2022-10-15-8.png)\n\t\t- 在上图中, $A=1$的时候, B的概率上升了, $A=0$的时候, B的概率下降了.\n\t\t - 此时阴影部分的面积之和表示$P(B)=P(B\\mid A)P(A)+P(B\\mid\\neg A)P(\\neg A)$\n\t\t - 注意B在左右的概率分布变化不一定要一个上升一个下降, 它们的变化是完全无关的. 它们可以都上升或者都下降:\n\t\t ![](notes/2021/2021.12/assets/img_2022-10-15-9.png) \n\t\t 这样, 我们就用几何直观表达了两个相关变量的概率分布. 每一个小长方形都代表了一种AB的取值情况.\n\n- 那么, Bayes定理怎样从图形里面得到呢?\n\t将小长方形与概率对应起来, 我们有:\n![400](notes/2021/2021.12/assets/img_2022-10-15-10.png)\n\n## Update of Prior Beliefs\n- 现在我们能够用Update of Information的观点来看待Bayes定理了, 在上面的图像里面, 我们假设事件A代表患病, 事件B代表检测为阳性. \n\t- 在我们还没有进行检测的时候, 我们对于患病的理解概率的理解是这样的;\n\t\t![200](notes/2021/2021.12/assets/img_2022-10-15-5.png)\n\t- 但是现在进行了检测B, 意味着我们对于是否患病有了更多的信息. 当我们再次计算患病概率的时候, 我们应该更新之前的认识, 在新的图形上面计算患病的后验概率:\n\t- ![400](notes/2021/2021.12/assets/img_2022-10-15-10.png)\n\n下面这个视频片段也许解释的更清楚一点:\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/HZGCoVF3YvM?start=60\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n- 在日常生活中, 我们每天都在做大大小小的决定. 在大部分情况下, 我们都会综合权衡\"过去的经验\"和\"当下的具体情况\", 然后做出一个\"最佳\"的决定.\n- 其实我们过去的\"经验\"就相当于先验概率$P(A)$, 而具体情况就是新出现的事件$B$. 综合权衡的过程就是在看这个新出现的事件B对于我们过去的经验产生了怎样的影响(求后验概率$P(A\\mid B)$)\n\n\n\n## Change of Perspective\n- 下面这个观点是我自己探索出来的, 这个观点侧重于从个体角度上来理解Bayes定理:\n\n- 首先我们需要介绍概率分布的第二个表示方法.\n\t- 前面我们将$A, B$的概率分布表示为下图的形式:\n\t\t![300](notes/2021/2021.12/assets/Pasted%20image%2020211219225436.png)\n\t\t观察这个图, 我们可以看到A的概率分布表示很简单, 但是B的概率分布表示很复杂.\n\t\t我们可以理解为我们先知道了A的概率分布, 然后再在此基础上添加了B(从下向上看):\n\t\t![300](notes/2021/2021.12/assets/Pasted%20image%2020211219225601.png)\n- 这显然对于B是十分不公平的, 我们用可以类似的思路变化一下表示方法:\n\t![230](notes/2021/2021.12/assets/Pasted%20image%2020211219225717.png)\n\t注意图像里面每个小长方形仍然对应不同情况下的概率, 只是形状变了, 面积没有变.\n\t- 我们可以理解为先确定了B, 然后再添加了A的概率分布:\n\t\t![300](notes/2021/2021.12/assets/Pasted%20image%2020211219225838.png)\n\t\t\n对比一下两个表示:\n![](notes/2021/2021.12/assets/Pasted%20image%2020211219230041.png)\n- 对于左边的图, 我们可以理解为一种\"总体的角度\". 即对于一大群人, 有患病的, 也有不患病的, 而后来加入的B的概率分布解释了这个检测方法对于不同情况的人群的效果.\n- 对于右边的图, 我们这可以理解为一种\"个体的角度\", 即对于一个人, ta做了检测后要么为阳性要么为阴性, 没有其他情况, 而后来加入的A的分布给出了这个人不同检测结果的患病概率.\n\n- 而Bayes' Theorem就是在这两种视角之间转换的方式.\n\n现在我们来观察Bayes的公式:\n$$P(A \\mid B)=\\frac{P(B \\mid A) P(A)}{P(B)}$$\n如果你去做了癌症检查, 得到了阳性结果(事件B), 那么你一定很想知道你真的有癌症的概率有多大 (即概率$P(A\\mid B)$ ). 用右边的图来理解, 即你已经确定B发生了, 可以只关注图片的下半部分: \n![450](notes/2021/2021.12/assets/Pasted%20image%2020211219230710.png)\n\n而我们常常这样计算$P(A\\mid B)$:\n$$P(A \\mid B)=\\frac{P(B \\mid A) P(A)}{P(B \\mid A) P(A)+P(B \\mid \\neg A) P(\\neg A)}$$\n其中$P(A), P(B \\mid A), P(B \\mid \\neg A)$分别表示疾病的发生率, 检测方法的Sensitivity和Specificity, 计算时通常它们都是已知的或者可以使用频率近似代替概率.\n- 用图形表示就是: (我们只需要关注阴影部分, 即已经确定的范围)\n![400](notes/2021/2021.12/assets/img_2022-10-15-10.png)\n\n注意我们计算这个概率使用的是左边的图, 即\"总体的角度\", 而我们得到的结果是\"个体的角度\"里面的指标$P(A\\mid B)$ : 一个已经检测为阳性的人的患病概率. 也就是说, Bayes定理实现了视角的切换(Change of Perspective)\n\n- 注意总体角度和个人角度其实是可以交换的, 比如我们将左边的图看作个体角度, 则公式:\n\t$$P(B \\mid A)=\\frac{P(A \\mid B) P(B)}{P(A)}$$\n\t可以理解为在一个人已经患病的情况下, ta检测得到阳性结果的概率(检测方法的Sensitivity).\n\t\n\t\n### 拓展\n有了直观的表示方法, Bayes定理可以很容易地推广到多个类的情况:\n\n- 如果A有多个类:\n$$\\begin{aligned}\n\u0026P(B)=\\sum_{j} P\\left(B \\mid A_{j}\\right) P\\left(A_{j}\\right) \\\\\n\u0026\\Rightarrow P\\left(A_{i} \\mid B\\right)=\\frac{P\\left(B \\mid A_{i}\\right) P\\left(A_{i}\\right)}{\\sum_{j} P\\left(B \\mid A_{j}\\right) P\\left(A_{j}\\right)}\n\\end{aligned}$$\n用图形表示就是:\n![](notes/2021/2021.12/assets/Pasted%20image%2020211219232617.png)\n\n- 如果B分成了BCD三个互斥的情况, 用图形表示就是:\n![Bayes More Situations](notes/2021/2021.12/assets/Bayes%20More%20Situations.svg)\n\n\n\n\n","lastmodified":"2022-10-15T14:06:29.13849866Z","tags":null},"/notes/2021/2021.12/Union_Bound-%E5%B8%83%E5%B0%94%E4%B8%8D%E7%AD%89%E5%BC%8F-Booles_inequality":{"title":"Union_Bound-布尔不等式-Boole's_inequality","content":"# Union Bound: 布尔不等式\n\n\u003cdiv align=\"right\"\u003e 2021-12-04\u003c/div\u003e\n\nTags: #Math/Statistics \n\n\n- This website explained it well:\n[The Union Bound and Extension](https://www.probabilitycourse.com/chapter6/6_2_1_union_bound_and_exten.php)\n\n## Intuition\n\n$$\\begin{aligned}\nP(A \\cup B) \u0026=P(A)+P(B)-P(A \\cap B) \\\\\n\u0026 \\leq P(A)+P(B) .\n\\end{aligned}$$\n\n同样, 对于三个随机变量也有相应的不等关系:\n\n$$\\begin{aligned}\nP(A \\cup B \\cup C) \u0026=P((A \\cup B) \\cup C) \\\\\n\u0026 \\leq P(A \\cup B)+P(C) \\\\\n\u0026 \\leq P(A)+P(B)+P(C)\n\\end{aligned}$$\n\nIn general, using induction we prove the following:\n\n**The Union Bound**\n\nFor any events $A_{1}, A_{2}, \\ldots, A_{n}$, we have\n$$\nP\\left(\\bigcup_{i=1}^{n} A_{i}\\right) \\leq \\sum_{i=1}^{n} P\\left(A_{i}\\right)\n$$\n\n展开就是:\n\n$$\n\\mathbb{P}\\left(A_{1} \\bigcup A_{2} \\bigcup \\cdots\\right) \\leq \\mathbb{P}\\left(A_{1}\\right)+\\mathbb{P}\\left(A_{2}\\right)+\\cdots\n$$","lastmodified":"2022-10-15T14:06:29.13849866Z","tags":null},"/notes/2021/2021.12/in-situ":{"title":"in situ","content":"# in situ\n\n\u003cdiv align=\"right\"\u003e 2021-12-09\u003c/div\u003e\n\nTags: #English #Latin \n\n在原地\n\n_**In situ**_ ([/ɪn ˈsɪtjuː, - ˈsaɪtjuː, - ˈsiː-/](https://en.wikipedia.org/wiki/Help:IPA/English \"Help:IPA/English\"); often not italicized in English) is a Latin phrase that translates literally to \"on site\" or \"in position.\" It can mean \"locally\", \"on site\", \"on the premises\", or \"in place\" to describe where an event takes place and is used in many different context\n\n","lastmodified":"2022-10-15T14:06:29.162498918Z","tags":null},"/notes/2021/2021.12/mentionnote-something-in-passing-%E9%A1%BA%E4%BE%BF%E6%8F%90%E5%88%B0":{"title":"mention(note) something in passing - 顺便提到","content":"# mention/note something in passing\n\n\u003cdiv align=\"right\"\u003e 2021-12-24\u003c/div\u003e\n\nTags: #English\n\n[mention/note something in passing | meaning of mention/note something in passing in Longman Dictionary of Contemporary English | LDOCE](https://www.ldoceonline.com/dictionary/mention-note-something-in-passing)\n\nif you say something in passing, you mention it while you are mainly talking about something else.\n\n- He did mention his brother’s wife, but only in passing. \n\n- He noted, in passing, that he had lasted longer than Texas Sen.\n- Like many more, presumably, we mention Ribblehead in passing.\n- In Exodus the quails were mentioned only in passing.","lastmodified":"2022-10-15T14:06:29.162498918Z","tags":null},"/notes/2021/2021.6/%E4%BB%A3%E6%8D%A2%E5%AF%86%E7%A0%81Substitution-Cipher%E4%B8%8E%E7%BD%AE%E6%8D%A2%E5%AF%86%E7%A0%81Permutation-Cipher":{"title":"代换密码（Substitution Cipher）与置换密码（Permutation Cipher）","content":"# 代换密码与置换密码（Substitution Cipher \u0026 Permutation Cipher）\n\nTags: #Math #Cryptography  #Course \n\n\u003e 分不清楚这两个完全是翻译的锅\n\n## 置换（不是置换密码）Permutation（Not Permutation Cipher)\n首先置换是数学上的一种操作，是对一组确定的元素进行重新排列\n- 元素不变\n- 只改变顺序\n\n![|100](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Permutations_RGB.svg/220px-Permutations_RGB.svg.png)\n\nWikipedia:\n\u003e In [mathematics](https://en.wikipedia.org/wiki/Mathematics \"Mathematics\"), a **permutation** of a [set](https://en.wikipedia.org/wiki/Set_(mathematics) \"Set (mathematics)\") is, loosely speaking, an arrangement of its members into a [sequence](https://en.wikipedia.org/wiki/Sequence \"Sequence\") or [linear order](https://en.wikipedia.org/wiki/Linear_order \"Linear order\"), or if the set is already ordered, a rearrangement of its elements. The word \"permutation\" also refers to the act or process of changing the linear order of an ordered set\n\n## 辨析\n当以替换式密码与[置换式密码（Transposition Cipher)](https://zh.wikipedia.org/w/index.php?title=%E7%BD%AE%E6%8F%9B%E5%BC%8F%E5%AF%86%E7%A2%BC\u0026action=edit\u0026redlink=1)（或称转位式密码或移转式密码(Permutation Cipher属于Transposition Cipher的一种）相比较时，可以发现转位式密码只是改变明文中单元的位置，而单元本身没有转变；相反，替换式密码只是转换单元，但密文中单元的位置没有改变\n\n## 代换密码 Substitution Cipher\n![500](notes/2021/2021.6/assets/img_2022-10-15.png)\n\n又称**替换密码、取代式密码**\n\n关键是”**元素变，位置不变**“，观察上图，原来有两个黄色的方块，加密后变成了两个红色的圆圈。\n这说明 **\"元素类型/分布/组成\"** 发生了变化，原来是正方形，现在变成了圆形；原来是黄色最多，现在变成红色最多了。\n\n为什么**代换**密码中涉及到**置换（Substitution）**？\n![](notes/2021/2021.6/assets/img_2022-10-15-1.png)\n因为他的意思其实是，”更换26个字幕对应的元素相当于将两个Permutation上下对应地放起来“\n$$\n\\begin{alignat}{}\n\u0026A\u0026B\u0026C\u0026D\u0026E\\\\\n\u0026\\downarrow\n\u0026\\downarrow\n\u0026\\downarrow\n\u0026\\downarrow\n\u0026\\downarrow\n\\\\\n\u0026E\u0026A\u0026D\u0026C\u0026B\n\\end{alignat}\n$$\n\n## 置换密码 Permutation Cipher\n![](notes/2021/2021.6/assets/img_2022-10-15-2.png)\n先阅读这个网站：\n[Transposition Cipher - Columnar Transposition Cipher](https://crypto.interactive-maths.com/columnar-transposition-cipher.html)\n再阅读这个：\n[Permutation Cipher](https://crypto.interactive-maths.com/permutation-cipher.html)\n\n可以看出，置换密码即一种一段一段地交换位置的加密方式，如果这个”一小段\"的长度是整个字符串，那么便是一种Transposition Cipher\n\n在整个过程中，只有元素顺序发生了变化，元素组成没有变化。\nIn classical cryptography, a permutation cipher is a transposition cipher in which the key is a permutation.\n[reference](https://en.wikibooks.org/wiki/Cryptography/Permutation_cipher)\n\n\n","lastmodified":"2022-10-15T14:06:29.178499091Z","tags":null},"/notes/2021/2021.6/%E5%9B%BE%E7%81%B5%E5%BD%92%E7%BA%A6-Turing-Reduction":{"title":"图灵归约 Turing Reduction","content":"\n**把还没解决的问题归约到已经解决的问题上**\n\n**用已经解决的问题去解决还没解决的问题**\n\n\u003e 密码学原理与实践 Page 167\n\u003e \n\u003e ![](notes/2021/2021.6/assets/img_2022-10-15-3.png)\n\n假设我们已经存在一个解决问题 A 的算法 $G(x)$\n\n一个A到B的图灵归约即利用$G(x)$构造一个解决问题B的算法$H(x)$, 并且$H(x)$是多项式时间的.\n\n\u003e ![](notes/2021/2021.6/assets/img_2022-10-15-4.png)\n\n\u003e https://zhuanlan.zhihu.com/p/194313998\n\u003e 这篇文章译自[reductions-and-jokes](https://rjlipton.wpcomstaging.com/2020/02/28/reductions-and-jokes/)\n\u003e \n\u003e \u003e 一个物理学家和一个数学家正坐在教师休息室里。突然间，休息室里的咖啡机着火了。物理学家就拿了一个垃圾桶，把里面的垃圾清空，跑到水池前，给垃圾桶灌满水，随后扑灭了火。由于这个咖啡机着过一次火了，大家都同意把垃圾桶装满水放在这个咖啡机旁边。  \n\u003e \u003e 第二天，同样的两个人又坐在同样的休息室里，咖啡机又一次着火了。这一次，数学家站了起来，把装满水的垃圾桶拿了起来，把里面的水倒掉，又放了一些垃圾在里面，交给了物理学家。这样就把问题归约到了一个之前已经解决过的问题上。\n\u003e \n\u003e 虽然这个笑话是讽刺数学家的，但确实很好地解释了归约这个概念。其想法很简单：我们现在遇到了个问题，可以把它转化到一个某个已解决的问题上，而不是一定要直接解决这个问题。从这个意义上来说，归约其实是一种比较懒的解决问题的方式。\n\u003e \n\u003e Instead of putting out a fire, the following [video](https://thumbs.gfycat.com/DifficultVapidAmericanredsquirrel-size_restricted.gif?fbclid=IwAR2AXbtag_WFTP9bmipr4JOhvViHAQbvEgE8h1oCdG_71IttR28EgcSTqhg) is about retrieving a shoe that is floating away.\n\u003e ![https://thumbs.gfycat.com/DifficultVapidAmericanredsquirrel-size_restricted.gif?fbclid=IwAR2AXbtag_WFTP9bmipr4JOhvViHAQbvEgE8h1oCdG_71IttR28EgcSTqhg](https://thumbs.gfycat.com/DifficultVapidAmericanredsquirrel-size_restricted.gif?fbclid=IwAR2AXbtag_WFTP9bmipr4JOhvViHAQbvEgE8h1oCdG_71IttR28EgcSTqhg)\n\n\n\u003e ## Another Example\n\u003e \n\u003e Here is an example of reductions that are not so silly and a little less simple.\n\u003e \n\u003e Imagine that Alice and Bob are at it again. Bob wants to be able to multiply integers fast and he plans on building a hardware system that stores the answers in a table. Then his hardware system will be able to compute the product of two integers by just looking up the answers. Okay, there are really better ways to do this, but just play along for the moment.\n\u003e \n\u003e [![](https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2020/02/tables2.jpg?resize=200%2C200\u0026ssl=1)](https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2020/02/tables2.jpg?ssl=1)\n\u003e \n\u003e Bob’s table is big and he is troubled. The above table has ![{100}](https://s0.wp.com/latex.php?latex=%7B100%7D\u0026bg=ffffff\u0026fg=000000\u0026s=0\u0026c=20201002) entries just to multiply numbers less than ![{10}](https://s0.wp.com/latex.php?latex=%7B10%7D\u0026bg=ffffff\u0026fg=000000\u0026s=0\u0026c=20201002). Clearly for a more extensive table the cost grows fast. He asks his friend Alice for some help. She says:”Just store the diagonal values and I can show you how to handle the general case.” Here is her old trick.\n\u003e \n\u003e ![\\displaystyle  a \\times b = \\frac{\\left(\\left(a + b\\right)^{2} - a^{2} - b^{2}\\right)}{2}. ](https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++a+%5Ctimes+b+%3D+%5Cfrac%7B%5Cleft%28%5Cleft%28a+%2B+b%5Cright%29%5E%7B2%7D+-+a%5E%7B2%7D+-+b%5E%7B2%7D%5Cright%29%7D%7B2%7D.+\u0026bg=ffffff\u0026fg=000000\u0026s=0\u0026c=20201002)\n\u003e \n\u003e Using this allows Bob to just store the diagonal of the multiplication table, and forget all the rest. It is a powerful reduction that shows:\n\u003e \n\u003e _One can reduce integer multiplication to addition and taking the square of a number._\n\u003e \n\u003e For example,\n\u003e \n\u003e ![\\displaystyle  \\begin{array}{rcl}        37 \\times 15 \u0026=\u0026 ( 52^{2} - 37^{2} - 15^{2} )/2 \\\\              \u0026=\u0026 (2704 - 1369 - 225)/2 \\\\            \u0026=\u0026 555. \\end{array} ](https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++++++++37+%5Ctimes+15+%26%3D%26+%28+52%5E%7B2%7D+-+37%5E%7B2%7D+-+15%5E%7B2%7D+%29%2F2+%5C%5C++++++++++++++%26%3D%26+%282704+-+1369+-+225%29%2F2+%5C%5C++++++++++++%26%3D%26+555.+%5Cend%7Barray%7D+\u0026bg=ffffff\u0026fg=000000\u0026s=0\u0026c=20201002)\n\n\n\n\n","lastmodified":"2022-10-15T14:06:29.178499091Z","tags":null},"/notes/2021/2021.6/Diffie-Hellman%E9%97%AE%E9%A2%98":{"title":"Diffie-Hellman问题","content":"# Diffie-Hellman问题\n\n## 两个Diffie-Hellman问题\n### Computation Diffie-Hellman / CDH\n即给定一个基数与两个指数, 计算合并的指数\n- 给定$(\\alpha, \\alpha^b, \\alpha^c)$, **求** $\\alpha^{bc}$\n\n### Decision Diffie-Hellman / DDH\n即给你三个指数, 让你判断最后一个是不是前两个的合并\n- 给定 $(\\alpha^b, \\alpha^c, \\alpha^d)$, **判断有没有** $\\alpha^d=\\alpha^{bc}\\space?$\n\n**注意:** 给定的 $\\alpha$ 是 $n$ 阶元素, 所以上面的运算都是 $mod\\space  \\alpha^n$ 的\n\n\n### 辨析: CDH / DDH\n- CDH是计算一个数, DDH是判断计算结果是否成立\n- 计算难度比较: \n\t- ![](notes/2021/2021.6/assets/img_2022-10-15-5.png)(来自课件)\n- 直觉上理解: 把可能的答案给出来了让你判断对不对(DDH)肯定比让你自己求答案(CDH)简单, 而要你把对数求出来(离散对数问题)是最难的, 因为和CDH相比, 这相当于先把每个部分的指数算出来, 再计算CDH里面的合并的元素(即先用$\\alpha^b,\\alpha^c$算$b,c$ 再计算$b\\times c,$再算出$\\alpha^{bc}$)\n\n## DDH $\\propto_T$ CDH $\\propto_T$ Discrete Logarithm / 三个问题的图灵归约关系\n[图灵归约 Turing Reduction](notes/2021/2021.6/图灵归约%20Turing%20Reduction.md)\n\n### CDH $\\propto_T$ Discrete Logarithm\n很简单, 正如前面已经提到的一样, 可以先用 $\\alpha^b,\\alpha^c$ 算  $b,c$ 再计算 $b\\times c,$ 再算出 $\\alpha^{bc}$.\n\u003e ![](notes/2021/2021.6/assets/img_2022-10-15-6.png)\n\nCDH $\\propto_T$ Discrete Logarithm 说明了 Discrete Logarithm 至少和 CDH 一样难\n\n这样想更清晰: **Discrete Logarithm至少比CDH难**\n\n### DDH $\\propto_T$ CDH\n因为我们都可以求正确的$\\alpha^{bc}$了,我们只需要验证$\\alpha^{bc}$和$\\alpha^{d}$是不是一样就可以解决DDH了\n\u003e ![](notes/2021/2021.6/assets/img_2022-10-15-7.png)\n\nDDH $\\propto_T$ CDH**说明了CDH至少比DDH难**\n\n### 一个思考, DDH / CDH / Discrete Logarithm到底哪个最难?\n\n\u003e *一个错误想法:*\n\u003e \n\u003e 有 DDH $\\propto_T$ CDH, *~~说明 DDH 至少是和 CDH 一样难的(因为解决了 CDH 一定就可以解决 DDH)~~*\n\u003e \n\u003e 有 CDH $\\propto_T$ Discrete Logarithm, *~~说明 CDH 至少是和 Discrete Logarithm 一样难的(因为解决了 Discrete Logarithm 一定就可以解决 CDH)~~*\n\u003e \n\u003e *~~这样看来, 好像 Discrete Logarithm 最简单. CDH 第二简单, DDH 最难, 但是为什么实际是反过来的呢?~~*\n\n**正确想法:**\n\n有DDH $\\propto_T$ CDH, 说明CDH至少是和DDH一样难的(因为解决了CDH一定就可以解决DDH)\n\n有CDH $\\propto_T$ Discrete Logarithm, 说明Discrete Logarithm至少CDH是和一样难的(因为解决了Discrete Logarithm一定就可以解决CDH)\n\n这样看来, **Discrete Logarithm最难, CDH第二难,  DDH最简单**\n\n[第十一个知识点：DLP,CDH和DDH问题都是什么](https://www.cnblogs.com/zhuowangy2k/p/11901028.html)\n\n\u003e ……\n\u003e \n\u003eCDH是和DLP相关的,但是哪个更难呢?如果我能有效率的解决DLP,那么我就可以找出$a$,然后轻松的计算出$g^{ab}$就像Bob做的那样,因此我们就解决了CDH.所以我们说能解决DLP那么一定能解决CDH,这就是说DLP至少和CDH一样难.\n\u003e\n\u003e……\n\u003e\n\u003e如果对手能够解决DDH(输出正确的x的概率大于1/2).那么就是说$G,g^a,g^b$一定泄露了一些关于$g^{ab}$的信息,使得攻击者能把它从随机的元素中分辨出来,尽管不能直接计算出来.而且很明显,如果对手能解决CDH问题,那么它可以有效率的解决DDH,因为它已经可以得到$g^{ab}$ 的值.这意味着,CDH至少和DDH一样难.\n\u003e \n\u003e 这就是我们这篇中讨论的三个问题,我们给出了一个简明的证明对他们的困难性进行排序:DLP最难,然后是CDH,最后是DDH.就像我们看到的那样,DLP有时候是简单的,会让CDH和DDH都变简单.因此群$G$和生成器$g$的选择在做密码学的时候是十分重要的!\n\n\n### 解CDH的算法和解ElGamal的算法是等价的\nin ElGamal:\n![](notes/2021/2021.6/assets/img_2022-10-15-8.png)\n\n**注意$\\alpha, \\beta$都是公钥**\n\n$$\n\\begin{align}\ny_1\u0026=\\alpha^k\\\\\nK\u0026=\\beta^k=\\alpha^{ak}\\\\\ny_2\u0026=xK=x\\beta^k=x\\alpha^{ak}\\\\\n\u0026\\Downarrow\\\\\nx=d_k(y_1,y_2)\u0026=y_2\\cdot (\\alpha^{ak})^{-1}=y_2\\cdot (y_1^a)^{-1}\n\\end{align}\n$$\n\n#### $OracleCDH\\space\\Rightarrow\\space ElGamal$\n$\\delta=OracleCDH(\\alpha,\\beta,y_1)=OracleCDH(\\alpha,\\alpha^k,\\alpha^k)$\n\n由此算出 $\\delta=\\alpha^{ak}$,就相当于$y_1^a$了\n\n所以$x=y_2\\cdot\\delta^{-1}$\n\n#### $CDH \\quad\\Leftarrow\\quad OracleElGamal$\n$$\\begin{align}\nx\u0026=OracleElGamal(\\alpha,\\beta,(y_1,y_2))\\\\\n\u0026=OracleElGamal(\\alpha,\\alpha^a,(\\alpha^k,y_2))\\\\\n\u0026=y_2\\cdot (\\alpha^{ak})^{-1}\n\\end{align}\n$$\n所以$\\alpha^{ak}=y_2\\cdot (y_2\\cdot (\\alpha^{ak})^{-1})^{-1}=\\alpha^{ak}$\n\n\n","lastmodified":"2022-10-15T14:06:29.162498918Z","tags":null},"/notes/2021/2021.6/Function_Procedure_Difference_%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B%E5%92%8C%E5%87%BD%E6%95%B0%E7%9A%84%E5%8C%BA%E5%88%AB":{"title":"Function_Procedure_Difference_(存储)过程和函数的区别","content":"# Difference Between Function and Procedure\n\n\n[Link](https://www.jianshu.com/p/2eaa094adf9e)\n\n\n函数有1个返回值,而存储过程是通过参数返回的,可以有多个或者没有\n\n#### E.g.\n\n```cpp\nZZ x, a, n;  \nx = InvMod(a, n);  _// functional form_  \nInvMod(x, a, n);   _// procedural form_\n```","lastmodified":"2022-10-15T14:06:29.162498918Z","tags":null},"/notes/2021/2021.6/Hamming_Distance_%E6%B1%89%E6%98%8E%E8%B7%9D%E7%A6%BB":{"title":"Hamming_Distance_汉明距离","content":"# Hamming Distance / 汉明距离\n\n\n\n汉明距离是对于两个**相同长度**的字符串而言, the number of positions at which the corresponding symbols are different(相同的位置上对应字符不同的位置个数)\n\n### 图例\n带色线条是路径示意\n\n![3-bit binary cube|350](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b4/Hamming_distance_3_bit_binary.svg/1280px-Hamming_distance_3_bit_binary.svg.png)\n\nTwo example distances: 100→011 has distance 3; 010→111 has distance 2\n\n![3-bit binary cube Hamming distance examples|350](https://upload.wikimedia.org/wikipedia/commons/thumb/6/6e/Hamming_distance_3_bit_binary_example.svg/1280px-Hamming_distance_3_bit_binary_example.svg.png)\n\n![4-bit binary tesseract|600](https://upload.wikimedia.org/wikipedia/commons/thumb/b/bf/Hamming_distance_4_bit_binary.svg/1920px-Hamming_distance_4_bit_binary.svg.png)\n![4-bit binary tesseract Hamming distance examples|600](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b4/Hamming_distance_4_bit_binary_example.svg/1920px-Hamming_distance_4_bit_binary_example.svg.png)\n\n## Hamming Weight / 汉明重量\n一个字符串与相同长度的全零字符串之间的汉明距离\n\nThe Hamming weight of a string is the number of symbols that are different from the zero-symbol of the alphabet used. \nIt is thus equivalent to the Hamming distance from the **all-zero string of the same length**. \n","lastmodified":"2022-10-15T14:06:29.162498918Z","tags":null},"/notes/2021/2021.6/Hash%E5%87%BD%E6%95%B0_Pt.1_%E6%95%B0%E6%8D%AE%E5%AE%8C%E6%95%B4%E6%80%A7":{"title":"Hash函数_Pt.1_数据完整性","content":"# 4.1 Hash 函数与数据完整性\n\n\nHash 函数的主要目的就是为了**保持数据的完整性**\n与\"加密\"不同, hash函数的目的不是为了让一个消息对其他人\"不可知,不可理解\", 而是为了保证这条信息没有被篡改, 为了\"不变质\"地保存一条信息, 即保持这条信息原来的模样\n\nHash函数有两种, 一种是带密钥的, 一种是不带密钥的, 两种在应用场景上有一些区别\n\n不带密钥的Hash函数可以在一段时间内验证数据的完整性。\n想象一个Oracle(先知, 谕示器), 你提供给他一条信息, 她总会返回给你一个独特的信息，通过比对这个信息，你就能够知道自己的信息从上次询问到现在之间是否被篡改了（但是这个信息本身并没有什么实际含义）\n\n注意, 你必须要安全的保存 $y=h(x)$, 否则坏蛋可以同时更换你的 $x$ -\u003e $x^\\prime$ 和 $y$ -\u003e $y^\\prime=h(x^\\prime)$, 你在询问的时候依然有 $h(x^\\prime)=y^\\prime$, 满足校验条件.\n\n\n带密钥的Hash函数则可以在通讯中保证数据的完整性（Message_Authentication_Code，MAC, 消息校验码）\n你和 Bob 都事先约定好了一个密钥 K，通过这个密钥 K 可以生成对应的 Hash 函数 $h_k$, 你在与 Bob 通讯的时候，同时发送消息 $x$ 和 $y=h_k(x)$, 这样 Bob 在收到消息后便可以校验 x 和 y 是否对应, 以此来判断消息是否被篡改了\n\n注意, 你必须要确信消息的确来自 Bob, 否则坏蛋仍然可以同时伪装成 Bob 向你发送 $x^\\prime$ 和 $y^\\prime$, 这依然满足校验条件\n\n\n\n\n","lastmodified":"2022-10-15T14:06:29.162498918Z","tags":null},"/notes/2021/2021.6/Hash%E5%87%BD%E6%95%B0_Pt.2_%E6%94%BB%E5%87%BB%E6%96%B9%E6%B3%95_%E5%AE%89%E5%85%A8%E6%80%A7":{"title":"Hash函数_Pt.2_攻击方法_安全性","content":"# Hash函数的攻击方法/安全性\n\n\u003e 密码学原理与实践 page99\n\n## 理想的安全性:随机谕示模型(Random Oracle Model)\n\n**完全的随机对应性:**\n\n从名称来理解, 即理想的Hash函数相当于一个\"先知\", 能够对每一个$x$给出一个完全随机的$hash(x)$, 并且计算$hash(x)$的唯一方法便是询问Oracle(谕示器)\n\n在这个假设下,有如下定理: \n\u003e 密码学原理与实践 page94\n\u003e \n\u003e ![img_2022-10-15-9](notes/2021/2021.6/assets/img_2022-10-15-9.png)\n\n**直观理解:** \n\n用一部分x去确定hash函数后,剩下的x取任意可能密文的概率是相同的.\n\n即密文与明文的对应完全随机, 密文不取决于明文的任何性质(当然明文每次对应的密文得一样)\n\n这就要求我们考虑如何设计一个hash函数, 既能保留某种 *\"独特性\"*,  (即不能把所有的明文都对应到同一个密文上,不同的x对应不同的y)\n\n又能保留某种 *独立性* 即x对应不同的y都是等概率的, 不能因为这个x的某些特征决定它取某些y的概率更高\n\n## 攻击方法\n在随机谕示模型下,我们对三种问题有以下算法:\n(注意,这些算法都是Las Vegas算法, 即不一定成功的算法)\n我们规定这个算法最多计算Q次hash值, 这个算法的平均成功率为 $\\varepsilon$\n\n### 原像问题\n对于一个密文y,我们尝试用随机的Q个x去验证是不是y的原像\n\n**成功率:**\n1-Q次都找不到的概率\n$$\n1-(1-\\frac{1}{M})^Q\n$$\n### 第二原像问题\n对于一个x,我们尝试用另外Q-1个$x^\\prime$去验证是不是与x hash值相同\n\n**成功率:**  \n1-(Q-1次都找不到的概率)\n$$\n1-(1-\\frac{1}{M})^{Q-1}\n$$\n\n### 碰撞问题\n验证 Q 个 x 中有没有 y 值相同的两个 x\n\n**成功率:**\n1-(Q个x hash值各不相同的概率)\n$$\n1-\\frac{M}{M}\\cdot\\frac{M-1}{M}\\cdots\\frac{M-(Q-1)}{M}\n$$\n不要因为你不能控制碰撞对便认为碰撞问题没有实际威胁\n\n#### 生日悖论\n从碰撞问题, 我们可以引出著名的生日悖论, 这两个问题在统计意义上是相同的, 即多个独立事件发生重复情形的概率\n通过以下两个近似, 我们可以估算出样本总数(班级人数)与发生碰撞的概率(至少有两个人生日是同一天的概率)之间的关系:\n\n![img_2022-10-15-10](notes/2021/2021.6/assets/img_2022-10-15-10.png)\n\n最后的结果是:\n$$\nQ \\approx \\sqrt{2 M \\ln \\frac{1}{1-\\epsilon}}\n$$\n其中,M是所有可能的情形(一年中的365天), 如果我们取$\\epsilon=0.5, Q\\approx 22.3$即23人的班级里面就有50%的可能性有至少有两个人生日相同.\n\n单独来看, 一个人取生日是一年中任意一天的概率是$\\frac1{365}$, 是很小的, 但是当多个独立事件反复发生, 发生相同情形的\"巧合\"的概率却出奇的高(23个人相比365显然是很小的一个数)!\n\n这个结论对于 Hash 函数的设计有什么启发意义呢?\n\n所有可能的生日对应 Hash 函数的所有可能取值\n\n班级人数则相当于上面算法里面的尝试次数Q\n\n类似的, 如果我们的Hash函数的所有可能取值只有365个, 则只要尝试23次就很有可能找到一对hash值相同的数据了! 这显然不太安全(找到一对相同的看起来还不太严重,但是从下面的规约关系能看出, 解决了碰撞问题, 也解决了)\n\n## 安全性准则的比较/攻击方法的包含(归约)关系\n\n![img_2022-10-15-11](notes/2021/2021.6/assets/img_2022-10-15-11.png)\n![img_2022-10-15-12](notes/2021/2021.6/assets/img_2022-10-15-12.png)\n\n由上图中的箭头x--\u003ey的含义是: if a hash function is secure in the $xxx$-sense, then it is secure in the $yyy$-sense.\n\n书中下面两个问题的证明思路都是\"逆否证法\"\n\n### 碰撞稳固与第二原像稳固\n![img_2022-10-15-13](notes/2021/2021.6/assets/img_2022-10-15-13.png)\n\n- 如果 **碰撞稳固** 则 **第二原像稳固**\n\n**证明方法:**\n\n证明如果有一个函数能够解决第二原像问题,则可以利用这个函数去解决碰撞问题\n那么如果一个函数的碰撞问题不可被解决(即碰撞稳固),那么它的第二原像问题也不可被解决(也是第二原像稳固的).\n\n注意: \"解决碰撞问题\"的含义是 **\"可以找到碰撞对\"** 而不是 **\"不会有碰撞了\"**\n\n### 碰撞稳固与原像稳固\n![img_2022-10-15-14](notes/2021/2021.6/assets/img_2022-10-15-14.png)\n\n- ~~前提: $\\bcancel{|\\mathcal{X}|\\geq 2|\\mathcal{Y}|}$~~\n- ~~如果 **碰撞稳固** 则 **原像稳固**--~~\n\n**注意：** 不能这样说！书上只说明了如果我们有100%的把握解决原像问题，则有\n$$\n\\mathcal{\\frac{|X|-|Y|}{|X|}}\n$$\n的把握解决碰撞问题\n\n而碰撞稳固（不能解决碰撞问题）并不能推出原像稳固（不能解决原像问题）\n（除非$\\mathcal{X\u003e\u003eY}$, 即明文空间极大于密文空间)\n\n\u0026emsp; **证明方法:**\n证明的是如果存在一个可以100%解决原像问题的Las Vegas算法，那么可以利用这个算法构造一个解决碰撞问题成功率是\n$$\n\\mathcal{\\frac{|X|-|Y|}{|X|}}\n$$\n的算法(在$|\\mathcal{X}|\\geq 2|\\mathcal{Y}|$的时候把握是50%,如果$|\\mathcal{X}|=|\\mathcal{Y}|$则完全不可能找到碰撞\n\n(为什么呢? 按理说如果有两个x指向同一个y,还是可以找到碰撞的啊?\n\n![|200](notes/2021/2021.6/assets/img_2022-10-15-15.png)\n\n\n--\u003e原因是在书中有这么一句话:\n\n![密码学原理与实践 page99](notes/2021/2021.6/assets/img_2022-10-15-16.png)\n\n所以当$|\\mathcal{X}|=|\\mathcal{Y}|$的时候, x和y一定是一一对应的))\n\n![img_2022-10-15-17](notes/2021/2021.6/assets/img_2022-10-15-17.png)\n\n\n","lastmodified":"2022-10-15T14:06:29.162498918Z","tags":null},"/notes/2021/2021.6/Hash%E5%87%BD%E6%95%B0_Pt.3_%E8%BF%AD%E4%BB%A3Hash%E5%87%BD%E6%95%B0":{"title":"Hash函数_Pt.3_迭代Hash函数","content":"# 迭代Hash函数\n\n\n## 迭代Hash函数的基本结构\n\n迭代Hash函数是一种用**有限长度Hash函数**来处理**无限长度的数据**的方法\n\n下面这张图形象地表示了迭代Hash函数地加密步骤:\n![img_2022-10-15-18](notes/2021/2021.6/assets/img_2022-10-15-18.png)\n\n这样的构造方法可以概括为以下的三个步骤:\n\n我们以有限的Hash函数$compress$为基础\n$$\n\\{0,1\\}^{m+t}\\longrightarrow \\{0,1\\}^{m}\n$$\n这是一个将长度从$m+t$缩减到$m$的有限长度hash函数\n\n1. **预处理**\n这一步将明文切分成长度为**t**的小段.\n\n--\u003e最后剩余的怎么办?\n1. 在末尾添加整个字符串的长度\n2. 如果还不够,用0补足\n--\u003e为什么要添加字符串的长度?\n这是为了使预处理函数$x\\mapsto y$为单射, 保证整体的抗碰撞性质\n反例:只添加0\n1001_0       -\u003e 1001_0000\n1001_000  -\u003e 1001_0000\n这两个串构成了一个碰撞对\n\n2. **处理**\n这一步逐步循环调用$compress$, 知道得到最后的结果$z_r$,它的长度为$m$.\n\n注意:\n- 一开始有一个公开的初始值比特串 $IV$, 与$y_1$一同构成$compress$的第一次输入\n- 我们每次缩减长度为t的串, 一共进行$$r\\times t=\\Big\\lceil \\frac{x}{t}\\Big\\rceil$$次,其中r是y的个数\n\n\n3. **输出变换**\n这一步把长度为$m$的结果映射到$l$, 使得最后的迭代Hash函数是一个长度上$i\\mapsto l$的映射, i为任意自然数\n\n- 这一步是可选的\n\n\n## 迭代Hash函数的实例\n\n### Merkle-Damgard结构\n根据有限hash函数Compress中t的大小进行分类, Merkle-Damgard结构的函数有两种形式：\n#### Compress中 $t\\geqslant2$ 的Merkle-Damgard结构\n对比观察算法伪代码和图解即可（放大看）\n![伪代码](notes/2021/2021.6/assets/img_2022-10-15-19.png)\n![手绘图解](notes/2021/2021.6/assets/img_2022-10-15.jpg)\n\n比较Merkle-Damgard结构和迭代Hash的一般结构：\n1. 预处理\n\t1.  MD结构把x拆成了大小为t-1的小段（这也是为什么要单独讨论$t=1$ 的情形），否则$t-1=0，y$压根没法分 \n\t2.  在最后补齐的时候是在末尾添上空缺位个数的二进制，在中间补零\n\t\t\n\t\t![手绘图解——预处理](notes/2021/2021.6/assets/img_2022-10-15-20.png)\n\t\t\n2. 处理\n\t1. 在每次调用Compress的时候在最中间插入了一个1（注意是1个1不是0！）\n对应t-1长度的y，每次Compress输入的长度依然是$m+t$\n\n   ![手绘图解_输入长度](notes/2021/2021.6/assets/img_2022-10-15-21.png)\n#### Compress中 $t=1$ 的Merkle-Damgard结构\n没法把x分成t-1长的小段了，我们怎么对x进行合理的拆分，方便我们利用Compress函数呢？\n我们可以逐位对x进行某种代换，\n\n\n## 论证迭代Hash函数的安全性\n[Hash函数的攻击方法 安全性](notes/2021/2021.6/Hash函数_Pt.2_攻击方法_安全性.md#Hash函数的攻击方法%20安全性)\n\n我们之前讨论的安全性只是针对于有限的Hash函数Compress的，但是我们可以将从这继续推广，得到迭代Hash函数安全性的一些结论\n\n不是所有迭代Hash函数都有Merkle-Damgard结构的优秀性质:\n\u003e 杨礼珍老师课件 第 4 章 Hash 函数.Pdf#page=28\n\n### Merkle-Damgard结构的安全性\n#### 碰撞稳固\n- 如果Compress是碰撞稳固的，那么Merkle-Damgard函数也是碰撞稳固的\n\n**证明：**\n因为Merkle-Damgard结构有两种情形，所以也要分类讨论\n我们证明的思路依然是”逆否证法“，即我们需要证明如果Merkle-Damgard结构中存在一个碰撞，那么Compress函数里面也一定可以找到一个碰撞\n\n##### $t\\geq 2$的Merkle-Damgard结构\n因为Compress是把原字符串分成许多小块进行Hash的，所以我们主要就是确定是在原字符串中的哪一部分能够找到一个”不一致“（碰撞）\n$$\n\\begin{array}{l}\ny(x)=y_{1}\\left\\|y_{2}\\right\\| \\cdots \\| y_{k+1} \\\\\ny\\left(x^{\\prime}\\right)=y_{1}^{\\prime}\\left\\|y_{2}^{\\prime}\\right\\| \\cdots \\| y_{\\ell+1}^{\\prime}\n\\end{array}\n$$\n**分类讨论：**\n1. 在两个字符串长度模$t-1$结果不一样时, 很容易想到在最后添加的$y_{k+1}$一定不一样,这样Compress在最后一步一定有碰撞\n\u003e 情况 1: $|x| \\equiv\\left|x^{\\prime}\\right|(\\bmod t-1)$ 。\n\u003e \n\u003e 这里 $d \\neq d^{\\prime}$ 且 $y_{k+1} \\neq y_{l+1}^{\\prime}$, 我们有\n\u003e \n\n$$\\begin{aligned}\n\\operatorname{compress}\\left(g_{k}\\|1\\| y_{k+1}\\right) \u0026=g_{k+1} \\\\\n\u0026=h(x) \\\\\n\u0026=h\\left(x^{\\prime}\\right) \\\\\n\u0026=g_{\\ell+1}^{\\prime} \\\\\n\u0026=\\operatorname{compress}\\left(g^{\\prime}_l\\|1\\| y^{\\prime}_{\\ell+1} \\right)\n\\end{aligned}$$\n\n\u003e \n\u003e 因为 $y_{k+1} \\neq y_{\\ell+1}^{\\prime}$, 所以找到了 $h$ 的一个碰撞。\n \n2. 在最后找不到不一致, 就只有向前回溯. 现在两个字符串末尾一样(模$t-1$结果相同), 所以只有两种情形:\n\t\u003e 情况 2: $|x| \\equiv\\left|x^{\\prime}\\right|(\\bmod t-1)$ 。\n\t\u003e 为了便于讨论，分成两种更细的情况:\n\t- 两个字符串长度一模一样\n\t- 两个字符串长度相差$t-1$的整数倍\n\n\n\t1. **对于第1种情形:**\n\t因为$z_1$在中间连接两个部分用的是0, 而剩下的$z$都用的是1, 所以在开头一定能找到一个碰撞.\n\t\n\t\u003e 情况 2a: $|x| \\neq\\left|x^{\\prime}\\right|$ 。\n\t\u003e 不失一般性，设 $\\left|x^{\\prime}\\right|\u003e|x|$, 因此 $\\ell\u003ek$ 。按照情况 $2 \\mathrm{a}$ 类似的过程，假定没有找到 compress 的碰撞，最后总有\n\t\u003e $$\n\t\u003e \\begin{aligned}\n\t\u003e \\operatorname{compress}\\left(0^{m+1} \\| y_{1}\\right) \u0026=g_{1} \\\\\n\t\u003e \u0026=g_{\\ell-k+1}^{\\prime} \\\\\n\t\u003e \u0026=\\operatorname{compress}\\left(g_{\\ell-k}^{\\prime}\\|1\\| y_{\\ell-k+1}^{\\prime}\\right)\n\t\u003e \\end{aligned}\n\t\u003e $$\n\t\u003e 但\n\t\u003e $$\n\t\u003e 0^{m+1} \\| y_{1}\n\t\u003e $$\n\t\u003e 的第 $(m+1)$ 比特是 0, 而\n\t\u003e $$\n\t\u003e g_{\\ell-k}^{\\prime}\\|1\\| y_{\\ell-k+1}^{\\prime}\n\t\u003e $$\n\t\u003e 的第 $(m+1)$ 比特是 1 。因此必然会找到 compress 的一个碰撞。 \n\n\t2. **对于第2种情形,**\n\t\t我们逐步倒推, 要么在某一步找到一个不一样, 要么没有不一样(而这与假设$y(x)\\neq y(x^\\prime)$ 相矛盾)\n\n\t\u003e 情况 2b : $|x|=\\left|x^{\\prime}\\right|$ 。\n\t\u003e 此时有 $k=\\ell$ 和 $y_{k+1}=y_{k+1}^{\\prime}$, 像情况 1 中一样，我们有:\n\t\u003e $$\n\t\u003e \\begin{aligned}\n\t\u003e \\operatorname{compress}\\left(g_{k}\\|1\\| y_{k+1}\\right) \u0026=g_{k+1} \\\\\n\t\u003e \u0026=h(x) \\\\\n\t\u003e \u0026=h\\left(x^{\\prime}\\right) \\\\\n\t\u003e \u0026=g_{k+1}^{\\prime} \\\\\n\t\u003e \u0026=\\operatorname{compress}\\left(g_{k}^{\\prime}\\|1\\| y_{k+1}^{\\prime}\\right)\n\t\u003e \\end{aligned}\n\t\u003e $$\n\t\u003e 如果 $g_{k} \\neq g_{k}^{\\prime}$, 则找到了 compress 的碰撞，所以可假定 $g_{k}=g_{k}^{\\prime}$ 。则有\n\t\u003e $$\n\t\u003e \\begin{aligned}\n\t\u003e \\operatorname{compress}\\left(g_{k-1}\\|1\\| y_{k}\\right) \u0026=g_{k} \\\\\n\t\u003e \u0026=g_{k}^{\\prime} \\\\\n\t\u003e \u0026=\\operatorname{compress}\\left(g_{k-1}^{\\prime}\\|1\\| y_{k}^{\\prime}\\right)\n\t\u003e \\end{aligned}\n\t\u003e $$\n\t\u003e 或者找到 compress 的一个碰撞，或者 $g_{k-1}=g_{k-1}^{\\prime}$ 并且 $y_{k}=y_{k}^{\\prime}$ 。假定没有找到碰撞，重复止 述过程，最后得到\n\t\u003e $$\n\t\u003e \\begin{aligned}\n\t\u003e \\operatorname{compress}\\left(0^{m+1} \\| y_{1}\\right) \u0026=g_{1} \\\\\n\t\u003e \u0026=g_{1}^{\\prime} \\\\\n\t\u003e \u0026=\\operatorname{compress}\\left(0^{m+1} \\| y_{1}^{\\prime}\\right)\n\t\u003e \\end{aligned}\n\t\u003e $$\n\t\u003e 如果 $y_{1} \\neq y_{1}^{\\prime}$, 则找到了 compress 的一个碰撞，因此可假定 $y_{1}=y_{1}^{\\prime}$ 。这样对 $1 \\leqslant i \\leqslant k+1$ 都有 $y_{i}=y_{i}^{\\prime}$, 所以 $y(x)=y\\left(x^{\\prime}\\right)$ 。但因为映射 $x \\mapsto y(x)$ 是单射，这意味着 $x=x^{\\prime}$ 。而我们假定了 $x \\neq x^{\\prime}$, 这就产生了矛盾。\n\n因为讨论了所有的情况，也就证明了所期望的结论。\n\n\u003e 其实这里不应该把书上的证明抄下来的, 这样降低了我的思考的浓度,让这篇笔记变得陌生了, 如果担心不够详细, 可以列出书上的页码, 在将来如果需要整理成文章,也可以到时候再来排版\n\n##### $t = 1$的Merkle-Damgard结构\n\n**关键:** \n- 逐比特预处理\n\t预处理的关键便是如何在把长度变得规整的同时构造一个**单射**\n\t先利用这个函数逐比特预处理:\n\t$$\n\t\\begin{align}\n\tf(0)\u0026=0\\\\\n\tf(1)\u0026=01\\\\\n\t\\end{align}\n\t$$\n\t然后在开头添加11:\n\t$y\\leftarrow 11||f(x_1)||f(x_2)||\\cdots||f(x_n)$\n\t(这是为了保证无碰撞性质, (构造无后缀性质保证无碰撞性质))\n- 逐比特压缩\n\t利用 $compress: \\{0,1\\}^{m+1}\\rightarrow \\{0,1\\}^m$\n\t被压缩的第一个串是$0^m||y_1$\n\n\n\n\n","lastmodified":"2022-10-15T14:06:29.162498918Z","tags":null},"/notes/2021/2021.6/Hash%E5%87%BD%E6%95%B0_Pt.4_%E5%AE%89%E5%85%A8Hash%E7%AE%97%E6%B3%95_SHA-1":{"title":"Hash函数_Pt.4_安全Hash算法_SHA-1","content":"# SHA-1\n\nTags: #Cryptography  #Math  #Course #Hash \n\n**SHA-1是一个具有160bit消息摘要的迭代Hash函数**\n\n## 主要思想\nSHA-1 的分组大小是 512bit, 意味着每一次迭代处理 512bit 的数据\n\nSHA-1建立在对比特串面向**字**的操作上, 意味着在处理512bit的时候是每次32bit, ~~一共16次~~, 一共80次. (为什么变多了? 因为在循环里面需要将16个字扩充到80个字, 如下图)\n\n![](notes/2021/2021.6/assets/img_2022-10-15-22.png)\n\n(刚开始我一直看不懂这里, 觉得数组下标超限了, 但仔细看, 意思其实是根据原来的16个字利用异或与循环左移构建新的字)\n\nSHA-1在循环的时候使用了5个中间变量$A,B,C,D,E$, 大小为一个字, 不通过断更新它们的值来进行处理\n\n[Youtube - An Intuitive Introduction](https://www.youtube.com/watch?v=DMtFhACPnTY)\n![](notes/2021/2021.6/assets/img_2022-10-15-23.png)\n同时这个视频里面还指出了其他地方都没有提到的一点: SHA-1 里面的所有加法都是循环加法(溢出位变成最低位), 这样会损失信息, 让 Hash 变得更安全.\n\n**(但是这一点其他地方都没有提到, 待验证!)**\n#todo\n\n老师的课件中把SHA-1与Merkle-Damgard结构的对应关系写的很清晰\n\n[Merkle-Damgard结构](notes/2021/2021.6/Hash函数_Pt.3_迭代Hash函数.md#Merkle-Damgard结构)\n\n![](notes/2021/2021.6/assets/img_2022-10-15-24.png)\n![](notes/2021/2021.6/assets/img_2022-10-15-25.png)\n\n\n\n红色\"do\"里面的部分如下图例:\n\n![File:SHA-1.svg](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e2/SHA-1.svg/365px-SHA-1.svg.png)\n\n\n## 预处理:\n注意的一点是SHA-1能处理的串是有最大长度的, 为$2^{64}-1$\n\n同时因为在预处理的时候需要填充x的长度$l$, 从上面可以知道$|l|\\leq 64\\ bits$, 如果$l$长度不够, 在左边加0, (在左边加0也符合正常的大小表示方法(用0\n填充没有用到的高位))\n\n预处理方法:\n$$y\\leftarrow x||一个1||许多0||l$$\n0 有多少个呢? 让 y 的长度 mod 512=0 即可\n\n更清晰一点的话, 是:\n\n![|480](notes/2021/2021.6/assets/img_2022-10-15-26.png)\n\n## 处理\n直觉理解上面已经谈过了, 详细的话可以看\u003c密码学原理与实践 page118\u003e或者官方文档:[RFC3174](https://datatracker.ietf.org/doc/html/rfc3174#section-6)\n\n## 结果变换\n不需要变换, 最后的$H_0||H_1||H_2||H_3||H_4$连起来, 便是一个$32bits\\cdot 5=160\\ bits$的消息摘要.\n\n\n## Further Questions\n- SHA-1的这种迭代结构与Merkel-Damgard结构的关系是什么?(History, 是Merkel首创了这种结构吗?)\n\n[SHA-1的Wikipedia写的蛮好](https://en.wikipedia.org/wiki/SHA-1)\n\n","lastmodified":"2022-10-15T14:06:29.162498918Z","tags":null},"/notes/2021/2021.6/Hash%E5%87%BD%E6%95%B0_Pt.5_%E6%B6%88%E6%81%AF%E8%AE%A4%E8%AF%81%E7%A0%81_MAC":{"title":"Hash函数_Pt.5_消息认证码_MAC","content":"# 消息认证码 MAC Message Authentication Code\n\n\n## 消息认证码是什么\n\n生成方式: 带密钥的Hash函数, Hash值可以不用在安全的信道上传输, 只需要开始的时候协商一个密钥$K$就可以了\n\n用于核验在**不安全信道上传输的消息**有没有被修改\n\n## 不安全的构造方式\n一个很直观的方式便是把密钥$K$加到明文x里面,一起Hash, 既然攻击者不知道$K$是什么, 他也应该无法计算$h_K(x)=h(x_K). (x_K$为加入了K的x)\n\n但是以下论证告诉我们一些简单的构造方式并不安全, 攻击者即使不知道K是什么, 也可以利用一个有效对$(x,h_K(x))$计算新的有效对$(x^\\prime,h_K(x^\\prime)$\n\n### 直接把K作为$IV$\n(IV 即 Compress 的初始输入 [见迭代Hash函数的基本结构](notes/2021/2021.6/Hash函数_Pt.3_迭代Hash函数.md#迭代Hash函数的基本结构))\n\n#### 无预处理\n假设$x^{\\prime}=x||w$, 则可以这样计算新的$h_K(x^\\prime)$\n$$h_{K}\\left(x \\| w\\right)=\\operatorname{compress}\\left(h_{K}(x) \\| w\\right)=\\operatorname{compress}\\left(y\\| w\\right)$$\n\n**直觉理解:**\n为什么还是能够算出新的有效对, 即使我们并不知道K? 这说明这种构造方式没有很好的隐藏K, 我们以为K,没有泄露, 其实K是隐藏在$h_K(x)$里面的\n\n#### 有预处理(padding)\n我们还是能够计算$h_K(x||pad(x)||w||(pad(x^\\prime))$此时$x^\\prime=x||pad(x)|w$\n\n\n### 嵌套MAC和HMAC\n嵌套 MAC 即合成两个带密钥的 Hash 族来建立一个 MAC 算法\n\n$$(g \\circ h)_{(K, L)}(x)=h_{L} \\left(g_{K}(x)\\right)$$\n\n外面的$h_L$是一个安全的\"小MAC\", 里面的$g_K$是一个碰撞稳固的带密钥的Hash族, 它们共同构建了一个安全的\"大MAC\" \n$\\Rightarrow$ **\"大 MAC\"=先带密钥 Hash, 再生成 MAC**\n\n#### 对嵌套MAC的攻击\n有三种攻击方式 , 分别是:\n- 对嵌套MAC的假冒者（“大MAC攻击”）$\\longrightarrow$(直接攻击整个, 企图找到$x^\\prime$ 的$h_{L}(g_{K}(x^\\prime))$)\n- 对小MAC的假冒者（“小MAC攻击”）$\\longrightarrow$(攻击$h_L$, 即攻击外层的MAC, 对于$x^\\prime$ ,企图找到$h_{L}(x^\\prime)$\n- 当密钥是保密的，对Hash族的碰撞-探测者（“未知-密钥碰撞攻击”）$\\longrightarrow$(攻击$g_K$, 即攻击内层的的MAC, 对于$x^\\prime$ ,企图找到$h_{L}(x^\\prime)$([之前讨论过的碰撞问题](notes/2021/2021.6/Hash函数_Pt.2_攻击方法_安全性.md#碰撞问题))\n\n书上证明了一个结论, 这个结论说明: \n\n如果对小MAC的攻击至多有$\\epsilon_2$的成功率, 对Hash函数的(不知道密钥的)碰撞攻击最多有$\\epsilon_1$的成功率, 那么对总的嵌套MAC的攻击至多有$\\epsilon_1+\\epsilon_2$的成功率\n\n(即$\\epsilon\\leqslant \\epsilon_1+\\epsilon_2$)\n\n#### HMAC\nHMAC是利用不带密钥的Hash函数构造的嵌套MAC\n下面是利用[SHA-1](notes/2021/2021.6/Hash函数_Pt.4_安全Hash算法_SHA-1.md)构造的HMAC:\n$$\n\\operatorname{HMAC}_{K}(x)=\\mathrm{SHA}-1((K \\oplus \\text { opad }) \\| \\mathrm{SHA}-1((K \\oplus \\text { ipad }) \\| x))\n$$\n其中, 密钥K的长度是512bits, \n![](notes/2021/2021.6/assets/img_2022-10-15-27.png)\n\n![image-20210620164647868](notes/2021/2021.6/assets/img_2022-10-15-28.png)\n\n总体上来看, HMAC把密钥与两个常数异或构成两个新的密钥 , 然后分别放入嵌套的SHA-1中与明文一起进行加密, (处理后的密钥放在前面)\n\n注意第二次计算仅仅需要利用一次Compress, 所以HMAC也相当于\n\n![image-20210620165121073](notes/2021/2021.6/assets/img_2022-10-15-29.png)\n\n### CBC-MAC\n\nCBC-MAC和DES/AES的CBC工作模式非常相似, 这是一种由分组密码构造MAC的方式.\n\n\u003e 杨礼珍老师课件 第 4 章 Hash 函数.pdf#page=47\n![](notes/2021/2021.6/assets/img_2022-10-15-30.png)\n其实IV相当于没有, 因为$0^t\\oplus x_1=x_1$\n\n\n[Nice Youtube Video](https://www.youtube.com/watch?v=BsWsJfIisvY)\n![](notes/2021/2021.6/assets/img_2022-10-15-31.png)\n![](notes/2021/2021.6/assets/img_2022-10-15-32.png)\n在视频里面强调了, 如果你引入了IV, 反而会损失安全性\n\n这个带长度的没看懂 #没看懂\n![](notes/2021/2021.6/assets/img_2022-10-15-33.png)\n生日攻击没看懂\n\n\n\n\n\n\n","lastmodified":"2022-10-15T14:06:29.162498918Z","tags":null},"/notes/2021/2021.7/Difference_between_Git-Bash_Git_CMD":{"title":"Difference_between_Git Bash_Git_CMD","content":"# Git Bash or Git Cmd?\n\n\u003cdiv align=\"right\"\u003e 2021-07-27\u003c/div\u003e\n\nTags: #Git \n\n## 它们是什么?\n在为Windows Terminal设置Git界面的时候遇到了这个问题, Git提供了三种操控方式: Git GUI, Git Bash和Git Cmd, 第一个是图形界面, 那么后面两个命令行界面有什么区别呢?\n\nBash是Unix Shell的一种\nUnix，一种操作系统，Linux是Unix的一种\nShell，“为用户提供用户界面”的软件，比如Windows里面的Cmd\n\n\u003eCLI与GUI:\n\u003e 通常将shell分为两类：命令行与图形界面。命令行壳层提供一个[命令行界面](https://zh.wikipedia.org/wiki/%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%95%8C%E9%9D%A2 \"命令行界面\")（CLI）；而图形壳层提供一个[图形用户界面](https://zh.wikipedia.org/wiki/%E5%9C%96%E5%BD%A2%E4%BD%BF%E7%94%A8%E8%80%85%E4%BB%8B%E9%9D%A2)（GUI）。[^2]\n\n## 如何选择?\n\n\u003e **Git CMD** is just like regular Windows command prompt with the `git` command. It lets you use all of Git features through command line. Useful if you are already familiar with Windows cmd and you only work on Windows.\n\u003e \n\u003e **Git Bash** emulates a [bash](https://en.wikipedia.org/wiki/Bash_(Unix_shell)) environment on windows. It lets you use all git features in command line plus most of [standard unix commands](https://ss64.com/bash/). Useful if you are used to Linux and want to keep the same habits.\n\u003e \n\u003e **Git GUI** is a **G**raphical **U**ser **I**nterface letting you use Git without touching command line. It is an alternative among other Git clients. Since Git GUI is very minimal, you could also look at [other alternatives](https://git-scm.com/download/gui/windows) if GUIs interest you.\n\u003e \n\u003e It is up to you to decide which you want to use. As many others, I recommend you to learn Git with command line before switching to a graphical interface. If you don't know which to choose between Git Bash and Git CMD, I'd go for Git Bash since bash is a really useful tool to learn.[^1]\n\n\n\n[^1]: https://stackoverflow.com/questions/45034549/difference-between-git-gui-git-bash-git-cmd\n[^2]: https://zh.wikipedia.org/wiki/%E6%AE%BC%E5%B1%A4","lastmodified":"2022-10-15T14:06:29.178499091Z","tags":null},"/notes/2021/2021.8/%E5%87%B8%E4%BC%98%E5%8C%96%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98":{"title":"凸优化与线性回归问题","content":"# Gradient Descent \u0026 Convex Optimization / 凸优化\n\n\u003cdiv align=\"right\"\u003e 2021-08-02\u003c/div\u003e\n\nTags: #MachineLearning #ConvexOptimization #Math \n\n在[这里(和下面的引用里面)](notes/2021/2021.8/Part.5_Gradient_Descent(ML_Andrew.Ng.).md), 我们特殊的线性规划的损失函数一定是一个凸函数, 那么在其他情况下, 线性规划还是凸函数吗, 线性规划问题会陷入局部最优的问题中去吗?\n\n\u003e Note that, while gradient descent can be susceptible to local minima in general, the optimization problem we have posed here for linear regression has only one global, and no other local, optima; thus gradient descent always converges (assuming the learning rate α is not too large) to the global minimum. Indeed, J is a convex quadratic function. Here is an example of gradient descent as it is run to minimize a quadratic function.\n\u003e \n\u003e ![](notes/2021/2021.8/assets/img_2022-10-15.png)\n\u003e \n\u003e The ellipses shown above are the contours of a quadratic function.[^1]\n\t\n- 凸优化问题与机器学习有着很密切的联系, 需要进一步了解\n\n\n[^1]: https://www.coursera.org/learn/machine-learning/supplement/U90DX/gradient-descent-for-linear-regression","lastmodified":"2022-10-15T14:06:29.482502366Z","tags":null},"/notes/2021/2021.8/%E5%A6%82%E4%BD%95%E5%8A%AA%E5%8A%9B%E5%B7%A5%E4%BD%9C_Paul_Graham":{"title":"如何努力工作_Paul_Graham","content":"# 如何勤奋工作\n\n\u003cdiv align=\"right\"\u003e 2021-08-16\u003c/div\u003e\n\nTags: #Essay #PaulGraham #Translation \n\n\u003cdiv align=\"right\"\u003e 2021.6\u003c/div\u003e\n\n关于如何勤奋工作似乎没什么好学的. 任何上过学的人都知道勤奋工作需要付出些什么, 即使他们选择不去这么做. 要知道有些12岁的小孩也工作得非常勤奋. 如今， 每当思考这个问题的时候, 我总会发现自己相比学生时期有了更多的感悟.\n\n我的感悟之一是: 如果你想要做点了不起的事情, 那么努力工作是必须的. 在童年的时候, 我还对此不太确定: 那时学校里不同功课的难度并不相同, 有的功课不费什么力气就能做好. 而且我看那些出色的大人们工作的时候, 他们似乎毫不费力就能做好. 也许, 只要你足够聪明, 就有方法躲过繁重的工作? 现在, 我明白了其实并没有这样的方法.\n\n之所以学校里面有些学科简单很多，是因为学校对它们的要求不高. 而之所以那些大人们工作看起来毫不费力是因为他们已经练习了许多年, 他们让自己的工作变得看起来很简单.\n\n当然, 那些工作出色的人常常的确有许多天赋. 勤奋工作有三个要素: 天赋, 练习, 和努力. 只具备其中两项的话, 你一样也能做得很好, 但是要成为佼佼者, 你必须同时具备这三个要素: 你需要有才华, 勤加练习并且十分努力. [^3]\n\n比尔·盖茨可以说是他同时代里面最聪明的商人之一, 但同时他也是里面最勤奋的之一. \"我在二十几岁的时候从来没有休息过一天, \" 他说, \"一天也没有. \" 利昂内尔·梅西(Lionel Messi)也是这样, 他天赋异禀, 但是当他年轻时候的教练们谈到他的时候, 他们记住的是梅西的付出和必胜的决心. 如果我一定要选一个话, P·G·伍德豪斯(P. G. Wodehouse)是我心目中二十世纪最伟大的英国作家. 写作对于他来说当然比任何人都更得心应手, 但也没有人工作得比他更努力. 在74岁的时候, 他这样说:\n\n\u003e 正如我所说的那样, 我对自己的每一本新书, 都有 \"这次又在文学花园里面摘到了一颗柠檬 \" 的感觉. 但我觉得这是一件好事: 这让我始终保持活力, 书里每一句话我都要重写十遍, 有时甚至二十遍.\n\n对你来说, 这可能听起来有点极端. 但比尔盖茨好像听起来更极端: 谁能十年里一天也不休息? 他们两个有着比任何人都多的天赋, 却也工作的比任何人都勤奋. 这两个特质正是你需要同时具备的.\n\n听起来, 这是太显然不过的了, 但实际上却很难完全做到. “才华和勤奋的区别并不大.[^1]”, 这个观点一部分来自于主流文化(这在主流文化里面根深蒂固), 一部分也是因为才华和勤奋两者都有的\"离群者\"太少见了. 如果说天赋和坚持努力的动力都很少见的话, 那么两者都具备的人可以说是\"平方少见(rare squared)\", 一般来说, 有其中一点的人常常缺乏另一点. 但是想要出众的话, 你需要同时具备两点. 既然你不能改变自己的天资, 那么尽自己所能地勤奋工作, 便是成就伟大事业的唯一选择了.\n\n如果你面前有外加给你的, 定义明确的目标(就像在学校里面那样), 那么要做到勤奋工作其实很简单. 在这种情况下, 有一些技巧: 不欺骗自己, 不要拖延(这也是欺骗自己的一种形式), 不要分心, 遇到挫折不要放弃. 但是这些原则很小的孩子也能做到, 只要他们愿意.\n\n我在还是孩子的时候学到的是: 如何实现那些没有明确定义, 也不是外加给自己的目标. 如果你想要有所成就, 你得适应以上两种情况.\n\n最基本的是: 你需要自己想要工作, 而不是让别人催促你. 现在, 如果我没有在努力工作, 心里的警钟就会响起. 当我努力工作的时候, 并不一定会有所收获, 但是如果我没有努力工作, 我一定什么也得不到, 这会让我感觉很糟.[^2]\n\n我小时候并没有突然领悟到这一点. 像其他小孩子一样, 我喜欢学习或者完成某样新东西时候的成就感. 随着年龄的增长, 这慢慢变成了在自己无所事事的时候的一种厌恶感. 一个我记得很清楚的时间是, 我从13岁起便不再看电视了. \n\n我接触的一些人也是在大约这个年纪决定开始认真做事的. 当我问Patrick Collison他是什么时候开始受不了无所事事的, 他说:\n\n\u003e 我觉得是13, 14岁左右. 我清楚的记着我坐在客厅里面, 望着外面, 寻思自己为什么在浪费自己的暑假. \n\n也许是青春期会发生一些变化吧. 这就解释的通了.\n\n奇怪的是, \"开始认真做事\"前面最大的阻碍很可能是学校. 学校让工作(学习, 至少他们所指的所谓的\"学习\")变得无意义又乏味. 在我全身心投入一件事之前, 我必须要知道什么才是\"真正的工作\", 我花了一些时间才学到这一点, 因为即使是在大学里面, 许多工作也是没有意义的, 甚至有时一整个部门都是没用的. 但是在我终于知道真正的工作是什么之后, 我发现自己想要完成工作的欲望与工作本身达到了完美的契合.\n\n我猜想大多数人都得先认清什么才是\"真正的工作\", 然后才有机会爱上工作. 在_《一个数学家的辩白》(A Mathematician's Apology)_ 里面对这有一句少有的精辟描述:\n\n\u003e 我不记得我在儿时对数学有什么热情, 这对于一个数学家来说并不是一个光彩的想法. 我当时以为数学就是考试和奖学金, 我想要打败其他同龄人, 而数学成为了我打败他们最干脆的方式.\n\n他直到大学才了解到数学的真正内涵, 当时他在阅读Jordan的 _Cours d'analyse_:\n\n\u003e 我永远也忘不了我在阅读那部杰作时候的惊异心情, 了解到我这一代数学家最初的灵感来源, 我第一次懂得了数学的真正含义.\n\n为了理解什么是\"真正的工作\", 你需要破除两种虚假的观念. 第一种正是哈代(Hardy[^4])在学校里遇到的那种. 为了便于教授, 学科知识会被扭曲, 而扭曲程度常常如此之大以至于使它们变得面目全非.[^5] 另一种观念是某些工作本质上的问题: 有些工作本身就是虚伪的, 或者只是纯粹的琐事.\n\n真正的工作有着某种实在的东西. 我并不是说每个人都要去写牛顿的《原理》, 而是说真正的工作会让你感觉到它的重要性. 这的确是一个模糊的标准, 但是我是有意说的这么模糊的, 我们需要涵盖很广的范围.[^6]\n\n一旦你弄清了什么是真正的工作, 你接下来需要弄清楚自己每天要花多少时间在它上面. 你不可能把清醒的每一分钟都花在这件事上面, 因为对于许多工作而言, 投入到精力超出了一定限度, 结果会越来越糟糕.\n\n这个限度取决于人和工作本身. 我做过好几种不同的工作, 每一个工作的限度都是不一样的. 对于难一点的写作或者编程任务, 我的限度是一天5个小时. 然而当我经营一家初创公司的时候, 我可以不间断地工作, 至少前三年是这样, 如果我再做久一点的话, 我可能就需要偶尔放放假了.[^7]\n\n找到限度的唯一方法是超越限度. 要培养自己对工作结果好坏的敏感度, 这样你就能够通过工作成果的好坏来判断自己是不是工作得太狠了. 这时, 对自己诚实非常重要: 一方面, 你可以注意到自己是不是懈怠了, 另一方面, 你也可以注意到自己是不是工作得太多了. 如果你觉得拼命地工作是一件光荣的事, 赶紧丢掉这个想法, 因为你只不过是在得到更差的结果罢了, 你这样做仅仅是为了荣誉感, 如果不是为了给别人看, 那么就是为了给你自己看.[^8]\n\n找到刻苦工作的限度是一个长期的, 持续的过程, 不是一蹴而就的. 工作本身的难度和你自己的精力每个小时都在发生变化, 所以你需要持续评价自己的工作强度和工作效果.\n\n努力也并不意味着要一直强迫自己去工作. 有些人可能的确是这样做的, 但是我觉得我的经验适用于大多数人: 我只需要偶尔在开始一个项目的时候或者临近某个检查的时候催促自己, 那个时候我很容易拖延. 但是只要我开始做事了, 我就会一直做下去.\n\n我坚持的动力取决于我做的工作. 当我在Viaweb工作的时候, 我的动力是对失败的恐惧. 我当时几乎不会拖延, 因为总有事情要做, 如果我能够做点什么来拉开我和追赶我的猛兽的距离, 为什么要拖延呢?[^9] 然而现在驱使我写文章的动力是文章里面不完美的地方. 在动笔之前我会折腾几天, 像一只狗在趴下之前都会转几圈, 思考趴在哪里. 但是只要我开始写了, 我并不需要刻意督促自己去工作, 因为总有一些错误或者缺漏在督促我. \n\n我的确会花一些精力让自己着眼于重要的部分. 许多问题都有一个最难的核心, 周围环绕的是一些相对简单的部分. 努力工作意味着尽自己所能地着眼于核心, 有些时候你可能会感到力不从心, 有些日子你可能只能做些简单的, 外围的工作. 但是你始终应该努力着眼于核心, 努力不拖延. \n\n你应该怎么生活也是这些\"硬核问题\"之一. 在核心的部分有更重要更艰难的问题, 而不那么重要的问题则在边缘. 所以你不仅在从事具体工作的时候需要偶尔做出改变,  你不时也需要进行一些重大的, 转变生活轨迹的调整, 来确保自己始终在做重要的事. 原则还是一样的, 勤奋工作意味着抓住核心, 抓住最困难的问题. \n\n我说的\"核心\"指的是真正的核心, 而不是大家认为的的核心. 大家公认的最重要的问题常常是错误的, 无论是整体上还是具体的领域中. 如果你的意见和大家相异, 而且你对了, 那将是一个创造新事物的宝贵时机.\n\n更有野心的工作常常是更困难的, 尽管应该承认这一点, 但是你也不应该只根据问题的困难度来决定做什么. 如果你发现某个重要的工作对你来说比其他人更容易, 要么是因为你碰巧很擅长这件事, 你有天赋, 要么是因为你找到了一个新方法, 或者只是因为你做这件事更有动力.无论如何都要完成它. 有些最棒的工作是由那些找到困难问题的简单策略的人完成的.\n\n除了学习什么是真正的工作之外, 你还需要弄清楚你适合什么工作. 这也不是说你一定要找到最符合你天赋的工作, 如果你7英尺高, 你也不一定要去打篮球. 你适合什么工作不仅仅取决于你的天赋, 也取决于你的兴趣, 甚至兴趣还要更重要一点. [对于一个领域的强烈兴趣](http://www.paulgraham.com/genius.html)比任何约束都更能激发你工作的热情.\n\n发现你的兴趣可能比发现你的天赋更难. 天赋的种类比兴趣要少, 并且你往往从童年就开始发掘自己的天赋了, 相比之下, 兴趣更为微妙, 你可能到二十多岁才发现自己的兴趣, 甚至更迟. 你感兴趣的东西有可能还不存在. 并且你还要排除一切更为强大的干扰: 你真的对$x$感兴趣吗? 会不会只是因为$x$能赚很多钱?或者是因为$x$是一个很风光的工作?或者是因为你父母想让你从事$x$?[^10]\n\n弄清楚要做什么工作的困难程度对每个人来说差别非常大. 这是我自童年以来学到的关于工作最重要的事情之一. 还是一个孩子的时候, 你会以为每个人都有一个使命, 每个人需要做的就是弄清楚它是什么. 在电影里的确是这样, 在那些讲给孩子们听的线性的名人传记里面也是这样. 有时候生活的确是这样的, 有的人在童年时就发现了自己想干什么, 他们只需要坚持做就行了, 比如莫扎特. 但是另一些人, 比如牛顿, 换了一种又一种工作. 也许我们作为后来人能从中找出一个工作当作他们的使命——我们会希望牛顿多花点时间在数学和物理上面, 少花点时间在炼金术和神学上面——但是这仅仅是后见之明带来的错觉. 对于牛顿来说, 没有任何声音在指引他.\n\n所以虽然有一部分人的生活很快就能确定下来, 另外也有一部分人的生活永远也不会确定下来. 对于后面这部分人, 确定要做什么不是勤奋工作的\"序曲\", 而是勤奋工作本身的一部分, 两者就像一组联立方程式一样. 对于这部分人, 我前面提到的过程有了第三个部分: 除了认识到自己的工作强度和工作的效果, 你还需要思考自己是该继续现在的工作还是换一个新的领域. 如果你工作得很努力但是效果不够好, 你就该换一个工作了. 这听起来很简单, 但是做起来很难. 如果你在工作第一天非常努力但是效果不理想, 你当然不应该放弃, 上手需要时间. 但是多少时间又是合适的呢? 如果你之前得心应手的工作不再顺手, 你又应该怎么办? 你这时又该给自己多少时间? [^11]\n\n究竟什么样的结果才能称得上好结果? 这很难评判. 如果你是在几乎没有前期工作的领域里面摸索, 那么你甚至不知道好的结果长什么样子. 在历史上, 错误地评估自己工作重要性的例子比比皆是.\n\n评判一项工作是否有价值的最可靠标准是你是否觉得它有意思. 这听起来过分主观了, 甚至主观得有点危险. 但是这大概是你能够得到的最精确的结果了. 做这项工作的人是你自己. 谁还能比你自己更有资格评判这项工作的重要性呢? 又能有什么标准比\"是不是有趣\"更好呢?\n\n但是要想实践这个标准, 你需要对自己完全诚实. 的确, 这是如何勤奋工作这个问题里最突出的一点, 这里面每一个环节都取决于你对自己的诚实程度.\n\n勤奋工作并不是榨干自己的每一份精力. 这是一个复杂的主题, 它是一个需要每一点都恰到好处的动态系统: 你需要认清\"真正的工作\"是什么, 弄清自己适合做什么, 尽你所能抓住问题的核心, 在每一刻都清楚的认识自己的现状与潜力, 并且确保投入恰到好处的精力来保证最好的结果. 这虽然是一个复杂的网络, 但只要你始终保持诚实与清醒, 它便会自动达到最优, 而你也将会异乎常人的高效.\n\n## 英语原文\n[如何努力工作_Paul_Graham](notes/2021/2021.8/如何努力工作_Paul_Graham.md)\n\n\n\n[^3]: 在\"关于天才的公交车票理论\"一文中, 我提到伟大事业的三个要素是天赋, 决心和兴趣. 那是前一阶段的要素: 因为决心和兴趣催生了练习和努力.\n[^1]:  原文: here's a faint xor between talent and hard work.潜在的意思是只要有一个就行了 [译注]\n[^2]: 这里的\"分辨率\"是一天, 而不是一小时. 你可能在洗澡的时候, 甚至睡觉的时候想到一个问题的答案, 但是这的前提是你前一天一定在努力解决那个问题.偶尔休假是件好事, 我喜欢学习新东西, 而不是知识躺在沙滩上.\n[^4]: 上文《一个数学家的辩白》的作者 [译注]\n[^5]: 孩子在学校里面学到的最\"原汁原味\"的东西是体育, 这诚然是因为许多运动项目都是起源于学校里面的游戏, 但是至少在这一部分, 孩子们在做和成年人一样的事. 在普通的美国高中里, 你可以选择假装做正经事, 或者正经地做些模仿的事, 老实说, 后者并不差.\n[^6]: 知道你想要做什么并不意味着你有能力做这件事. 许多人花了很多时间做他们不想做的工作, 尤其是早年的时候, 但是如果你清楚自己想要做什么, 你至少知道应该往哪边努力.\n[^7]: 高强度工作有着更短的有效工作时间, 这可能是当你有了孩子以后的一个解决方案: 做更难的事. 我实际上就是这么干的, 尽管不是故意的.\n[^8]: 在有些文化里面提倡表演性质的刻苦工作, 我个人并不喜欢这个主意, 因为: 1. 这只不过是对重要的事的模仿而已, 2. 这是对人们精力无意义的消耗. 我懂得不多, 没有资格评价这个现象的好坏, 但是我猜测这个现象并不好.\n[^9]: 初创企业的人们拼命工作的原因之一就是初创企业会倒闭, 并且如果他们倒闭了, 这个失败将会是决定性且非常显眼的\n[^10]: 想要赚很多钱没什么不好意思的. 你总是需要解决生计问题的, 并且想要一次性赚很多钱的想法也没什么不对. 我认为对金钱本身感兴趣也是可以的, 随你便. 只要你知道自己的初心. 不要无意间让对金钱的渴望掩盖了你真正感兴趣的东西.\n[^11]: 许多人在个人的小项目里面遇到过这个问题. 但是舍弃一个小项目还是要比放弃一整个工作要容易许多. 你越坚定, 放弃就越困难. 这时的你就像西班牙流感病人, 在对抗自己的免疫系统: 你告诉自己不能放弃, 应该再努力一点. 这时, 谁又能说你做的不对呢?","lastmodified":"2022-10-15T14:06:29.482502366Z","tags":null},"/notes/2021/2021.8/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%88%86%E5%B8%83%E4%B8%8E%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E7%9A%84%E8%81%94%E7%B3%BB_Relation_of_Laplace_distribution-_and_Gaussian_distribution":{"title":"拉普拉斯分布与高斯分布的联系_Relation_of_Laplace_distribution _and_Gaussian_distribution","content":"# Gaussian distribution, Laplace distribution: The Relation\n\n\u003cdiv align=\"right\"\u003e 2021-07-31\u003c/div\u003e\n\nTags: #Math/Statistics #GaussianDistribution #LaplaceDistribution\n\n拉普拉斯分布, 概率密度函数:\n![](notes/2021/2021.7/assets/img_2022-10-15-1.png)\n![](notes/2021/2021.7/assets/img_2022-10-15-2.png)\n\n![概率密度函数](notes/2021/2021.9/正态分布_高斯分布_Normal_Distribution-Gaussian_Distribution.md#概率密度函数)\n\n- Look at the formula for the PDF in the infobox -- it's just the Gaussian with  $|\\boldsymbol{x}-\\boldsymbol{\\mu}|$ instead of $(\\boldsymbol{x}-\\boldsymbol{\\mu})^{2}$)[^2]\n\n- 拉普拉斯分布的概率密度函数让我们联想到[正态分布](https://zh.wikipedia.org/wiki/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83 \"正态分布\")，但是，**正态分布**是用相对于 _μ_ **[平均值](https://zh.wikipedia.org/wiki/%E5%B9%B3%E5%9D%87%E5%80%BC)的差的平方**来表示，而**拉普拉斯概率密度**用相对于**平均值的差的[绝对值](https://zh.wikipedia.org/wiki/%E7%BB%9D%E5%AF%B9%E5%80%BC \"绝对值\")** 来表示。因此，拉普拉斯分布的尾部比正态分布更加平坦。[^1]\n[^1]: https://zh.wikipedia.org/zh-hans/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%88%86%E5%B8%83\n[^2]: [Why_do_cost_functions_use_the_square_error](notes/2021/2021.8/Why_do_cost_functions_use_the_square_error.md#^b7e1c9)","lastmodified":"2022-10-15T14:06:29.482502366Z","tags":null},"/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E7%9A%84%E6%B1%82%E5%AF%BC":{"title":"矩阵的求导","content":"# 对矩阵的求导_Matrix_Derivative\n\n\u003cdiv align=\"right\"\u003e 2021-08-16\u003c/div\u003e\n\nTags: #Matrix #Derivative #Calculus #MachineLearning \n\n- 在学习吴恩达机器学习CS229的时候为了推导Normal Equation的公式, 接触到了函数对于矩阵的求导, 因为许久没有接触微积分, 并且知识跨度太大, 许久没有看懂, 故在此笔记中慢慢梳理.\n- Learning Materials:\n\t- Pili HU, Matrix Calculus, https://github.com/hupili/tutorial/tree/master/matrix-calculus\n\t- A Matrix Algebra Approach to Artificial Intelligence by Xian-Da Zhang\n\t- [矩阵求导术](https://zhuanlan.zhihu.com/p/24709748)\n\t- [矩阵导数计算器](http://www.matrixcalculus.org/)\n\t- \n---\n## 不同的情况\n涉及到矩阵的导数运算有以下情况:\n- 标量函数(Scalar Function) : 将变量(可能是矩阵或者向量)映射到 $\\rightarrow\\mathbb{R}_{}$\n- 向量函数(Vector Function):  变量$\\rightarrow\\mathbb{R}_{n}$\n- 矩阵函数(Matrix Function): 变量$\\rightarrow\\mathbb{R}_{m\\times n}$\n\n具体的映射表示如下:[^1]\n![](notes/2021/2021.7/assets/img_2022-10-15.png)\n\n## A Walk Through\n[Link to File](https://project.hupili.net/tutorial/hu2012-matrix-calculus/hu2012matrix-calculus.pdf)\n### Highlights\n矩阵导数与微分的联系 \n$$d f=\\operatorname{tr}\\left(\\left(\\frac{\\partial f}{\\partial X}\\right)^{T} d X\\right)$$ ^e0894d\n\n\n## 与梯度的关系[^2]\n从下面的叙述可以看出, 一个$\\mathbb R_{m\\times n}\\mapsto \\mathbb R_{n}$函数$f(A)$, 其对于$A$梯度便是对于自变量$A$的导数矩阵.\n\n所以在吴恩达的讲义里面的$\\nabla$符号是梯度, 但是严格的来说应该是导数$\\large \\frac {\\partial f} {\\partial x}$\n[Normal_Equation_Proof_2_Matrix_Method](notes/2021/2021.8/Normal_Equation_Proof_2_Matrix_Method.md)\n\n### 实值函数相对于向量和矩阵的梯度\n相对于 $\\mathrm{n} \\times 1$ 向量 $\\mathrm{x}$ 的梯度算子记作 $\\nabla_{\\boldsymbol{x}}$, 定义为\n$$\n\\nabla_{\\boldsymbol{x}} \\stackrel{\\text { def }}{=}\\left[\\frac{\\partial}{\\partial x_{1}}, \\frac{\\partial}{\\partial x_{2}}, \\cdots, \\frac{\\partial}{\\partial x_{n}}\\right]^{T}=\\frac{\\partial}{\\partial \\boldsymbol{x}}\n$$\n\n#### 对向量的梯度\n\n以 $\\mathrm{n} \\times 1$ 实向量 $\\mathrm{x}$ 为变元的实标量函数 $\\mathrm{f}(\\mathrm{x})$ 相对于 $\\mathrm{x}$ 的梯度为 $\\mathrm{n} \\times 1$ 列向量 $\\mathbf{x}$, 定义为\n$$\n\\nabla_{\\boldsymbol{x}} f(\\boldsymbol{x}) \\stackrel{\\text { def }}{=}\\left[\\frac{\\partial f(\\boldsymbol{x})}{\\partial x_{1}}, \\frac{\\partial f(\\boldsymbol{x})}{\\partial x_{2}}, \\cdots, \\frac{\\partial f(\\boldsymbol{x})}{\\partial x_{n}}\\right]^{T}=\\frac{\\partial f(\\boldsymbol{x})}{\\partial \\boldsymbol{x}}\n$$\nm维行向量函数 $\\boldsymbol{f}(\\boldsymbol{x})=\\left[f_{1}(\\boldsymbol{x}), f_{2}(\\boldsymbol{x}), \\cdots, f_{m}(\\boldsymbol{x})\\right]$ 相对于n维实向量 $\\mathbf{x}$ 的梯度为 $\\mathrm{n} \\times \\mathrm{m}$ 矩阵, 定义为\n$$\n\\nabla_{\\boldsymbol{x}} \\boldsymbol{f}(\\boldsymbol{x}) \\stackrel{\\operatorname{def}}{=}\\left[\\begin{array}{cccc}\n\\frac{\\partial f_{1}(\\boldsymbol{x})}{\\partial x_{1}} \u0026 \\frac{\\partial f_{2}(\\boldsymbol{x})}{\\partial x_{1}} \u0026 \\cdots \u0026 \\frac{\\partial f_{m}(\\boldsymbol{x})}{\\partial x_{1}} \\\\\n\\frac{\\partial f_{1}(\\boldsymbol{x})}{\\partial x_{2}} \u0026 \\frac{\\partial f_{2}(\\boldsymbol{x})}{\\partial x_{2}} \u0026 \\cdots \u0026 \\frac{\\partial f_{m}(\\boldsymbol{x})}{\\partial x_{2}} \\\\\n\\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\\n\\frac{\\partial f_{1}(\\boldsymbol{x})}{\\partial x_{n}} \u0026 \\frac{\\partial f_{2}(\\boldsymbol{x})}{\\partial x_{n}} \u0026 \\cdots \u0026 \\frac{\\partial f_{m}(\\boldsymbol{x})}{\\partial x_{n}}\n\\end{array}\\right]=\\frac{\\partial \\boldsymbol{f}(\\boldsymbol{x})}{\\partial \\boldsymbol{x}}\n$$\n#### 对矩阵的梯度\n\n标量函数 $f(\\boldsymbol{A})$ 相对于 $\\mathrm{m} \\times \\mathrm{n}$ 实矩阵 $\\mathrm{A}$ 的梯度为 $\\mathrm{m} \\times \\mathrm{n}$ 矩阵, 简称梯度矩阵, 定义为\n$$\n\\nabla_{A} f(\\boldsymbol{A})\\stackrel{\\text { def }}{=}\\left[\\begin{array}{cccc}\n\\frac{\\partial f(A)}{\\partial a_{11}} \u0026 \\frac{\\partial f(A)}{\\partial a_{12}} \u0026 \\cdots \u0026 \\frac{\\partial f(A)}{\\partial a_{1 n}} \\\\\n\\frac{\\partial f(A)}{\\partial a_{21}} \u0026 \\frac{\\partial f(A)}{\\partial a_{22}} \u0026 \\cdots \u0026 \\frac{\\partial f(A)}{\\partial a_{2 n}} \\\\\n\\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\\n\\frac{\\partial f(A)}{\\partial a_{m 1}} \u0026 \\frac{\\partial f(A)}{\\partial a_{m 2}} \u0026 \\cdots \u0026 \\frac{\\partial f(A)}{\\partial a_{m n}}\n\\end{array}\\right]=\\frac{\\partial f(\\boldsymbol{A})}{\\partial \\boldsymbol{A}}\n$$\n\n\n\n#### 法则\n\n以下法则适用于实标量函数对向量的梯度以及对矩阵的梯度.\n\n- 线性法则: 若 $f(\\boldsymbol{A})$ 和 $g(\\boldsymbol{A})$ 分别是矩阵A的实标量函数, $\\mathrm{c}_{1}$ 和 $\\mathrm{c}_{2}$ 为实常数, 则\n\n$$\n\\frac{\\partial\\left[c_{1} f(\\boldsymbol{A})+c_{2} g(\\boldsymbol{A})\\right]}{\\partial \\boldsymbol{A}}=c_{1} \\frac{\\partial f(\\boldsymbol{A})}{\\partial \\boldsymbol{A}}+c_{2} \\frac{\\partial g(\\boldsymbol{A})}{\\partial \\boldsymbol{A}}\n$$\n- 乘积法则: 若 $f(\\boldsymbol{A}), g(\\boldsymbol{A})$ 和 $h(\\boldsymbol{A})$ 分别是矩阵A的实标量函数, 则\n\n$$\n\\begin{aligned}\n\u0026\\frac{\\partial f(\\boldsymbol{A}) g(\\boldsymbol{A})}{\\partial \\boldsymbol{A}}=g(\\boldsymbol{A}) \\frac{\\partial f(\\boldsymbol{A})}{\\partial \\boldsymbol{A}}+f(\\boldsymbol{A}) \\frac{\\partial g(\\boldsymbol{A})}{\\partial \\boldsymbol{A}} \\\\\n\u0026\\frac{\\partial f(\\boldsymbol{A}) g(\\boldsymbol{A}) h(\\boldsymbol{A})}{\\partial \\boldsymbol{A}}=g(\\boldsymbol{A}) h(\\boldsymbol{A}) \\frac{\\partial f(\\boldsymbol{A})}{\\partial \\boldsymbol{A}}+f(\\boldsymbol{A}) h(\\boldsymbol{A}) \\frac{\\partial g(\\boldsymbol{A})}{\\partial \\boldsymbol{A}}+f(\\boldsymbol{A}) g(\\boldsymbol{A}) \\frac{\\partial h(\\boldsymbol{A})}{\\partial \\boldsymbol{A}}\n\\end{aligned}\n$$\n- 商法则: 若 $g(\\boldsymbol{A}) \\neq 0$, 则\n\n$$\n\\frac{\\partial f(\\boldsymbol{A}) / g(\\boldsymbol{A})}{\\partial \\boldsymbol{A}}=\\frac{1}{g(\\boldsymbol{A})^{2}}\\left[g(\\boldsymbol{A}) \\frac{\\partial f(\\boldsymbol{A})}{\\partial \\boldsymbol{A}}-f(\\boldsymbol{A}) \\frac{\\partial g(\\boldsymbol{A})}{\\partial \\boldsymbol{A}}\\right]\n$$\n- 链式法则：若A为 $m \\times n$ 矩阵，且 $y=f(\\boldsymbol{A})$ 和 $g(y)$ 分别是以矩阵 $\\mathbf{A}$ 和标量 $y$ 为变元的实标量函数, 则\n\n$$\n\\frac{\\partial g(f(\\boldsymbol{A}))}{\\partial \\boldsymbol{A}}=\\frac{d g(y)}{d y} \\frac{\\partial f(\\boldsymbol{A})}{\\partial \\boldsymbol{A}}\n$$\n\n\n\n\n\n\n\n\n\n\n[^1]:A Matrix Algebra Approach to Artificial Intelligence\n[^2]: https://zh.wikipedia.org/zh-sg/%E6%A2%AF%E5%BA%A6 ","lastmodified":"2022-10-15T14:06:29.482502366Z","tags":null},"/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E8%BF%B9%E7%9A%84%E6%80%A7%E8%B4%A8":{"title":"矩阵迹的性质","content":"# 矩阵迹的性质\n\n\u003cdiv align=\"right\"\u003e 2021-08-16\u003c/div\u003e\n\nTags: #Trace #Matrix #Math\n\n- 标量可以直接套上迹： $a=\\operatorname{tr}(a)$\n\n- $\\mathrm{tr}AB = \\mathrm{tr}BA$  ^tracecommutative\n\n![](notes/2021/2021.7/assets/tr1.drawio.svg)\n左边: \n$$\n\\begin{align}\n\u0026\\sum^n_i a_{1i}b_{i1}+\\sum^n_i a_{2i}b_{i2}+\\cdots+\\sum^n_i a_{mi}b_{im} \\\\ = \u0026\\sum^m_j\\sum^n_ia_{ji}b_{ij}\n\\\\ = \u0026\\sum^m_i\\sum^n_j a_{ij}b_{ji}\n\\end{align}\n$$\n右边:\n$$\n\\begin{align}\n\u0026\\sum^m_i b_{1i}a_{i1}+\\sum^m_i b_{2i}a_{i2}+\\cdots+\\sum^m_i b_{ni}a_{in} \\\\ = \u0026\\sum^n_j\\sum^m_i b_{ji}a_{ij}\n\\\\ = \u0026\\sum^m_i\\sum^n_j a_{ij}b_{ji}\n\\end{align}\n$$\n\n左边=右边\n\n上面的式子其实就相当于把A,B其中一个翻过来, 和另一个叠在一起, 对应位置乘起来, 再加起来:\n![](notes/2021/2021.7/assets/tr2.drawio.svg)\n\n推广:\n只要\"环形的\"顺序不变, 矩阵相乘的迹就不变 \n$$\\mathrm{tr}ABC = \\mathrm{tr}CAB = \\mathrm{tr}BCA$$\n$$\\mathrm{tr}ABCD = \\mathrm{tr}DABC = \\mathrm{tr}CDAB = \\mathrm{tr}BCDA$$\n\n\n- 对于方阵, 还有以下性质:\n\t- $\\mathrm{tr}A=\\mathrm{tr}A^T \\Rightarrow$ 因为旋转轴不变\n\t\t- flip the matrix around its rotary line, which is the \"trace line\", and the \"trace line\" is the only thing that doesn't change when flipping the matrix.\n\t- $\\mathrm{tr}(A+B)=\\mathrm{tr}A+\\mathrm{tr}B$\n\t- $\\mathrm{tr}(aA)=a\\mathrm{tr}A$\n","lastmodified":"2022-10-15T14:06:29.482502366Z","tags":null},"/notes/2021/2021.8/Different_Gradient_Descent_Methods":{"title":"Different_Gradient_Descent_Methods","content":"## Batch Gradient Descent, 批梯度下降, BGD\n每一次把所有数据都用来更新参数, Use ALL the training examples.\n$$\n\\begin{array}{l}\n\\text { repeat until convergence }\\{\\\\\n\\begin{array}{cc}\n\u0026\\theta_{j}:=\\theta_{j}-\\alpha \\frac 1 m \\sum_{i=1}^{m} \\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right) x_j^{(i)} \n\\end{array}\\\\\n\\text { \\} }\n\\\\\\\\ \\text { (simultaneously update } \nj=0, \\cdots ,j=n)\n\\end{array}\n$$\n## Stochastic/Incremental Gradient Descent, 随机梯度下降 SGD\n\n$$\n\\begin{aligned}\nLoop \u0026\\quad \\{\\qquad\\\\\n\u0026\\text { for } \\mathrm{i}=1 \\text { to } \\mathrm{m},\\{ \\\\\n\u0026\\qquad \\theta_{j}:=\\theta_{j}+\\alpha\\left(y^{(i)}-h_{\\theta}\\left(x^{(i)}\\right)\\right) x_{j}^{(i)} \\qquad\\quad(\\text { for every j} \\ )\\\\\n\\quad\u0026\\qquad\\quad\\}\\\\\n\u0026\\quad\\}\n\\end{aligned}\n$$\n上面这个式子最大的不同是: 没有了求和符号, 取而代之的是一个for循环. 这意味着随机梯度下降每一次只根据一个样本进行参数更新.(each time we encounter a training example, we update the parameters according to the gradient of the error with respect to that single training example only.)\n\n这样的好处是速度快, 每一次计算都在更新参数, 而不像BGD那样把所有样本都算一遍才能更新一次参数. \n坏处是可能不会Converge, 最终得到的结果可能只是一个近似解, 最后得到一种震荡的状态.\n一种解决方法是随着训练的进行不断减小学习率, 这样便可以增大收敛的几率.\n\n## Mini-batch Gradient Descent\n这种方法综合了上面两种方法的优缺点, 每次选一个小样本来更新参数, 达到了一个很好的折中.\n\n![](notes/2021/2021.7/assets/img_2022-10-15-3.png)\n\n\n## Other Methods\n其实还有很多方法, [这篇文章](https://ruder.io/optimizing-gradient-descent/)是一个很好的总结.\n\n吴恩达的深度学习课程里面有一节专门讲优化方法: #todo\nhttps://www.coursera.org/lecture/deep-neural-network/mini-batch-gradient-descent-qcogH ^06f99c","lastmodified":"2022-10-15T14:06:29.478502323Z","tags":null},"/notes/2021/2021.8/Here_goes_nothing_meaningetymology":{"title":"Here_goes_nothing!_meaning\u0026etymology","content":"# Here goes nothing!\n\n\u003cdiv align=\"right\"\u003e 2021-08-02\u003c/div\u003e\n\nTags: #English \n\n![](notes/2021/2021.7/assets/img_2022-10-15.gif)\n\n## ref.1\nIndicates a lack of confidence or certainty about the activity about to be tried.\n\n    Well, I checked everything and I think it's wired up correctly, so I guess all that's left is to turn it on. Here goes nothing!\n[^1]\n## ref.2\nWhen we are certain the outcome of an/a activity/task will be a failure or when we know we are going to be unsuccessful but we still do it.\n\n\tWhen going to give a speech on feminism , for which she wasn't prepared for, Brooklyn whispered to herself: Here goes nothing.\n[^2]\n\n## ref.3\nI'm going to attempt this, even though I doubt that I'll be successful, do well, or enjoy it. Often used somewhat ironically, in a way that indicates optimism despite the possibility of failure. \n\n\t- \"Here goes nothing,\" he muttered, as he began the final exam. \n\t\n\t- My rocket is ready for launch! Here goes nothing!\n[^3]\n\n[^1]: https://en.wiktionary.org/wiki/here_goes_nothing\n[^2]: https://www.urbandictionary.com/define.php?term=Here%20Goes%20Nothing\n[^3]: https://idioms.thefreedictionary.com/here+goes+nothing","lastmodified":"2022-10-15T14:06:29.478502323Z","tags":null},"/notes/2021/2021.8/How_to_Work_Hard-Paul_Graham":{"title":"How_to_Work_Hard-Paul_Graham","content":"# How to Work Hard\n\n\u003cdiv align=\"right\"\u003e 2021-08-16\u003c/div\u003e\n\nTags: #Essay #PaulGraham #Translation\n\n## Chinese Translation\n[[notes/2021/2021.8/如何努力工作_Paul_Graham]]\n\n## English Version\nJune 2021  \n  \nIt might not seem there's much to learn about how to work hard. Anyone who's been to school knows what it entails, even if they chose not to. There are 12 year olds who work amazingly hard. And yet when I ask if I know more about working hard now than when I was in school, the answer is definitely yes.  \n  \nOne thing I know is that if you want to do great things, you'll have to work very hard. I wasn't sure of that as a kid. Schoolwork varied in difficulty; one didn't always have to work super hard to do well. And some of the things famous adults did, they seemed to do almost effortlessly. Was there, perhaps, some way to evade hard work through sheer brilliance? Now I know the answer to that question. There isn't.  \n  \nThe reason some subjects seemed easy was that my school had low standards. And the reason famous adults seemed to do things effortlessly was years of practice; they made it look easy.  \n  \nOf course, those famous adults usually had a lot of natural ability too. There are three ingredients in great work: natural ability, practice, and effort. You can do pretty well with just two, but to do the best work you need all three: you need great natural ability _and_ to have practiced a lot _and_ to be trying very hard. [[1](http://www.paulgraham.com/hwh.html#f1n)]  \n  \nBill Gates, for example, was among the smartest people in business in his era, but he was also among the hardest working. \"I never took a day off in my twenties,\" he said. \"Not one.\" It was similar with Lionel Messi. He had great natural ability, but when his youth coaches talk about him, what they remember is not his talent but his dedication and his desire to win. P. G. Wodehouse would probably get my vote for best English writer of the 20th century, if I had to choose. Certainly no one ever made it look easier. But no one ever worked harder. At 74, he wrote\n\n\u003e with each new book of mine I have, as I say, the feeling that this time I have picked a lemon in the garden of literature. A good thing, really, I suppose. Keeps one up on one's toes and makes one rewrite every sentence ten times. Or in many cases twenty times.\n\nSounds a bit extreme, you think. And yet Bill Gates sounds even more extreme. Not one day off in ten years? These two had about as much natural ability as anyone could have, and yet they also worked about as hard as anyone could work. You need both.  \n  \nThat seems so obvious, and yet in practice we find it slightly hard to grasp. There's a faint xor between talent and hard work. It comes partly from popular culture, where it seems to run very deep, and partly from the fact that the outliers are so rare. If great talent and great drive are both rare, then people with both are rare squared. Most people you meet who have a lot of one will have less of the other. But you'll need both if you want to be an outlier yourself. And since you can't really change how much natural talent you have, in practice doing great work, insofar as you can, reduces to working very hard.  \n  \nIt's straightforward to work hard if you have clearly defined, externally imposed goals, as you do in school. There is some technique to it: you have to learn not to lie to yourself, not to procrastinate (which is a form of lying to yourself), not to get distracted, and not to give up when things go wrong. But this level of discipline seems to be within the reach of quite young children, if they want it.  \n  \nWhat I've learned since I was a kid is how to work toward goals that are neither clearly defined nor externally imposed. You'll probably have to learn both if you want to do really great things.  \n  \nThe most basic level of which is simply to feel you should be working without anyone telling you to. Now, when I'm not working hard, alarm bells go off. I can't be sure I'm getting anywhere when I'm working hard, but I can be sure I'm getting nowhere when I'm not, and it feels awful. [[2](http://www.paulgraham.com/hwh.html#f2n)]  \n  \nThere wasn't a single point when I learned this. Like most little kids, I enjoyed the feeling of achievement when I learned or did something new. As I grew older, this morphed into a feeling of disgust when I wasn't achieving anything. The one precisely dateable landmark I have is when I stopped watching TV, at age 13.  \n  \nSeveral people I've talked to remember getting serious about work around this age. When I asked Patrick Collison when he started to find idleness distasteful, he said\n\n\u003e I think around age 13 or 14. I have a clear memory from around then of sitting in the sitting room, staring outside, and wondering why I was wasting my summer holiday.\n\nPerhaps something changes at adolescence. That would make sense.  \n  \nStrangely enough, the biggest obstacle to getting serious about work was probably school, which made work (what they called work) seem boring and pointless. I had to learn what real work was before I could wholeheartedly desire to do it. That took a while, because even in college a lot of the work is pointless; there are entire departments that are pointless. But as I learned the shape of real work, I found that my desire to do it slotted into it as if they'd been made for each other.  \n  \nI suspect most people have to learn what work is before they can love it. Hardy wrote eloquently about this in _A Mathematician's Apology_:\n\n\u003e I do not remember having felt, as a boy, any _passion_ for mathematics, and such notions as I may have had of the career of a mathematician were far from noble. I thought of mathematics in terms of examinations and scholarships: I wanted to beat other boys, and this seemed to be the way in which I could do so most decisively.\n\nHe didn't learn what math was really about till part way through college, when he read Jordan's _Cours d'analyse_.\n\n\u003e I shall never forget the astonishment with which I read that remarkable work, the first inspiration for so many mathematicians of my generation, and learnt for the first time as I read it what mathematics really meant.\n\nThere are two separate kinds of fakeness you need to learn to discount in order to understand what real work is. One is the kind Hardy encountered in school. Subjects get distorted when they're adapted to be taught to kids — often so distorted that they're nothing like the work done by actual practitioners. [[3](http://www.paulgraham.com/hwh.html#f3n)] The other kind of fakeness is intrinsic to certain types of work. Some types of work are inherently bogus, or at best mere busywork.  \n  \nThere's a kind of solidity to real work. It's not all writing the _Principia_, but it all feels necessary. That's a vague criterion, but it's deliberately vague, because it has to cover a lot of different types. [[4](http://www.paulgraham.com/hwh.html#f4n)]  \n  \nOnce you know the shape of real work, you have to learn how many hours a day to spend on it. You can't solve this problem by simply working every waking hour, because in many kinds of work there's a point beyond which the quality of the result will start to decline.  \n  \nThat limit varies depending on the type of work and the person. I've done several different kinds of work, and the limits were different for each. My limit for the harder types of writing or programming is about five hours a day. Whereas when I was running a startup, I could work all the time. At least for the three years I did it; if I'd kept going much longer, I'd probably have needed to take occasional vacations. [[5](http://www.paulgraham.com/hwh.html#f5n)]  \n  \nThe only way to find the limit is by crossing it. Cultivate a sensitivity to the quality of the work you're doing, and then you'll notice if it decreases because you're working too hard. Honesty is critical here, in both directions: you have to notice when you're being lazy, but also when you're working too hard. And if you think there's something admirable about working too hard, get that idea out of your head. You're not merely getting worse results, but getting them because you're showing off — if not to other people, then to yourself. [[6](http://www.paulgraham.com/hwh.html#f6n)]  \n  \nFinding the limit of working hard is a constant, ongoing process, not something you do just once. Both the difficulty of the work and your ability to do it can vary hour to hour, so you need to be constantly judging both how hard you're trying and how well you're doing.  \n  \nTrying hard doesn't mean constantly pushing yourself to work, though. There may be some people who do, but I think my experience is fairly typical, and I only have to push myself occasionally when I'm starting a project or when I encounter some sort of check. That's when I'm in danger of procrastinating. But once I get rolling, I tend to keep going.  \n  \nWhat keeps me going depends on the type of work. When I was working on Viaweb, I was driven by fear of failure. I barely procrastinated at all then, because there was always something that needed doing, and if I could put more distance between me and the pursuing beast by doing it, why wait? [[7](http://www.paulgraham.com/hwh.html#f7n)] Whereas what drives me now, writing essays, is the flaws in them. Between essays I fuss for a few days, like a dog circling while it decides exactly where to lie down. But once I get started on one, I don't have to push myself to work, because there's always some error or omission already pushing me.  \n  \nI do make some amount of effort to focus on important topics. Many problems have a hard core at the center, surrounded by easier stuff at the edges. Working hard means aiming toward the center to the extent you can. Some days you may not be able to; some days you'll only be able to work on the easier, peripheral stuff. But you should always be aiming as close to the center as you can without stalling.  \n  \nThe bigger question of what to do with your life is one of these problems with a hard core. There are important problems at the center, which tend to be hard, and less important, easier ones at the edges. So as well as the small, daily adjustments involved in working on a specific problem, you'll occasionally have to make big, lifetime-scale adjustments about which type of work to do. And the rule is the same: working hard means aiming toward the center — toward the most ambitious problems.  \n  \nBy center, though, I mean the actual center, not merely the current consensus about the center. The consensus about which problems are most important is often mistaken, both in general and within specific fields. If you disagree with it, and you're right, that could represent a valuable opportunity to do something new.  \n  \nThe more ambitious types of work will usually be harder, but although you should not be in denial about this, neither should you treat difficulty as an infallible guide in deciding what to do. If you discover some ambitious type of work that's a bargain in the sense of being easier for you than other people, either because of the abilities you happen to have, or because of some new way you've found to approach it, or simply because you're more excited about it, by all means work on that. Some of the best work is done by people who find an easy way to do something hard.  \n  \nAs well as learning the shape of real work, you need to figure out which kind you're suited for. And that doesn't just mean figuring out which kind your natural abilities match the best; it doesn't mean that if you're 7 feet tall, you have to play basketball. What you're suited for depends not just on your talents but perhaps even more on your interests. A [deep interest](http://www.paulgraham.com/genius.html) in a topic makes people work harder than any amount of discipline can.  \n  \nIt can be harder to discover your interests than your talents. There are fewer types of talent than interest, and they start to be judged early in childhood, whereas interest in a topic is a subtle thing that may not mature till your twenties, or even later. The topic may not even exist earlier. Plus there are some powerful sources of error you need to learn to discount. Are you really interested in x, or do you want to work on it because you'll make a lot of money, or because other people will be impressed with you, or because your parents want you to? [[8](http://www.paulgraham.com/hwh.html#f8n)]  \n  \nThe difficulty of figuring out what to work on varies enormously from one person to another. That's one of the most important things I've learned about work since I was a kid. As a kid, you get the impression that everyone has a calling, and all they have to do is figure out what it is. That's how it works in movies, and in the streamlined biographies fed to kids. Sometimes it works that way in real life. Some people figure out what to do as children and just do it, like Mozart. But others, like Newton, turn restlessly from one kind of work to another. Maybe in retrospect we can identify one as their calling — we can wish Newton spent more time on math and physics and less on alchemy and theology — but this is an [illusion](http://www.paulgraham.com/disc.html) induced by hindsight bias. There was no voice calling to him that he could have heard.  \n  \nSo while some people's lives converge fast, there will be others whose lives never converge. And for these people, figuring out what to work on is not so much a prelude to working hard as an ongoing part of it, like one of a set of simultaneous equations. For these people, the process I described earlier has a third component: along with measuring both how hard you're working and how well you're doing, you have to think about whether you should keep working in this field or switch to another. If you're working hard but not getting good enough results, you should switch. It sounds simple expressed that way, but in practice it's very difficult. You shouldn't give up on the first day just because you work hard and don't get anywhere. You need to give yourself time to get going. But how much time? And what should you do if work that was going well stops going well? How much time do you give yourself then? [[9](http://www.paulgraham.com/hwh.html#f9n)]  \n  \nWhat even counts as good results? That can be really hard to decide. If you're exploring an area few others have worked in, you may not even know what good results look like. History is full of examples of people who misjudged the importance of what they were working on.  \n  \nThe best test of whether it's worthwhile to work on something is whether you find it interesting. That may sound like a dangerously subjective measure, but it's probably the most accurate one you're going to get. You're the one working on the stuff. Who's in a better position than you to judge whether it's important, and what's a better predictor of its importance than whether it's interesting?  \n  \nFor this test to work, though, you have to be honest with yourself. Indeed, that's the most striking thing about the whole question of working hard: how at each point it depends on being honest with yourself.  \n  \nWorking hard is not just a dial you turn up to 11. It's a complicated, dynamic system that has to be tuned just right at each point. You have to understand the shape of real work, see clearly what kind you're best suited for, aim as close to the true core of it as you can, accurately judge at each moment both what you're capable of and how you're doing, and put in as many hours each day as you can without harming the quality of the result. This network is too complicated to trick. But if you're consistently honest and clear-sighted, it will automatically assume an optimal shape, and you'll be productive in a way few people are.  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n**Notes**  \n  \n[1] In \"The Bus Ticket Theory of Genius\" I said the three ingredients in great work were natural ability, determination, and interest. That's the formula in the preceding stage; determination and interest yield practice and effort.  \n  \n[2] I mean this at a resolution of days, not hours. You'll often get somewhere while not working in the sense that the solution to a problem comes to you while taking a [shower](http://www.paulgraham.com/top.html), or even in your sleep, but only because you were working hard on it the day before.  \n  \nIt's good to go on vacation occasionally, but when I go on vacation, I like to learn new things. I wouldn't like just sitting on a beach.  \n  \n[3] The thing kids do in school that's most like the real version is sports. Admittedly because many sports originated as games played in schools. But in this one area, at least, kids are doing exactly what adults do.  \n  \nIn the average American high school, you have a choice of pretending to do something serious, or seriously doing something pretend. Arguably the latter is no worse.  \n  \n[4] Knowing what you want to work on doesn't mean you'll be able to. Most people have to spend a lot of their time working on things they don't want to, especially early on. But if you know what you want to do, you at least know what direction to nudge your life in.  \n  \n[5] The lower time limits for intense work suggest a solution to the problem of having less time to work after you have kids: switch to harder problems. In effect I did that, though not deliberately.  \n  \n[6] Some cultures have a tradition of performative hard work. I don't love this idea, because (a) it makes a parody of something important and (b) it causes people to wear themselves out doing things that don't matter. I don't know enough to say for sure whether it's net good or bad, but my guess is bad.  \n  \n[7] One of the reasons people work so hard on startups is that startups can fail, and when they do, that failure tends to be both decisive and conspicuous.  \n  \n[8] It's ok to work on something to make a lot of money. You need to solve the money problem somehow, and there's nothing wrong with doing that efficiently by trying to make a lot at once. I suppose it would even be ok to be interested in money for its own sake; whatever floats your boat. Just so long as you're conscious of your motivations. The thing to avoid is _unconsciously_ letting the need for money warp your ideas about what kind of work you find most interesting.  \n  \n[9] Many people face this question on a smaller scale with individual projects. But it's easier both to recognize and to accept a dead end in a single project than to abandon some type of work entirely. The more determined you are, the harder it gets. Like a Spanish Flu victim, you're fighting your own immune system: Instead of giving up, you tell yourself, I should just try harder. And who can say you're not right?  \n  \n  \n  \n**Thanks** to Trevor Blackwell, John Carmack, John Collison, Patrick Collison, Robert Morris, Geoff Ralston, and Harj Taggar for reading drafts of this.  \n  \n\n  \n\n![](https://sep.yimg.com/ca/Img/trans_1x1.gif)\n\n![](https://sep.yimg.com/ca/I/paulgraham_2272_1423)[Arabic Translation](https://world.hey.com/amna/post-09ff9372)![](https://sep.yimg.com/ca/Img/trans_1x1.gif)  \n\n![](https://sep.yimg.com/ca/Img/trans_1x1.gif)\n\n  \n\n  \n  \n\n---","lastmodified":"2022-10-15T14:06:29.478502323Z","tags":null},"/notes/2021/2021.8/Linear_RegressionGradient_Descent":{"title":"Linear_Regression\u0026Gradient_Descent","content":"%%下面这里我一来就想要写一个最普适的情况, 但是弄得能难懂, 吴恩达在这里比我讲的清晰多了%%\n\n\n把梯度下降方法应用到我们的线性回归问题里面, 可以得到我们Hypothesis函数参数更新的方法(如何求Cost Function最小值Minimal的方法):\n\t$$\\begin{align*} \n\t\\text{repeat until convergence: } \n\\lbrace \u0026 \\newline \\theta_0 := \u0026 \\theta_0 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)}) \\newline \n\\theta_1 := \u0026 \\theta_1 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m}\\left((h_\\theta(x^{(i)}) - y^{(i)}) x^{(i)}_1\\right) \\newline \n\t\\rbrace\u0026 \n\t\\end{align*}$$\n\n下面我们用一个一般的形式, 详细解释一下应用的过程:\n\n- [Algorithm](notes/2021/2021.8/Part.5_Gradient_Descent(ML_Andrew.Ng.).md#Algorithm)\n\t这是一个有$n$个变量(Feature  $x$), $n+1$个参数( $\\theta$ )的损失函数的梯度下降算法:\n$$\n\\begin{array}{l}\n\\text { repeat until convergence }\\{\\\\\n\\begin{array}{cc}\n\\theta_{j}:=\\theta_{j}-\\alpha \\frac{\\Large\\partial}{\\Large\\partial \\Large\\theta_{j}}   J\\left(\\theta_{0},\\cdots ,\\theta_{n}\\right) \u0026 \\text { (simultaneously update } \nj=0, \\cdots ,j=n)\n\\end{array}\\\\\n\\text { \\} }\n\\end{array}\n$$\n\n- [Cost Function](notes/2021/2021.8/Part.3_Linear_Regression(ML_Andrew.Ng.).md#Cost%20Function)\n\t这是一个平方损失函数\n$$J\\left(\\theta_{0},\\cdots ,\\theta_{n}\\right)=\\frac{1}{2 m} \\sum_{i=1}^{m}\\left(\\hat{y}^{(i)}-y^{(i)}\\right)^{2}=\\frac{1}{2 m} \\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2}$$\n\n其中 $\\mathrm{m}$ 是数据点的个数, $x^{(i)}, y^{(i)}$ 是单个数据点, 这里$x^{(i)}$应当是一个矢量. $\\theta_{0}, \\cdots ,\\theta_{n}$ 是Hypothesis里面的参数, 也是Cost Function里面的变量.\n\n- 我们的Hypothesis是$h_\\theta$, 具体的表达式未指明, 暂定为\n\t$$\\begin{align}\n\t\u0026h_\\theta= \\theta_n {\\large f_{\\normalsize n}}(x^{(i)})+\\cdots , +\\theta_1 {\\large f_{\\normalsize 1}}(x^{(i)})+\\theta_0\\quad  \\\\\u0026and\\quad ( { f_{\\normalsize 0}}(x^{(i)})=1)\n\t\\end{align}\n\t$$\n\t对于我们的线性回归问题, $f_j(x^{(i)})=x_j^{(i)}$\n\t\n\t\n\n下面把Cost Function$:J\\left(\\theta_{0},\\cdots ,\\theta_{n}\\right)$带入上面的梯度下降公式, 得出具体的梯度下降表达式.\n\n首先是对于$\\theta_j$的偏导数计算, 即 $\\frac{\\Large\\partial}{\\Large\\partial \\Large\\theta_{j}} J(\\theta)$ . \n为了使思路清晰, 我们先计算对于一个数据点的平方误差的偏导数$\\frac{\\Large\\partial}{\\Large\\partial \\Large\\theta_{j}}  K(\\theta)$, 根据导数的性质, 有$\\frac{\\Large\\partial}{\\Large\\partial \\Large\\theta_{j}} J(\\theta)=\\frac{\\Large1}{\\Large2 m} \\sum_{i=1}^{m} \\frac{\\Large\\partial}{\\Large\\partial \\Large\\theta_{j}}  K(\\theta)$ :\n\n\n\n$$\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\theta_{j}} K(\\theta) \n\u0026=\\frac{\\partial}{\\partial \\theta_{j}}\\left(\\hat{y}-y\\right)^{2}\\\\\n\n\u0026=\\frac{\\partial}{\\partial \\theta_{j}} \\left(h_{\\theta}(x)-y\\right)^{2} \\\\\n\n\u0026= 2 \\left(h_{\\theta}(x)-y\\right) \\cdot \\frac{\\partial}{\\partial \\theta_{j}}\\left(h_{\\theta}(x)-y\\right) \\\\\n\n\u0026=2\\left(h_{\\theta}(x)-y\\right) \\cdot \\frac{\\partial}{\\partial \\theta_{j}}\\left(\\sum_{i=0}^{n} \\theta_{i} f_i(x)-y\\right) \\\\\n\n\u0026=2\\left(h_{\\theta}(x)-y\\right) f_{j}(x)\n\n\\end{aligned}\n$$\n带入所有样本点, 计算$\\frac{\\Large\\partial}{\\Large\\partial \\Large\\theta_{j}} J(\\theta)$:\n$$\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\theta_{j}} J(\\theta) \n\u0026=\\frac{1}{2 m} \\sum_{i=1}^{m} \\frac{\\partial}{\\partial \\theta_{j}}  K(\\theta)\\\\\n\n\u0026=\\frac{1}{2 m} \\sum_{i=1}^{m} 2\\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right) f_{j}(x^{(i)}) \\\\\n\n\u0026=2\\cdot\\frac{1}{2}\\cdot \\frac 1 m \\sum_{i=1}^{m} \\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right) f_{j}(x^{(i)}) \\\\\n\n\u0026=\\frac 1 m \\sum_{i=1}^{m} \\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right) f_{j}(x^{(i)}) \\\\\n\n\\end{aligned}\n$$\n\n- 注意在第三行的$2\\cdot{\\Large\\frac1 2}$, [这里就是为什么Cost Function里面要加入一个$\\frac1 2$, 可以让Gradient的形式更好看](notes/2021/2021.8/Part.3_Linear_Regression(ML_Andrew.Ng.).md#Cost%20Function)      ([Regarding the $\\frac1 2$ term](notes/2021/2021.8/Why_do_cost_functions_use_the_square_error.md#Regarding%20the%20frac1%202%20term)) ^021e6f\n\n对于我们的线性回归问题, :\n$$\\frac{\\partial}{\\partial \\theta_{j}} J(\\theta) =\\frac 1 m \\sum_{i=1}^{m} \\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right)x_j^{(i)} $$\n再乘上学习率$\\alpha$, 即是每一次参数$\\theta$变化的大小, 与旧参数相减即得到新的参数.\n\n- [Batch Gradient Descent 批梯度下降 BGD](notes/2021/2021.8/Different_Gradient_Descent_Methods.md#Batch%20Gradient%20Descent%20批梯度下降%20BGD):这里$i$的求和范围是$1$~$m$, 代表每一次都利用所有样本点来更新参数. \n\n","lastmodified":"2022-10-15T14:06:29.478502323Z","tags":null},"/notes/2021/2021.8/Mean_Squared_Error_%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE":{"title":"Mean_Squared_Error_均方误差","content":"# Mean Squared Error\n\n\u003cdiv align=\"right\"\u003e 2021-07-31\u003c/div\u003e\n\nTags: #MachineLearning #CostFunction\n\nMean Square Error: 平均平方误差, 简称均方差, MSE, 又称 Mean Squared **Deviation** (MSD)\n\n均方差的形式很简单, 但是也有许多问题值得思考\n- 为什么采用平方, 而不是绝对值, 三次方等等\n\n## StackExchange上面一个很好的解释\n[Why_do_cost_functions_use_the_square_error/为什么损失函数要使用均方误差](notes/2021/2021.8/Why_do_cost_functions_use_the_square_error.md)\n\n## 为什么MSE是合理的\n\n\u003e 均方误差有非常好的几何意义, 它对应了常用的**欧几里得距离**或简称\"欧氏距离\" (Euclidean distance). 基于均方误差最小化来进行模型求解的方法称为\"最小二乘法\" (least square method). 在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小.[^1]\n\n^0a7c67\n\n## 为什么前面有$\\frac{1}{2m}$\n\n$$J\\left(\\theta_{0}, \\theta_{1}\\right)=\\frac{1}{2 m} \\sum_{i=1}^{m}\\left(\\hat{y}^{(i)}-y^{(i)}\\right)^{2}=\\frac{1}{2 m} \\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2}$$[Cost Function ML_Andrew.Ng.](notes/2021/2021.8/Part.3_Linear_Regression(ML_Andrew.Ng.).md#Cost%20Function)\n\t\nThe mean is halved $\\left(\\frac{1}{2}\\right)$ as a convenience for the computation of the gradient descent, as the derivative term of the square function will cancel out the $\\frac{1}{2}$ term.[^2]\n\nAlso:\n[Regarding the frac1 2 term](notes/2021/2021.8/Why_do_cost_functions_use_the_square_error.md#Regarding%20the%20frac1%202%20term)\n\n\t\n\t\n[^1]: 周志华, 机器学习, 第三章, 线性模\n[^2]: 吴恩达机器学习的解释","lastmodified":"2022-10-15T14:06:29.478502323Z","tags":null},"/notes/2021/2021.8/Norm_of_a_Vector-Matrix":{"title":"Norm_of_a_Vector-Matrix","content":"# 向量/矩阵的范数\n\n\u003cdiv align=\"right\"\u003e 2021-08-20\u003c/div\u003e\n\nTags: #Norm #Math #MachineLearning #Regularization\n\nhttps://zh.wikipedia.org/wiki/%E8%8C%83%E6%95%B0\n\n![](notes/2021/2021.7/assets/img_2022-10-15-4.png)\n\n","lastmodified":"2022-10-15T14:06:29.478502323Z","tags":null},"/notes/2021/2021.8/Normal_Equation_Proof_2_Matrix_Method":{"title":"Normal_Equation_Proof_2_Matrix_Method","content":"## 首先补充一点矩阵的知识:求导、迹的性质\n\n矩阵的求导和矩阵的迹是密不可分的\n\n### 矩阵的求导\n\n![矩阵的求导](notes/2021/2021.8/矩阵的求导.md)\n\n### 矩阵的迹\n\n![矩阵迹的性质](notes/2021/2021.8/矩阵迹的性质.md)\n\n### 证明中需要的一些其他性质\n\n结合矩阵的求导, 还有以下性质:\n- $$\\nabla_{A} \\operatorname{tr} A B =B^{T}$$ \n\t- 结合[这里](notes/2021/2021.8/矩阵迹的性质.md#^tracecommutative)对$\\operatorname{tr} A B$的推导, 可以看出对于矩阵$A$每一个位置单独求偏导, 都会得到$b_{ji}$, 即$B^T$对应的位置.\n\t- 或者可以从导数的角度来证明:\n\t标量函数$f=\\operatorname{tr}AB$\n\t$$\\begin{align}  \n\tdf \u0026= d\\ \\operatorname{tr}AB  \\\\\n\t   \u0026= \\operatorname{tr}d(AB) \\\\\n\t   \u0026= \\operatorname{tr}BdA \\\\\n\t   \\end{align}$$\n\t   联系:![矩阵导数与微分的联系](notes/2021/2021.8/矩阵的求导.md#^e0894d)\n\t所以: $$\\frac{\\partial f}{\\partial A}=B^T$$\n\t证毕\n- $$\\nabla_{A^{T}} f(A) =\\left(\\nabla_{A} f(A)\\right)^{T}$$ \n$$\\begin{align}\n\\nabla_{A^T} f(A)\n\u0026=\\left[\\begin{array}{ccc}\n\\frac{\\partial f}{\\partial A^T_{11}} \u0026 \\cdots \u0026 \\frac{\\partial f}{\\partial A^T_{1 n}} \\\\\n\\vdots \u0026 \\ddots \u0026 \\vdots \\\\\n\\frac{\\partial f}{\\partial A^T_{m 1}} \u0026 \\cdots \u0026 \\frac{\\partial f}{\\partial A^T_{m n}}\n\\end{array}\\right]\\\\\n\u0026=\\left[\\begin{array}{ccc}\n\\frac{\\partial f}{\\partial A_{11}} \u0026 \\cdots \u0026 \\frac{\\partial f}{\\partial A_{1 n}} \\\\\n\\vdots \u0026 \\ddots \u0026 \\vdots \\\\\n\\frac{\\partial f}{\\partial A_{m 1}} \u0026 \\cdots \u0026 \\frac{\\partial f}{\\partial A_{m n}}\n\\end{array}\\right]^T\\\\\n\u0026=\\left(\\nabla_{A} f(A)\\right)^{T}\n\\end{align}$$\n\n- $$\\nabla_{A} \\operatorname{tr} A B A^{T} C =C A B+C^{T} A B^{T} $$\n\t证明: \n\t标量函数$f=\\operatorname{tr} A B A^{T} C$\n\t$$\\begin{align}  \n\tdf \u0026= d\\ \\operatorname{tr} (A B A^{T} C)  \\\\\n\t   \u0026= \\operatorname{tr}(d(A B A^{T} C)) \\\\\n\t   \u0026= \\operatorname{tr}(dA (B A^{T} C)+A dB (A^{T} C)+(A B) dA^{T} (C)+(A B A^{T}) dC) \\\\\n\t   \u0026= \\operatorname{tr}(dA (B A^{T} C)+(A B) dA^{T} (C)) \\\\\n\t   \u0026= \\operatorname{tr}((B A^{T} C)dA)+\\operatorname{tr}( (CA B) dA^{T})) \\\\\n\t\u0026= \\operatorname{tr}((B A^{T} C)dA)+\\operatorname{tr}( (CAB)^T (dA^{T})^T)) \\\\\n\t\u0026= \\operatorname{tr}((B A^{T} C)dA)+\\operatorname{tr}( (CAB)^T dA) \\\\\n\t\u0026= \\operatorname{tr}\\left(\\left(BA^{T}C+ (CAB)^T\\right) dA\\right)\n\t   \\end{align}$$\n\t所以: $$\\frac{\\partial f}{\\partial A}=\\left(BA^{T}C+ (CAB)^T\\right)^T=C A B+C^{T} A B^{T}$$\n\t证毕\n\t\n- $$\\nabla_{A}|A| =|A|\\left(A^{-1}\\right)^{T}$$\n\t参见吴恩达讲义里面的证明:\n\t![](notes/2021/2021.7/assets/Pasted%20image%2020210817213317.png)\n\t\n\t\n## 然后是证明:\n内积的另一种表述:  $z^{T} z=\\sum_{i} z_{i}^{2}$ :\n$$\n\\begin{aligned}\n\\frac{1}{2}(X \\theta-\\vec{y})^{T}(X \\theta-\\vec{y}) \u0026=\\frac{1}{2} \\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2} \\\\\n\u0026=J(\\theta)\n\\end{aligned}\n$$\n为了最小化 $J$, 我们对 $\\theta$求导. 结合上面的补充性质, 我们有:\n$$\n\\nabla_{A^{T} } \\operatorname{tr} A B A^{T} C=B^{T} A^{T} C^{T}+B A^{T} C\n$$\n这个在下面会用到.\n求导有:\n$$\\begin{aligned}\n\\nabla_{\\theta} J(\\theta) \u0026=\\nabla_{\\theta} \\frac{1}{2}(X \\theta-\\vec{y})^{T}(X \\theta-\\vec{y}) \\\\\n\\text{(展开)}\u0026=\\frac{1}{2} \\nabla_{\\theta}\\left(\\theta^{T} X^{T} X \\theta-\\theta^{T} X^{T} \\vec{y}-\\vec{y}^{T} X \\theta+\\vec{y}^{T} \\vec{y}\\right) \\\\\n\\text{(标量的迹就是它自己)}\u0026=\\frac{1}{2} \\nabla_{\\theta} \\operatorname{tr}\\left(\\theta^{T} X^{T} X \\theta-\\theta^{T} X^{T} \\vec{y}-\\vec{y}^{T} X \\theta+\\vec{y}^{T} \\vec{y}\\right) \\\\\n\u0026=\\frac{1}{2} \\nabla_{\\theta}\\left(\\operatorname{tr} \\theta^{T} X^{T} X \\theta-2 \\operatorname{tr} \\vec{y}^{T} X \\theta\\right) \\\\\n\\text{(利用上面的推论)}\u0026=\\frac{1}{2}\\left(X^{T} X \\theta+X^{T} X \\theta-2 X^{T} \\vec{y}\\right) \\\\\n\u0026=X^{T} X \\theta-X^{T} \\vec{y}\n\\end{aligned}$$\n\n零导数为零, 有$X^{T} X \\theta=X^{T} \\vec{y}$ , 所以 $\\theta=(X^{T} X )^{-1}X^{T} \\vec{y}$\n证毕. ","lastmodified":"2022-10-15T14:06:29.478502323Z","tags":null},"/notes/2021/2021.8/Part.10_Octave_TutorialML_Andrew.Ng.":{"title":"Part.10_Octave_Tutorial(ML_Andrew.Ng.)","content":"# Octave Tutorial\n\n\u003cdiv align=\"right\"\u003e 2021-08-18\u003c/div\u003e\n\nTags: #Octave #MachineLearning \n\n- 还是要在实践中学习Octave\n\n\n## 为什么吴恩达说Octave比Python好呢?  \n- 或许这里涉及到编程与建模的区别? \n\t\t建模的目的是快速实现一个模型, 像 `Matlab` `Octave` `Labview`之类的软件的目标就是快速建模, 而编程语言像是Python之类的, 他们的目的偏向于 建立一个模型的可靠的应用实例, 不仅要实现, 还需要可靠, 性能需要优化\n\t\t但是像IPython Console, Jupyter Notebook之类的交互式编程界面是否已经打破了这两个之间的隔阂? \t\n\t\t\n\t\t\n\t- Cousera 上面Machine Learning 大概开始于2011\n\t\t\n\t\t\u003eIn October 2011, the \"applied\" version of the Stanford class (CS229a) was hosted on ml-class.org and launched, with over 100,000 students registered for its first edition[^1]\n\t- IPython Notebook \u0026 Jupyter Notebook 也大致自2011-2015年间逐渐起步, 后来才逐步变得流行起来.\n\t\t \u003e-  IPython在0.12版本（2011年12月）中添加了Notebook界面，2015年更名为Jupyter Notebook[^2]\n\t\t \u003e - 据《大西洋》杂志报道，在2018年初，用户对Jupyter的兴趣超过了Mathematica Notebook界面的流行程度[^2]\n\t- 故一种合理的猜测是在Andrew Ng 的课程录制的时候, Python 的交互性还没有得到很好的发展与推广, 时至今日(2021), Python作为机器学习建模工具的易用性值得关注.\n\n## 实用技巧: 向量化Vectorization\n\n相比于利用循环, 在需要对一个矩阵里面的元素进行相似的操作的时候, 我们可以利用矩阵乘法来进行相同的操作, 这样我们可以利用**性能优化过的函数**, 加快程序性能. \n\nE.g. $$\\sum x_i^2=\\vec X^T \\vec X$$ \n\n\n[^1]: https://en.wikipedia.org/wiki/Andrew_Ng\n[^2]: https://zh.wikipedia.org/zh-sg/Jupyter#Jupyter_Notebook","lastmodified":"2022-10-15T14:06:29.478502323Z","tags":null},"/notes/2021/2021.8/Part.11_ClassificationML_Andrew.Ng.":{"title":"Part.11_Classification(ML_Andrew.Ng.)","content":"# Classification\n\n\u003cdiv align=\"right\"\u003e 2021-08-19\u003c/div\u003e\n\nTags: #MachineLearning  #Classification \n\n- 分类问题最简单的情况是二分类问题(Binary Classification), 更一般的情况是多分类问题.\n\n- 分类问题与回归问题最大的不同是其对输出的要求是离散的, 线性函数/回归在分类问题上面不适用. \n\n\n","lastmodified":"2022-10-15T14:06:29.478502323Z","tags":null},"/notes/2021/2021.8/Part.12_Logistic_RegressionML_Andrew.Ng.":{"title":"Part.12_Logistic_Regression(ML_Andrew.Ng.)","content":"# Logistic Regression\n\n\u003cdiv align=\"right\"\u003e 2021-08-19\u003c/div\u003e\n\nTags: #LogisticRegression #MachineLearning #Classification \n\n## Logistic Function\n![Logistic Function](notes/2021/2021.8/Sigmoid_Function.md#Logistic%20Function)\n\n\n## Hypothesis Representation\n- 我们可以通过对线性回归的方法进行一些小改动来匹配回归问题, 在线性回归的时候, $h(x)$的输出与分类问题的\"值域\"偏差较大, 比如在二分类问题里面, 要求$y=0\\ or\\ 1$, 但是$h(x)$会输出大于一或者小于零的数. \n\n- 为了解决这个问题, 我们将$h(x)$作为逻辑斯蒂函数的输入, 将任意输出匹配到$(0,1)$区间里面去, 方便分类.\n$$\\begin{aligned}\n\u0026h_{\\theta}(x)=g\\left(\\theta^{T} x\\right) \\\\\n\u0026g(z)=\\frac{1}{1+e^{-z}}\\\\\n\u0026z=\\theta^{T} x \\\\\n\\end{aligned}$$\n\n- Hypothesis如此表述之后,  $h(x)$的值就可以理解为二分类问题中, $x$被归为$1$的概率大小.\n\t严谨的表述是: \n\t$$h_θ(x)=P(y=1|x;θ)=1−P(y=0|x;θ)$$\n\t$$P(y=0|x;θ)+P(y=1|x;θ)=1$$\n\t其中 \" $;θ$ \" 的含义是: $θ$是参数(Parameterized by theta).\n\t\n\t\n## Decision Boundary\n假如我们采用这样的分类方法:\n$$\\begin{aligned}\n\u0026h_{\\theta}(x) \\geq 0.5 \\rightarrow y=1 \\\\\n\u0026h_{\\theta}(x)\u003c0.5 \\rightarrow y=0\n\\end{aligned}$$\n\n那么分类结果最终由$h(x)$的值决定:\n$$\\begin{aligned}\nθ^Tx≥0⇒y=1\\\\\nθ^Tx\u003c0⇒y=0\n\\end{aligned}$$\n\n一个例子:\n![](notes/2021/2021.7/assets/Pasted%20image%2020210819153316.png)\n图中划分数据集的线便是这个$h(x)$, Decision Boundary, 在曲线上面的点$h(x)\\geq 0$, 分类结果为1, 在曲线下面的点$h(x)\u003c 0$, 分类结果为0.\n\n注意Decision Boundary是$h(x)$的性质, 即使上图不画数据点, Boundary依然存在.\n\n## Nonlinearity\n\n就像线性回归可以推广为多项式回归一样, Logistic 回归也可以有非线性的决策边界:\n![](notes/2021/2021.7/assets/Pasted%20image%2020210819154110.png)\n![](notes/2021/2021.7/assets/Pasted%20image%2020210819154123.png)","lastmodified":"2022-10-15T14:06:29.478502323Z","tags":null},"/notes/2021/2021.8/Part.13_Cost_Function-Logistic_RegressionML_Andrew.Ng.":{"title":"Part.13_Cost_Function-Logistic_Regression(ML_Andrew.Ng.)","content":"# Cost Function - Logistic Regression\n\n\u003cdiv align=\"right\"\u003e 2021-08-19\u003c/div\u003e\n\nTags: #CostFunction #LogisticRegression #MachineLearning \n\n## Representation\n如果我们采用[线性回归的损失函数](notes/2021/2021.8/Part.4_Cost_Function_Intuition(ML_Andrew.Ng.).md): [均方误差](notes/2021/2021.8/Mean_Squared_Error_均方误差.md), 那么因为Logistic 回归的$h(x)$里面有形式很复杂的Logistic函数, 损失函数将不再是[凸函数](notes/2021/2021.8/凸优化与线性回归问题.md), 将会很难最小化, 所以我们需要考虑另外的损失函数形式:\n\n我们采用这样的对数形式\nUpdate: 这其实是[Cross_Entropy-交叉熵](notes/2022/2022.2/Cross_Entropy-交叉熵.md)\n\n$$\n\\begin{array}{ll}\nJ(\\theta)=\\frac{1}{m} \n\\sum_{i=1}^{m} \\operatorname{Cost}\\left(h_{\\theta}\\left(x^{(i)}\\right), y^{(i)}\\right) \u0026 \\\\\n\\\\\n\\operatorname{Cost}\\left(h_{\\theta}(x), y\\right)=-\\log \\left(h_{\\theta}(x)\\right) \u0026 \\text { if } \\mathrm{y}=1 \\\\\n\n\\operatorname{Cost}\\left(h_{\\theta}(x), y\\right)=-\\log \\left(1-h_{\\theta}(x)\\right) \u0026 \\text { if } \\mathrm{y}=0\n\\end{array}$$\n\n## Intuition\n- 在$y=1$时: 我们的损失函数在接近0的时候(错误的一端)趋向于无穷大, 在等于1的时候(正确的一端)达到最小值0.\n$$\\operatorname{Cost}\\left(h_{\\theta}(x), y\\right)=-\\log \\left(h_{\\theta}(x)\\right)\n$$\n![300](notes/2021/2021.7/assets/img_2022-10-15-19.png)\n\n- 在$y=0$时: 我们的损失函数在接近1的时候(错误的一端)趋向于无穷大, 在等于0的时候(正确的一端)达到最小值0.\n$$\\operatorname{Cost}\\left(h_{\\theta}(x), y\\right)=-\\log \\left(1-h_{\\theta}(x)\\right) $$\n![250](notes/2021/2021.7/assets/img_2022-10-15-20.png)\n\n这样, 总体上, 在预测值与真实值越接近的时候损失函数越接近于0.\n\n$$\\begin{array}{ll}\nCost(h_θ(x),y)=0 \\text{  if  } h_θ(x)=y\\\\\nCost(h_θ(x),y)→∞ \\text{  if  } y=0 \\text{ and } h_θ(x)→1\\\\\nCost(h_θ(x),y)→∞ \\text{  if  } y=1 \\text{ and } h_θ(x)→0\\\\\n\\end{array}$$\n\n这样的损失函数形式确保了logistic regression的 $J(θ)$ 是凸函数.\n### 证明\n[证明Logistic回归的损失函数是凸函数](notes/2021/2021.9/证明Logistic回归的损失函数是凸函数.md)\n\n\n## 更简洁的形式\n我们可以把两种情况写成一个式子:\n$$\\operatorname{Cost}\\left(h_{\\theta}(x), y\\right)=-y \\log \\left(h_{\\theta}(x)\\right)-(1-y) \\log \\left(1-h_{\\theta}(x)\\right)$$\n(观察上面的式子: 在$y=1$的时候, $1-y=0$, 在$y=0$的时候, $1-y=1$)\n\n### Cost Function\n所以损失函数可以表示为:\n$$\n\\begin{align}\nJ(\\theta)\u0026=\\frac{1}{m} \n\\sum_{i=1}^{m} \\operatorname{Cost}\\left(h_{\\theta}\\left(x^{(i)}\\right), y^{(i)}\\right)  \\\\\n\u0026=-\\frac{1}{m} \\sum_{i=1}^{m}\\left[y^{(i)} \\log \\left(h_{\\theta}\\left(x^{(i)}\\right)\\right)+\\left(1-y^{(i)}\\right) \\log \\left(1-h_{\\theta}\\left(x^{(i)}\\right)\\right)\\right]\\\\\n\\end{align}\n$$\n(注意提出来的负号)\n\n[向量化](notes/2021/2021.8/Part.10_Octave_Tutorial(ML_Andrew.Ng.).md#实用技巧%20向量化Vectorization)的表示为:\n$$\n\\begin{aligned}\nh\u0026=g(X \\theta) \\\\\nJ(\\theta)\u0026=-\\frac{1}{m} \\cdot\\left[y^{T} \\log (h)+(1-y)^{T} \\log (1-h)\\right]\n\\end{aligned}\n$$","lastmodified":"2022-10-15T14:06:29.478502323Z","tags":null},"/notes/2021/2021.8/Part.14_Logistic_RegressionGradient_DescentML_Andrew.Ng.":{"title":"Part.14_Logistic_Regression\u0026Gradient_Descent(ML_Andrew.Ng.)","content":"# Logistic Regression \u0026 Gradient Descent\n\n\u003cdiv align=\"right\"\u003e 2021-08-19\u003c/div\u003e\n\nTags: #LogisticRegression  #GradientDescent  #MachineLearning \n\n- **Gradient Descent:**\n![Algorithm](notes/2021/2021.8/Part.5_Gradient_Descent(ML_Andrew.Ng.).md#Algorithm)\n\n- **Cost Function:**\n![Cost Function](notes/2021/2021.8/Part.13_Cost_Function-Logistic_Regression(ML_Andrew.Ng.).md#更简洁的形式#Cost%20Function)\n\n## 推导\n损失函数里面的$g(x)$为Logistic函数, [Logistic的导函数](notes/2021/2021.8/Sigmoid_Function.md#Logistic%20Function)为:\n$$\\begin{aligned}\n\\frac {d}{dx}g(x)\u0026=g(x)\\left(1-g(x)\\right)\\\\\n\\end{aligned}$$\n\n求偏导数$\\frac{\\Large\\partial}{\\Large\\partial \\Large\\theta_{j}} J(\\theta)$:\n\n为了使思路清晰, 我们先计算对于一个数据点的偏导数, 即先计算求和符号的后半部分: $\\frac{\\Large\\partial}{\\Large\\partial \\Large\\theta_{j}}  C(\\theta)$, 根据导数的性质, 有$\\frac{\\Large\\partial}{\\Large\\partial \\Large\\theta_{j}} J(\\theta)=-\\frac{\\Large1}{\\Large m} \\sum_{i=1}^{m} \\frac{\\Large\\partial}{\\Large\\partial \\Large\\theta_{j}}  C^{(i)}(\\theta)$ :\n\n$$\\begin{aligned}\n\\frac{\\partial}{\\partial \\theta_{j}} C(\\theta) \n\u0026=\\frac{\\partial}{\\partial \\theta_{j}}\\left(\\ y \\log \\left(h_{\\theta}\\left(x\\right)\\right)\n+\\left(1-y\\right) \\log \\left(1-h_{\\theta}\\left(x\\right)\\right)\\ \\right)\\\\\n\n\u0026=\\left(y \\frac{1}{g\\left(\\theta^Tx\\right)}-(1-y) \\frac{1}{1-g\\left(\\theta^Tx\\right)}\\right) \\frac{\\partial}{\\partial \\theta_{j}} g\\left(\\theta^Tx\\right) \\\\\n\u0026=\\left(y \\frac{1}{g\\left(\\theta^Tx\\right)}-(1-y) \\frac{1}{1-g\\left(\\theta^Tx\\right)}\\right) g\\left(\\theta^Tx\\right)\\left(1-g\\left(\\theta^Tx\\right)\\right) \\frac{\\partial}{\\partial \\theta_{j}} \\theta^Tx\\\\\n\u0026=\\left(y\\left(1-g\\left(\\theta^Tx\\right)\\right)-(1-y) g\\left(\\theta^Tx\\right)\\right) x_{j} \\\\\n\u0026=\\left(y-yg\\left(\\theta^Tx\\right)+yg\\left(\\theta^Tx\\right)-g\\left(\\theta^Tx\\right)\\right) x_{j} \\\\\n\u0026=\\left(y-h_{\\theta}(x)\\right) x_{j}\n\\end{aligned}$$\n\n(注意: 证明里面$\\theta,x$均为列向量, 有所不同的是: 在上面向量化的损失函数里面, $X_{m\\times n}$是每一行为一个数据)\n\n带入所有样本点, 计算 $\\frac{\\Large\\partial}{\\Large\\partial \\Large\\theta_{j}} J(\\theta)$ :\n$$\\begin{aligned}\n\\frac{\\partial}{\\partial \\theta_{j}} J(\\theta) \n\u0026=-\\frac{1}{m} \\sum_{i=1}^{m} \\frac{\\partial}{\\partial \\theta_{j}}   C^{(i)}(\\theta)\\\\\n\u0026=\\frac{1}{m} \\sum_{i=1}^{m}\\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right) x^{(i)}_{j}\n\\end{aligned}$$\n\n## 结果\n带入梯度下降里面:\n$$\n\\begin{array}{l}\n\\text { repeat until convergence }\\{\\\\\n\\begin{array}{cc}\n\u0026\\theta_{j}:=\\theta_{j}-\\alpha \\frac 1 m \\sum_{i=1}^{m} \\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right) x_j^{(i)} \n\\end{array}\\\\\n\\text { \\} }\n\\\\\\\\ \\text { (simultaneously update } \nj=0, \\cdots ,j=n)\n\\end{array}\n$$\n\n向量化后的公式:\n$$\\theta:=\\theta-\\alpha\\frac{1}{m} X^{T}(g(X \\theta)-\\vec{y})$$\n\n- 比较Linear Regression里面的梯度下降公式: \n[Relation_Between_Linear_Regression\u0026Gradient_Descent_梯度下降和线性回归的关系](notes/2021/2021.8/Relation_Between_Linear_Regression\u0026Gradient_Descent_梯度下降和线性回归的关系.md)\n发现是完全一样的.","lastmodified":"2022-10-15T14:06:29.478502323Z","tags":null},"/notes/2021/2021.8/Part.15_Advanced_OptimizationML_Andrew.Ng.":{"title":"Part.15_Advanced_Optimization(ML_Andrew.Ng.)","content":"# Advanced Optimization\n\n\u003cdiv align=\"right\"\u003e 2021-08-19\u003c/div\u003e\n\nTags: #Octave #MachineLearning #GradientDescent #LinearRegression #LogisticRegression \n\n\n- **More sophisticated, faster way to optimize parameters**: \n\t- Conjugate gradient\n\t- BFGS\n\t- L-BFGS\n\n\n[Link:其他Gradient_Descent Different_Gradient_Descent_Methods](notes/2021/2021.8/Different_Gradient_Descent_Methods.md)\n\n在[Octave](notes/2021/2021.8/Part.10_Octave_Tutorial(ML_Andrew.Ng.).md)里面, 只需要写出怎么计算$J(\\theta)$, $\\frac{\\Large\\partial}{\\Large\\partial \\Large\\theta_{j}} J(\\theta)$即可调用内置的库函数快速计算参数值.\n\n```matlab\nfunction [jVal, gradient] = costFunction(theta)\n  jVal = [...code to compute J(theta)...];\n  gradient = [...code to compute derivative of J(theta)...];\nend\n```\n\n```matlab\noptions = optimset('GradObj', 'on', 'MaxIter', 100);\ninitialTheta = zeros(2,1);\n   [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);\n```\n\n- MATLAB里面是`optimoptions`函数\n- 在第二次作业(Logistic Regression)里面有对这个方法更具体的介绍.","lastmodified":"2022-10-15T14:06:29.478502323Z","tags":null},"/notes/2021/2021.8/Part.16_MulticlassClassification-One_vs_RestML_Andrew.Ng.":{"title":"Part.16_MulticlassClassification-One_vs_Rest(ML_Andrew.Ng.)","content":"# One vs Rest\n\n\u003cdiv align=\"right\"\u003e 2021-08-19\u003c/div\u003e\n\nTags: #MulticlassClassification #Classification #MachineLearning \n\n- AKA: One vs All\n\n## MulticlassClassification\n![](notes/2021/2021.7/assets/img_2022-10-15-21.png)\n\n$$\\begin{aligned}\n\u0026y \\in\\{0,1 \\ldots n\\} \\\\\n\u0026h_{\\theta}^{(0)}(x)=P(y=0 \\mid x ; \\theta) \\\\\n\u0026h_{\\theta}^{(1)}(x)=P(y=1 \\mid x ; \\theta) \\\\\n\u0026\\cdots \\\\\n\u0026h_{\\theta}^{(n)}(x)=P(y=n \\mid x ; \\theta) \\\\\n\u0026\\text { prediction }=\\max _{i}\\left(h_{\\theta}^{(i)}(x)\\right)\n\\end{aligned}$$\n\n## One vs Rest\n将二分类问题应用到多分类里面, 即对每一个分类分别训练一个二分类模型$h^{(i)}(x)$ , 因为$h^{(i)}(x)$的大小可以看作属于这个分类的概率, 在预测的时候将数据点带入$n$个$h^{(i)}(x)$里面, 取概率最大的分类作为预测结果.\n![](notes/2021/2021.7/assets/img_2022-10-15-22.png)\n![](notes/2021/2021.7/assets/img_2022-10-15-23.png)","lastmodified":"2022-10-15T14:06:29.478502323Z","tags":null},"/notes/2021/2021.8/Part.17_Overfitting_UnderfittingML_Andrew.Ng.":{"title":"Part.17_Overfitting_Underfitting(ML_Andrew.Ng.)","content":"# Overfitting Underfitting\n\n\u003cdiv align=\"right\"\u003e 2021-08-20\u003c/div\u003e\n\nTags: #Overfitting #Underfitting #MachineLearning\n\n![](notes/2021/2021.7/assets/img_2022-10-15-24.png)\n\n## Underfitting\nUnderfitting 的另一 种表述是这个模型有 \"High Bias\", 直观上理解, 这个模型对数据集有着先入为主的\"偏见\", \"不允许\"数据集为二次的, 导致预测效果不好.\nBias=Preconception\n\n原因: 模型太简单/使用的特征太少\n\n## Overfitting\nOverfitting的另一种表述则是 \"High Variance\", 即这个模型有着太多的可能性, 而我们的数据太少, 我们现有的数据不足以确定这个模型 / 基于现有的数据, 这个模型有着很高的变数.\n\nThe term high variance is another historical or technical one. But, the intuition is that, if we're fitting such a high order polynomial, then, the hypothesis can fit, you know, it's almost as if it can fit almost any function and this face of possible hypothesis is just too large, it's too variable. And we don't have enough data to constrain it to give us a good hypothesis.\n\n原因: 模型太复杂\n\n## 解决方法\n\n1) 减少特征数目\n\t-  手动筛选\n\t-  利用降维算法\n\n2) Regularization 正则化\n\t- 保留所有特征, 但是减小参数的Magnitude\n\t- Regularization works well when we have a lot of slightly useful features.\n\n\n- 在以前的关于为什么要利用均方误差的笔记里面也有关于Bias_Variance_Trade-off 的阐述:\n![](notes/2021/2021.8/Why_do_cost_functions_use_the_square_error.md#^91cd90)\n\n## 后续思考\n1. Source: [4.6. 暂退法（Dropout） — 动手学深度学习 2.0.0-beta0 documentation](https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/dropout.html#id1)\n\t- 线性模型的 High Bias 一定程度上是因为线性模型没有考虑到特征之间的交互作用。 对于每个特征，线性模型必须指定正的或负的权重，而忽略其他特征。\n\t- 深度神经网络位于偏差-方差谱的另一端。 与线性模型不同，神经网络并不局限于单独查看每个特征，而是学习特征之间的交互。 例如，神经网络可能推断“尼日利亚”和“西联汇款”一起出现在电子邮件中表示垃圾邮件， 但单独出现则不表示垃圾邮件。\n\n\t- 即使我们有比特征多得多的样本，深度神经网络也有可能过拟合。 2017 年，一组研究人员通过在随机标记的图像上训练深度网络。 这展示了神经网络的极大灵活性，因为人类很难将输入和随机标记的输出联系起来， 但通过随机梯度下降优化的神经网络可以完美地标记训练集中的每一幅图像。 想一想这意味着什么？ 假设标签是随机均匀分配的，并且有 10 个类别，那么分类器在测试数据上很难取得高于 10%的精度， 那么这里的泛化差距就高达 90%，如此严重的过拟合。\n\n\n---\n吴恩达练习二里面的例子:\n![](notes/2021/2021.8/assets/Pasted%20image%2020210911160311.png)\n![](notes/2021/2021.8/assets/Pasted%20image%2020210911160325.png)\n\n![400](notes/2021/2021.8/assets/lambda=0.png)\n![400](notes/2021/2021.8/assets/lambda=1.png)\n![400](notes/2021/2021.8/assets/lambda=10.png)\n![400](notes/2021/2021.8/assets/lambda=100.png)","lastmodified":"2022-10-15T14:06:29.478502323Z","tags":null},"/notes/2021/2021.8/Part.1_Supervised_LearningML_Andrew.Ng.":{"title":"Part.1_Supervised_Learning(ML_Andrew.Ng.)","content":"# Supervised Learning-Introduction\n\n\u003cdiv align=\"right\"\u003e 2021-07-27\u003c/div\u003e\n\nTags: #MachineLearning #SupervisedLearning\n\n## What is supervised learning?\nSupervised learning refers to the fact that we gave the algorithm a dataset in which \"Right Answers\" were given.\n监督学习需要给机器标签, 指出正确答案是什么\n\n## What topics does supervised learning include?\n### Regression\nPredict a continuous valued output.\n根据有限的信息预测出连续的变化范围\n\n### Classification\n根据有限的输出做出离散的(Discrete)判断\n","lastmodified":"2022-10-15T14:06:29.478502323Z","tags":null},"/notes/2021/2021.8/Part.2_Unsupervised_LearningML_Andrew.Ng.":{"title":"Part.2_Unsupervised_Learning(ML_Andrew.Ng.)","content":"# Unsupervised Learning-Introduction\n\n\u003cdiv align=\"right\"\u003e 2021-07-27\u003c/div\u003e\n\nTags: #MachineLearning #UnsupervisedLearning\n\n## What is unsupervised learning?\nIn Unsupervised Learning, we're given data that looks different than data that looks like this that doesn't have any labels or that all has the same label or really no labels.\n监督学习不需要给机器正确答案, 机器会自动找出规律\n\n## What topics does unsupervised learning include?\n### Clustering\n在一组数据里面找出规律, 把相似的归为一类\n\n### Cocktail Party Algorithm\nfind structure in a chaotic environment, 在混乱的数据里面找到独立的成分/结构\n","lastmodified":"2022-10-15T14:06:29.478502323Z","tags":null},"/notes/2021/2021.8/Part.3_Linear_RegressionML_Andrew.Ng.":{"title":"Part.3_Linear_Regression(ML_Andrew.Ng.)","content":"# Linear Regression\n\n\u003cdiv align=\"right\"\u003e 2021-07-31\u003c/div\u003e\n\nTags: #MachineLearning #SelfLearning\n\n## Model Representation\n- [Supervised Learning](notes/2021/2021.8/Part.1_Supervised_Learning(ML_Andrew.Ng.).md)\n- [Regression Problem](notes/2021/2021.8/Part.1_Supervised_Learning(ML_Andrew.Ng.).md#Regression)\n### Structure\n基于训练集, 我们希望通过学习算法得到一个Hypothesis函数$h$, 在房价预测问题上. 输入房子的大小, 得到估计的价格. \n![](notes/2021/2021.7/assets/img_2022-10-15-5.png)\n对于单变量的线性回归问题(Univariate Linear Regression), 可以表现为如下形式:\n$$ h_\\theta(x)=\\theta_1 x+\\theta_0$$\n其中$h_\\theta$可以简记为$h$\n\n对于训练数据:\n\n- **A pair** $(x^{(i)} , y^{(i)} )$ is called a **training example**\n\n- The dataset that we’ll be using to learn—**a list of m training examples** $(x^{(i)},y^{(i)})\\ , (i=1,...,m)$ — is called a **training set**.\n\n\n## Cost Function\n损失函数是用来衡量Hypothesis function的精确度的, 损失函数可以衡量Hypothesis在整个数据集上面平均误差\n\n下面是一个名叫\"[平方误差函数/Squared Error Function/Mean Squared Error](notes/2021/2021.8/Mean_Squared_Error_均方误差.md)\"的损失函数:\n$$J\\left(\\theta_{0}, \\theta_{1}\\right)=\\frac{1}{2 m} \\sum_{i=1}^{m}\\left(\\hat{y}^{(i)}-y^{(i)}\\right)^{2}=\\frac{1}{2 m} \\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2}$$\n分开来看, $J\\left(\\theta_{0}, \\theta_{1}\\right)$实际上是$\\frac 1 2\\overline{x}$, $\\overline{x}$是预测值与真实值误差的平方\n\nLink:\n[Why_do_cost_functions_use_the_square_error](notes/2021/2021.8/Why_do_cost_functions_use_the_square_error.md)\n\n### 直观感受\n[Part.4_Cost_Function_Intuition](notes/2021/2021.8/Part.4_Cost_Function_Intuition(ML_Andrew.Ng.).md)\n\n## 推广:多项式回归\n- Our hypothesis function need not be linear (a straight line) if that does not fit the data well.\n- We can **change the behavior or curve** of our hypothesis function by making it a quadratic, cubic or square root function (or any other form).","lastmodified":"2022-10-15T14:06:29.478502323Z","tags":null},"/notes/2021/2021.8/Part.4_Cost_Function_IntuitionML_Andrew.Ng.":{"title":"Part.4_Cost_Function_Intuition(ML_Andrew.Ng.)","content":"# Cost Function Intuition: Linear Regression\n\n\u003cdiv align=\"right\"\u003e 2021-08-02\u003c/div\u003e\n\nTags: #MachineLearning #CostFunction #LinearRegression \n\n\n## 2-Dimension Intuition\n首先简化一下我们的问题, 现在只有三个数据点$(1,1),(2,2),(3,3)$, 我们的Hypothesis Function$:h=\\theta_1 x$ 只有一个参数$\\theta_1$表示斜率, Cost Function还是:\n$$\nJ\\left(\\theta_{0}, \\theta_{1}\\right)=\\frac{1}{2 m} \\sum_{i=1}^{m}\\left(\\hat{y}^{(i)}-y^{(i)}\\right)^{2}=\\frac{1}{2 m} \\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2}\n$$\n则我们大概可以把Cost Function的变化过程表示成这样: \n![](notes/2021/2021.7/assets/img_2022-10-15-1.gif)\n可以看出, 斜率为1的时候Cost Function有最小值1, 此时Hypothesis最优.\n\n\n## 3-Dimension Intuition\n三维的情况即Cost有两个参数$\\theta_0, \\theta_1$, 如果$z$轴表示Cost Function的大小那么会是一个碗装的曲面, 表示在一个平面里面可以用等高线图(Contour Map)来表示. \n![](notes/2021/2021.7/assets/img_2022-10-15-6.png)\n最优的情况即曲面的最低处\n![](notes/2021/2021.7/assets/img_2022-10-15-7.png)\n\n![可视化损失函数的困难](notes/2022/2022.2/可视化损失函数的困难.md)","lastmodified":"2022-10-15T14:06:29.478502323Z","tags":null},"/notes/2021/2021.8/Part.5_Gradient_DescentML_Andrew.Ng.":{"title":"Part.5_Gradient_Descent(ML_Andrew.Ng.)","content":"# Gradient Descent\n\n\u003cdiv align=\"right\"\u003e 2021-08-02\u003c/div\u003e\n\nTags: #MachineLearning #GradientDescent \n\n梯度下降是一种最小化损失函数的标准方法\nSo we have our hypothesis function and we have a way of measuring how well it fits into the data. Now we need to estimate the parameters in the hypothesis function. That's where gradient descent comes in.\n\n## Intuition\n![](notes/2021/2021.7/assets/img_2022-10-15-8.png)\n这个曲面是在CostFunction空间里面的, XY坐标表示Hypothesis里面的参数$\\theta_0,\\theta_1 \\cdots$\nCostFunction代表Hypothesis与真实值的偏差, CostFunction的目标是使自己的值最小, 即Hypothesis与真实值的偏差最小.\n更新哪个参数就相当于在那个参数的方向上走一步.\n\n## Algorithm\n$$\n\\begin{array}{l}\n\\text {repeat until convergence }\\{\\\\\n\\begin{array}{cc}\n\\theta_{j}:=\\theta_{j}-\\alpha \\frac{\\Large\\partial}{\\Large\\partial \\Large\\theta_{j}}   J\\left(\\theta_{0},\\cdots ,\\theta_{n}\\right) \u0026 \\text { (simultaneously update } \nj=0, \\cdots ,j=n)\n\\end{array}\\\\\n\\text { \\} }\n\\end{array}\n$$\n- 里面的$:=$表示\"赋值\"\n\n## Learning Rate / 学习率\n~~即每一步的长度~~\n- 每一步长度的比例大小, 上方公式里面的  $\\Large\\alpha$\n- 相当于一个人腿的长度, **并不能直接等同于每一步的长度**, 因为每一步的长度还和偏导数的大小有关\n\t![|500](notes/2021/2021.7/assets/img_2022-10-15-9.png)\n### No need to decrease Learning Rate overtime\n![|300](notes/2021/2021.7/assets/img_2022-10-15-10.png)\n\nAs we approach a local minimum, gradient descent will automatically take smaller steps. So, no need to decrease α over time.\n\n### 合理调整学习率\n调整每一步的大小比例\n![](notes/2021/2021.7/assets/img_2022-10-15-11.png)\n![](notes/2021/2021.7/assets/img_2022-10-15-2.gif)\n![](notes/2021/2021.7/assets/img_2022-10-15-3.gif)\n### 需要同时赋值\n![](notes/2021/2021.7/assets/img_2022-10-15-12.png)\n- 这样才能够保证走的方向是梯度最大的方向\n- 如果一前一后地赋值, 那么走的路线是这样ZigZag形状的, 可能能够运作, 但显然不是我们想要的运作方式.\n\t![](notes/2021/2021.7/assets/img_2022-10-15-13.png)\n\t\n\t\n## Different Gradient Descent Methods\n![Different_Gradient_Descent_Methods](notes/2021/2021.8/Different_Gradient_Descent_Methods.md)\n\n\n## [Linear Regression \u0026 Gradient Descent](notes/2021/2021.8/Linear_Regression\u0026Gradient_Descent.md)\n![Linear_Regression\u0026Gradient_Descent](notes/2021/2021.8/Linear_Regression\u0026Gradient_Descent.md)\n\n![Relation_Between_Linear_Regression\u0026Gradient_Descent_梯度下降和线性回归的关系](notes/2021/2021.8/Relation_Between_Linear_Regression\u0026Gradient_Descent_梯度下降和线性回归的关系.md)\n\n## [Logistic Regression \u0026 Gradient Descent](notes/2021/2021.8/Part.14_Logistic_Regression\u0026Gradient_Descent(ML_Andrew.Ng.).md)\n![Part.14_Logistic_Regression\u0026Gradient_Descent(ML_Andrew.Ng.)](notes/2021/2021.8/Part.14_Logistic_Regression\u0026Gradient_Descent(ML_Andrew.Ng.).md)\n\n\n\n## [[notes/2021/2021.8/凸优化与线性回归问题]]\n\n![凸优化与线性回归问题](notes/2021/2021.8/凸优化与线性回归问题.md)\n\t","lastmodified":"2022-10-15T14:06:29.478502323Z","tags":null},"/notes/2021/2021.8/Part.6_Linear_Algerba_ReviewML_Andrew.Ng.":{"title":"Part.6_Linear_Algerba_Review(ML_Andrew.Ng.)","content":"# Linear Algebra Review \n\n\u003cdiv align=\"right\"\u003e 2021-08-04\u003c/div\u003e\n\nTags: #Math #Math/LinearAlgebra\n\n- **Scalar**: 标量, A physical quantity that is completely described by its magnitude.\n- Vector有两种Index方式: 0-indexed \u0026 1-indexed就是指第一个元素序号是0还是1\n\n- 一个把数据带入回归方程的一个矩阵技巧\n\t![](notes/2021/2021.7/assets/img_2022-10-15-14.png)\n- 交换律: **Commutative Property**\n- 结合律: **Associative Property**\n- 单位矩阵: **Identity Matrix** \n\t表示为$I$ : Denoted $I$\n- 逆: **Inverse**\n- 转置: **Transpose**\n","lastmodified":"2022-10-15T14:06:29.478502323Z","tags":null},"/notes/2021/2021.8/Part.7_Feature_ScalingML_Andrew.Ng.":{"title":"Part.7_Feature_Scaling(ML_Andrew.Ng.)","content":"# Feature Scaling\n\n\u003cdiv align=\"right\"\u003e 2021-08-06\u003c/div\u003e\n\nTags: #MachineLearning #FeatureEngineering \n\n![](notes/2021/2021.7/assets/img_2022-10-15-15.png)[^2]\n\n深入阅读的链接:\nhttps://sebastianraschka.com/Articles/2014_about_feature_scaling.html\n\n## When to Use\n- 在梯度下降的时候, 缩放数据可以让梯度变化更平滑\n- ![](notes/2021/2021.7/assets/img_2022-10-15-16.png)\n\u003e If an algorithm uses gradient descent, then the difference in ranges of features will cause different step sizes for each feature. To ensure that the gradient descent moves smoothly towards the minima and that the steps for gradient descent are updated at the same rate for all the features, we scale the data before feeding it to the model. Having features on a similar scale will help the gradient descent converge more quickly towards the minima.\n\u003e \n\u003e Specifically, in the case of Neural Networks Algorithms, feature scaling benefits optimization by:\n\u003e \n\u003e -   It makes the training faster\n\u003e -   It prevents the optimization from getting stuck in local optima\n\u003e -   It gives a better error surface shape\n\u003e -   Weight decay and Bayes optimization can be done more conveniently\n- 在以距离为基础的算法里面, 放缩数据可以让数据分布更均匀\n\u003e Distance-based algorithms like KNN, K-means, and SVM are most affected by the range of features. This is because behind the scenes they are using distances between data points to determine their similarity and hence perform the task at hand. Therefore, we scale our data before employing a distance-based algorithm so that all the features contribute equally to the result.\n- 在主成分分析里面, 放缩数据可以让凸显出数据的\"变化\", (一个数量级很大的数据变一点点\u003e\u003e一个数量级很小的数据变化几倍)\n\u003e In [PCA](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html) we are interested in the components that maximize the variance. If one component (e.g. age) varies less than another (e.g. salary) because of their respective scales, PCA might determine that the direction of maximal variance more closely corresponds with the ‘salary’ axis, if those features are not scaled. As a change in the age of one year can be considered much more important than the change in salary of one euro, this is clearly incorrect.\n\n\n## Normalization 归一化\n\n![](notes/2021/2021.7/assets/img_2022-10-15-17.png)\n\n$$x^\\prime= \\frac{x-x_{min}}{x_{max}-x_{min}}$$\n\n- 可以自己调控数据分布的范围, 比如你想让数据分布在$[a,b]$范围内, 公式变为:\n$$x^{\\prime}=a+\\left(\\frac{x-\\min (x)}{\\max (x)-\\min (x)}\\right)(b-a)$$\n\n- 归一化对离群值十分敏感\n\n- 会缩小很大的数据, 会改变数量级\n## Standardization 标准化\n![Probability density function for the Normal distribtion|500](https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Normal_Distribution_PDF.svg/1920px-Normal_Distribution_PDF.svg.png)\n- 拿正态分布的数据做例子: 就相当于把其他颜色的曲线都变成红色的那条标准正态曲线\n\n$$x^\\prime= \\frac{x-\\mu}{\\sigma}$$\n$\\mu$是均值, $\\sigma$是标准差(方差的平方根)\n\n- 对离群值不是那么敏感\n\n- 标准化也改变数量级, 会减去均值\n\n### 如何选择[^1]\n**Normalization** 在数据不符合正态分布的时候比较适用, 像KNN这种对数据分布没有要求的模型更适用于归一化\n\n在神经网络里面常常要求数据分布在0-1之间, 这时候归一化必不可少; 另一个例子是图像处理的时候常常会把数据缩小到一个范围(比如0-255), 在这时标准化更加适用. \n\n\n**Standardization** 在数据满足正态分布的时候更加适用, 并且在放缩的时候没有范围限制, (不像归一化可以明确的规定一个范围$[a,b]$)\n\n在聚类中, 标准化在比较不同特征的相似性的时候很好用(why? #todo), 另一个例子是PCA的时候常常用标准化来突出数据分布的差异度, 而不是用归一化把最大的变成一. ^375f2a\n\n- 总之: \n\t- Standardization 适用于**正态分布**的数据, \n\t- Normalization 适用于**非正态分布**的数据\n- Normalization里面离群值对数据的影响显著\n- 不知道怎么办就都试试, 比较哪一个效果最好\n\n\n## Feature Scaling \u0026 Regression\n在多项式回归里面, 数据放缩很重要, 因为级数增长很快\n\n\n## Don't Confuse Regularization Normalization \u0026 Standardization\n - **Regularization:** 正则化 \n - **Normalization:** 归一化\n - **Standardization:** 标准化\n\n\n\n[^1]: https://www.atoti.io/when-to-perform-a-feature-scaling/\n[^2]:https://sebastianraschka.com/Articles/2014_about_feature_scaling.html ","lastmodified":"2022-10-15T14:06:29.478502323Z","tags":null},"/notes/2021/2021.8/Part.8_Train_Gradient_DescentML_Andrew.Ng.":{"title":"Part.8_Train_Gradient_Descent(ML_Andrew.Ng.)","content":"# Train Gradient Descent\n\n\u003cdiv align=\"right\"\u003e 2021-08-06\u003c/div\u003e\n\nTags: #GradientDescent #MachineLearning \n\n## 判断收敛(Convergence)的方法\n\n- 画出Cost Function - Iteration图, 平缓后收敛\n- 相邻周期变化值小于一个很小的值$\\Delta$\n\n## 寻找正常的学习率\n- 只要学习率$\\alpha$足够小, 损失函数一定是递减的(可以严格证明)\n- 如果学习率波动或者递增, 常常是因为学习率过大\n- 学习率过大也有一定几率导致收敛缓慢\n- 学习率过小会导致收敛过慢\n- 合适的方法是类似于二分法的思路, 用一系列的值去尝试, \n\te.g. $0.001, 0.003, 0.006, 0.01\\cdots$\n\n","lastmodified":"2022-10-15T14:06:29.478502323Z","tags":null},"/notes/2021/2021.8/Part.9_Normal_EquationML_Andrew.Ng.":{"title":"Part.9_Normal_Equation(ML_Andrew.Ng.)","content":"# Normal Equation\n\n\u003cdiv align=\"right\"\u003e 2021-08-14\u003c/div\u003e\n\nTags: #MachineLearning #NormalEquation #LinearRegression \n\n- Normal Equation 是解 **线性回归(Linear Regression)** 问题的一种代数方法.\n\n## Definition\n\nThe value of $\\theta$ that minimizes $J(\\theta)$ can be given in closed form by the equation\n$$\n\\theta=\\left(X^{T} X\\right)^{-1} X^{T} \\vec{y}\n$$\n\n其中\n$$\nX_{m\\times (n+1)}=\\left[\\begin{array}{c}\n-\\left(x^{(1)}\\right)^{T}- \\\\\n-\\left(x^{(2)}\\right)^{T}- \\\\\n\\vdots \\\\\n-\\left(x^{(m)}\\right)^{T}-\n\\end{array}\\right] .\n$$\n(其中$x$是$n+1$维列向量)\n$$\n\\vec{y}_{m\\times 1}=\\left[\\begin{array}{c}\ny^{(1)} \\\\\ny^{(2)} \\\\\n\\vdots \\\\\ny^{(m)}\n\\end{array}\\right] .\n$$\n\n## Intuitive\n- 这实际上是在求$X$矩阵列空间:$\\mathrm{Col}(X)$中, 最接近向量$\\vec{y}$的向量$X\\vec\\theta$(投影$\\mathrm{proj}_{\\mathrm{Col(}X\\mathrm )}(\\ \\vec y\\ )$[^3]), 这里$\\theta$可以看作是这个投影在列空间里面的坐标.\n- 这里的列向量就是所有样本里面的每一个单独的Feature构成的向量.\n- 这样看来, 问题的求解就是要在特征构成的\"特征空间\"里面找到一个点, 这个点最接近真实值$\\vec y$ (这其实是[Linear Regression](notes/2021/2021.8/Part.3_Linear_Regression(ML_Andrew.Ng.).md)的Intuition, Normal Equation方法促使我用线性代数的角度来看待这个问题)\n![](notes/2021/2021.7/assets/img_2022-10-15-18.png)[^2]\n上图是__线性代数及其应用__此书里面一个形象的图例, 参数$\\hat x / \\theta$的维数与特征数相同(n或者n+1), 而每一个特征的\"长度\"是样本数m, 真实值的数量也是m\n- 注意列空间的维数很可能不是$m$, 但是真实值向量$\\vec y$是在$\\mathbb R_m$里面取的, 所以我们常常需要求一个近似解$X\\vec\\theta$\n\t- 如果列空间的维数就是m, 那么我们能够把损失函数降为0, 即拟合曲线经过所有样本点. [^4]\n\n\u003e 具体的数学知识参见Linear Algebra and Its Applications by David C. Lay 第6章\n\n## 推导\n假设要得到$\\theta$, 使$X\\theta$尽可能靠近$\\vec y$\n### 通过向量空间推导\n\u003e 详细推导见上方提及的书Linear Algebra and Its Applications by David C. Lay 第6章\n\n大概思路:\n$X$ 的列向量垂直于$(\\vec y -\\mathrm {proj}_{Col(X)}\\ \\vec y)=(\\vec y-X\\theta)$, 所以$X$的列与$(\\vec y-X\\theta)$的内积为0, 也就相当于$X^T$与$(\\vec y-X\\theta)$的矩阵积为0 $\\Rightarrow X^T(\\vec y-X\\theta)=0 \\Rightarrow X^T\\vec y=X^TX\\theta$\n\n然后如果$X$的列向量独立, 那么$X^TX$可逆, 那么$\\theta=(X^TX)^{-1}X^T\\vec y$\n\n### 求导数\n![Normal_Equation_Proof_2_Matrix_Method](notes/2021/2021.8/Normal_Equation_Proof_2_Matrix_Method.md)\n\n\n## 局限性\n- 首先, 正规方程法做的只是尽可能地让拟合的曲线与样本点的差别最小, 即使曲线通过所有的样本点也不代表模型的预测效果很好, 因为还要考虑模型选取的合理性(曲线还是直线, etc), 数据里面的噪声等等因素. 这样看来正规方程法其实只是一种解方程的方法.\n\n- Normal Equation方法解决的问题都具有这样的形式: $$y=X\\beta+\\varepsilon$$(其中$\\varepsilon$是余差向量, 相当于预测值与真实值的误差), 这样的方程称为**线性模型**.[^1] \n\n- 线性模型可以进行直线拟合也可以进行曲线拟合, 所求的最优解都是最小二乘拟合.\n\t- 曲线的一个例子:\n\n$$\t\\begin{aligned}\n\u0026\\left[\\begin{array}{c}\ny_{1} \\\\\ny_{2} \\\\\n\\vdots \\\\\ny_{n}\n\\end{array}\\right]=\\left[\\begin{array}{ccc}\n1 \u0026 x_{1} \u0026 x_{1}^{2} \\\\\n1 \u0026 x_{2} \u0026 x_{2}^{2} \\\\\n\\vdots \u0026 \\vdots \u0026 \\vdots \\\\\n1 \u0026 x_{n} \u0026 x_{n}^{2}\n\\end{array}\\right]\\left[\\begin{array}{c}\n\\beta_{0} \\\\\n\\beta_{1} \\\\\n\\beta_{2}\n\\end{array}\\right]+\\left[\\begin{array}{c}\n\\varepsilon_{1} \\\\\n\\varepsilon_{2} \\\\\n\\vdots \\\\\n\\varepsilon_{n}\n\\end{array}\\right]\\\\\n\u0026\\quad y\\quad=\\quad\\quad X \\quad \\quad\\quad \\quad \\beta\\quad+\\quad\\varepsilon\n\\end{aligned}$$\n\n- 一般的, 一个有两个独立变量$u,v$的线性模型可以用以下方程来预测:\n$$\ny=\\beta_{0} f_{0}(u, v)+\\beta_{1} f_{1}(u, v)+\\cdots+\\beta_{k} f_{k}(u, v)\n$$\n此处, $f_{0}, \\cdots, f_{k}$ 是某类已知函数, $\\beta_{0}, \\cdots, \\beta_{k}$ 是未知权值.\n\n## Normal Equation \u0026 Noninvertibility\n因为方程是$\\theta=\\left(X^{T} X\\right)^{-1} X^{T} \\vec{y}$ , 在以下两种情况的时候, 可能会出现$(X^{T} X)$不可逆的情况:\n- 有多余的特征 / 特征之间线性相关 $\\rightarrow$ 删除多于特征即可\n- 特征数目大于样本数目: $n\u003em$, $\\rightarrow$ 删除一些特征或者利用\"正则化Regularization\"\n- [Don't Confuse Regularization Normalization and Standardization](notes/2021/2021.8/Part.7_Feature_Scaling(ML_Andrew.Ng.).md#Don't%20Confuse%20Regularization%20Normalization%20Standardization)\n\n在Octave语言里面 `pinv` (pseudo inverse) 可以用于求不可逆矩阵的逆 (`inv` 只能用于可逆矩阵)\n\n## Normal Equation \u0026 Gradient Descent\n\nNormal Equation方法只能解决线性回归问题(或者, 更一般地: 多重回归的线性模型[^1]), 相比之下, 梯度下降能解决更多机器学习模型的参数问题\n\n下面是对两个方法的一个比较:\n\n| Gradient Descent     |  Normal Equation    |\n| ---- | ---- |\n|  Need to choose alpha    |   No need to choose alpha   |\n| Needs many iterations     |   No need to iterate   |\n| $O(kn^2)$ | $O(n^3)$, need to calculate inverse of $X^TX$ |\n| Works well when n is large | Slow if n is very large |\n\n\n在正规方程法里面, 计算矩阵的逆的时间复杂度是$\\mathcal{O}\\left(n^{3}\\right) .$ 所以在特征数量很大的时候, 这个方法的速度会很慢. 在实际应用中, 如果n \u003e 10,000 就需要考虑使用梯度下降这种迭代形式来求参数的最优解了.\n\n### 在应用的时候需要注意的地方\n在完成作业的时候, 我发现利用Normal Equation 方法和 梯度下降方法所得到的$\\theta$值不一样:\n![](notes/2021/2021.7/assets/Pasted%20image%2020210819132727.png)![](notes/2021/2021.7/assets/Pasted%20image%2020210819132842.png)\n\n这是因为在梯度下降里面我们运用了Feature Scaling, 而在Normal Equation方法里面, 我们不需要对变量进行放缩.\n\n最后他们的预测结果是完全一样的(考虑误差):\n![](notes/2021/2021.7/assets/Pasted%20image%2020210819133047.png)![](notes/2021/2021.7/assets/Pasted%20image%2020210819133106.png)\n\n\n[^1]: Linear Algebra and Its Applications (4th Edition) by David C. Lay 第6.6节, 最小二乘法在线性模型中的应用:\n\n\t\u003e 例4表明，多重回归的线性模型和前面例题中的简单回归模型具有同样的抽象形式，线性代数为我们理解所有线性模型内在的一般原理提供了帮助，定义只要$X$适当，关于$β$的标准方程就具有相同的矩阵形式，不管包含多少变量，这样，对$X^TX$可逆的任何线性模型，最小二乘中的$\\hat β$总可由$\\left(X^{T} X\\right)^{-1} X^{T} \\vec{y}$ 计算得到.\n\t\n[^2]: Linear Algebra and Its Applications (4th Edition) by David C. Lay 第6.5节\n[^3]: 欧氏距离最小, 即[最小二乘法](notes/2021/2021.8/Mean_Squared_Error_均方误差.md#^0a7c67), 也即[均方误差](notes/2021/2021.8/Mean_Squared_Error_均方误差.md)作为损失函数要最小化\n[^4]: 这时可能有一解或者多解, 联系线性代数里面线性方程组的知识, 如果有多解, 那么说明列向量并不是独立的, 说明特征不是独立的. 具体参见 Linear Algebra and Its Applications (4th Edition) by David C. Lay 第6.5节","lastmodified":"2022-10-15T14:06:29.478502323Z","tags":null},"/notes/2021/2021.8/Relation_Between_Linear_RegressionGradient_Descent_%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%92%8C%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E5%85%B3%E7%B3%BB":{"title":"Relation_Between_Linear_Regression\u0026Gradient_Descent_梯度下降和线性回归的关系","content":"# 梯度下降法和线性回归的关系\n\n\u003cdiv align=\"right\"\u003e 2021-08-05\u003c/div\u003e\n\nTags: #MachineLearning #LinearRegression  #GradientDescent\n\n\n```mermaid\ngraph TD\n\tA([梯度下降])--\u003eB([梯度下降+平方损失])--\u003eC([梯度下降+平方损失+线性回归])\n\n```\n\n\n\n- 梯度下降法公式\n$$\n\\begin{array}{l}\n\\text { repeat until convergence }\\{\\\\\n\\begin{array}{cc}\n\\theta_{j}:=\\theta_{j}-\\alpha \\frac{\\Large\\partial}{\\Large\\partial \\Large\\theta_{j}}   J\\left(\\theta_{0},\\cdots ,\\theta_{n}\\right) \u0026 \\text { (simultaneously update } \nj=0, \\cdots ,j=n)\n\\end{array}\\\\\n\\text { \\} }\n\\end{array}\n$$\n\n---\n- 梯度下降 + Cost Function=平方损失\n$$J\\left(\\theta_{0},\\cdots ,\\theta_{n}\\right)=\\frac{1}{2 m} \\sum_{i=1}^{m}\\left(\\hat{y}^{(i)}-y^{(i)}\\right)^{2}=\\frac{1}{2 m} \\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2}$$\n\n所以\n\n$$\n\\frac{\\partial}{\\partial \\theta_{j}} J(\\theta) \n=\\frac 1 m \\sum_{i=1}^{m} \\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right) f_{j}(x^{(i)}) \n$$\n\n公式变为:\n$$\n\\begin{array}{l}\n\\text { repeat until convergence }\\{\\\\\n\\begin{array}{cc}\n\u0026\\theta_{j}:=\\theta_{j}-\\alpha \\frac 1 m \\sum_{i=1}^{m} \\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right) f_{j}(x^{(i)}) \n\\end{array}\\\\\n\\text { \\} }\n\\\\\\\\ \\text { (simultaneously update } \nj=0, \\cdots ,j=n)\n\\end{array}\n$$\n---\n- 梯度下降 + 平方损失 + Hypothesis Function=线性回归\n\n\t对于我们的线性回归问题, $f_j(x^{(i)})=x_j^{(i)}$\n\t\n\t公式变成了: ^fe416b\n\n$$\n\\begin{array}{l}\n\\text { repeat until convergence }\\{\\\\\n\\begin{array}{cc}\n\u0026\\theta_{j}:=\\theta_{j}-\\alpha \\frac 1 m \\sum_{i=1}^{m} \\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right) x_j^{(i)} \n\\end{array}\\\\\n\\text { \\} }\n\\\\\\\\ \\text { (simultaneously update } \nj=0, \\cdots ,j=n)\n\\end{array}\n$$","lastmodified":"2022-10-15T14:06:29.478502323Z","tags":null},"/notes/2021/2021.8/Sigmoid-Definition":{"title":"Sigmoid-Definition","content":"# Sigmoid\n\n\u003cdiv align=\"right\"\u003e 2021-08-19\u003c/div\u003e\n\nTags: #Sigmoid\n\nSigmoid means resembling the lower-case Greek letter sigma (uppercase Σ, lowercase σ, lowercase in word-final position ς) or the Latin letter S[^1]\n\n\n[^1]: Wikipedia","lastmodified":"2022-10-15T14:06:29.478502323Z","tags":null},"/notes/2021/2021.8/Sigmoid_Function":{"title":"Sigmoid_Function","content":"# Sigmoid Function\n\n\u003cdiv align=\"right\"\u003e 2021-08-19\u003c/div\u003e\n\nTags: #Sigmoid #MachineLearning #ActivationFunction\n\n## 什么是Sigmoid函数?\n- [Sigmoid的含义是](notes/2021/2021.8/Sigmoid-Definition.md)像S型的, 所以Sigmoid函数便是具有S形状的一类函数.\n\n- Sigmoid函数把整个实数域上的任意数映射到一个有限的区间里面: $(0,1)$\n- 在分类问题里面,  it's useful for transforming an arbitrary-valued function into a function better suited for classification.\n\n## Logistic Function\n- 逻辑斯蒂函数是Sigmoid函数之一.\n$$S(x)=\\frac{1}{1+e^{-x}}=\\frac{e^{x}}{e^{x}+1}=1-S(-x)$$\n\n![](notes/2021/2021.7/assets/img_2022-10-15-25.png)\n下图中蓝色曲线为导函数\n$$\\frac {d}{dx}S(x)=\\frac{e^{-x}}{(1+e^{-x})^2}\n=\\left(\\frac{1}{1+e^{-x}}\\right)\\left(1-\\frac{1}{1+e^{-x}}\\right)=S(x)\\left(1-S(x)\\right)$$\n![](notes/2021/2021.7/assets/Logistic.svg)\n\n![](notes/2021/2021.8/assets/derivative%20Sigmoid.png)","lastmodified":"2022-10-15T14:06:29.478502323Z","tags":null},"/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error":{"title":"Why_do_cost_functions_use_the_square_error","content":"# Why do cost functions use the square error?\n\n\u003cdiv align=\"right\"\u003e 2021-07-31\u003c/div\u003e\n\nTags: #MachineLearning #CostFunction #MeanSquareError\n\nReference: [StackExchange: why-do-cost-functions-use-the-square-error?](https://datascience.stackexchange.com/questions/10188/why-do-cost-functions-use-the-square-error?newreg=50bfd55599464f059209bd22b6898660)\n\nStackExchange上面一个关于均方差的一个很好的解释, 翻译如下:\n\n## Question:\n\u003eI'm just getting started with some machine learning, and until now I have been dealing with linear regression over one variable.\nI have learnt that there is a hypothesis, which is:\n$h_{\\theta}(x)=\\theta_{0}+\\theta_{1} x$\nTo find out good values for the parameters $\\theta_{0}$ and $\\theta_{1}$ we want to minimize the difference between the calculated result and the actual result of our test data. So we subtract\n$h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}$\nfor all $i$ from 1 to $m$. Hence we calculate the sum over this difference and then calculate the average by multiplying the sum by $\\frac{1}{m}$. So far, so good. This would result in:\n$\\frac{1}{m} \\sum_{i=1}^{m} \\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)$\nBut this is not what has been suggested. Instead the course suggests to take the square value of the difference, and to multiply by $\\frac{1}{2 m}$. So the formula is:\n$\\frac{1}{2 m} \\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2}$\nWhy is that? Why do we use the square function here, and why do we multiply by $\\frac{1}{2 m}$ instead of $\\frac{1}{m} ?$\n\n\n我是一个机器学习的初学者, 现在正在学习一元线性回归问题.\n我学到了下面这个假设函数:\n$h_{\\theta}(x)=\\theta_{0}+\\theta_{1} x$\n为了找到参数$\\theta_{0}$ 和 $\\theta_{1}$ 的最优值, 我们需要使预测值与真实值之间的误差最小, 所以我们把他们相减: $h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}$  其中 $i$ 取遍 $1$ 到 $m$.\n然后我们计算所有误差的和, 并且乘上 $\\frac{1}{m}$得到误差的平均数, 得到:\n$\\frac{1}{m} \\sum_{i=1}^{m} \\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)$\n但是这并不是正确的公式, 课程里面说我们需要把误差进行平方, 然后乘以$\\frac{1}{2 m}$, 所以应该是\n$\\frac{1}{2 m} \\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2}$\n为什么要这样做? 为什么我们需要将误差进行平方, 并且乘上$\\frac{1}{2 m}$ 而不是 $\\frac{1}{m} ?$\n\n## Answer:\nYour loss function would not work because it incentivizes setting $\\theta_{1}$ to any finite value and $\\theta_{0}$ to $-\\infty$.\n\nLet's call $r(x, y)=\\frac{1}{m} \\sum_{i=1}^{m} \\left(h_{\\theta}\\left(x^{(i)}\\right)-y\\right)$ the residual for $h$.\n\nYour goal is to make $r$ as close to zero as possible, not just minimize it. A high negative value is just as bad as a high positive value.\n\nEDIT: You can counter this by artificially limiting the parameter space $\\boldsymbol{\\Theta}$ (e.g. you want $\\left|\\theta_{0}\\right|\u003c\\mathbf{1 0}$ ). In this case, the optimal parameters would lie on certain points on the boundary of the parameter space. See [https://math.stackexchange.com/q/896388/12467](https://math.stackexchange.com/q/896388/12467). This is not what you want.\n\n## Why do we use the square loss\nThe squared error forces $h(x)$ and $y$ to match. It's minimized at $\\boldsymbol{u}=v$, if possible, and is always $\\geq 0$, because it's a square of the real number $\\boldsymbol{u}-\\boldsymbol{v}$.\n\n$|\\boldsymbol{u}-\\boldsymbol{v}|$ would also work for the above purpose, as would $(\\boldsymbol{u}-\\boldsymbol{v})^{2 n}$, with $\\boldsymbol{n}$ some positive integer. The first of these is actually used (it's called the $\\ell_{1}$ loss; you might also come across the $\\ell_{2}$ loss, which is another name for squared error).\n\nSo, why is the squared loss better than these? This is a *deep* question related to the link between Frequentist and Bayesian inference. In short, the squared error relates to **Gaussian Noise**.\n\nIf your data does not fit all points exactly, i.e. $h(x)-y$ is not zero for some point no matter what $\\theta$ you choose (as will always happen in practice), that might be because of noise. In any complex system there will be many small **independent** causes for the difference between your model $h$ and reality $y$ : measurement error, environmental factors etc. By the [Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem) $(\\mathrm{CLT})$, the total noise would be distributed Normally, i.e. according to the **Gaussian distribution**. We want to pick the best fit $\\boldsymbol{\\theta}$ taking this noise distribution into account. Assume $\\boldsymbol{R}=\\boldsymbol{h}(\\boldsymbol{X})-\\boldsymbol{Y}$, the part of $\\mathbf{y}$ that your model cannot explain, follows the Gaussian distribution $\\mathcal{N}(\\mu, \\sigma)$. We're using capitals because we're talking about random variables now.\n\nThe Gaussian distribution has two parameters, mean $\\mu=\\mathbb{E}[R]=\\frac{1}{m} \\sum_{i}\\left(h_{\\theta}\\left(X^{(i)}\\right)-Y^{(i))}\\right)$ and variance $\\sigma^{2}=E\\left[R^{2}\\right]=\\frac{1}{m} \\sum_{i}\\left(h_{\\theta}\\left(X^{(i)}\\right)-Y^{(i))}\\right)^{2}$. See [here](https://math.stackexchange.com/questions/518281/how-to-derive-the-mean-and-variance-of-a-gaussian-random-variable) to understand these terms better.\n\n- Consider $\\boldsymbol{\\mu}$, it is the systematic error of our measurements. Use $\\boldsymbol{h}^{\\prime}(\\boldsymbol{x})=\\boldsymbol{h}(\\boldsymbol{x})-\\boldsymbol{\\mu}$ to correct for systematic error, so that $\\boldsymbol{\\mu}^{\\prime}=\\mathbb{E}\\left[\\boldsymbol{R}^{\\prime}\\right]=\\mathbf{0}$ (exercise for the reader). Nothing else to do here.\n\n- $\\sigma$ represents the random error, also called noise. Once we've taken care of the systematic noise component as in the previous point, the best predictor is obtained when $\\boldsymbol{\\sigma}^{2}=\\frac{1}{m} \\sum_{i}\\left(h_{\\theta}\\left(X^{(i)}\\right)-Y^{(i))}\\right)^{2}$ is minimized. Put another way, the best predictor is the one with the tightest distribution (smallest variance) around the predicted value, i.e. smallest variance. **Minimizing the the least squared loss is the same thing as minimizing the variance**! That explains why the least squared loss works for a wide range of problems. The underlying noise is very often Gaussian, because of the $\\mathrm{CLT}$, and minimizing the squared error turns out to be the *right* thing to do!\n\nTo simultaneously take both the mean and variance into account, we include a _bias_ term in our classifier (to handle systematic error μ), then minimize the square loss.\n\nFollowup questions:\n- **Least squares loss = Gaussian error. Does every other loss function also correspond to some noise distribution?** Yes. For example, the $\\ell_{1}$ loss (minimizing absolute value instead of squared error) corresponds to the [Laplace distribution](https://en.wikipedia.org/wiki/Laplace_distribution) (Look at the formula for the PDF in the infobox -- it's just the Gaussian with  $|\\boldsymbol{x}-\\boldsymbol{\\mu}|$ instead of $(\\boldsymbol{x}-\\boldsymbol{\\mu})^{2}$). A popular loss for probability distributions is the [KL-divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence). -The Gaussian distribution is very well motivated because of the **Central Limit Theorem**, which we discussed earlier. When is the Laplace distribution the right noise model? There are some circumstances where it comes about naturally, but it's more commonly as a regularizer [to enforce **sparsity**](https://math.stackexchange.com/q/1904767/12467): the $\\ell_{1}$ loss is the _least convex_ among all convex losses.\n ^b7e1c9\n\t-  As [Jan](https://datascience.stackexchange.com/users/14904/jan-van-der-vegt) mentions in the comments, the minimizer of _squared_ deviations is the mean and the minimizer of the sum of **absolute** deviations is the **median**. Why would we want to find the median of the residuals instead of the mean? Unlike the mean, the median isn't thrown off by one very large outlier. So, the ℓ1 loss is used for increased robustness. Sometimes a combination of the two is used.\n- **Are there situations where we minimize both the Mean and Variance?** Yes. Look up [Bias-Variance Trade-off](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). Here, we are looking at a set of classifiers $h_\\theta \\in H$ and asking which among them is best. If we ask which _set_ of classifiers is the best for a problem, minimizing both the bias and variance becomes important. It turns out that there is always a trade-off between them and we use **regularization** to achieve a compromise. ^91cd90\n\n## Regarding the $\\frac1 2$ term\n\nThe 1/2 does not matter and actually, neither does the $m$ - they're both constants. The optimal value of $\\theta$ would remain the same in both cases.\n\n- The expression for the gradient becomes prettier with the $\\frac1 2$, because the 2 from the square term cancels out.\n\t- When writing code or algorithms, we're usually concerned more with the gradient, so it helps to keep it concise. You can check progress just by checking the norm of the gradient. The loss function itself is sometimes omitted from code because it is used only for validation of the final answer.\n- The $m$ is useful if you solve this problem with gradient descent. Then your gradient becomes the average of $m$ terms instead of a sum, so its' scale does not change when you add more data points.\n\t- I've run into this problem before: I test code with a small number of points and it works fine, but when you test it with the entire dataset there is loss of precision and sometimes over/under-flows, i.e. your gradient becomes `nan` or `inf`. To avoid that, just normalize w.r.t. number of data points.\n- These aesthetic decisions are used here to maintain consistency with future equations where you'll add **regularization** terms. If you include the $m$, the regularization parameter λ will not depend on the dataset size $m$ and it will be more interpretable across problems.\n\n\n---\n你的损失函数并不正确, 因为它倾向于将$\\theta_{1}$ 设置为任意有限值,并且将 $\\theta_{0}$ 设置为 $-\\infty$.\n\n我们不妨把$r(x, y)=\\frac{1}{m} \\sum_{i=1}^{m} \\left(h_{\\theta}\\left(x^{(i)}\\right)-y\\right)$ 称为 $h$的残差.\n\n你的目标是让$r$ **尽可能地接近0**, **不是让其尽可能地小**. 一个(绝对值)很大的负数和一个很大的整数一样糟糕.\n\n**附**: 你也可以人为限制参数的变化范围 $\\boldsymbol{\\Theta}$ (比如: 令$\\left|\\theta_{0}\\right|\u003c\\mathbf{1 0}$ ). 此时,你的方法得到的最优参数会是很靠近边界的一个值. (参见 [https://math.stackexchange.com/q/896388/12467](https://math.stackexchange.com/q/896388/12467)). 这并不是我们想要的结果.\n\n## 为什么要使用平方误差\n平方误差会让 $h(x)$ 靠近 $y$. 它在 $\\boldsymbol{u}=v$的时候取得最小值,  并且因为它是实数 $\\boldsymbol{u}-\\boldsymbol{v}$的平方, 它始终$\\geq 0$.\n\n$|\\boldsymbol{u}-\\boldsymbol{v}|$ 也有一样的效果,  正如$(\\boldsymbol{u}-\\boldsymbol{v})^{2 n}$,  ($\\boldsymbol{n}$ 是任意正数)也一样. 绝对值误差其实在实际问题中也用到了 (称为 $\\ell_{1}$ 误差; 你有时也会看到 $\\ell_{2}$ 误差, 这是平方误差的另一种称呼).\n\n所以为什么平方误差比它们都好? 这个问题其实十分深入, 它涉及到了频率学派推断[^1]和贝叶斯推断[^2]之间的联系. 简而言之, 平方误差其实和**高斯噪声**有关. [^3]\n\n如果你的预测值和真实值总是对不上, 也就是无论$\\theta$选什么值, 总有一些点$h(x)-y$ 不为零(这很常见). 那么很可能你的数据有噪声. 在一个复杂的系统中, 许多微小但是**相互独立**的因素会使 $h$ 和真实值 $y$不一样 : 比如测量误差, 环境因素等等. 根据[中心极限定理](https://en.wikipedia.org/wiki/Central_limit_theorem)  Central Limit Theorem$(\\mathrm{CLT})$ , 整体上这些噪声会呈正态分布, 也就是说, 它们服从 **高斯分布**(也称正态分布). 我们在选取 $\\boldsymbol{\\theta}$ 的时候, 也需要尽可能地考虑到这些因素. 假设 $\\boldsymbol{R}=\\boldsymbol{h}(\\boldsymbol{X})-\\boldsymbol{Y}$, 其中$\\mathbf{y}$的有一部分是你的模型无法解释的噪声, 服从高斯分布$\\mathcal{N}(\\mu, \\sigma)$. (我们之所以使用大写字母, 是因为它们都代表随机变量)\n\n高斯分布包含两个变量: 期望值$\\mu=\\mathbb{E}[R]=\\frac{1}{m} \\sum_{i} h_{\\theta}\\left(X^{(i)}\\right)-Y^{(i))}$ 和方差 $\\sigma^{2}=E\\left[R^{2}\\right]=\\frac{1}{m} \\sum_{i}\\left(h_{\\theta}\\left(X^{(i)}\\right)-Y^{(i))}\\right)^{2}$. 如果你想要了解更多,可以参考 [这个链接](https://math.stackexchange.com/questions/518281/how-to-derive-the-mean-and-variance-of-a-gaussian-random-variable) .\n\n- 对于 $\\boldsymbol{\\mu}$, 它表示我们测量的系统误差. 我们可以利用$\\boldsymbol{h}^{\\prime}(\\boldsymbol{x})=\\boldsymbol{h}(\\boldsymbol{x})-\\boldsymbol{\\mu}$ 来修正系统误差, 所以 $\\boldsymbol{\\mu}^{\\prime}=\\mathbb{E}\\left[\\boldsymbol{R}^{\\prime}\\right]=\\mathbf{0}$ (你可以试一试). 这样我们便已经修正了系统误差.\n\n- $\\sigma$ 代表系统的随机误差, 也称作*噪声*. 在修正了系统误差之后, 最好的预测结果在$\\boldsymbol{\\sigma}^{2}=\\frac{1}{m} \\sum_{i}\\left(h_{\\theta}\\left(X^{(i)}\\right)-Y^{(i))}\\right)^{2}$ 最小的时候取得. 换句话说, 最好的预测结果在预测值周围有着最紧密的分布 (最小的方差) . **取平方误差最小值的过程就是取方差最小值的过程**! 这也是最小二乘法[^4]适用面如此之广的原因. 一个系统里面隐藏的噪声常常是成正态分布的, 根据中心极限定理, 我们需要使平方误差最小化!\n\n为了同时考虑数学期望和方差, 我们在判别器里面引入了一个*偏置* (为了修正系统误差μ), 然后最小化平方误差.\n\n进一步的问题:\n- **最小二乘法 对应 高斯误差. 其他类型的损失函数也有对应的误差分布吗?** \n\t是的. 比如 $\\ell_{1}$ 误差 (使绝对值之和最小而不是平方误差最小) 对应着 [拉普拉斯分布(Laplace distribution)](https://en.wikipedia.org/wiki/Laplace_distribution) (注意观察一下拉普拉斯分布的概率密度函数 -- 相当于把高斯分布 $(\\boldsymbol{x}-\\boldsymbol{\\mu})^{2}$  的换成 $|\\boldsymbol{x}-\\boldsymbol{\\mu}|$).[^5] 另一个常见的损失函数对应的误差分布是 [KL-散度（相对熵）](notes/2022/2022.2/KL_Divergence-KL散度.md). 我们之前提到过，因为中心极限定理，高斯分布非常常见. 那么拉普拉斯分布又在什么时候适用呢? 的确有的问题符合拉普拉斯分布, 但是拉普拉斯分布更常被用于\"正则化\"来[保证**稀疏性(Sparsity)**](https://math.stackexchange.com/q/1904767/12467):  $\\ell_{1}$ 损失是所有凸损失函数里面凸起最小的. ^269677\n\t- 正如Jan在评论里面提到的一样, **平均值**使**平方误差**最小, 而**中位数**使**绝对误差**最小, 为什么($\\ell_{1}$损失中)我们要去找残差的中位数而不是均值呢? 这是因为不像平均值, 中位数不会被很大的离群值干扰. 所以$\\ell_{1}$损失被用来增加鲁棒性, 有时也会综合利用两者.\n\t- 附: 答案下方的评论:\n\t\t- 抱歉再问一下, 为什么不用绝对值而是用平方误差? – Alexander Suraphel Sep 5 '17 at 16:42\n\t\t- 绝对误差也可以, 但是你就是在找中位数而不是平均值了, 你可以用一小组数据来试一试, 观察不同估计值对于两个损失函数的影响 - Jan van der Vegt Oct 26 '17 at 10:58\n\n- **有没有同时最小化数学期望和方差的方法?** \n\t有. 参见 [偏差-方差权衡_Bias-Variance Trade-off](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). 比方说我们需要从一堆分类器$h_\\theta \\in H$ 里面选出最好的分类器.  如果我们需要找出*一组*最好的分类器, 那么就需要同时最小化偏差(Bias, 预期期望和真实值的差距) 和方差. 其实现实中总是需要在这两个量之间做出取舍的, 而这个取舍的过程常常通过**正则化**来实现.\n\n## 关于损失函数里面的$\\frac1 2$ \n\n其实这个 1/2 并不重要,  $m$ 也是 - 他们都是常数.  $\\theta$ 的最优值与它们无关.\n\n- 加上 $\\frac1 2$会让梯度的表达式更好看, [因为平方项的2被约掉了](notes/2021/2021.8/Linear_Regression\u0026Gradient_Descent.md#^021e6f).\n\t- 在设计算法和写代码的时候, 我们通常更关注梯度, 所以让梯度的表达式更简洁是很有用的, 你可以通过检查梯度的范数来检查表达式, 而在代码中损失函数有时会被省略掉, 因为它只有在核验最终答案的时候才会被用到.\n- 在你使用梯度下降法的时候, $m$ 是很有用的. 这样你的梯度就是$m$项的平均值, 而不是他们的和, 这样在你改变数据量的时候, 梯度的数量级不会改变. \n\t- 我遇到过这样的问题: 在少量的数据上面代码运行的很好, 但是当我用整个数据集来测试的时候就出现了精度的损失, 有时候甚至会出现上/下溢, 也就是说, 梯度变成了`nan` 或者 `inf`. 只要用数据项的个数$m$来规格化数据即可\n- 这其中也有审美的因素在, 如果将来可能添加更多的公式, 此举可以保持\"一致性\", 因为这样如果将来的公式里面包含**正则化**的部分, 那么正则化的参数λ将不依赖于数据集大小$m$, 这样更有利于不同规格问题之间的统一.\n\n\n[^1]: https://en.wikipedia.org/wiki/Frequentist_inference\n[^2]: https://en.wikipedia.org/wiki/Bayesian_inference\n[^3]: https://en.wikipedia.org/wiki/Gaussian_noise\n[^4]: 在高中学习的时候并没有讲解最小二乘法名字的含义, 现在看来, 可以理解成\"取最小的平方误差的方法\"\n[^5]: [Related_Post](notes/2021/2021.8/拉普拉斯分布与高斯分布的联系_Relation_of_Laplace_distribution%20_and_Gaussian_distribution.md)","lastmodified":"2022-10-15T14:06:29.478502323Z","tags":null},"/notes/2021/2021.9/%E6%AD%A3%E5%88%99%E9%A1%B9%E4%B8%8D%E5%BD%B1%E5%93%8D%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%87%B8%E6%80%A7":{"title":"正则项不影响线性回归损失函数的凸性","content":"# 正则项不影响线性回归损失函数的凸性\n\n\u003cdiv align=\"right\"\u003e 2021-09-10\u003c/div\u003e\n\nTags: #MachineLearning #Regularization #GradientDescent #LinearRegression #ConvexOptimization \n\n\n- **Question:** 加上正则项以后函数还是凸的吗? 梯度下降还适用吗?\n- 还是适用的, 证明如下\n\n## 首先, 如何证明一个函数为凸函数?\n如果$f$是二阶可微的，那么如果$f$的定义域是凸集，并且$\\forall x\\in dom(f), \\nabla^2 f(x)\\geqslant0$，那么$f$ 就是一个凸函数.[^1]\n- 严格凸函数则要求二阶导数恒大于零\n- $dom(f)$意指函数$f$的定义域(Domian)\n\n## 我们首先证明没有正则项的$J(\\theta)$是凸的\n$$\\begin{aligned}\n\\frac{\\partial}{\\partial \\theta_{j}} J(\\theta) \n\n\u0026=\\frac{1}{2 m} \\sum_{i=1}^{m} \\frac{\\partial}{\\partial \\theta_{j}} \\left(h_{\\theta}(x)-y\\right)^{2} \\\\\n\n\u0026=\\frac{1}{2 m} \\sum_{i=1}^{m} 2 \\left(h_{\\theta}(x)-y\\right) \\cdot \\frac{\\partial}{\\partial \\theta_{j}}\\left(h_{\\theta}(x)-y\\right) \\\\\n\n\u0026=\\frac{1}{ m} \\sum_{i=1}^{m} \\left(h_{\\theta}(x)-y\\right) x_j \\\\\n\\end{aligned}$$\n\n$$\\begin{aligned}\n\\frac{\\partial^2}{\\partial \\theta_{j}^2} J(\\theta) \n\n\u0026=\\frac{\\partial}{\\partial \\theta_{j}}\\left(\\frac{\\partial}{\\partial \\theta_{j}} J(\\theta)\\right) \\\\\n\n\u0026=\\frac{1}{ m} \\sum_{i=1}^{m}\\frac{\\partial}{\\partial \\theta_{j}}  \\left(h_{\\theta}(x)-y\\right) x_j \\\\\n\n\u0026=\\frac{1}{ m} \\sum_{i=1}^{m}x_j^2 \\\\\n\\end{aligned}$$\n显然是凸的.\n\n## 然后加上正则项\n$$\\begin{aligned}\n\\frac{\\partial}{\\partial \\theta_{j}} J(\\theta) \n\n\u0026=\\frac{1}{2 m} \\left[\\sum_{i=1}^{m} \\frac{\\partial}{\\partial \\theta_{j}} \\left(h_{\\theta}(x)-y\\right)^{2} \n+\\lambda \\frac{\\partial}{\\partial \\theta_{j}} \\sum_{i=1}^{n} \\theta_{i}^{2}\\right]\\\\\n\n\u0026=\\frac{1}{2 m}\\left[ \\sum_{i=1}^{m} 2 \\left(h_{\\theta}(x)-y\\right) x_j+2\\lambda\\theta_{j}\\right]\\\\\n\n\u0026=\\lambda\\theta_{j}+\\frac{1}{ m} \\sum_{i=1}^{m} \\left(h_{\\theta}(x)-y\\right) x_j \\\\\n\\end{aligned}$$\n\n$$\\begin{aligned}\n\\frac{\\partial^2}{\\partial \\theta_{j}^2} J(\\theta) \n\n\u0026=\\frac{\\partial}{\\partial \\theta_{j}}\\left(\\frac{\\partial}{\\partial \\theta_{j}} J(\\theta)\\right) \\\\\n\n\u0026=\\frac{\\partial}{\\partial \\theta_{j}}\\left(\\lambda\\theta_{j}+\\frac{1}{ m} \\sum_{i=1}^{m} \\left(h_{\\theta}(x)-y\\right) x_j\\right) \\\\\n\n\u0026=\\lambda+\\frac1{m} \\sum_{i=1}^m x_j^2\n\\end{aligned}$$\n当然在$\\lambda\u003e0$的时候上式恒大于零, 根据上面的定理, 损失函数一定是凸函数, 证毕.\n\n\n[^1]:更详细的推导详见知乎文章: https://zhuanlan.zhihu.com/p/210252556 ","lastmodified":"2022-10-15T14:06:29.494502495Z","tags":null},"/notes/2021/2021.9/%E6%AD%A3%E5%88%99%E9%A1%B9%E4%B8%8D%E5%BD%B1%E5%93%8DLogistic%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%87%B8%E6%80%A7":{"title":"正则项不影响Logistic回归损失函数凸性","content":"# 正则项不影响Logistic回归损失函数凸性\n\n\u003cdiv align=\"right\"\u003e 2021-09-11\u003c/div\u003e\n\nTags: #MachineLearning #LogisticRegression #Regularization #ConvexOptimization #CostFunction \n\n## 首先, 没有加正则项的二阶导数如下\n![二阶导数](notes/2021/2021.9/证明Logistic回归的损失函数是凸函数.md#二阶导数)\n\n\n## 那么只需要计算正则项的二阶导数\n$$\\begin{align}\nJ(\\theta)\u0026=P(\\theta)+\\frac\\lambda{2m}\\sum^n_{i=1}\\theta_i^2\n\\end{align}$$\n\n$$\\begin{aligned}\n\\frac{\\partial^2}{\\partial \\theta_{j}^2}\n\\left(\\frac\\lambda{2m}\\sum^n_{i=1}\\theta_i^2\\right)\u0026=\n\\frac{\\lambda}{m}\\frac{\\partial}{\\partial \\theta_{j}} \\theta_{j}\\\\\n\u0026=\\frac{\\lambda}{m}\u003e0\n\\end{aligned}$$\n所以损失函数还是凸的\n","lastmodified":"2022-10-15T14:06:29.494502495Z","tags":null},"/notes/2021/2021.9/%E6%AD%A3%E5%88%99%E9%A1%B9%E4%BC%9A%E6%B6%88%E9%99%A4%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E6%B3%95%E5%8F%AF%E8%83%BD%E7%9A%84%E4%B8%8D%E5%8F%AF%E9%80%86%E6%80%A7":{"title":"正则项会消除正规方程法可能的不可逆性","content":"# Normal Equation Non-invertibility \u0026 Regularization\n\n\u003cdiv align=\"right\"\u003e 2021-09-10\u003c/div\u003e\n\nTags: #NormalEquation  #Regularization \n\n\n![](notes/2021/2021.9/Part.19_Regularized_Linear_Regression(ML_Andrew.Ng.).md#^72311a)\n\n$(X^{T} X+\\lambda \\cdot L)$的内部如下图所示: \n![](notes/2021/2021.9/assets/NormalEquationRegualrization.svg)\n最关键的位置就是最左上角的那个地方, 如果那个位置不为0 (或者第一行/列上某个位置不为零, 也可以移过去), 那么容易知道这个矩阵一定可逆(满秩), 因为L就是单位矩阵除去第一个1.\n![](notes/2021/2021.9/assets/NormalEquationRegualrization2.svg)\n\n如果上面第一行第一列元素全部为零, 那么一定是因为第一个特征($\\theta_0$对应的特征)构成的向量为零向量(X里面黄色的部分), 但是如果是这样, 将这一列与其他特征交换, 便可在乘积矩阵的第一行/列得到非零的元素(因为不可能有两个特征都是零向量, 如果有, 那么便是多余的向量, 删除便可(emm这样一想好像零向量本来就是多余的))\n从而得知最后的矩阵加和一定是满秩的, 证毕.","lastmodified":"2022-10-15T14:06:29.494502495Z","tags":null},"/notes/2021/2021.9/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83_%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_Normal_Distribution-Gaussian_Distribution":{"title":"正态分布_高斯分布_Normal_Distribution-Gaussian_Distribution","content":"# 正态分布\n\n\u003cdiv align=\"right\"\u003e 2021-09-16\u003c/div\u003e\n\nTags: #Math/Statistics\n\n## 概率密度函数\n正态分布, 概率密度函数:\n$$f(x)=\\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{\\Large -\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}}$$\nor\n$$f(x)=\\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}\\right)$$\n\n## 重要性质\n- Mean $(\\mu)$ and standard deviation $(\\sigma)$\n$$\n\\begin{aligned}\n\u0026\\mu=E(X)=\\int_{-\\infty}^{\\infty} x p(x) d x \\\\\n\u0026\\sigma^{2}=E\\left\\{(X-\\mu)^{2}\\right\\}=\\int_{-\\infty}^{\\infty}(x-\\mu)^{2} p(x) d x\n\\end{aligned}\n$$\n\n- Probability within any particular number of standard deviations of $\\mu$\n$$\n\\begin{aligned}\np\\{\\mu-k \\sigma \\leq x \\leq \\mu+k \\sigma\\} \u0026=\\int_{\\mu-k \\sigma}^{\\mu+k \\sigma} \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left[-\\frac{(x-\\mu)^{2}}{2 \\sigma^{2}}\\right] d x \\\\\n\u0026=\\frac{1}{\\sqrt{2 \\pi}} \\int_{-k}^{k} \\exp \\left[-\\frac{y^{2}}{2}\\right] d y\n\\end{aligned}\n$$\n\n- 线性变换后: \n如果 $X \\sim N\\left(\\mu,\\sigma^{2}\\right)$ 且 $a, b$是实数, 那么\n$$a X+b \\sim N\\left(a \\mu+b,(a \\sigma)^{2}\\right)$$\n\n- 正态分布的和还是正态分布[^1]\n$$\\begin{aligned}\n\u0026X \\sim N\\left(\\mu_{X}, \\sigma_{X}^{2}\\right) \\\\\n\u0026Y \\sim N\\left(\\mu_{Y}, \\sigma_{Y}^{2}\\right) \\\\\n\u0026Z=X+Y\n\\end{aligned}$$\n则\n$$Z \\sim N\\left(\\mu_{X}+\\mu_{Y}, \\sigma_{X}^{2}+\\sigma_{Y}^{2}\\right)$$\n\n## 记忆公式\n- 注意$\\sigma$在根号外面\n- 指数是负的($x=\\mu$的时候等于0, 同时取得最大值)\n\n\n## 与拉普拉斯分布的联系\n\n[拉普拉斯分布与高斯分布的联系_Relation_of_Laplace_distribution _and_Gaussian_distribution](notes/2021/2021.8/拉普拉斯分布与高斯分布的联系_Relation_of_Laplace_distribution%20_and_Gaussian_distribution.md)\n\n## Higher Dimensions\n$$\\begin{aligned}\n\u0026p\\{x\\}=\\frac{1}{(\\sqrt{2 \\pi})^{n / 2}|C|^{1 / 2}} \\exp \\left[-\\frac{1}{2}(x-\\mu)^{T} C^{-1}(x-\\mu)\\right] \\\\\n\u0026x=\\left[\\begin{array}{c}\nx_{1} \\\\\n\\cdots \\\\\nx_{n}\n\\end{array}\\right] \\quad \\mu=\\left[\\begin{array}{c}\n\\mu_{1} \\\\\n\\cdots \\\\\n\\mu_{n}\n\\end{array}\\right] \\quad C=\\left[\\begin{array}{ccc}\n\\sigma_{11}^{2} \u0026 \\ldots \u0026 \\sigma_{1 n}^{2} \\\\\n\\cdots \u0026 \u0026 \\ldots \\\\\n\\sigma_{m 1}^{2} \u0026 \\ldots \u0026 \\sigma_{m n}^{2}\n\\end{array}\\right]\n\\end{aligned}$$\n\n\n[^1]: [Sum of normally distributed random variables - Wikipedia](https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables)","lastmodified":"2022-10-15T14:06:29.494502495Z","tags":null},"/notes/2021/2021.9/%E7%8B%AC%E7%AB%8B%E5%90%8C%E5%88%86%E5%B8%83-IID":{"title":"独立同分布-IID","content":"# 独立同分布 Independent and identically distributed\n\n\u003cdiv align=\"right\"\u003e 2021-09-16\u003c/div\u003e\n\nTags: #Math/Statistics\n\n## 定义\n- 在概率论与统计学中，独立同分布（英语：**Independent and identically distributed**，或称独立同分配，缩写为**iid**、 **i.i.d.**、**IID**）是指一组随机变量中每个变量的概率分布都相同，且这些随机变量互相独立.\n\n\n\n","lastmodified":"2022-10-15T14:06:29.494502495Z","tags":null},"/notes/2021/2021.9/%E8%AF%81%E6%98%8ELogistic%E5%9B%9E%E5%BD%92%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%98%AF%E5%87%B8%E5%87%BD%E6%95%B0":{"title":"证明Logistic回归的损失函数是凸函数","content":"# 证明Logistic回归的损失函数是凸函数\n\n\u003cdiv align=\"right\"\u003e 2021-09-11\u003c/div\u003e\n\nTags: #MachineLearning #LogisticRegression #ConvexOptimization #CostFunction \n\n![首先 如何证明一个函数为凸函数](notes/2021/2021.9/正则项不影响线性回归损失函数的凸性.md#首先%20如何证明一个函数为凸函数)\n\n## 证明\n### 原函数\n[Part.13_Cost_Function-Logistic_Regression(ML_Andrew.Ng.)](notes/2021/2021.8/Part.13_Cost_Function-Logistic_Regression(ML_Andrew.Ng.).md)\n$$\\begin{aligned}\nh\u0026=g(X \\theta) \\\\\nJ(\\theta)\u0026=-\\frac{1}{m} \\cdot\\left[y^{T} \\log (h)+(1-y)^{T} \\log (1-h)\\right]\n\\end{aligned}$$\n### 一阶导数\n在梯度下降里面我们已经求出了一阶导数了:\n![推导](notes/2021/2021.8/Part.14_Logistic_Regression\u0026Gradient_Descent(ML_Andrew.Ng.).md#推导)\n\n### 二阶导数\n$$h(x)=g\\left(\\theta^Tx\\right) $$\n$$\\begin{aligned}\n\\frac{\\partial}{\\partial \\theta_{j}} h(x)\u0026=\n\\frac{\\partial}{\\partial \\theta_{j}} g\\left(\\theta^Tx\\right)\\\\\n\u0026=g\\left(\\theta^Tx\\right)\\left(1-g\\left(\\theta^Tx\\right)\\right) \\frac{\\partial}{\\partial \\theta_{j}} \\theta^Tx\\\\\n\u0026=g\\left(\\theta^Tx\\right)\\left(1-g\\left(\\theta^Tx\\right)\\right)x_j\\\\\n\u0026=h(x)\\left(1-h(x)\\right)x_j\n\\end{aligned}$$\n\n\n$$\\begin{aligned}\n\\frac{\\partial^2}{\\partial \\theta_{j}^2} J(\\theta) \n\u0026=\\frac{\\partial}{\\partial \\theta_{j}}\\left[\\frac{1}{m} \\sum_{i=1}^{m}\\left(h(x^{(i)})-y^{(i)}\\right) x^{(i)}_{j}\\right]\\\\\n\u0026=\\frac{1}{m} \\sum_{i=1}^{m}\\left(\\frac{\\partial}{\\partial \\theta_{j}}h(x^{(i)})\\right) x^{(i)}_{j} \\\\\n\u0026=\\frac{1}{m} \\sum_{i=1}^{m}g\\left(\\theta^Tx\\right)\\left(1-g\\left(\\theta^Tx\\right)\\right)\\left(x_j^{(i)}\\right)^2 \\\\\n\u0026=\\frac{1}{m} \\sum_{i=1}^{m}h(x)\\left(1-h(x)\\right)\\left(x_j^{(i)}\\right)^2\n\\end{aligned}$$\n- 接下来判断二阶导数的负号: 对于Sigmoid函数的导函数部分, 观察函数图像, 蓝色曲线为导函数始终大于零:\n[Logistic Function](notes/2021/2021.8/Sigmoid_Function.md#Logistic%20Function)\n![Logistic](notes/2021/2021.7/assets/Logistic.svg)\n同时$\\left(x_j^{(i)}\\right)^2$也非负, 所以二阶导数恒大于等于零, 函数为凸函数, 证毕.\n\n","lastmodified":"2022-10-15T14:06:29.494502495Z","tags":null},"/notes/2021/2021.9/CMU15-445_1_Lecture_Note":{"title":"CMU15-445_1_Lecture_Note","content":"# Course Intro \u0026 Relational Algebra\n\n\u003cdiv align=\"right\"\u003e 2021-09-08\u003c/div\u003e\n\nTags: #Database \n \n## Overview\n- 这门课是关于数据库的DBMS的设计和实现的, 而不是关于如何使用一个数据库的\n- 2 classes per week, supplementary  reading  materials.\n\n- TextBook: Database System Concepts: The best among all the textbooks - Andy Pavlo\n\n- Project: build your own storage manager from scratch. A relatively complex software yet not a full-fledged system. Domino-way of building. \n\n\n## Database\n\na collection of data that's related together in some way that's try's to model some aspects of real world. \na useful database is the one which you can ask question about it.\n\nFlat Files:\n- CSV File: Comma Separated Value File\n\t- ![](notes/2021/2021.9/assets/Pasted%20image%2020210908111847.png)\n\t- If we want to find out which year the artist \"Ice Cube\" went solo,  we could write the following python code to iterate through the file above:\n```py\nfor line in file:\n\trecord = parse(line)\n\tif record[0]==\"Ice Cube\": \n\t\tprint int(record[1])\n ```\n- Implementation\n\t- However, there could be many issues in this way of storing information.\n\t- For example, this method is slow and becomes problematic when sharing data between programs(e.g. write to the same file at the same time).\n- Durability Problems\n- Data Integrity Problems\n\nDBMS:\n- Allows apps to store and analyze info in a database\n- 不用重复造轮子\n- We'll focus on In-disk DB\n- Definition, Creation, Query, Update and Administration of databases are the essential functionalities of a general-purpose DBMS. \n\n### History of DBMSs\n- **Early Stage**\n\tTight coupling between logical and physical layers.\n\t- People: Ted Codd - Purposed Relational Model\n\n**Data Model**: a collection of concepts for describing the data in a database. -\u003e The Higher Concept\n**Schema**: a description of a particular collection of data, Using a given data model. -\u003e The Actual Plan\n- **There are a bunch of data models:**\n\t- ![](notes/2021/2021.9/assets/Pasted%20image%2020210922111326.png)\n\t- ![](notes/2021/2021.9/assets/Pasted%20image%2020210922111354.png)\n\t- ![](notes/2021/2021.9/assets/Pasted%20image%2020210922111439.png)\n\t- ![](notes/2021/2021.9/assets/Pasted%20image%2020210922111456.png)\n\n**Relational Model: Three Parts**\n- Structure: The definitions of relations and their contents.\n- Integrity: Ensure the database's contents satisfy constraints.\n- Manipulation: How to access and modify a database's contents\n\n**Relation: Definition**\nUnordered set that contain the relation of attributes that represent entities.\n\n- Tuple: a set of attribute values(Domains) in the relation.(所以Tuple不只是三元组哈)\n- `NULL`可以放入任何Domain\n\n**Primary Key**\nUniquely identify a single tuple.\n\n## DML: Data Manipulation Languages\n有两种:\n- Procedural: 描述DBMS应该怎么找到目标数据\n\t- 对应 **Relational Algebra** -\u003e我们这门课研究的对象\n- Non-Procedural: 描述需要什么数据, 但是不给出怎么找到这些数据.\n\t- 对应Relational Calculus\n\n\n## Relational Algebra\nDescribe the fundamental operations to retrieve and manipulate tuples in a relation(Based on Set Algebra)\n```mermaid\ngraph LR\nA([realtion1])--\u003eB[\u003cSome Operations\u003e]\nC([realtion2])--\u003eB\nD([...])--\u003eB\nE([realtionN])--\u003eB\nB--\u003eF([New Relation])\n```\n\n关系代数是Procedural的, 描述的是筛选数据所要进行的操作.\n![](notes/2021/2021.9/assets/Pasted%20image%2020210926102310.png)\n在课件里面介绍了以上操作:\n- Select\n- Projection\n- Union\n- Intersection\n- Difference\n- Product\n- Join\n\n- 在课件里面举了一个例子, 如果我们用Procedural Language(比如 关系代数)来描述我们需要的数据, 那么可能会出现效率低的情况, 最佳的策略是描述我们需要的数据(Non-Procedural的), 让DBMS来决定怎样查找效率高, 这也是现在数据库里面的现行标准.(*de facto* standard)\n\n\n\n\n\n\n\n\n\n\n\n","lastmodified":"2022-10-15T14:06:29.482502366Z","tags":null},"/notes/2021/2021.9/Codecademy_SQL_Tutorial-Notes-Database":{"title":"Codecademy_SQL_Tutorial-Notes-Database","content":"# SQL Tutorial\n\n\u003cdiv align=\"right\"\u003e 2021-09-26\u003c/div\u003e\n\nTags: #Database #SQL #SQLite\n\n- This course use SQLite\n\n**复习材料:**\n- [SQL Commands - Glossary](https://www.codecademy.com/articles/sql-commands)\n- [CheatSheet](https://www.codecademy.com/learn/learn-sql/modules/learn-sql-manipulation/cheatsheet)\n\n下面只叙述CheatSheet里面没有的内容\n\n\n\n## Clauses\n`CREATE TABLE` is a _clause_. Clauses perform specific tasks in SQL. By convention, clauses are written in capital letters. Clauses can also be referred to as commands.\n`INSERT`, `SELECT` etc. are also clauses.\n中文: 子句?\n## INSERT\n- insert attribute 的顺序可以交换, 只要Tuple里面的顺序一一对应即可.\n```sql\n INSERT INTO celebs (id, name, age)\n VALUES (1, 'Justin Bieber', 22);\n\n INSERT INTO celebs (name, id, age)\n VALUES ('Jeremy Lin', 3, 26);\n```\n\n- Multiple tuples: Quick way:\n```sql\nINSERT INTO table (col1, col2, col3)\nVALUES\n(row1_val1, row1_val2, row1_val3),\n(row2_val1, row2_val2, row2_val3),\n(row3_val1, row3_val2, row3_val3);\n```\n\n## ALTER\nOrder:\n### Question\n\nIn the context of this [exercise 89](https://www.codecademy.com/paths/data-science/tracks/dspath-why-data-python-basics/modules/dspath-introduction-to-sql/lessons/manipulation/exercises/alter), can we add a column at a specific position to a table?\n\n### Answer\n\nNo, unfortunately, you cannot specify what position to add a column to a table.\n\nBy default, a new column will always be added at the end of the table. For most intents and purposes, this should not affect much, since you can always select the columns in any order, for instance, like\n\n```\nSELECT col3, col1, col2\n```\n\nIf column order is very important, then an alternative is to create a new table and add the columns in the specific order they should appear.\n\n## DELETE\n```sql\nDELETE FROM celebs \nWHERE twitter_handle IS NULL;\n```\n- 这里的`IS NULL`可以换成`= NULL`吗?\n\t- 不可以, 没法删除\n\n\n## SQLite\n- In SQLite, a database is stored in **a single file** — a trait that distinguishes it from other database engines.\n- Drawbacks\n\t- Security and Data Integrity issue\n\t- No Data-Type Verification\n\t- Less advanced features\n\t- SQLite’s maintainers consider it to be among the [most replicated pieces of software in the world](https://www.sqlite.org/mostdeployed.html).\n\t- \n\n## Part 2 Queries\n### AS\n- AS不改变原来的relation\n- 你可以用逗号分隔两个查询重命名的项: \n```sql\nSELECT name AS movie, imdb_rating AS IMDb\nFROM movies;\n```\n\n### WHERE\n- WHERE里面的相等是一个等号: `=`\n\n### LIKE\n- wildcard: `_` -\u003e a single character\n- `%` is a wildcard character that matches zero or more missing letters in the pattern.\n- `LIKE` is **not** case sensitive.\n- 如果要查找`_` 或`%`, 利用反斜杠表示转义字符: `\\_`  `\\%`\n\n\u003e ### Question\n\u003e \n\u003e Can we apply the `LIKE` operator to values other than `TEXT`?\n\u003e \n\u003e ### Answer\n\u003e \n\u003e Yes, you can apply the `LIKE` operator to numerical values as well.\n\u003e \n\u003e Whenever you use `LIKE` however, you must always wrap the pattern within a pair of quotations, whether for matching a number or a string.\n\u003e \n\u003e #### Example\n\u003e \n\u003e ```\n\u003e /* \n\u003e This will select movies where the id number\n\u003e starts with 2 and is followed by any two numbers.\n\u003e */\n\u003e SELECT * \n\u003e FROM movies\n\u003e WHERE id LIKE '2__';\n\u003e ```\n\n\n### BETWEEN\n```sql\nSELECT *\nFROM movies\nWHERE year BETWEEN 1990 AND 1999;\n```\n这个范围是 [1990, 1999], 包括了1999,\n\n但是如果是字符串的话, 就变得稍微有一点复杂:\n看这个:\n```sql\nSELECT *\nFROM movies\nWHERE name BETWEEN 'A' AND 'J';\n```\n'Jaw' 会包括在里面吗? -\u003e 不会\n但是'J'会包括在里面\n\n因为其实是这个顺序:\n'A', 'Aa' ...... 'J', 'Ja',...........\n到'J'那里就停下来了.\n\n### CASE\n```sql\nSELECT name,\n CASE\n  WHEN imdb_rating \u003e 8 THEN 'Fantastic'\n  WHEN imdb_rating \u003e 6 THEN 'Poorly Received'\n  ELSE 'Avoid at All Costs'\n END AS 'Review'\nFROM movies;\n```\n- 注意第一行最后有一个逗号!\nELSE不是必须的\n```sql\nSELECT *,\n  CASE\n    WHEN review \u003e 4.5 THEN 'Extraordinary'\n    WHEN review \u003e 4   THEN 'Excellent'\n    WHEN review \u003e 3   THEN 'Good'\n    WHEN review \u003e 2   THEN 'Fair'\n    ELSE 'Poor'\n  END AS 'New Review'\nFROM nomnom;\n```\nCASE是从上向下匹配的, 所以第二行的不用再说明要小于4.5:\n![](notes/2021/2021.9/assets/img_2022-10-15.png)\n\n## Multiple Tables\n### Inner Join\n![](https://content.codecademy.com/courses/learn-sql/multiple-tables/inner-join.gif)\n\n### LEFT JOIN\n![](https://content.codecademy.com/courses/learn-sql/multiple-tables/left-join.gif)\n\n\n\n\n## 学完了\n有三个网站来回顾学习内容：\n- [学前预览](https://www.codecademy.com/learn/learn-sql/modules/learn-sql-manipulation)\n- [CheatSheet](https://www.codecademy.com/learn/learn-sql/modules/learn-sql-multiple-tables/cheatsheet)\n- [Glossary Article](https://www.codecademy.com/articles/sql-commands)","lastmodified":"2022-10-15T14:06:29.482502366Z","tags":null},"/notes/2021/2021.9/De-facto":{"title":"De facto","content":"# De facto\n\n\u003cdiv align=\"right\"\u003e 2021-09-26\u003c/div\u003e\n\nTags: #English #Latin \n\nIn law and government, de facto (/deɪ ˈfæktoʊ, di-, də-/ day FAK-toh, dee -⁠; Latin: de facto [deː ˈfaktoː], \"in fact\") describes practices that exist in reality, even though they are not officially recognized by laws.\n\nIt is commonly used to refer to what happens in practice, in contrast with de jure (\"by law\"), which refers to things that happen according to law. ","lastmodified":"2022-10-15T14:06:29.482502366Z","tags":null},"/notes/2021/2021.9/OK_should_be_in_capital-lettersor_okay":{"title":"OK_should_be_in_capital letters(or_okay)","content":"#English\n\n- OK/okay is a word with uncertain origin.\n- Both \"OK\" and \"okay\" is acceptable, but ok is only used for convenience.\n\n\nSources:\n- https://www.writing-skills.com/ok-ok-okay-how-do-you-write-ok\n- https://english.stackexchange.com/questions/108213/must-ok-only-be-written-in-capital-letters\n- https://www.lifehacker.com.au/2013/08/its-not-ok-to-write-ok/\n\n","lastmodified":"2022-10-15T14:06:29.482502366Z","tags":null},"/notes/2021/2021.9/Part.18_Regularization_IntuitionML_Andrew.Ng.":{"title":"Part.18_Regularization_Intuition(ML_Andrew.Ng.)","content":"# Regularization: Intuition\n\n\u003cdiv align=\"right\"\u003e 2021-09-10\u003c/div\u003e\n\nTags: #MachineLearning #Regularization \n\n如果我们约束的参数\"加大权重\", 那么在优化的时候就会重点最小化那些加了权重的参数.\nE.g.\n$$\n\\theta_{0}+\\theta_{1} x+\\theta_{2} x^{2}+\\theta_{3} x^{3}+\\theta_{4} x^{4}\n$$\nWe'll want to eliminate the influence of $\\theta_{3} x^{3}$ and $\\theta_{4} x^{4}$. Without actually getting rid of these features or changing the form of our hypothesis, we can instead modify our cost function:\n$$\n\\min _{\\theta} \\frac{1}{2 m} \\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2}+1000 \\cdot \\theta_{3}^{2}+1000 \\cdot \\theta_{4}^{2}\n$$\n后面的两项可以约束$\\theta_{3}$和$\\theta_{4}$, 减小它们的影响.\n\n更一般的形式如下: \n$$\n\\min _{\\theta} \\frac{1}{2 m} \\left[\\sum_{i=1}^{m}\n\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2}\n+\\lambda \\sum_{j=1}^{n} \\theta_{j}^{2}\\right]\n$$\n\n- The $\\lambda$, or lambda, is the regularization parameter. It determines how much the costs of our theta parameters are inflated.\n- 注意$j$从1开始, 我们通常不约束$\\theta_0$ .\n\n","lastmodified":"2022-10-15T14:06:29.482502366Z","tags":null},"/notes/2021/2021.9/Part.19_Regularized_Linear_RegressionML_Andrew.Ng.":{"title":"Part.19_Regularized_Linear_Regression(ML_Andrew.Ng.)","content":"# Regularization \u0026 Linear Regression\n\n\u003cdiv align=\"right\"\u003e 2021-09-10\u003c/div\u003e\n\nTags: #MachineLearning #Regularization #GradientDescent #LinearRegression #NormalEquation \n\n## Regularization \u0026 Gradient Descent\n添加了正则项之后有两点需要注意:\n- $\\theta_0$需要单独处理 (不需要正则约束, 损失函数不一样)\n- $\\theta_1 \\sim \\theta_n$ 因为需要正则化, 损失函数$J(\\theta)$发生了变化, 梯度需要重新计算\n- [正则项不影响线性回归损失函数的凸性](notes/2021/2021.9/正则项不影响线性回归损失函数的凸性.md)\n\n同时考虑上面两点, 梯度下降更新公式变为了: \n\n$$\n\\begin{aligned}\nRe\u0026peat\\ \\{\\\\\n\u0026\\theta_{0}:=\\theta_{0}-\\alpha \\frac{1}{m} \\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right) x_{0}^{(i)} \\\\\n\u0026\\theta_{j}:=\\theta_{j}-\\alpha\\left[ \\frac{1}{m} \\sum_{i=1}^{m}\n\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right) x_{j}^{(i)}+\\frac{\\lambda}{m} \\theta_{j}\\right]\n\\quad\\quad j \\in\\{1,2 \\ldots n\\}\n\\\\ \\}\n\\end{aligned}\n$$\n要是把方括号打开, 第二行的更新公式可以变为:\n$$\n\\theta_{j}:=\\theta_{j}\\left(1-\\alpha\\frac\\lambda m\\right)\n-\\alpha\\frac{1}{m} \\sum_{i=1}^{m}\n\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right) x_{j}^{(i)}\n\\quad\\quad j \\in\\{1,2 \\ldots n\\}\n$$\n因为$\\left(1-\\alpha\\frac\\lambda m\\right)$一定小于1, 所以这个更新公式每次都会缩小一点点$\\theta_i$, 而公式的后半部分和没有正则化的公式是完全一样的.\n\n\n## Regularization \u0026 Normal Equation\n- 没有正则化的公式:\t\n$$\n\t\\theta=\\left(X^{T} X\\right)^{-1} X^{T} \\vec{y}\t\n$$\n([Definition](notes/2021/2021.8/Part.9_Normal_Equation(ML_Andrew.Ng.).md#Definition))\n\n- 加入正则项以后: \n\n$$\\begin{aligned}\n\u0026\\theta=(X^{T} X+\\lambda \\cdot L)^{-1} X^{T} \\vec y \\\\\n\u0026\\text { where } L=\\left[\\begin{array}{cccc}\n0 \u0026 \u0026 \u0026 \u0026 \\\\\n\u0026 1 \u0026 \u0026 \u0026 \\\\\n\u0026 \u0026 1 \u0026 \u0026 \\\\\n\u0026 \u0026 \u0026 \\ddots \u0026 \\\\\n\u0026 \u0026 \u0026 \u0026 1\n\\end{array}\\right]_{(n+1)\\times(n+1)}\n\\end{aligned}$$\n\n^72311a\n\n$L$的第一个0可以理解为不用正则化$\\theta_0$\n\n- 在没有正则化以前$(X^{T} X+\\lambda \\cdot L)$可能不可逆, 但是正则化以后是一定可逆的:\n\t证明:[[notes/2021/2021.9/正则项会消除正规方程法可能的不可逆性]] \n\n\n","lastmodified":"2022-10-15T14:06:29.482502366Z","tags":null},"/notes/2021/2021.9/Part.20_Regularized_Logistic_RegressionML_Andrew.Ng.":{"title":"Part.20_Regularized_Logistic_Regression(ML_Andrew.Ng.)","content":"# Regularized Logistic Regression\n\n\u003cdiv align=\"right\"\u003e 2021-09-11\u003c/div\u003e\n\nTags: #MachineLearning #LogisticRegression #Regularization \n\n## 回顾一下没有正则化的情况\n### 损失函数\n[更简洁的形式](notes/2021/2021.8/Part.13_Cost_Function-Logistic_Regression(ML_Andrew.Ng.).md#更简洁的形式)\n$$\\begin{align}\nJ(\\theta)\n\u0026=-\\frac{1}{m} \\sum_{i=1}^{m}\\left[y^{(i)} \\log \\left(h_{\\theta}\\left(x^{(i)}\\right)\\right)+\\left(1-y^{(i)}\\right) \\log \\left(1-h_{\\theta}\\left(x^{(i)}\\right)\\right)\\right]\\\\\n\\end{align}$$\nor 向量化的\n$$\\begin{aligned}\nh\u0026=g(X \\theta) \\\\\nJ(\\theta)\u0026=-\\frac{1}{m} \\cdot\\left[y^{T} \\log (h)+(1-y)^{T} \\log (1-h)\\right]\n\\end{aligned}$$\n### 梯度下降公式\n[结果](notes/2021/2021.8/Part.14_Logistic_Regression\u0026Gradient_Descent(ML_Andrew.Ng.).md#结果)\n$$\\begin{aligned}\n\\frac{\\partial}{\\partial \\theta_{j}} J(\\theta) \n\u0026=\\frac{1}{m} \\sum_{i=1}^{m}\\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right) x^{(i)}_{j}\n\\end{aligned}$$\n- 是和线性回归完全一样的, 所以梯度下降公式也一样:\n$$\\begin{array}{l}\n\\text { repeat until convergence }\\{\\\\\n\\begin{array}{cc}\n\u0026\\theta_{j}:=\\theta_{j}-\\alpha \\frac 1 m \\sum_{i=1}^{m} \\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right) x_j^{(i)} \n\\end{array}\\\\\n\\text { \\} }\n\\\\\\\\ \\text { (simultaneously update } \nj=0, \\cdots ,j=n)\n\\end{array}$$\n---\n## 正则化以后的损失函数\n$$\\begin{align}\nJ(\\theta)\n\u0026=-\\frac{1}{m} \\sum_{i=1}^{m}\n\\left[y^{(i)} \\log \\left(h\\left(x^{(i)}\\right)\\right)+\\left(1-y^{(i)}\\right) \\log \\left(1-h\\left(x^{(i)}\\right)\\right)\\right]\n+\\frac\\lambda{2m}\\sum^n_{i=1}\\theta_i^2\\\\\n\u0026=P(\\theta)+\\frac\\lambda{2m}\\sum^n_{i=1}\\theta_i^2\n\\end{align}$$\n需要注意的就是正则项应该是加上去的, 原理损失函数前面的负号是为了\"反转\"$log$函数.\n\n## 正则化以后的梯度下降\n\n在计算偏导数的时候, 利用偏导数的性质, 我们只需要在最后加上正则项的偏导数即可:\n$$\\begin{aligned}\n\\frac{\\partial}{\\partial \\theta_{j}} J(\\theta) \n\u0026=\\frac{\\partial}{\\partial \\theta_{j}}P(\\theta)+\n\\frac\\lambda{2m}\\frac{\\partial}{\\partial \\theta_{j}}\\sum^n_{i=1}\\theta_i^2\\\\\n\u0026=\\frac{\\partial}{\\partial \\theta_{j}}P(\\theta)+\n\\frac\\lambda{m}\\theta_{j} \\\\\n\u0026=\\frac{1}{m} \\sum_{i=1}^{m}\\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right) x^{(i)}_{j}\n+\\frac\\lambda{m}\\theta_{j}\n\\end{aligned}$$\n- 注意正则项在求和符号的外面\n\n带入梯度更新公式有:\n$$\n\\begin{aligned}\nRe\u0026peat\\ \\{\\\\\n\u0026\\theta_{0}:=\\theta_{0}-\\alpha \\frac{1}{m} \\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right) x_{0}^{(i)} \\\\\n\u0026\\theta_{j}:=\\theta_{j}-\\alpha\\left[ \\frac{1}{m} \\sum_{i=1}^{m}\n\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right) x_{j}^{(i)}+\\frac{\\lambda}{m} \\theta_{j}\\right]\n\\quad\\quad j \\in\\{1,2 \\ldots n\\}\n\\\\ \\}\n\\end{aligned}\n$$\n要是把方括号打开, 第二行的更新公式可以变为:\n$$\n\\theta_{j}:=\\theta_{j}\\left(1-\\alpha\\frac\\lambda m\\right)\n-\\alpha\\frac{1}{m} \\sum_{i=1}^{m}\n\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right) x_{j}^{(i)}\n\\quad\\quad j \\in\\{1,2 \\ldots n\\}\n$$\n因为$\\left(1-\\alpha\\frac\\lambda m\\right)$一定小于1, 所以这个更新公式每次都会缩小一点点$\\theta_i$, 而公式的后半部分和没有正则化的公式是完全一样的.\n\n- 上面这部分是直接从线性回归那里拷贝过来的, 两者唯一的不同就是$h(x)$的定义不同\n\n## 证明正则以后还是凸的\n[正则项不影响Logistic回归损失函数凸性](notes/2021/2021.9/正则项不影响Logistic回归损失函数凸性.md)\n","lastmodified":"2022-10-15T14:06:29.482502366Z","tags":null},"/notes/2021/2021.9/Part.21_Neural_Network_IntroductionML_Andrew.Ng.":{"title":"Part.21_Neural_Network_Introduction(ML_Andrew.Ng.)","content":"# Neural Network - Introduction\n\n\u003cdiv align=\"right\"\u003e 2021-09-11\u003c/div\u003e\n\nTags: #NeuralNetwork #MachineLearning \n\n## Non-linear Hypotheses\n![](notes/2021/2021.9/assets/img_2022-10-15-1.png)\n所以随着特征数量的提高, 如果需要更加复杂的假设函数, 特征数量会爆炸式地增长.\n比如在图片处理的时候, 每一个像素都是一个特征(RGB彩色像素甚至是三个特征), 那么特征的数目将会是十分庞大的:\n![](notes/2021/2021.9/assets/img_2022-10-15-2.png)\n\n## The “one learning algorithm” hypothesis\n![](notes/2021/2021.9/assets/img_2022-10-15-3.png)\n\n## Neurons in the Brain\n![The Typical Structure of a Neuron](https://www.cusabio.com/statics/images/Structure-Neuron.jpg)\n- 树突: Dendrite\n- 轴突: Axon\n- 神经脉冲: Spike\n","lastmodified":"2022-10-15T14:06:29.482502366Z","tags":null},"/notes/2021/2021.9/Part.22_Model_Representation-Neural_NetworkML_Andrew.Ng.":{"title":"Part.22_Model_Representation-Neural_Network(ML_Andrew.Ng.)","content":"# Model Representation - NN\n\n\u003cdiv align=\"right\"\u003e 2021-09-11\u003c/div\u003e\n\nTags: #NeuralNetwork #MachineLearning \n\n## Hypothesis\n![](notes/2021/2021.9/assets/Neuron.svg)\n一个神经元(Neuron / Activation Unit)的输出计算公式由如下公式给出:\n$$h_\\Theta(x)=\na = g(x_0\\theta_0+x_1\\theta_1+\\cdots+x_n\\theta_n)$$\n是线性的. ($g(x)$是Sigmoid Function)\n\n神经网络的结构如下:\n![](notes/2021/2021.9/assets/Network.svg)\n可以看出, 每一层(Layer)都有许多节点组成, 所有的e节点一层层组成一个网络, 构成了比线性Hypothesis更复杂的结构.\n\n\u003e **进一步思考:** 为什么一定要是一层一层的呢? 为什么不是图结构的呢?\n\n每一个节点的权重(Weights, 即$\\theta$)都构成一个行向量, 所有的行向量组成这一层的权重矩阵$\\Theta^{(i)}$\n![](notes/2021/2021.9/assets/ThetaWeights.svg)\n\n\n\n以上图为例:\n$$\\begin{aligned}\na_{1}^{(2)} \u0026=g\\left(\\Theta_{10}^{(1)} x_{0}+\\Theta_{11}^{(1)} x_{1}+\\Theta_{12}^{(1)} x_{2}+\\Theta_{13}^{(1)} x_{3}\\right) \\\\\na_{2}^{(2)} \u0026=g\\left(\\Theta_{20}^{(1)} x_{0}+\\Theta_{21}^{(1)} x_{1}+\\Theta_{22}^{(1)} x_{2}+\\Theta_{23}^{(1)} x_{3}\\right) \\\\\na_{3}^{(2)} \u0026=g\\left(\\Theta_{30}^{(1)} x_{0}+\\Theta_{31}^{(1)} x_{1}+\\Theta_{32}^{(1)} x_{2}+\\Theta_{33}^{(1)} x_{3}\\right) \\\\\nh_{\\Theta}(x) \u0026=a_{1}^{(3)}=g\\left(\\Theta_{10}^{(2)} a_{0}^{(2)}+\\Theta_{11}^{(2)} a_{1}^{(2)}+\\Theta_{12}^{(2)} a_{2}^{(2)}+\\Theta_{13}^{(2)} a_{3}^{(2)}\\right)\n\\end{aligned}$$\n\n![](notes/2021/2021.9/assets/WeightsMatrix.svg)\n\nThe dimensions of these matrices of weights is determined as follows:\n\nIf network has $s_j$ units in layer $j$ and $s_{j+1}$ units in layer $j+1$, then $Θ^{(j)}$ will be of dimension $s_{j+1}×(s_j+1)$. If network has $s_j$ units in layer $j$ and $s_{j+1}$ units in layer $j+1$, then $\\Theta^{(j)}$ will be of dimension $s_{j+1} \\times (s_j + 1)$.\n\nThe $+1$ comes from the addition in $\\Theta^{(j)}$ of the \"bias nodes,\" $x_0$ and $\\Theta_0^{(j)}$. In other words the output nodes will not include the bias nodes while the inputs will.","lastmodified":"2022-10-15T14:06:29.482502366Z","tags":null},"/notes/2021/2021.9/Part.23_Forward_Propagation-Neural_NetworkML_Andrew.Ng.":{"title":"Part.23_Forward_Propagation-Neural_Network(ML_Andrew.Ng.)","content":"# Forward Propagation\n\n\u003cdiv align=\"right\"\u003e 2021-09-12\u003c/div\u003e\n\nTags: #NeuralNetwork #MachineLearning \n\n## Vectorized Implementation\n$$\\begin{aligned}\na_{1}^{(2)} \u0026=g\\left(\\Theta_{10}^{(1)} x_{0}+\\Theta_{11}^{(1)} x_{1}+\\Theta_{12}^{(1)} x_{2}+\\Theta_{13}^{(1)} x_{3}\\right) \\\\\na_{2}^{(2)} \u0026=g\\left(\\Theta_{20}^{(1)} x_{0}+\\Theta_{21}^{(1)} x_{1}+\\Theta_{22}^{(1)} x_{2}+\\Theta_{23}^{(1)} x_{3}\\right) \\\\\na_{3}^{(2)} \u0026=g\\left(\\Theta_{30}^{(1)} x_{0}+\\Theta_{31}^{(1)} x_{1}+\\Theta_{32}^{(1)} x_{2}+\\Theta_{33}^{(1)} x_{3}\\right) \\\\\n\\end{aligned}$$\n我们把Sigmoid函数里面的部分用$z$代替:\n$$\\begin{aligned}\n\u0026a_{1}^{(2)}=g\\left(z_{1}^{(2)}\\right) \\\\\n\u0026a_{2}^{(2)}=g\\left(z_{2}^{(2)}\\right) \\\\\n\u0026a_{3}^{(2)}=g\\left(z_{3}^{(2)}\\right)\n\\end{aligned}$$\n同时:\n$$\\begin{aligned}\n\u0026X = \\left[\\begin{array}{cccc}\nx_0  \\\\x_1  \\\\x_2  \\\\x_3  \\\\\n\\end{array}\\right]\n=a^{(1)}\n\\end{aligned}$$\n\n第一步变为:\n$$\\begin{aligned}\n\\Theta^{(1)}X =Z^{(1)}= \\left[\\begin{array}{cccc}\nz_1^{(2)}  \\\\z_2^{(2)}  \\\\z_3^{(2)}  \\\\\n\\end{array}\\right]\n\\end{aligned}$$\n\n![](notes/2021/2021.9/assets/Forward_Propagation_p1.svg)\n带入Sigmoid函数得到第二层activation units的输出值:\n$$\\begin{aligned}\nA^{(1)}= Sigmoid(\\left[\\begin{array}{cccc}\nz_1^{(2)}  \\\\z_2^{(2)}  \\\\z_3^{(2)}  \\\\\n\\end{array}\\right]) = \\left[\\begin{array}{cccc}\na_1^{(2)}  \\\\a_2^{(2)}  \\\\a_3^{(2)}  \\\\\n\\end{array}\\right]\n\\end{aligned}$$\n![](notes/2021/2021.9/assets/Forward_Propagation_p2.svg)\n同时加上Bias Unit.\n\n下一层重复上述步骤, 直到得到最终的结果\n![](notes/2021/2021.9/assets/Forward_Propagation_p3.svg)\n\n前向传播是从输入开始, 逐步向前, 利用权重计算输出的过程.\n\n总览:\n![](notes/2021/2021.9/assets/Forward_Propagation_all.svg)\n\n## 与Logistic Regression的联系\n\n在最后一步的时候, 我们进行的操作其实和Logistic Regression一模一样, 每一个Activation Unit的计算过程都可以理解为一次Logistic Regression.\n\n\n![](notes/2021/2021.9/assets/Pasted%20image%2020210912142652.png)","lastmodified":"2022-10-15T14:06:29.482502366Z","tags":null},"/notes/2021/2021.9/Part.24_Neural_Network-ExamplesML_Andrew.Ng.":{"title":"Part.24_Neural_Network-Examples(ML_Andrew.Ng.)","content":"# Examples of Neural Network\n\n\u003cdiv align=\"right\"\u003e 2021-09-12\u003c/div\u003e\n\nTags: #NeuralNetwork #MachineLearning \n \n - 我们可以通过以下直观的组合过程, 体会神经网络利用Linearity构建Non-Linearity的过程.\n \n## OR function\n![](notes/2021/2021.9/assets/img_2022-10-15-4.png)\n## AND function\n![](notes/2021/2021.9/assets/img_2022-10-15-5.png)\n## NOT function\n![](notes/2021/2021.9/assets/img_2022-10-15-6.png)\n\n## Putting together -\u003e XNOR Function\n![](notes/2021/2021.9/assets/img_2022-10-15-7.png)\n![](notes/2021/2021.9/assets/img_2022-10-15-8.png)","lastmodified":"2022-10-15T14:06:29.482502366Z","tags":null},"/notes/2021/2021.9/Part.25_Multiclass_Classification-Neural_NetworkML_Andrew.Ng.":{"title":"Part.25_Multiclass_Classification-Neural_Network(ML_Andrew.Ng.)","content":"# Multiclass Classification\n\n\u003cdiv align=\"right\"\u003e 2021-09-12\u003c/div\u003e\n\nTags: #MachineLearning #NeuralNetwork \n\n\n\n![](notes/2021/2021.9/assets/img_2022-10-15-9.png)\n\nWe can define our set of resulting classes as y:\n$$\ny^{(i)}=\\left[\\begin{array}{l}\n1 \\\\\n0 \\\\\n0 \\\\\n0\n\\end{array}\\right],\\left[\\begin{array}{l}\n0 \\\\\n1 \\\\\n0 \\\\\n0\n\\end{array}\\right],\\left[\\begin{array}{l}\n0 \\\\\n0 \\\\\n1 \\\\\n0\n\\end{array}\\right],\\left[\\begin{array}{l}\n0 \\\\\n0 \\\\\n0 \\\\\n1\n\\end{array}\\right]\n$$\nEach $y^{(i)}$ represents a different image corresponding to either a car, pedestrian, truck, or motorcycle. **The inner layers, each provide us with some new information which leads to our final hypothesis function.** The setup looks like:\n$$\n\\left[\\begin{array}{c}\nx_{0} \\\\\nx_{1} \\\\\nx_{2} \\\\\n\\cdots \\\\\nx_{n}\n\\end{array}\\right] \\rightarrow\\left[\\begin{array}{c}\na_{0}^{(2)} \\\\\na_{1}^{(2)} \\\\\na_{2}^{(2)} \\\\\n\\ldots\n\\end{array}\\right] \\rightarrow\\left[\\begin{array}{c}\na_{0}^{(3)} \\\\\na_{1}^{(3)} \\\\\na_{2}^{(3)} \\\\\n\\cdots\n\\end{array}\\right] \\rightarrow \\ldots \\rightarrow\\left[\\begin{array}{l}\nh_{\\Theta}(x)_{1} \\\\\nh_{\\Theta}(x)_{2} \\\\\nh_{\\Theta}(x)_{3} \\\\\nh_{\\Theta}(x)_{4}\n\\end{array}\\right]\n$$","lastmodified":"2022-10-15T14:06:29.482502366Z","tags":null},"/notes/2021/2021.9/Part.26_Probabilistic_Interpretation_of_MSEML_Andrew.Ng.":{"title":"Part.26_Probabilistic_Interpretation_of_MSE(ML_Andrew.Ng.)","content":"# 均方差的合理性 - 概率解释\n\n\u003cdiv align=\"right\"\u003e 2021-09-16\u003c/div\u003e\n\nTags: #MachineLearning #Math/Statistics #MeanSquareError #CostFunction \n\n## 之前的一些讨论\n- [Mean_Squared_Error-均方误差](notes/2021/2021.8/Mean_Squared_Error_均方误差.md)\n- [Why_do_cost_functions_use_the_square_error](notes/2021/2021.8/Why_do_cost_functions_use_the_square_error.md)\n\n## CS229 - Probabilistic Interpretation\n\n- [独立同分布-IID](notes/2021/2021.9/独立同分布-IID.md)\n- [[notes/2021/2021.9/正态分布_高斯分布_Normal_Distribution-Gaussian_Distribution]]\n\nPrerequisite: [似然函数](https://zh.wikipedia.org/zh-hans/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0)\n\u003e 我需要进一步学习概率论(贝叶斯统计)\n\n---\n\n下面叙述从概率角度该怎么理解均方差的合理性, 其实是最大似然估计的思想, 和Bayesian估计的思想很像.\n\n- **前提:** 误差$\\epsilon$是**独立同分布(IID)** 的且服从**正态分布(Normal Distribution)** \n\t- **理论基础:** 中心极限定理.\n\n\n我们这样表示输入和输出的关系: 其中$x^{(i)}$是输入, $y^{(i)}$是输出, $\\theta^{T}$是参数向量, $\\epsilon^{(i)}$表示误差.\n$$y^{(i)}=\\theta^{T} x^{(i)}+\\epsilon^{(i)}$$\n\n根据我们的假设, 误差服从正态分布: \n\n$$\n\\begin{aligned}\n\\epsilon^{(i)}\u0026\\sim\\mathcal{N}\\left(0, \\sigma^{2}\\right)\\quad\\Rightarrow\n\\\\p\\left(\\epsilon^{(i)}\\right)\u0026=\\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{\\left(\\epsilon^{(i)}\\right)^{2}}{2 \\sigma^{2}}\\right)\n\\end{aligned}$$\n将输入输出的关系带进去, 可以得到$y^{(i)}$的概率密度分布:\n$$\np\\left(y^{(i)} \\mid x^{(i)} ; \\theta\\right)=\\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{\\left(y^{(i)}-\\theta^{T} x^{(i)}\\right)^{2}}{2 \\sigma^{2}}\\right)\n$$\n\n联系[正态分布的表达式](notes/2021/2021.9/正态分布_高斯分布_Normal_Distribution-Gaussian_Distribution.md#概率密度函数):\n\n$$f(x)=\\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}\\right)$$\n\n其中的$\\theta^{T} x^{(i)}$就是数学期望$\\mu$, 所以我们可以这样表示:\n\n$$y^{(i)} \\mid x^{(i)} ; \\theta \\sim \\mathcal{N}\\left(\\theta^{T} x^{(i)}, \\sigma^{2}\\right)$$\n\n- **下面是关键的一步:**\n\n我们总是想要根据给定的$X$ (the design matrix, which contains all the $x^{(i)}$’s), 调整$\\theta$, 来得到对于输出$Y$的最佳预测.\n\n所以我们给出这个问题的似然函数:\n$$L(\\theta)=L(\\theta ; X, \\vec{y})=p(\\vec{y} \\mid X ; \\theta)$$\n它表示在给定的$\\theta$下, 由训练集里面的$X$得到对应的$Y$的\"似然性/可能性/合理性\"\n\n根据似然函数的定义(就相当于条件概率), 对于我们的训练集, $L(\\theta)$表示如下:\n$$\\begin{aligned}\nL(\\theta) \u0026=\\prod_{i=1}^{m} p\\left(y^{(i)} \\mid x^{(i)} ; \\theta\\right) \\\\\n\u0026=\\prod_{i=1}^{m} \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{\\left(y^{(i)}-\\theta^{T} x^{(i)}\\right)^{2}}{2 \\sigma^{2}}\\right)\n\\end{aligned}$$\n- 注意是连乘, 因为是联合概率\n\n根据**极大似然估计**的思想, 我们想要知道$L(\\theta)$取得最大值的时候的$\\theta$值, 因为最大化这个函数十分复杂, 我们可以取对数(因为对数函数是严格递增的, 而这个的值域也在对数函数的定义域里面)\n\n我们用$\\ell(\\theta)$表示$log\\ likelihood$:\n$$\\begin{aligned}\n\\ell(\\theta) \u0026=\\log L(\\theta) \\\\\n\u0026=\\log \\prod_{i=1}^{m} \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{\\left(y^{(i)}-\\theta^{T} x^{(i)}\\right)^{2}}{2 \\sigma^{2}}\\right) \\\\\n\u0026=\\sum_{i=1}^{m} \\log \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{\\left(y^{(i)}-\\theta^{T} x^{(i)}\\right)^{2}}{2 \\sigma^{2}}\\right) \\\\\n\u0026=m \\log \\frac{1}{\\sqrt{2 \\pi} \\sigma}-\\frac{1}{\\sigma^{2}} \\cdot \\frac{1}{2} \\sum_{i=1}^{m}\\left(y^{(i)}-\\theta^{T} x^{(i)}\\right)^{2}\n\\end{aligned}$$\n所以要最大化$\\ell(\\theta)$相当于最小化 \n$$\\frac{1}{2} \\sum_{i=1}^{m}\\left(y^{(i)}-\\theta^{T} x^{(i)}\\right)^{2}$$\n这和平方误差和只差一个$\\frac 1 m$\n\n\n\u003e Note also that, in our previous discussion, **our final choice of $θ$ did not depend on what was $σ^2$,** and indeed we’d have arrived at the same result even if $σ^2$ were unknown. We will use this fact again later, when we talk about the **exponential family** and **generalized linear models**.","lastmodified":"2022-10-15T14:06:29.482502366Z","tags":null},"/notes/2021/2021.9/Part.27_Locally_Weighted_Linear_RegressionML_Andrew.Ng.":{"title":"Part.27_Locally_Weighted_Linear_Regression(ML_Andrew.Ng.)","content":"# Locally Weighted Linear Regression\n\n\u003cdiv align=\"right\"\u003e 2021-09-30\u003c/div\u003e\n\nTags: #MachineLearning #LinearRegression \n\nAbbreviation: LWR\n\n![](notes/2021/2021.9/assets/img_2022-10-15-10.png)\n\n上图展现了Underfitting \u0026 Overfitting的情况，而 \nLocally weighted linear regression (LWR) is an algorithm which, assuming there is sufficient training data, makes the choice of features less critical.\n\n## 对比\nIn the original linear regression algorithm, to make a prediction at a query point $x$ (i.e., to evaluate $h(x)$ ), we would:\n1. Fit $\\theta$ to minimize $\\sum_{i}\\left(y^{(i)}-\\theta^{T} x^{(i)}\\right)^{2}$.\n2. Output $\\theta^{T} x$.\n\nIn contrast, the locally weighted linear regression algorithm does the following:\n\n1. Fit $\\theta$ to minimize $\\sum_{i} w^{(i)}\\left(y^{(i)}-\\theta^{T} x^{(i)}\\right)^{2}$.\n2. Output $\\theta^{T} x$.\n\n不同:\n- 多了一个$w^{(i)}$, 对于每一次query, 我们都需要重新拟合$\\theta$\n\n## 详细解释\n\n其中$w^{(i)}$的作用是 给最接近这次查询目标$x$的样本点更大的权重(样本越接近查询目标, 那么就可能和查询目标\"更像\")\n\n一个常用的$w^{(i)}$是:\n\n$$w^{(i)}=\\exp \\left(-\\frac{\\left(x^{(i)}-x\\right)^{2}}{2 \\tau^{2}}\\right)$$\n向量形式:\n$$w^{(i)}=\n\\exp\\left(-\\frac{(x^{(i)}-x)^{T}(x^{(i)}-x)}\n{(2 \\tau^{2})}\\right)$$\n\n\n因为$w^{(i)}$的指数部分一定是非正的, 考虑指数函数的负半轴部分:\n![](notes/2021/2021.9/assets/img_2022-10-15-11.png)\n\n观察发现: $x$越接近$x^{(i)}$, 指数部分越接近$0$, $w^{(i)}$越接近$1$, 这部分特征在损失函数里面的权重越大, 会更着重于让这部分的损失越小, 那么就会更偏重于这部分的参数.\n反之, If $w^{(i)}$ is small, then the $\\left(y^{(i)}-\\theta^{T} x^{(i)}\\right)^{2}$ error term will be pretty much ignored in the fit.\n\n\n**bandwidth parameter**: The parameter $\\tau$ controls how quickly the weight of a training example falls off with distance of its $x^{(i)}$ from the query point $x ; \\tau$ is called *the bandwidth parameter*.\n(调整\"到底距离$x$多远才算重要的样本点\")\n\n- 虽然 $w^{(i)}$的这个形式和正态分布很像, 但是其实没有什么联系, 因为 $w^{(i)}$不是随机变量, 也不服从独立同分布.\n\n- LWR是一种**非参数算法**, 因为这个算法的输出和样本还有紧密的联系:\n- Locally weighted linear regression is the first example we’re seeing of a ***non-parametric algorithm***. The (unweighted) linear regression algorithm that we saw earlier is known as a ***parametric learning algorithm***, because it has a fixed, finite number of parameters (the $θ_i$’s), which are fit to the data. Once we’ve fit the $θ_i$’s and stored them away, **we no longer need to keep the training data around to make future predictions**. In contrast, to make predictions using locally weighted linear regression, **we need to keep the entire training set around**. The term “***non-parametric***” (roughly) refers to the fact that the amount of stuff we need to keep in order to represent the hypothesis h grows linearly with the size of the training set.","lastmodified":"2022-10-15T14:06:29.482502366Z","tags":null},"/notes/2021/2021.9/ROC_and_AUC_Graph":{"title":"ROC_and_AUC_Graph","content":"#  ROC AUC Clearly Explained\n\n\u003cdiv align=\"right\"\u003e 2021-09-30\u003c/div\u003e\n\nTags: #MachineLearning #ROC #AUC\n\n- **ROC:** Receiver Operator Characteristic 用来判断哪一个是最好的Classification Threshold.\n- **AUC:** the area under the curve, 用来判断哪一个是最好的模型\n\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/4jRBRDbJemM\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e","lastmodified":"2022-10-15T14:06:29.482502366Z","tags":null},"/notes/2021/2021.9/Underline_in_Markdown_MD%E4%B8%8B%E5%88%92%E7%BA%BF":{"title":"Underline_in_Markdown_MD下划线","content":"# Markdown里面的下划线\n\n\u003cdiv align=\"right\"\u003e 2021-09-18\u003c/div\u003e\n\nTags: #Markdown #HTML\n\n## `\u003cins\u003e \u003c/ins\u003e`\n**Example:**\n```md\n\u003cins\u003eThis line is UNDERLINED!\u003c/ins\u003e \n```\n\n\u003cins\u003eThis line is UNDERLINED!\u003c/ins\u003e \n\n## `\u003cu\u003e \u003c/u\u003e`\n**Example:** \n```md\n\u003cu\u003eThis line is UNDERLINED!\u003c/u\u003e \n```\n\n\u003cu\u003eThis line is UNDERLINED!\u003c/u\u003e \n\n","lastmodified":"2022-10-15T14:06:29.482502366Z","tags":null},"/notes/2021/2021.9/Why_printf_is_called_printf":{"title":"Why_printf_is_called_printf","content":"# Why `printf` is called `printf` \n\n\u003cdiv align=\"right\"\u003e 2021-09-06\u003c/div\u003e\n\nTags: #Programming #English \n\n## What f stands for in `printf` ?\n\nThe title of section **_7.3 Formatted output - printf_** on page 145 of original K\u0026R strongly suggest that the `f` stands for formatted:\n\nSearching in other sources will show that B's and C's `printf` [both seem to originate from BCPL's `writef` function](https://en.wikipedia.org/wiki/Printf_format_string#1960s:_BCPL.2C_ALGOL_68.2C_Multics_PL.2FI) which used already in 1966 the `%` formatting character.\n\nAlso worth to note that Algol68 also adopted `printf` function for formatted output. Yet the formatting logic was a little different.\n\nSource: https://softwareengineering.stackexchange.com/questions/317462/why-isnt-cs-most-basic-printing-function-named-print-instead-of-printf\n\n\n\n","lastmodified":"2022-10-15T14:06:29.482502366Z","tags":null},"/notes/2021/2021.9/i.e._Meaning":{"title":"i.e._Meaning","content":"# Latin Expression: i.e.\n\n\u003cdiv align=\"right\"\u003e 2021-09-11\u003c/div\u003e\n\nTags: #English \n\n## _id est (i.e.)_\n**that is** (literally \"**it is**\")\n\n\"**That is (to say)**\" in the sense of \"**that means**\" and \"**which means**\", or \"**in other words**\", \"**namely**\", or sometimes \"**in this case**\", depending on the context.\n\nused especially in writing before a piece of information that makes the meaning of something clearer or shows its true meaning\n也就是，即\n\n- The hotel is closed during low season, i.e. from October to March. 这家旅馆在淡季，即从10月到3月，关门停业。\n- The price must be more realistic, i.e. lower. 价格必须更切合实际些，也就是说要更低些。\n\n## More info\n- It is OK to use i.e. or e.g. in formal essays.[^1]\n- [List_of_Latin_abbreviations_on_Wikipedia](https://en.wikipedia.org/wiki/List_of_Latin_abbreviations)\n\n\n[^1]: Also, [[notes/2021/2021.9/OK_should_be_in_capital letters(or_okay)]]","lastmodified":"2022-10-15T14:06:29.494502495Z","tags":null},"/notes/2022/2022.1/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%AF%94%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%9B%B4%E9%AB%98%E6%95%88":{"title":"为什么反向传播比前向传播更高效","content":"找到了一个很好的解释: \n\n\u003e 为什么说反向传播算法很高效？要回答这个问题，让我们来考虑另一种计算梯度的方式。设想现在是神经网络研究的早期阶段，大概是在上世纪50年代或60年代左右，并且你是第一个想到使用梯度下降方法来进行训练的人！但是要实现这个想法，你需要一种计算代价函数梯度的方式。你回想了你目前关于演算的知识，决定试一下是否能用链式法则来计算梯度。但是琢磨了一会之后发现，代数计算看起来非常复杂，你也因此有些失落。所以你尝试着寻找另一种方法。你决定把代价单独当做权重的函数$C=C(w)$（我们一会再来讨论偏置）。将权重写作$w1,w2,…$，并且要对某个权重计算$∂C/∂w_j$。一个很明显的计算方式是使用近似：\n\u003e $$\\frac{\\partial C}{\\partial w_{j}} \\approx \\frac{C\\left(w+\\epsilon e_{j}\\right)-C(w)}{\\epsilon}$$\n\u003e 其中$\\epsilon$是一个大于零的极小数, $e_j$是第$j$个方向上的单位向量。换句话说，我们可以通过计算两个差距很小的$w_j$的代价，然后利用上面的等式来估计$∂C/∂w_j$。我们可以利用相同的思想来对偏置求偏导$∂C/∂b$。\n\u003e \n\u003e 这种方式看起来很不错。它的概念很简单，实现起来也很简单，只需要几行代码。当然了，他看起来要比使用链式法则来计算梯度靠谱多了！\n\u003e \n\u003e 然而遗憾的是，虽然这种方式看起来很美好，但当用代码实现之后就会发现，它实在是太慢了。要理解其中的原因的话，设想在我们的神经网络中有一百万个权重，对于每一个不同的权重$w_j$，为了计算$C(w+ϵe_j)$，我们需要计算$∂C/∂w_j$。这意味着为了计算梯度，我们需要计算一百万次代价函数，进而对于每一个训练样例，都需要在神经网络中前向传播一百万次。我们同样需要计算$C(w)$，因此总计需要一百万零一次前向传播。\n\u003e \n\u003e 反向传播的优点在于它仅利用一次前向传播就可以同时计算出所有的偏导$∂C/∂w_j$，随后也仅需要一次反向传播。大致来说，反向传播算法所需要的总计算量与两次前向传播的计算量基本相等（这应当是合理的，但若要下定论的话则需要更加细致的分析。合理的原因在于前向传播时主要的计算量在于权重矩阵的乘法计算，而反向传播时主要的计算量在于权重矩阵转置的乘法。很明显，它们的计算量差不多）。这与基于等式(46)的方法所需要的一百万零一次前向传播相比，虽然反向传播看起来更复杂一些，但它确实更更更更更快。\n\u003e \n\u003e 这种加速方式在1986年首次被人们所重视，极大地拓展了神经网络能够适用的范围，也导致了神经网络被大量的应用。当然了，反向传播算法也不是万能的。在80年代后期，人们终于触及到了性能瓶颈，在利用反向传播算法来训练深度神经网络（即具有很多隐含层的网络）时尤为明显。在本书后面的章节中我们将会看到现代计算机以及一些非常聪明的新想法是如何让反向传播能够用来训练深度神经网络的。\n\nSource: [为什么说反向传播算法很高效 · 神经网络与深度学习](https://hit-scir.gitbooks.io/neural-networks-and-deep-learning-zh_cn/content/chap2/c2s8.html) 原文无公式, 对照[英文原文: Neural networks and deep learning](http://neuralnetworksanddeeplearning.com/chap2.html#in_what_sense_is_backpropagation_a_fast_algorithm)添加了公式","lastmodified":"2022-10-15T14:06:29.510502668Z","tags":null},"/notes/2022/2022.1/%E7%9F%A9%E9%98%B5%E7%9A%84%E4%B8%8D%E5%90%8C%E4%B9%98%E6%B3%95-Hadamard-Kronecker":{"title":"矩阵的不同乘法-Hadamard-Kronecker","content":"# 矩阵的不同乘积\n\n\u003cdiv align=\"right\"\u003e 2022-02-01\u003c/div\u003e\n\nTags: #Matrix #Math \n\n## 一般的矩阵乘法\n![Matrix_multiplication_qtl1](notes/2022/2022.1/assets/Matrix_multiplication_qtl1.svg)\n\n## Hadamard Product $\\odot$\n对应位置的元素相乘\n$$  \\begin{bmatrix}\n    a_{11} \u0026 a_{12} \u0026 a_{13}\\\\\n    a_{21} \u0026 a_{22} \u0026 a_{23}\\\\\n    a_{31} \u0026 a_{32} \u0026 a_{33}\n  \\end{bmatrix} \\circ \\begin{bmatrix}\n    b_{11} \u0026 b_{12} \u0026 b_{13}\\\\\n    b_{21} \u0026 b_{22} \u0026 b_{23}\\\\\n    b_{31} \u0026 b_{32} \u0026 b_{33}\n  \\end{bmatrix} = \\begin{bmatrix}\n    a_{11}\\, b_{11} \u0026 a_{12}\\, b_{12} \u0026 a_{13}\\, b_{13}\\\\\n    a_{21}\\, b_{21} \u0026 a_{22}\\, b_{22} \u0026 a_{23}\\, b_{23}\\\\\n    a_{31}\\, b_{31} \u0026 a_{32}\\, b_{32} \u0026 a_{33}\\, b_{33}\n  \\end{bmatrix}$$\n  -  **Hadamard product** 符号表示为:  $A \\circ B$ or  $A \\odot B$\n![Hadamard_product_qtl1](notes/2022/2022.1/assets/Hadamard_product_qtl1.svg)\n\n## Kronecker Product $\\bigotimes$\n- 克罗内克积（英语：Kronecker product）是两个任意大小的矩阵间的运算，表示为$\\bigotimes$。克罗内克积是外积从向量到矩阵的推广，也是张量积在标准基下的矩阵表示。\n- 类似于外积:\n- ![300](notes/2022/2022.1/assets/Pasted%20image%2020220201164342.png)\n- 对于向量: \n\t- $$\\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix}$$\n\t\t和\n\t\t$$\\mathbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_m \\end{bmatrix}$$\n\t\ttheir outer product or Kronecker product is given by the $n \\times m$ matrix\n\t\t$$\\mathbf{v} \\otimes \\mathbf{w} = \\begin{bmatrix} v_1 w_1 \u0026\u0026 v_1 w_2 \u0026\u0026 \\cdots \u0026\u0026 v_1 w_m \\\\ v_2 w_1 \u0026\u0026 v_2 w_2 \u0026\u0026 \\cdots \u0026\u0026 v_2 w_m \\\\ \\vdots \u0026\u0026 \\vdots \u0026\u0026 \\ddots \u0026\u0026 \\vdots \\\\ v_n w_1 \u0026\u0026 v_n w_2 \u0026\u0026 \\cdots \u0026\u0026 v_n w_m\\end{bmatrix}$$\n- 进一步地, 对于 $2 \\times 2$ 矩阵 $A$ 和 $3 \\times 2$ 矩阵 $B$ 他们的Kronecker Product是$6 \\times 4$ 矩阵: \n\t$$\n\t\\begin{aligned}\n\t\\mathrm{A} \\otimes \\mathrm{B} \u0026=\\left[\\begin{array}{lll}\n\ta_{11} \\mathrm{~B} \u0026 a_{12} \\mathrm{~B} \\\\\n\ta_{21} \\mathrm{~B} \u0026 a_{22} \\mathrm{~B}\n\t\\end{array}\\right] \\\\\n\t\u0026=\\left[\\begin{array}{llll}\n\ta_{11} b_{11} \u0026 a_{11} b_{12} \u0026 a_{12} b_{11} \u0026 a_{12} b_{12} \\\\\n\ta_{11} b_{21} \u0026 a_{11} b_{22} \u0026 a_{12} b_{21} \u0026 a_{12} b_{22} \\\\\n\ta_{11} b_{31} \u0026 a_{11} b_{32} \u0026 a_{12} b_{31} \u0026 a_{12} b_{32} \\\\\n\ta_{21} b_{11} \u0026 a_{21} b_{12} \u0026 a_{22} b_{11} \u0026 a_{22} b_{12} \\\\\n\ta_{21} b_{21} \u0026 a_{21} b_{22} \u0026 a_{22} b_{21} \u0026 a_{22} b_{22} \\\\\n\ta_{21} b_{31} \u0026 a_{21} b_{32} \u0026 a_{22} b_{31} \u0026 a_{22} b_{32}\n\t\\end{array}\\right]\n\t\\end{aligned}\n\t$$\n\n\n### [Tensor Product](D2L-1-What_is_a_tensor.md#Tensor%20Product) vs Kronecker Product\n[Tensors for Beginners 13: Tensor Product vs Kronecker Product - YouTube](https://www.youtube.com/watch?v=qp_zg_TD0qE)\n\n\n","lastmodified":"2022-10-15T14:06:29.510502668Z","tags":null},"/notes/2022/2022.1/D2L-1-What_is_a_tensor":{"title":"D2L-1-What_is_a_tensor","content":"# What is a *tensor*?\n\n\u003cdiv align=\"right\"\u003e 2022-01-25\u003c/div\u003e\n\nTags: #Tensor #DeepLearning\n\n- 最初, **张量**是在物理和数学里面的一个概念, 后来深度学习借用了这个名词, 但是意义有所改变. \n- 在**数学与物理学的语境**里面, \"Tensor\"是一个抽象的概念, 用于表示在坐标变换下的一种不变量, 比如广义相对论中, 坐标的变换会引起观测的时空的变换。而爱因斯坦张量（Einstein tensor）是广义相对论中用来描述时空曲率的一个张量, 不随坐标的变换而变换.\n- 在**深度学习的语境**里面, \"Tensor\"是多维数组的一种表示方式\n\n- 要理清这其中的关系, 还需要稍微深入的认识一下张量是个什么东西.\n\n## 张量 - 含义一\n![](notes/2022/2022.1/assets/img_2022-10-15.png)\n- 在给定坐标系的情况下, 张量可以表示为多维数组的形式. 比如标量(Scalar)就是一个零阶的张量, 向量是一阶的张量, 矩阵是二阶的张量. 在这个含义上, 张量可以看作矩阵的更高阶的推广形式.\n- 但是这种定义张量的方式是片面的, 没有表明张量\"坐标系变换下的不变性\"这一个重要的特征, 后面我们会做简要说明.\n\n## 深度学习里面的张量\n搜索了多方资料[^1], 得知计算机科学里面的\"张量\"只是 N-d Array 的一种表述方法, 可以看作是矩阵表示法的一种多维推广, 而矩阵只是一种组织数字的二位形式而已.\n![](notes/2022/2022.1/assets/img_2022-10-15-1.png)\n### Tensorflow为什么叫\"Tensor\"flow\n根据Tensorflow给官方教程: [^2]\n\u003e 张量是具有统一类型（称为 `dtype`）的多维数组。您可以在 `tf.dtypes.DType` 中查看所有支持的 `dtypes`。\n\u003e 如果您熟悉 [NumPy](https://numpy.org/devdocs/user/quickstart.html)，就会知道张量与 `np.arrays` 有一定的相似性。\n\n和Wikipedia的介绍: [^3]\n\u003e The name TensorFlow derives from the operations that such neural networks perform on multidimensional data arrays, which are referred to as _[tensors](https://en.wikipedia.org/wiki/Tensor \"Tensor\")_.\n\n所以Tensorflow里面的Tensor其实就是多维数组的意思, 不具备数学定义上张量具备的其他性质.\n\n## 张量: 其他含义\n- Youtube上面的博主有一个关于张量的详细介绍[^4], 里面给出了张量的另外两种定义: \n\t- ![](notes/2022/2022.1/assets/img_2022-10-15-2.png)\n\t- ![](notes/2022/2022.1/assets/img_2022-10-15-3.png)\n- 考虑定义2. 从一阶张量, 即向量的角度来看, 不变的是坐标系里面的向量自身, 而能够改变的Components是向量的坐标表示, 这个坐标表示根据坐标系的变化而变换, 变化的方式能够用基变换矩阵来刻画.\n\t- ![](notes/2022/2022.1/assets/img_2022-10-15-4.png)\n- ![500](notes/2022/2022.1/assets/img_2022-10-15-5.png)\n\n### Covector\nCovector在正交基下面就是行向量, 可以看作是一个函数, 这个函数把向量映射成实数(内积).\n![](notes/2022/2022.1/assets/img_2022-10-15-6.png)\n- Covector的可视化: \n![](notes/2022/2022.1/assets/img_2022-10-15-7.png)\n- Covector的变换规则:  $\\epsilon$是covector的基向量, $\\alpha$是covector在$\\epsilon$下的坐标\n![](assets/Pasted%20image%2020220201004409.png)\n![](notes/2022/2022.1/assets/img_2022-10-15-8.png)\n### Tensor Product \n![|300](notes/2022/2022.1/assets/img_2022-10-15-9.png)\n- 一个很好的简介: [The Tensor Product, Demystified](https://www.math3ma.com/blog/the-tensor-product-demystified)\n![](notes/2022/2022.1/assets/img_2022-10-15-10.png)\n### Linear Map\n![](notes/2022/2022.1/assets/img_2022-10-15-11.png)\n\n![](notes/2022/2022.1/assets/img_2022-10-15-12.png)\n\n\n\n\n\n[^1]: 并没有找到严谨的权威资料. 找到的大多是和我有同样疑问的一些人的提问. 比如: [Does the word tensor in TensorFlow have the same meaning with tensor in physics or mathematics? - Quora](https://www.quora.com/Does-the-word-tensor-in-TensorFlow-have-the-same-meaning-with-tensor-in-physics-or-mathematics) [machine learning - Why the sudden fascination with tensors? - Cross Validated](https://stats.stackexchange.com/questions/198061/why-the-sudden-fascination-with-tensors/198064#198064)\n[^2]: [张量简介  |  TensorFlow Core](https://www.tensorflow.org/guide/tensor?hl=zh-cn)\n[^3]: [TensorFlow - Wikipedia](https://en.wikipedia.org/wiki/TensorFlow#History)\n[^4]: [Tensors for Beginners 0: Tensor Definition - YouTube](https://www.youtube.com/watch?v=TvxmkZmBa-k\u0026list=PLJHszsWbB6hrkmmq57lX8BV-o-YIOFsiG\u0026index=2)","lastmodified":"2022-10-15T14:06:29.494502495Z","tags":null},"/notes/2022/2022.1/D2L-10-%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D":{"title":"D2L-10-小批量随机梯度下降","content":"# 小批量随机梯度下降(Mini-Batch)是深度学习默认的求解方法\n\n\u003cdiv align=\"right\"\u003e 2022-02-02\u003c/div\u003e\n\nTags: #MachineLearning #GradientDescent #DeepLearning #Optimization \n\n- [Different_Gradient_Descent_Methods](notes/2021/2021.8/Different_Gradient_Descent_Methods.md)\n\n注意有两个点: 小批量(Mini-Batch), 随机(Stochastic) 梯度下降\n其中: \n- 小批量是因为在整个数据集上面训练一次又慢又贵\n\t- 同时小批量还能从多个相似的数据点中选一个代表来计算, 节约了计算资源\n\t- 但是样本不能太小, 太小的样本不适合用GPU并行计算\n- 随机是选取小样本的方法: 随机选取 ","lastmodified":"2022-10-15T14:06:29.494502495Z","tags":null},"/notes/2022/2022.1/D2L-11-%E6%B3%9B%E5%8C%96Generalization":{"title":"D2L-11-泛化(Generalization)","content":"# Generalization: 泛化\n\n\u003cdiv align=\"right\"\u003e 2022-02-08\u003c/div\u003e\n\nTags: #MachineLearning #DeepLearning \n\n- 线性回归恰好是一个在整个域中只有一个最小值的学习问题。 [^1]但是对于像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。 \n\n- 深度学习实践者很少会去花费大力气寻找这样一组参数，使得在_训练集_上的损失达到最小。 事实上，更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失， 这一挑战被称为_**泛化**_（generalization）。\n\n\n[^1]: 可以证明, 正则后的线性回归损失函数MSE依然是凸的: [正则项不影响线性回归损失函数的凸性](notes/2021/2021.9/正则项不影响线性回归损失函数的凸性.md)","lastmodified":"2022-10-15T14:06:29.494502495Z","tags":null},"/notes/2022/2022.1/D2L-2-Tensor%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C":{"title":"D2L-2-Tensor数据操作","content":"# Tensor数据操作\n\n\u003cdiv align=\"right\"\u003e 2022-02-01\u003c/div\u003e\n\nTags: #Tensor #DeepLearning \n\n## `[行, 列]`\n用冒号可以表示范围, 即一个子区域\n![](notes/2022/2022.1/assets/img_2022-10-15-13.png)\n注意还可以用双冒号间隔选择, 双冒号后的数字为间隔的周期\n![](notes/2022/2022.1/assets/img_2022-10-15-14.png)\n\n","lastmodified":"2022-10-15T14:06:29.494502495Z","tags":null},"/notes/2022/2022.1/D2L-3-%E4%BA%9A%E5%AF%BC%E6%95%B0":{"title":"D2L-3-亚导数","content":"# 亚导数\n\n\u003cdiv align=\"right\"\u003e 2022-02-01\u003c/div\u003e\n\nTags: #Math \n\n- 将导数拓展到了不可微的函数: \n\t- 举例: \n\t\t- ![](notes/2022/2022.1/assets/img_2022-10-15-15.png)\n\t\t- ![](notes/2022/2022.1/assets/img_2022-10-15-16.png)\n\n","lastmodified":"2022-10-15T14:06:29.494502495Z","tags":null},"/notes/2022/2022.1/D2L-4-%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC":{"title":"D2L-4-矩阵求导","content":"# 矩阵求导\n\n\u003cdiv align=\"right\"\u003e 2022-02-01\u003c/div\u003e\n\nTags: #Math #Matrix \n\n- 矩阵的求导一直很让人头疼😖\n- 之前的笔记: [矩阵的求导](notes/2021/2021.8/矩阵的求导.md)\n- 李沐这次的讲解方式不太一样，是从标量逐步推广到矩阵，还蛮清晰的。\n\n## 从标量到向量\n![400](notes/2022/2022.1/assets/img_2022-10-15-17.png)\n其中 $\\Large{\\frac{\\partial y}{\\partial x}, \\frac{\\partial \\mathbf y}{\\partial x}}$都很好理解, 尤其需要注意的是当求导的自变量$\\mathbf x$为向量的时候, 为\n$$\\mathbf{x}=\\left[\\begin{array}{c}\nx_{1} \\\\\nx_{2} \\\\\n\\vdots \\\\\nx_{n}\n\\end{array}\\right] \\quad \\frac{\\partial y}{\\partial \\mathbf{x}}=\\left[\\frac{\\partial y}{\\partial x_{1}}, \\frac{\\partial y}{\\partial x_{2}}, \\ldots, \\frac{\\partial y}{\\partial x_{n}}\\right]$$\n结果变成了一个行向量.\n### Useful Results\n![](notes/2022/2022.1/assets/img_2022-10-15-18.png)\n#### $\\frac{\\partial \\|\\mathbf{x}\\|^2}{\\partial \\mathbf x}=2\\mathbf{x^T}$\n$$\\mathbf{x}=\\left[\\begin{array}{c}\nx_{1} \\\\x_{2} \\\\\\vdots \\\\x_{n}\n\\end{array}\\right]\\quad \\|\\mathbf{x}\\|^2=\\sum^n_1x_i^2$$\n$$\\begin{aligned}\n\\frac{\\partial\\|\\mathbf{x}\\|^2}{\\partial \\mathbf{x}}\u0026=\n\\left[\\frac{\\partial x_i^2}{\\partial x_{1}}, \\frac{\\partial x_i^2}{\\partial x_{2}}, \\ldots, \\frac{\\partial x_i^2}{\\partial\nx_{n}}\\right]\\\\\n\u0026=\\left[2x_1, 2x_{2}, \\ldots, 2x_{n}\\right]\\\\\n\u0026=2\\mathbf{x^T}\n\\end{aligned}$$\n\n\n### 进一步\n而当$\\mathbf{x, y}$都是向量的时候, 可以这样理解:\n$$\\begin{aligned}\n\u0026\\mathbf{x}=\\left[\\begin{array}{c}\nx_{1} \\\\\nx_{2} \\\\\n\\vdots \\\\\nx_{n}\n\\end{array}\\right] \\quad \\mathbf{y}=\\left[\\begin{array}{c}\ny_{1} \\\\\ny_{2} \\\\\n\\vdots \\\\\ny_{m}\n\\end{array}\\right] \\\\\n\\end{aligned}$$\n$$\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}=\n\\left[\\begin{array}{c}\n\\frac{\\partial y_{1}}{\\partial \\mathbf{x}} \\\\\n\\frac{\\partial y_{2}}{\\partial \\mathbf{x}} \\\\\n\\vdots \\\\\n\\frac{\\partial y_{m}}{\\partial \\mathbf{x}}\n\\end{array}\\right]=\\begin{bmatrix}\n\\frac{\\partial y_{1}}{\\partial x_{1}}\u0026 \\frac{\\partial y_{1}}{\\partial x_{2}}\u0026 \\ldots\u0026 \\frac{\\partial y_{1}}{\\partial x_{n}} \\\\\n\\frac{\\partial y_{2}}{\\partial x_{1}}\u0026 \\frac{\\partial y_{2}}{\\partial x_{2}}\u0026 \\ldots\u0026 \\frac{\\partial y_{2}}{\\partial x_{n}} \\\\\n\u0026\u0026\\vdots \\\\\n\\frac{\\partial y_{m}}{\\partial x_{1}}\u0026 \\frac{\\partial y_{m}}{\\partial x_{2}}\u0026 \\ldots\u0026 \\frac{\\partial y_{m}}{\\partial x_{n}}\n\\end{bmatrix}$$\n\n也就是说: \n$$\\mathbf{x} \\in \\mathbb{R}^{n}, \\quad \\mathbf{y} \\in \\mathbb{R}^{m}, \\quad \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} \\in \\mathbb{R}^{m \\times n}$$\n\n### 直观含义\n![](notes/2022/2022.1/assets/img_2022-10-15-19.png)\n求导后得到梯度向量, 为增长最快的方向\n\n### Useful Results\n#### $\\frac{\\partial \\mathbf x}{\\partial \\mathbf x}=I$\n$$\\frac{\\partial \\mathbf{x}}{\\partial \\mathbf{x}}=\n\\begin{bmatrix}\n\\frac{\\partial x_{1}}{\\partial \\mathbf{x}} \\\\\n\\frac{\\partial x_{2}}{\\partial \\mathbf{x}} \\\\\n\\vdots \\\\\n\\frac{\\partial x_{m}}{\\partial \\mathbf{x}}\n\\end{bmatrix}=\\begin{bmatrix}\n\\frac{\\partial x_{1}}{\\partial x_{1}}\u0026 \\frac{\\partial x_{1}}{\\partial x_{2}}\u0026 \\ldots\u0026 \\frac{\\partial x_{1}}{\\partial x_{n}} \\\\\n\\frac{\\partial x_{2}}{\\partial x_{1}}\u0026 \\frac{\\partial x_{2}}{\\partial x_{2}}\u0026 \\ldots\u0026 \\frac{\\partial x_{2}}{\\partial x_{n}} \\\\\n\u0026\u0026\\vdots \\\\\n\\frac{\\partial x_{m}}{\\partial x_{1}}\u0026 \\frac{\\partial x_{m}}{\\partial x_{2}}\u0026 \\ldots\u0026 \\frac{\\partial x_{m}}{\\partial x_{n}}\n\\end{bmatrix}=\n\\begin{bmatrix}\n1 \u00260 \u0026\\ldots \u00260 \\\\\n0 \u00261 \u0026\\ldots \u00260 \\\\\n\\vdots \u0026\\vdots\u0026\\ddots \u0026\\vdots \\\\\n0 \u00260 \u0026\\ldots \u00261\n\\end{bmatrix}\n$$\n\n#### $\\frac{\\partial \\mathbf{Ax}}{\\partial \\mathbf x}=\\mathbf A$\n$$\\mathbf{x} \\in \\mathbb{R}^{n}, \\quad \\mathbf{A} \\in \\mathbb{R}^{m\\times n}$$\n令$\\mathbf r_i$代表矩阵$\\mathbf A$的行向量, 用 $\\langle \\mathbf{a} , \\mathbf{b} \\rangle$表示内积, 第二行为了简化表示, 使用了[Einstein Notation](notes/2022/2022.1/Einstein%20Notation.md): \n\n$$\\begin{aligned}\\frac{\\partial \\mathbf{Ax}}{\\partial \\mathbf{x}}\u0026=\n\\begin{bmatrix}\n\\frac{\\partial \\langle r_1 , \\mathbf x\\rangle}{\\partial \\mathbf{x}} \\\\\n\\frac{\\partial \\langle r_2 , \\mathbf x\\rangle}{\\partial \\mathbf{x}} \\\\\n\\vdots \\\\\n\\frac{\\partial \\langle r_m , \\mathbf x\\rangle}{\\partial \\mathbf{x}} \\\\\n\\end{bmatrix}\\\\\u0026=\n\\large{\\begin{bmatrix}\n\\frac{\\partial a_{1i}x_i}{\\partial x_{1}}\u0026 \n\\frac{\\partial a_{1i}x_i}{\\partial x_{2}}\u0026 \n\\ldots\u0026 \n\\frac{\\partial a_{1i}x_i}{\\partial x_{n}} \\\\\n\\frac{\\partial a_{2i}x_i}{\\partial x_{1}}\u0026 \n\\frac{\\partial a_{2i}x_i}{\\partial x_{2}}\u0026 \n\\ldots\u0026 \n\\frac{\\partial a_{2i}x_i}{\\partial x_{n}} \\\\\n\u0026\u0026\\vdots \\\\\n\\frac{\\partial a_{mi}x_i}{\\partial x_{1}}\u0026 \n\\frac{\\partial a_{mi}x_i}{\\partial x_{2}}\u0026 \n\\ldots\u0026 \n\\frac{\\partial a_{mi}x_i}{\\partial x_{n}}\n\\end{bmatrix}}\\\\\u0026=\n\\begin{bmatrix}\na_{11} \u0026a_{12} \u0026\\ldots \u0026a_{1n} \\\\\na_{21} \u0026a_{22} \u0026\\ldots \u0026a_{2n} \\\\\n\\vdots \u0026\\vdots\u0026\\ddots \u0026\\vdots \\\\\na_{m1} \u0026a_{m2} \u0026\\ldots \u0026a_{mn}\n\\end{bmatrix}=\\mathbf{A}\n\\end{aligned}$$\n#### $\\frac{\\partial \\mathbf{x^{T}A}}{\\partial \\mathbf x}=\\mathbf{A^T}$\n$$\\mathbf{x} \\in \\mathbb{R}^{n}, \\quad \\mathbf{A} \\in \\mathbb{R}^{\\color{red}{n\\times m}}$$\n令$\\mathbf c_i$代表矩阵$\\mathbf A$的列向量:\n$$\\begin{aligned}\\frac{\\partial \\mathbf{x^{T}A}}{\\partial \\mathbf{x}}\u0026=\n\\begin{bmatrix}\n\\frac{\\partial \\langle c_1 , \\mathbf x\\rangle}{\\partial \\mathbf{x}} \\\\\n\\frac{\\partial \\langle c_2 , \\mathbf x\\rangle}{\\partial \\mathbf{x}} \\\\\n\\vdots \\\\\n\\frac{\\partial \\langle c_n , \\mathbf x\\rangle}{\\partial \\mathbf{x}} \\\\\n\\end{bmatrix}\\\\\u0026=\n\\large{\\begin{bmatrix}\n\\frac{\\partial a_{1i}x_i}{\\partial x_{1}}\u0026 \n\\frac{\\partial a_{1i}x_i}{\\partial x_{2}}\u0026 \n\\ldots\u0026 \n\\frac{\\partial a_{1i}x_i}{\\partial x_{n}} \\\\\n\\frac{\\partial a_{2i}x_i}{\\partial x_{1}}\u0026 \n\\frac{\\partial a_{2i}x_i}{\\partial x_{2}}\u0026 \n\\ldots\u0026 \n\\frac{\\partial a_{2i}x_i}{\\partial x_{n}} \\\\\n\u0026\u0026\\vdots \\\\\n\\frac{\\partial a_{mi}x_i}{\\partial x_{1}}\u0026 \n\\frac{\\partial a_{mi}x_i}{\\partial x_{2}}\u0026 \n\\ldots\u0026 \n\\frac{\\partial a_{mi}x_i}{\\partial x_{n}}\n\\end{bmatrix}}\\\\\u0026=\n\\begin{bmatrix}\na_{11} \u0026a_{12} \u0026\\ldots \u0026a_{1n} \\\\\na_{21} \u0026a_{22} \u0026\\ldots \u0026a_{2n} \\\\\n\\vdots \u0026\\vdots\u0026\\ddots \u0026\\vdots \\\\\na_{m1} \u0026a_{m2} \u0026\\ldots \u0026a_{mn}\n\\end{bmatrix}=\\mathbf{A^T}\n\\end{aligned}$$\n##### 特例: $\\frac{\\partial \\mathbf{x^T}}{\\partial \\mathbf x}=\\mathbf{I}$\n$$\\frac{\\partial \\mathbf{x^T}}{\\partial \\mathbf x}=\n\\frac{\\partial\\mathbf{x^{T}I}}{\\partial \\mathbf x}=\\mathbf{I^T}=\\mathbf{I}$$\n\n### 复合运算\n![](notes/2022/2022.1/assets/img_2022-10-15-20.png)\n- 需要注意的是$u, v$都是标量, $\\mathbf{u,v}$则是向量, 上面可以看出**标量相乘的运算律**和**向量相乘(内积)的运算律**是不一样的, 向量内积需要转置, 并且交换位置.\n\n\n## 从向量到矩阵\n![](notes/2022/2022.1/assets/img_2022-10-15-21.png)\n\n","lastmodified":"2022-10-15T14:06:29.494502495Z","tags":null},"/notes/2022/2022.1/D2L-5-%E6%8B%93%E5%B1%95%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99":{"title":"D2L-5-拓展链式法则","content":"# 拓展的求导链式法则\n\n\u003cdiv align=\"right\"\u003e 2022-02-02\u003c/div\u003e\n\nTags: #Math #Derivative \n\n- 从标量到向量, 不仅符号需要对应上, 矩阵的形状也需要对应上\n\n$$\\begin{align}\n\u0026\\frac{\\partial y}{\\partial \\mathbf{x}}=\\frac{\\partial y}{\\partial u} \\frac{\\partial u}{\\partial \\mathbf{x}}\\\\\n\u0026\\small{(1, n)}\\quad{(1,1)(1,n)}\n\\end{align}$$\n\n$$\\begin{align}\n\u0026\\frac{\\partial y}{\\partial \\mathbf{x}}=\\frac{\\partial y}{\\partial \\mathbf{u}} \\frac{\\partial \\mathbf{u}}{\\partial \\mathbf{x}}\\\\\n\u0026\\small{(1, n)}\\quad{(1,k)(k,n)}\n\\end{align}$$\n\n$$\\begin{align}\n\u0026\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}=\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{u}} \\frac{\\partial \\mathbf{u}}{\\partial \\mathbf{x}}\\\\\n\u0026\\small{(m, n)}\\quad{(m,k)(k,n)}\n\\end{align}$$\n\n## 例子: 线性回归\n### 单个样本点的损失\n$$\\mathbf{x}, \\mathbf{w} \\in \\mathbb{R}^{n}, \\quad y \\in \\mathbb{R},\\quad z=(\\langle\\mathbf{x}, \\mathbf{w}\\rangle-y)^{2}$$\n- 计算:  $$\\frac{\\partial z}{\\partial \\mathbf{w}}$$\n\n**解**:\n\n- 利用链式法则: \n\t- 首先进行变量替换: \n\t\t- $a=\\langle\\mathbf{x}, \\mathbf{w}\\rangle$ , 为标量\n\t\t- $b=\\langle\\mathbf{x}, \\mathbf{w}\\rangle-y=a-y$, 为标量\n\t\t- $z=a^2$, 为标量\n\n\t- 我们有: $$\\begin{aligned}\n\t\\frac{\\partial z}{\\partial \\mathbf{w}} \u0026=\\frac{\\partial z}{\\partial b} \\frac{\\partial b}{\\partial a} \\frac{\\partial a}{\\partial \\mathbf{w}} \\\\\n\t\u0026=\\frac{\\partial b^{2}}{\\partial b} \\frac{\\partial (a-y)}{\\partial a} \\frac{\\partial\\langle\\mathbf{x}, \\mathbf{w}\\rangle}{\\partial \\mathbf{w}} \\\\\n\t(\\because \\langle\\mathbf{x}, \\mathbf{w}\\rangle=\\mathbf{x^T\\cdot w})\u0026=2 b \\cdot 1 \\cdot \\mathbf{x}^{T} \\\\\n\t\u0026=2(\\langle\\mathbf{x}, \\mathbf{w}\\rangle-y) \\mathbf{x}^{T}\n\t\\end{aligned}$$\n\t\n### n个样本点的损失\n$$\\mathbf{X} \\in \\mathbb{R}^{m \\times n}, \\quad \\mathbf{w} \\in \\mathbb{R}^{n}, \\quad \\mathbf{y} \\in \\mathbb{R}^{m}$$\n$$\nz=\\|\\mathbf{X} \\mathbf{w}-\\mathbf{y}\\|^{2}\n$$\n- 计算:  $$\\frac{\\partial z}{\\partial \\mathbf{w}}$$\n\n**解**:\n\n- 利用链式法则: \n\t- 首先进行变量替换: \n\t\t- $\\mathbf a=\\mathbf{Xw},\\quad \\mathbf{a} \\in \\mathbb{R}^{m}$\n\t\t- $\\mathbf b=\\mathbf{a-y,\\quad b} \\in \\mathbb{R}^{m}$\n\t\t- $z=\\|\\mathbf{b}\\|^2$, 为标量\n\n\t- 我们有: $$\\begin{aligned}\n\t\\frac{\\partial z}{\\partial \\mathbf{w}} \u0026=\\frac{\\partial z}{\\partial \\mathbf{b}} \\frac{\\partial \\mathbf{b}}{\\partial \\mathbf{a}} \\frac{\\partial \\mathbf{a}}{\\partial \\mathbf{w}} \\\\\n\t\u0026=\\frac{\\partial\\|\\mathbf{b}\\|^{2}}{\\partial \\mathbf{b}} \\frac{\\partial \\mathbf{a}-\\mathbf{y}}{\\partial \\mathbf{a}} \\frac{\\partial \\mathbf{X} \\mathbf{w}}{\\partial \\mathbf{w}} \\\\\n\t\u0026=2 \\mathbf{b}^{T} \\times \\mathbf{I} \\times \\mathbf{X} \\\\\n\t\u0026=2(\\mathbf{X} \\mathbf{w}-\\mathbf{y})^{T} \\mathbf{X}\n\t\\end{aligned}$$\n\t检查维度是否匹配:\n\t$$\\begin{aligned}\n\t\\frac{\\partial z}{\\partial \\mathbf{w}} \u0026=\\frac{\\partial z}{\\partial \\mathbf{b}} \\frac{\\partial \\mathbf{b}}{\\partial \\mathbf{a}} \\frac{\\partial \\mathbf{a}}{\\partial \\mathbf{w}} \\\\\n\t\\small{\\frac{1\\times 1}{n\\times 1}}\u0026=\\small{\\frac{1\\times 1}{m\\times 1}\\frac{m\\times 1}{m\\times 1}\\frac{m\\times 1}{n\\times 1}}\\\\\\small{1\\times n}\u0026=\\small{(1\\times m)\\ (m\\times m) (m\\times n)}\\end{aligned}$$\n\t\n\n\n\t\n\n","lastmodified":"2022-10-15T14:06:29.494502495Z","tags":null},"/notes/2022/2022.1/D2L-6-%E8%AE%A1%E7%AE%97%E5%9B%BE":{"title":"D2L-6-计算图","content":"# 计算图\n\n\u003cdiv align=\"right\"\u003e 2022-02-02\u003c/div\u003e\n\nTags: #MachineLearning #DeepLearning \n\n- 将计算表示为一个无环图\n\n[例子 线性回归](notes/2022/2022.1/D2L-5-拓展链式法则.md#例子%20线性回归):\n![](notes/2022/2022.1/assets/img_2022-10-15-22.png)\n\n- 计算图有两种构造方法:\n\t- **显式构造**\n\t\t- 主要应用于:  `Tensorflow/Theano/MXNet`\n\t\t- 例子: \n\t```python\n\tfrom mxnet import sym\n\ta = sym.var()\n\tb = sym.var()\n\tc = 2 * a + b\n\t# bind data into a and b later\n\t```\n\t- **隐式构造**\n\t\t- 主要应用于: `PyTorch/MXNet`\n\t\t- 例子: \n\t```python\n\tfrom mxnet import autograd, nd\n\twith autograd.record():\n\ta = nd.ones((2,1))#创建两个二维向量\n\tb = nd.ones((2,1))\n\tc = 2 * a + b\n\t```\n\t","lastmodified":"2022-10-15T14:06:29.494502495Z","tags":null},"/notes/2022/2022.1/D2L-7-%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC":{"title":"D2L-7-自动求导","content":"# 自动求导\n\n\u003cdiv align=\"right\"\u003e 2022-02-02\u003c/div\u003e\n\nTags: #DeepLearning \n\n在机器学习里面, 深度学习框架可以帮我们自动求导, 计算梯度.\n\n## 自动求导的两种方式\n基于链式法则, 求导有两种顺序:\n- 正向累积 \n$$\\frac{\\partial y}{\\partial x}=\\frac{\\partial y}{\\partial u_{n}}\\left(\\frac{\\partial u_{n}}{\\partial u_{n-1}}\\left(\\ldots\\left(\\frac{\\partial u_{2}}{\\partial u_{1}} \\frac{\\partial u_{1}}{\\partial x}\\right)\\right)\\right)$$\n- 反向累积、又称反向传递\n$$\n\\frac{\\partial y}{\\partial x}=\\left(\\left(\\left(\\frac{\\partial y}{\\partial u_{n}} \\frac{\\partial u_{n}}{\\partial u_{n-1}}\\right) \\ldots\\right) \\frac{\\partial u_{2}}{\\partial u_{1}}\\right) \\frac{\\partial u_{1}}{\\partial x}\n$$\n\n## 反向累积/传播\n反向传播分为两个阶段: 正向阶段和反向阶段\n\n例子: $$z=(\\langle\\mathbf{x}, \\mathbf{w}\\rangle-y)^{2}$$\n### 正向阶段\n![400](notes/2022/2022.1/assets/img_2022-10-15-23.png)\n根据[计算图](notes/2022/2022.1/D2L-6-计算图.md), 我们先按照箭头的方向**向前**计算一次, 并且存储每一步的中间结果. 在反向计算的时候, 我们需要用这些中间结果来计算梯度.\n\n### 反向阶段\n![Backpropagation](notes/2022/2022.1/assets/img_2022-10-15.gif)\n反向传播通过不断调用前一步的结果, 从前向后计算每一步的偏导数.\n\n\n## 为什么反向传播比前向传播更高效?\n![为什么反向传播比前向传播更高效](notes/2022/2022.1/为什么反向传播比前向传播更高效.md)\n\n\n## PyTorch的Autograd\n[PyTorch Autograd Explained - In-depth Tutorial - YouTube](https://www.youtube.com/watch?v=MswxJw-8PvE)","lastmodified":"2022-10-15T14:06:29.494502495Z","tags":null},"/notes/2022/2022.1/D2L-8-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E5%8F%AF%E4%BB%A5%E7%9C%8B%E4%BD%9C%E5%8D%95%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C":{"title":"D2L-8-线性模型可以看作单层神经网络","content":"\n$$y=w_{1} x_{1}+w_{2} x_{2}+\\ldots+w_{n} x_{n}+b$$\n![](notes/2022/2022.1/assets/img_2022-10-15-24.png)\n\nLinks: \n- [Part.3_Linear_Regression(ML_Andrew.Ng.)](notes/2021/2021.8/Part.3_Linear_Regression(ML_Andrew.Ng.).md)\n- [Part.24_Neural_Network-Examples(ML_Andrew.Ng.)](notes/2021/2021.9/Part.24_Neural_Network-Examples(ML_Andrew.Ng.).md)","lastmodified":"2022-10-15T14:06:29.494502495Z","tags":null},"/notes/2022/2022.1/D2L-9-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E6%96%B9%E5%90%91":{"title":"D2L-9-梯度下降的方向","content":"# 梯度下降的方向是梯度的反方向\n\n\u003cdiv align=\"right\"\u003e 2022-02-02\u003c/div\u003e\n\nTags: #GradientDescent #DeepLearning #MachineLearning \n\n- 梯度是一个函数**增长最快**的方向, 通常我们都是想获得**损失函数的最小值**, 所以需要沿着梯度的反方向来移动.\n\n- 注意这并不是一定的, [梯度下降](notes/2021/2021.8/Part.5_Gradient_Descent(ML_Andrew.Ng.).md)/上升只是一种优化方法而已, 如果我们想要优化的目标函数取得最大值, 那么就应该沿着梯度的方向变化.\n","lastmodified":"2022-10-15T14:06:29.494502495Z","tags":null},"/notes/2022/2022.1/Dummy_Variables":{"title":"Dummy_Variables","content":"# Dummy Variable\n\n\u003cdiv align=\"right\"\u003e 2022-02-10\u003c/div\u003e\n\nTags: #Math/Statistics \n\n[Dummy variable (statistics) - Wikipedia](https://en.wikipedia.org/wiki/Dummy_variable_(statistics))\n\u003e In statistics and econometrics, particularly in regression analysis, a dummy variable is one that takes only the value 0 or 1 to indicate the absence or presence of some categorical effect that may be expected to shift the outcome.\n\nDummy Variable 里面的Dummy可不是傻子的意思, 这里的Dummy的意思是\"假的, 辅助的\", 意思是这个变量仅仅用于表示某个因素存在与否, 而没有实际意义, 很类似于[Kronecker delta](notes/2022/2022.1/Kronecker%20delta%20-%20克罗内克δ函数.md) $\\delta_{ij}$在线性代数中的应用.\n\n一个实例可以很好的解释Dummy Variable的作用: \n\n- **Section 1**\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/9yTui_LoSOc?start=175\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n- **Section 2**\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/9yTui_LoSOc?start=944\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e","lastmodified":"2022-10-15T14:06:29.494502495Z","tags":null},"/notes/2022/2022.1/Einstein-Notation":{"title":"Einstein Notation","content":"# Einstein Notation\n\n\u003cdiv align=\"right\"\u003e 2022-02-01\u003c/div\u003e\n\nTags: #Tensor #EinsteinNotation \n\n- [Einstein notation - Wikipedia](https://en.wikipedia.org/wiki/Einstein_notation)\n- 一种求和符号的简化书写方式, 在张量运算中经常使用, 由爱因斯坦发明.\n\n\u003ciframe width=\"720\" height=\"480\" src=\"https://www.youtube.com/embed/SSSGA6ohkfw?start=340\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n\n\u003ciframe width=\"720\" height=\"480\" src=\"https://www.youtube.com/embed/CLrTj7D2fLM\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n\n","lastmodified":"2022-10-15T14:06:29.494502495Z","tags":null},"/notes/2022/2022.1/Kronecker-delta-%E5%85%8B%E7%BD%97%E5%86%85%E5%85%8B%CE%B4%E5%87%BD%E6%95%B0":{"title":"Kronecker delta - 克罗内克δ函数","content":"# Kronecker delta\n\n\u003cdiv align=\"right\"\u003e 2022-02-10\u003c/div\u003e\n\nTags: #Math \n\n$$\\delta_{ij} = \\left\\{\\begin{matrix} \n1 \u0026 (i=j)  \\\\ \n0 \u0026 (i \\ne j) \\end{matrix}\\right.$$\n\n在线性代数中，单位矩阵可以写作 $\\left(\\delta_{i j}\\right)_{i, j=1}^{n}$\n\n","lastmodified":"2022-10-15T14:06:29.494502495Z","tags":null},"/notes/2022/2022.1/One-hot_Encoding-%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81":{"title":"One-hot_Encoding-独热编码","content":"# 独热编码 One-hot Encoding\n\n\u003cdiv align=\"right\"\u003e 2022-02-09\u003c/div\u003e\n\nTags: #One-hot #DeepLearning #Encoding\n\n\n$$\\begin{array}{ll}\n    apple \u0026=\\quad [\\ 1\\quad 0\\quad 0\\ ] \\\\\n    banana \u0026=\\quad [\\ 0\\quad 1\\quad 0\\ ] \\\\\n    pineapple \u0026=\\quad [\\ 0\\quad 0\\quad 1\\ ]\n\\end{array}$$\n\n\u003e [Dive into Deep Learning](https://zh-v2.d2l.ai/chapter_linear-networks/softmax-regression.html#subsec-classification-problem): \n\u003e 统计学家很早以前就发明了一种表示分类数据的简单方法：_独热编码_（one-hot encoding）: \n\u003e [[notes/2022/2022.1/Dummy_Variables]]","lastmodified":"2022-10-15T14:06:29.494502495Z","tags":null},"/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%82%E6%95%B0%E4%B8%8D%E8%83%BD%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%BA%E5%90%8C%E4%B8%80%E4%B8%AA%E5%B8%B8%E6%95%B0":{"title":"为什么参数不能初始化为同一个常数","content":"# 深度学习: 参数化所固有的对称性\n\n\u003cdiv align=\"right\"\u003e 2022-02-19\u003c/div\u003e\n\nTags: #DeepLearning \n\n- [4.8.1.3. 打破对称性](https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html#id5 \"Permalink to this headline\")\n\n\u003e - 对于一个多层感知机, 假设隐藏层只有两个单元, 输出层只有一个输出单元。 想象一下，如果我们将隐藏层的所有参数初始化为 $W^{(1)}=c$， $c$ 为常量，会发生什么？ \n\u003e - 在这种情况下，在前向传播期间，两个隐藏单元采用相同的输入和参数， 产生相同的激活，该激活被送到输出单元。 在反向传播期间，根据参数 $W^{(1)}$ 对输出单元进行微分， 得到一个梯度，其元素都取相同的值。 因此，在一次梯度下降（例如，小批量随机梯度下降）之后， $W^{(1)}$ 的所有元素仍然有相同的值。 而每一次迭代永远都不会打破对称性，这也意味着隐藏层的行为就好像只有一个单元, 我们永远也无法利用多层网络强大的表达能力。 \n\u003e - 请注意，虽然小批量随机梯度下降不会打破这种对称性，但暂退法 [(Dropout-丢弃法)](notes/2022/2022.2/D2L-23-Dropout-丢弃法.md) 正则化可以。","lastmodified":"2022-10-15T14:06:29.606503702Z","tags":null},"/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE":{"title":"为什么Softmax回归不用MSE","content":"# 为什么Softmax (或者Logistic) 不用MSE作为损失函数?\n\n\u003cdiv align=\"right\"\u003e 2022-02-28\u003c/div\u003e\n\nTags: #DeepLearning #MachineLearning #SoftmaxRegression #LogisticRegression #CostFunction #MeanSquareError #CrossEntropy \n\n- **回顾:** \n\t- MSE假设样本误差i.i.d., 并且服从正态分布, 最小化MSE等价于极大似然估计. 通常用于回归问题. MSE基于输出与真实值的欧氏距离.\n\t- 最小化Cross Entropy等价于最小化KL散度, 相当于最小化输出概率分布与真实概率分布之间的区别. 通常用于分类问题\n\n- 在分类问题里面, 我们不使用MSE的原因主要有: \n\t- **分类问题里面MSE并不是一个凸函数**, 这可能会导致算法无法学习到最优的参数.\n\t- 交叉熵梯度的**变化趋势**与**值域**都更理想:\n\n\t- 交叉熵的**计算更简单**.\n\n- 下面我们依次探究这几点, 由于Logistic回归就是特殊的Softmax回归, 我们先讨论逻辑斯蒂回归, 然后再进一步讨论Softmax.\n\n## Logistic\n- 我们先进行公式推导, 然后进行相应的解释.\n\n### 公式推导\n- 对于一个样本 $(\\mathbf{x}, y)$ 回忆 [Logistic Regression的Hypothesis](notes/2021/2021.8/Part.12_Logistic_Regression(ML_Andrew.Ng.).md) (我们将偏置直接加到 $\\mathbf{w, x}$ 里面): \t\n\t$$h(\\mathbf{x})=\\operatorname{Sigmoid}(\\mathbf{w^T x})$$\n- **损失函数为交叉熵:** \n\t[Part.13_Cost_Function-Logistic_Regression(ML_Andrew.Ng.)](notes/2021/2021.8/Part.13_Cost_Function-Logistic_Regression(ML_Andrew.Ng.).md)]\n\t$$L\\left(h(\\mathbf{x}), y\\right)=-y \\log \\left(h(\\mathbf{x})\\right)-(1-y) \\log \\left(1-h(\\mathbf{x})\\right)$$\n\t- 对其求导[Part.14_Logistic_Regression\u0026Gradient_Descent(ML_Andrew.Ng.)](notes/2021/2021.8/Part.14_Logistic_Regression\u0026Gradient_Descent(ML_Andrew.Ng.).md)\n\t$$\\frac{\\partial}{\\partial w_{j}} L\\left(h(\\mathbf{x}), y\\right)\n\t=\\left(y-h(\\mathbf x)\\right) x_{j}$$\n\t- 对其求二阶导:\n\t\t[证明Logistic回归的损失函数是凸函数](notes/2021/2021.9/证明Logistic回归的损失函数是凸函数.md)\n\t$$\\frac{\\partial^2}{\\partial w_{j}^2} L\\left(h(\\mathbf{x}), y\\right)\n\t=h(\\mathbf x)\\left(1-h(\\mathbf x)\\right) x^2_{j}$$\n\t- 需要注意的是上面都是对于一个样本的情况, [对于多个样本需要加上求和符号](notes/2022/2022.2/可视化损失函数的困难.md)\n\t\n- **损失函数为MSE:** \n\t$$L\\left(h(\\mathbf{x}), y\\right)=\\frac 1 2(y-h(\\mathbf{x}))^2$$\n\t- 对其求导: \n\t$$\\begin{aligned}\\frac{\\partial}{\\partial w_{j}} L\\left(h(\\mathbf{x}), y\\right)\u0026=-(y-h(\\mathbf{x}))\\frac{\\partial \\operatorname{Sigmoid}(\\mathbf{w^T x})}{\\partial w_{j}}\\\\\n\t\u0026=-(y-h(\\mathbf{x}))h'(\\mathbf{x})\\frac{\\partial\\mathbf{w^T x}}{\\partial w_{j}}\\\\\n\t\u0026=-(y-h(\\mathbf{x}))h'(\\mathbf{x})x_{j}\\\\\n\t\\text{(变成预测值的函数)}\u0026=-(y-h(\\mathbf{x}))(1-h(\\mathbf{x}))h(\\mathbf{x})x_{j}\n\t\\end{aligned}$$ \n\t- 求二阶导数(过程略):\n\t\t$$\\begin{aligned}\\frac{\\partial^2}{\\partial w_{j}^2} L\\left(h(\\mathbf{x}), y\\right)\u0026=-\\left(3\\cdot h^2(\\mathbf{x})-2\\cdot h(\\mathbf{x}) -2y\\cdot h(\\mathbf{x})+y\\right)h'(\\mathbf{x})(x_j)^2\n\t\\end{aligned}$$ \n\t\n### 解释 \n- 可视化的函数页面: [Cross Entropy Loss and MSE – GeoGebra](https://www.geogebra.org/m/buakfzn8)\n\t- 注: 这个例子假设有三个样本: $(a,1);(b,1);(c,0)$, 其中输入: $abc$ 都是可以调节的\n\n#### 分类问题里面MSE并不是一个凸函数, 这可能导致算法无法学习到最优的参数.\n我们作出MSE的二阶导函数图像如下: \n![](notes/2022/2022.2/assets/img_2022-10-15-1.png)\n可以看到并不是恒为非负的, 这说明函数并不是凸函数.\n\n- 而交叉熵的二阶导图像为: \n![](notes/2022/2022.2/assets/img_2022-10-15-2.png)\n- 这是一个凸函数: [详细证明见: 证明Logistic回归的损失函数是凸函数](notes/2021/2021.9/证明Logistic回归的损失函数是凸函数.md)\n\n#### 交叉熵梯度的变化趋势与值域都更理想\n- **值域:** 相比均方误差，交叉熵的梯度大小更均匀,  而MSE梯度过小且不够均匀, 容易出现梯度消失的问题.\n- 下图是原函数的图像. 可以看到交叉熵的坡度很\"平稳\", 梯度下降能够愉快地滑到最低值.\n![](notes/2022/2022.2/assets/img_2022-10-15-3.png)\n\n- 我们再做出其梯度图像: \n\t![](notes/2022/2022.2/assets/img_2022-10-15-4.png)\n- 我们发现: MSE的梯度不仅范围小, 而且值的变化还很反常, 在距离最优值(梯度零点)很远的地方反而变得很小. \n\t- 这导致的后果是: 如果参数初始化在距离最优值很远的地方, 训练没有进展. \n\n- 我们再从数学上分析一下其中的原因: \n\t- **交叉熵的一阶导数**为: \t$$\\frac{\\partial}{\\partial w_{j}}L\\left(h(\\mathbf{x}), y\\right)=\\left(y-h(\\mathbf x)\\right) x_{j}$$\n\t- **MSE的一阶导数**为: $$\\frac{\\partial}{\\partial w_{j}} L\\left(h(\\mathbf{x}), y\\right)=(y-h(\\mathbf{x}))h'(\\mathbf{x})x_{j}$$ \n\t- MSE的梯度表达式里面多了一项Sigmoid函数的导数, 而Sigmoid函数的导数长下面这个样子(红色虚线): \n\t\t![](notes/2022/2022.2/assets/img_2022-10-15-5.png)\n\t\t可以看到在绝对值较大的地方, Sigmoid的导数会变得很小, 这也是MSE梯度不理想的原因.\n- (来自一篇博客[^2]) ....Finally, it reminds me of something said in DL-book by Bengio, 'You must have some log form loss to cancel the exponential part when your output is sigmoid'\n\t- 如果你网络的最后一层是Sigmoid, 那么你的损失函数需要一些 $\\log$ 的部分来抵消掉(Sigmoid里面的)指数部分.\n\n#### 交叉熵的计算更简单\n- 这是因为: \n\t- **交叉熵的一阶导数**为: \t$$\\frac{\\partial}{\\partial w_{j}}L\\left(h(\\mathbf{x}), y\\right)=\\left(y-h(\\mathbf x)\\right) x_{j}$$\n\t- **MSE的一阶导数**为: $$\\frac{\\partial}{\\partial w_{j}} L\\left(h(\\mathbf{x}), y\\right)=(y-h(\\mathbf{x}))h'(\\mathbf{x})x_{j}$$ \n- 很明显交叉熵少了一个 $h'(\\mathbf{x})$\n\n\n#### 另注\n- 在有些解说里面, 作者做出的图像是损失函数关于模型输出的曲线, 我认为这虽然有一定道理, 但也是不太合理的: \n\t![如何得到损失函数的图像](notes/2022/2022.2/assets/img_2022-10-15-6.png)\n\t- 我们可以看到要得到损失与参数的关系, 不仅需要经过损失函数, 还需要经过模型, 而模型并不一定是线性的. 所以\"预测值-损失\"图像并不能真实地反映损失函数是如何影响梯度的大小与变化速度, 进而影响参数更新过程的.\n\n- 举个例子: 下图展示了真实值 $y=1$ 时, 梯度 $\\frac{\\partial}{\\partial w_{j}} L\\left(h(\\mathbf{x}), y\\right)$ 相对于模型预测值 $h(\\mathbf{x})$ 的变化图像: \n\t- 蓝色为交叉熵, 绿色为MSE\t\n\t![350](notes/2022/2022.2/assets/img_2022-10-15.png)\n- 虽然看起来交叉熵的变化更平稳, 而且单调性很好, 但是因为自变量是模型输出, 我们并不知道实际上参数更新时到底平不平稳. 而且这个图给人一种错觉: 好像要是模型一开始的输出小于0.25, 那么就永远也学习不到正确的参数了.\n\n- 下图是一个正确的例子, 图中也可以看出交叉熵作为损失的优越性.[^1]\n![500](notes/2022/2022.2/assets/img_2022-10-15-7.png)\n\n\n\n## Softmax\nSoftmax的情况太复杂了, 我们给出一些其他论文里面的论证: \n\n- 一个验证实验的结果: [Sigmoid-MSE vs. Softmax Cross-Entropy – Weights \u0026 Biases](https://wandb.ai/ayush-thakur/dl-question-bank/reports/Sigmoid-MSE-vs-Softmax-Cross-Entropy--VmlldzoyMDA3ODQ)\n\t- MSE也能够训练, 但是精确度要低一点\n\n- 有一篇论文[^4]探究了交叉熵与平方损失的不同, 对于梯度, MSE在Softmax里面有同样的问题: 距离最优值较远的时候梯度也很小: \n![](notes/2022/2022.2/assets/img_2022-10-15-37.png)\n\n## Others\n- 网上看到这张图蛮有意思的, 但是他好像说的不太对[^3]\n![](notes/2022/2022.2/assets/img_2022-10-15-8.png)\n\n[^1]:这篇文章是Xavier初始化的文章(zotero://select/items/@glorot2010understanding)\n[^2]: [Sigmoid in cross-entropy and mean-squared-error](http://antosny.github.io/2017/10/16/sigmoid-cross-entropy-mean-sqared-error/)\n[^3]: [损失函数的可视化——浅论模型的参数空间与正则_机器学习杂货铺1号店-CSDN博客](https://blog.csdn.net/LoseInVain/article/details/83473975)\n[^4]:P. Golik, P. Doetsch, and H. Ney, “Cross-entropy vs. squared error training: a theoretical and experimental comparison,”(zotero://select/items/@Golik2013CrossentropyVS) 2013. doi: [10.21437/Interspeech.2013-436](https://doi.org/10.21437/Interspeech.2013-436) . ","lastmodified":"2022-10-15T14:06:29.606503702Z","tags":null},"/notes/2022/2022.2/%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2-Affine_Transformation":{"title":"仿射变换-Affine_Transformation","content":"# 仿射变换 Affine Transformation\n\n\u003cdiv align=\"right\"\u003e 2022-02-10\u003c/div\u003e\n\nTags: #Math/LinearAlgebra \n\n仿射变换就是平移后的线性变换:\n\n![](notes/2022/2022.2/assets/img_2022-10-15.gif)\n[^1]\nHere is an Interaction: \n[Affine transformations / Kjerand Pedersen / Observable](https://observablehq.com/@kjerandp/affine-transformations)\n![](notes/2022/2022.2/assets/img_2022-10-15-1.gif)\n\n\n- 有趣的是, 我们可以在高维度通过线性变换来完成仿射变换[^2]\n\n\u003ciframe src=\"https://commons.wikimedia.org/wiki/File:Affine_transformations.ogv?embedplayer=yes\" width=\"512\" height=\"512\" frameborder=\"0\" \u003e\u003c/iframe\u003e \n\n[^3]\n\n\u003e - 普通矩阵向量乘法总将原点映射至原点，因此无法呈现平移（原点必须映射至其他点）。借由于所有向量上扩增一坐标 “1”，我们将原空间映至更高维空间的一个子集合以进行变换。在该空间中，原本之空间占有了扩长坐标一的1的子集合。 因此原空间的原点可在(0,0, ... 0, 1)。原空间的平移可借由更高维度空间的线性变换来达成（即为[错切变换](https://zh.wikipedia.org/wiki/%E9%8C%AF%E5%88%87 \"错切\")）。在高维度中的坐标即为 [齐次坐标](https://zh.wikipedia.org/wiki/%E9%BD%8A%E6%AC%A1%E5%BA%A7%E6%A8%99 \"齐次坐标\")的一例。 假如原空间为欧几里德空间), 则更高维空间为[实射影空间](https://zh.wikipedia.org/wiki/%E5%AE%9E%E5%B0%84%E5%BD%B1%E7%A9%BA%E9%97%B4 \"实射影空间\")。\n\u003e - 使用齐次坐标的优点为，借由相对应矩阵之乘积，可将任意数目的仿射变换[结合](https://zh.wikipedia.org/wiki/%E5%A4%8D%E5%90%88%E5%87%BD%E6%95%B0 \"复合函数\")为一。此性质被大量运用于 [计算机图形](https://zh.wikipedia.org/wiki/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2 \"计算机图形\"), [计算机视觉](https://zh.wikipedia.org/wiki/%E8%A8%88%E7%AE%97%E6%A9%9F%E8%A6%96%E8%A6%BA \"计算机视觉\") 与 [机器人学](https://zh.wikipedia.org/wiki/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AD%A6 \"机器人学\")。[^3]\n\n\n\n\n\n[^1]: [Understanding Transformations in Computer Vision: | by Felix Liu | Towards Data Science](https://towardsdatascience.com/understanding-transformations-in-computer-vision-b001f49a9e61)\n[^2]: [如何通俗地讲解「仿射变换」这个概念？ - 知乎](https://www.zhihu.com/question/20666664)\n[^3]: [仿射变换 - 维基百科，自由的百科全书](https://zh.wikipedia.org/zh-cn/%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2)","lastmodified":"2022-10-15T14:06:29.606503702Z","tags":null},"/notes/2022/2022.2/%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%8F%82%E6%95%B0%E5%A4%A7%E5%B0%8F%E7%9A%84%E8%AE%A1%E7%AE%97":{"title":"卷积层参数大小的计算","content":"![卷积层权重大小的计算](notes/2022/2022.2/assets/卷积层权重大小的计算.svg)\n- **输入通道数**决定了每一个卷积核的\"厚度\"\n- **输出通道数**决定了卷积核的\"个数\"\n- **卷积核的大小**则和输出的尺寸密切相关\n- 其实 **层数(厚度)** 和 **面积(尺寸)** 没有什么联系, 是两个比较独立的参数\n","lastmodified":"2022-10-15T14:06:29.606503702Z","tags":null},"/notes/2022/2022.2/%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%9B%B0%E9%9A%BE":{"title":"可视化损失函数的困难","content":"## 可视化损失函数的困难\n- 我们先来看看Loss Function的形式: $$L_{\\theta}\\ (\\hat{y},\\ y)=\\cdots$$\n\t- 里面有四个组成部分: $L, \\theta, \\hat{y}$ 和 $y$\n\t- 需要牢记的一点是: 选择Loss Function $L$ 的时候选择的是\"计算方式\", 也就是我们怎样计算模型输出 $\\hat y$ 和真实值 $y$, 而损失函数实际上是关于权重 $\\theta$ 的函数, 我们在反向传播的时候是对学习目标 $\\theta$ 求梯度, 而直观的梯度下降法也是在 $L$ 关于 $\\theta$ 的图像上逐步下降的.\n\t- 样本 $y$ 和损失函数 $L$ 影响着整个梯度地形图的形状, 而模型输出 $\\hat{y}$ 和权重 $\\theta$ 则代表等高线图里面的一个小人, 在训练时不断移动.\n\t- 容易忽略的一点是: 影响\"损失地形图\"的不仅有 $L$,  还有样本 $y$. 没有训练样本, 损失函数只是一个计算方式而已.\n\n\t- 要得到\"损失地形图\", 需要经过如下几步: \n\t\t![如何得到损失函数的图像](notes/2022/2022.2/assets/img_2022-10-15-6.png)\n\t\t- 可以看到: 因为参数 $\\theta$ 需要经过**模型**和**损失函数**两次变换, 而且模型的参数 $\\theta$ 通常数目庞大, 所以实际图像是非常复杂的.\n\n\t\t- 尽管实际图像非常复杂, 我们在学习的时候还是可以构造简单的特例, 来理解模型训练中的一些问题, 比如: \n\t\t\t- [为什么Softmax回归不用MSE](notes/2022/2022.2/为什么Softmax回归不用MSE.md)","lastmodified":"2022-10-15T14:06:29.606503702Z","tags":null},"/notes/2022/2022.2/%E5%A5%BD%E7%9A%84%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%B9%E8%B4%A8":{"title":"好的预测模型的特质","content":"# \"好\"的预测模型的特征\n\n\u003cdiv align=\"right\"\u003e 2022-02-14\u003c/div\u003e\n\nTags: #DeepLearning \n\n- **泛化性的角度:** \n\t- 我们期待“好”的预测模型能在未知的数据上有很好的表现： 经典泛化理论认为，**为了缩小训练和测试性能之间的差距，应该以简单的模型为目标。** \n\t- 简单性以较小维度的形式展现. 此外, $L_2$ 正则化的有效性也说明, 参数的范数也代表了一种有用的简单性度量。\n\n- 简单性的另一个角度是**平滑性**，即函数不应该对其输入的微小变化敏感。 例如，当我们对图像进行分类时，我们预计向像素添加一些随机噪声应该是基本无影响的。 [1995年，克里斯托弗·毕晓普证明了 具有输入噪声的训练等价于Tikhonov正则化。](notes/2022/2022.2/D2L-22-权重衰减.md#^c88d1b) **这项工作用数学证实了“要求函数光滑”和“要求函数对输入的随机噪声具有适应性”之间的联系**。\n\n\t- 平滑的函数能较好地适应输入的随机噪声\n\n\t- [Dropout](notes/2022/2022.2/D2L-23-Dropout-丢弃法.md) 就是在前向传播过程中，计算每一内部层的同时注入噪声，从而增加模型的平滑性.\n","lastmodified":"2022-10-15T14:06:29.606503702Z","tags":null},"/notes/2022/2022.2/%E5%AF%B9%E4%BA%8E%E7%AD%89%E4%BB%B7%E7%9A%84%E7%BD%91%E7%BB%9C-%E5%B0%8F%E7%9A%84%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%8F%82%E6%95%B0%E6%9B%B4%E5%B0%91":{"title":"对于等价的网络, 小的卷积核参数更少","content":"- 我们知道 [2个3x3卷积核堆叠后等价于一个5x5卷积核](notes/2022/2022.2/2个3x3卷积核堆叠后等价于一个5x5卷积核.md), 以此为例我们来比较一下两者的参数大小.\n\n![卷积层参数大小的计算](notes/2022/2022.2/卷积层参数大小的计算.md)\n\n- 假设输入通道数为 $C_{in}$, 输出通道数为 $C_{out}$\n\n- 对于 $5\\times5$ 卷积, 参数数量为: $$5\\times5\\times C_{in} \\times C_{out}$$\n\n- 对于 $3\\times3$ 卷积, 假设第一次卷积的输出通道数为 $C_{mid}$\n\t- 一次 $3\\times3$ 卷积的参数数量为 $$3\\times3\\times \\textcolor{Orange}{C_{in}}\\times \\textcolor{ForestGreen}{C_{mid}}$$\n\t- 第二次卷积的参数数量为 $$3\\times3\\times \\textcolor{ForestGreen}{C_{mid}}\\times \\textcolor{RoyalBlue}{C_{out}}$$\n\t- 总的参数数量为: $$9\\times\\textcolor{ForestGreen}{C_{mid}}\\left(\\textcolor{Orange}{C_{in}}+\\textcolor{RoyalBlue}{C_{out}}\\right)$$\n\n- **如果输入输出频道数相同:** \n\t- $5\\times5$ 卷积: $$25\\times C^2$$\n\t- $3\\times3$ 卷积: $$18\\times C^2$$\n\t- 显然更小的卷积核参数数量更少\n\n\n\n\n- **如果输入输出频道数不同:**\n\t- 对于 $3\\times3$ 卷积: \n\t\t- 我们通常选取 $\\textcolor{ForestGreen}{C_{mid}}$ 为 $\\textcolor{Orange}{C_{in}},\\textcolor{RoyalBlue}{C_{out}}$ 的几何平均数, 也就是说: $$\\textcolor{ForestGreen}{C_{mid}}=\\sqrt{\\textcolor{Orange}{C_{in}}\\textcolor{RoyalBlue}{C_{out}}}$$\n\t\t如果 $\\textcolor{Orange}{C_{in}}\\times \\alpha=\\textcolor{RoyalBlue}{C_{out}}$, 则 $$\\textcolor{ForestGreen}{C_{mid}}=\\sqrt{\\alpha}\\times\\textcolor{Orange}{C_{in}}$$\n\t- 对比一下两者的参数数量: \n\t\t- $5\\times5$ 卷积: $$25\\times C_{in} \\times C_{out}$$\n\t\t- $3\\times3$ 卷积: $$9\\times \\sqrt{\\textcolor{Orange}{C_{in}}\\textcolor{RoyalBlue}{C_{out}}}\\left(\\textcolor{Orange}{C_{in}}+\\textcolor{RoyalBlue}{C_{out}}\\right)$$\n\t\t- 这个就不是很好比较了, 我们把图做出来看看: \n\t\t\t![](notes/2022/2022.2/assets/Pasted%20image%2020220304104453.png)[^1]\n\t\t\t\n\t\t- 可以看到一般的参数范围内, 都是小的卷积核参数更少. \n\n\n[^1]: [Different Kernel Size - Who's Parameter is larger – GeoGebra](https://www.geogebra.org/m/zq6vnwcp)","lastmodified":"2022-10-15T14:06:29.606503702Z","tags":null},"/notes/2022/2022.2/%E5%BD%92%E7%BA%B3%E5%81%8F%E7%BD%AE-Inductive-bias-learning-bias":{"title":"归纳偏置-Inductive bias - learning bias","content":"# Inductive Bias - 归纳偏置 / 归纳偏好\n\n\u003cdiv align=\"right\"\u003e 2022-02-26\u003c/div\u003e\n\nTags: #DeepLearning #MachineLearning \n\n\n- 当学习器去预测其未遇到过的输入的结果时，会做一些假设（Mitchell, 1980）。而学习算法中的归纳偏置（Inductive bias）则是这些假设的集合。[^1]\n\n- 一个典型的归纳偏置例子是[奥卡姆剃刀](https://zh.wikipedia.org/wiki/%E5%A5%A7%E5%8D%A1%E5%A7%86%E5%89%83%E5%88%80 \"奥卡姆剃刀\")，它假设最简单而又一致的假设是最佳的。这里的一致是指学习器的假设会对所有样本产生正确的结果。\n\n[^1]: Machine Learning - Mitchell Chapter 2.7 ","lastmodified":"2022-10-15T14:06:29.606503702Z","tags":null},"/notes/2022/2022.2/%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F%E4%B8%8E%E5%AE%87%E5%AE%99%E4%B8%AD%E7%9A%84%E6%89%80%E6%9C%89%E5%8E%9F%E5%AD%90":{"title":"灰度图像与宇宙中的所有原子","content":"\n\u003e - 考虑$28×28$的灰度图像。 如果每个像素可以取$256$个灰度值中的一个， 则有$256^{784}$个可能的图像。 这意味着指甲大小的低分辨率灰度图像的数量比宇宙中的原子[^3]要多得多。[^1]\n\n- 反过来想, 小小的$28×28$的灰度图像中, 一定有很多很多图像整个宇宙中都不存在, 因为要是一个图像只用一个原子就能表达的话[^2], 宇宙中的所有原子都表达不完这些图片.\n\n[^1]: https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/underfit-overfit.html#id2\n[^2]: 显然不够\n[^3]: $10^{80}$","lastmodified":"2022-10-15T14:06:29.606503702Z","tags":null},"/notes/2022/2022.2/2%E4%B8%AA3x3%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%A0%86%E5%8F%A0%E5%90%8E%E7%AD%89%E4%BB%B7%E4%BA%8E%E4%B8%80%E4%B8%AA5x5%E5%8D%B7%E7%A7%AF%E6%A0%B8":{"title":"2个3x3卷积核堆叠后等价于一个5x5卷积核","content":"- VGG16相比AlexNet的一个改进是**采用连续的几个3x3的卷积核代替AlexNet中的较大卷积核（11x11，7x7，5x5）**。\n\t- 对于给定的感受野（与输出有关的输入图片的局部大小），采用堆积的小卷积核优于大的卷积核，因为多层非线性层可以增加网络深度来保证学习更复杂的模式，而且代价还比较小（参数更少）。\n\t- **代价更小:**  [[notes/2022/2022.2/对于等价的网络, 小的卷积核参数更少]]\n\n- 简单来说，在VGG中，使用了3个3x3卷积核来代替7x7卷积核，使用了2个3x3卷积核来代替5x5卷积核，这样做的主要目的是在保证具有相同感知野的条件下，提升了网络的深度，在一定程度上提升了神经网络的效果。\n\n## 为什么使用2个3x3卷积核可以来代替5x5卷积核\n- 5x5卷积看做一个小的全连接网络在5x5区域滑动，我们可以先用一个3x3的滤波器卷积，然后再用一个全连接层连接这个3x3卷积输出.  同时, 这个全连接层我们也可以看做一个3x3卷积层。这样我们就可以用两个3x3卷积级联（叠加）起来代替一个 5x5卷积。\n\n- 具体如下图所示：\n![](notes/2022/2022.2/assets/img_2022-10-15.jpg)\n\n至于为什么使用3个3x3卷积核可以来代替 $7\\times7$ 卷积核，推导过程与上述类似。\n\nref: [一文读懂VGG网络 - 知乎](https://zhuanlan.zhihu.com/p/41423739)\n","lastmodified":"2022-10-15T14:06:29.510502668Z","tags":null},"/notes/2022/2022.2/CAPTCHA":{"title":"CAPTCHA","content":"\n完全自动图灵测试: \nCompletely Automated Public Turing test to tell Computer and Humans Apart","lastmodified":"2022-10-15T14:06:29.510502668Z","tags":null},"/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5":{"title":"Cross_Entropy-交叉熵","content":"# Cross Entropy - 交叉熵\n\n\u003cdiv align=\"right\"\u003e 2022-02-11\u003c/div\u003e\n\nTags: #InformationTheory #DeepLearning \n\n## Intuition\n- 熵是编码一个事件所需要的最短平均长度\n\t$$\\begin{aligned}H(p)\u0026=\\sum_{x_{i}} p\\left(x_{i}\\right) \\log \\frac{1}{p\\left(x_{i}\\right)} \\\\\n\t\u0026=-\\sum_{x} p(x) \\log p(x)\t\\end{aligned}$$\n- 而交叉熵则是一种特殊情况下编码的平均最短长度: 事件真实的概率分布是 $p(x)$, 但是我们以为事件的分布是 $q(x)$ .  \n\n\t- 因此, 交叉熵可以看作每个信息片段在错误分布 $q$ 下的期望编码位长度，而信息实际分布为 $p$。这就是期望 $\\operatorname{E}_p$ 是基于 $p$ 而不是 $q$ 的原因.[^3]\n\t$$\\begin{aligned}   H(p, q)\u0026=\\sum_{x_{i}} p\\left(x_{i}\\right) \\log \\frac{1}{q\\left(x_{i}\\right)} \\\\\n\t\u0026=-\\sum_{x} p(x) \\log q(x) \\end{aligned}$$\n\n\n\n## Formal Definition\n- 在指定集合下, 分布 $q$  相对于分布 $p$ 的交叉熵定义如下:\n$$H(p, q) = -\\operatorname{E}_p[\\log q]$$\n其中符号 $E_p[\\cdot]$ 的含义是 相对于分布 $p$ 的期望值.\n\n- 交叉熵的定义也可以从 [KL散度](notes/2022/2022.2/KL_Divergence-KL散度.md) $D_{\\mathrm{KL}}(p \\parallel q)$ 得出(KL散度也叫做$p$ 相对于 $q$的相对熵).\n$$H(p, q) = H(p) + D_{\\mathrm{KL}}(p \\parallel q)$$\n其中 $H(p)$ 是分布 $p$ 的 [熵(Entropy)](notes/2022/2022.2/Entropy-熵.md) .\n\n- 对于离散分布 $p$ 和 $q$ :\n$$H(p, q)=-\\sum_{x \\in \\mathcal{X}} p(x) \\log q(x)$$\n[^1]\n## 性质\n- 交叉熵不是对称的: *i.e.* $$H(p,q)\\neq H(q,p)$$\n\t- KL散度也不是对称的\n\t- 尽管不是对称的，但是无论是 $H(p,q)$ 还是 $H(p,q)$ 其实都可以作为概率相似程度的衡量标准\n\n\n- 交叉熵是非负的: $$H(p, q)=-\\sum_{x } p(x) \\log q(x)$$\n\t其中 $p(x), q(x)\\in[\\ 0,1\\ ], \\log q(x)\\leq0$\n\t\n## 交叉熵: 作为损失函数\n[交叉熵是怎样衡量输出和真实值的差别的呢?](notes/2022/2022.2/D2L-14-Cross%20Entropy%20as%20Loss.md)\n\n## Cross Entropy in Neural Networks\n![](notes/2022/2022.2/assets/img_2022-10-15-9.png)\n- 在[Softmax回归](notes/2022/2022.2/D2L-13-Softmax_Regression.md)里面, 前面的真实概率分布是[One-hot_Encoding-独热编码](notes/2022/2022.1/One-hot_Encoding-独热编码.md)的, 所以整个求和式里只有一项留下来了, 即模型输出类的预测概率的负对数$-\\log q(x)$\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/6ArSys5qHAU?start=170\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n[^1]: 其中$\\mathcal{X}$ 指的是测度论里面的\"支撑集\". Reference: [Support (measure theory) - Wikipedia](https://en.wikipedia.org/wiki/Support_(measure_theory))\n[^2]: [为什么用交叉熵做损失函数 - 知乎](https://zhuanlan.zhihu.com/p/70804197)\n[^3]: [交叉熵 - 维基百科，自由的百科全书](https://zh.wikipedia.org/wiki/%E4%BA%A4%E5%8F%89%E7%86%B5)   [Cross entropy - Wikipedia](https://en.wikipedia.org/wiki/Cross_entropy) [详解机器学习中的熵、条件熵、相对熵和交叉熵 - 知乎](https://zhuanlan.zhihu.com/p/35379531)","lastmodified":"2022-10-15T14:06:29.510502668Z","tags":null},"/notes/2022/2022.2/D2L-12-Predication_or_Inference-Difference":{"title":"D2L-12-Predication_or_Inference-Difference","content":"# Prediction or Inference? The Difference\n\n\u003cdiv align=\"right\"\u003e 2022-02-08\u003c/div\u003e\n\nTags: #DeepLearning #Math/Statistics #Inference #Prediction\n\n- 在深度学习里面, **给定特征估计目标**的过程通常称为_预测_（prediction）或_推断_（inference）。\n- 但是, 虽然 _推断_ 这个词已经成为深度学习的标准术语，但其实 _推断_ 这个词有些用词不当。 **在统计学中，_推断_ 更多地表示基于数据集估计参数。**[^1] 当深度学习从业者与统计学家交谈时，术语的误用经常导致一些误解。[^2]\n\n\n[^1]: 如Bayesian Inference: [Bayesian_Estimation(Inference)贝叶斯估计](notes/2021/2021.12/Bayesian_Estimation(Inference)贝叶斯估计.md)\n[^2]: https://zh-v2.d2l.ai/chapter_linear-networks/linear-regression.html","lastmodified":"2022-10-15T14:06:29.510502668Z","tags":null},"/notes/2022/2022.2/D2L-13-Softmax_Regression":{"title":"D2L-13-Softmax_Regression","content":"# Softmax 回归\n\n\u003cdiv align=\"right\"\u003e 2022-02-11\u003c/div\u003e\n\nTags: #SoftmaxRegression #MachineLearning #Classification #MulticlassClassification \n\n- Softmax回归解决的是多分类问题[^1], 它可以看作是二分类的[Logistic_Regression](notes/2021/2021.8/Part.12_Logistic_Regression(ML_Andrew.Ng.).md)的推广.\n\n![Softmax函数](notes/2022/2022.2/Softmax函数.md)\n\n## Softmax回归\n- Softmax回归就是在线性回归的基础上套上一个Softmax函数, 取输出结果中概率最大的项作为预测结果.\n![](notes/2022/2022.2/assets/Pasted%20image%2020220211175631.png)\n\n### 交叉熵作为损失函数\n[D2L-14-Cross Entropy as Loss](notes/2022/2022.2/D2L-14-Cross%20Entropy%20as%20Loss.md)\n\n\n\n\n## Softmax and Argmax\n- 不像Softmax输出K个概率值, Argmax函数直接将输入向量里面最大的元素设置为1, 其他均置为0.\n- Argmax常常用于输出预测结果, 但是Argmax有个很严重的缺点: 它的结果没法用于反向传播优化参数: (因为它要么不可导要么导数为0)\n\n解释: \n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/KpKog-L9veg?start=185\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n### Softmax名称的由来\n\u003e Softmax的名字是怎么来的？ - 2WaveTech的文章 - 知乎 https://zhuanlan.zhihu.com/p/58859958\n\nSoftmax是\"Hardmax\"函数的Soft版本: \n![](notes/2022/2022.2/assets/Pasted%20image%2020220211181821.png)\n\n\n\n## 与Logistic(Sigmoid)回归的联系\n[Relation_between_Softmax_and_Logistic_Regression](notes/2022/2022.2/Relation_between_Softmax_and_Logistic_Regression.md)\n\n\n## 数值稳定性上面的一些细节\n\u003e softmax和cross-entropy是什么关系？ - 董鑫的回答 - 知乎 https://www.zhihu.com/question/294679135/answer/885285177\n\n\n\n[^1]: 和Logistic回归一样, 虽然它名字叫回归, 实际上解决的却是分类问题. 为什么呢? 搜索了一会儿没有得到结果😥\n","lastmodified":"2022-10-15T14:06:29.510502668Z","tags":null},"/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss":{"title":"D2L-14-Cross Entropy as Loss","content":"# 交叉熵作为损失函数\n\n\u003cdiv align=\"right\"\u003e 2022-02-11\u003c/div\u003e\n\nTags: #CostFunction  #DeepLearning  #CrossEntropy \n\n- 在作为损失函数的时候, 构成[交叉熵](notes/2022/2022.2/Cross_Entropy-交叉熵.md)的概率分布为:\n\t- 真实分布: $P^*$\n\t- 模型输出: $P$\n\n![450](notes/2022/2022.2/assets/img_2022-10-15-10.png)\n- 作为损失函数, 交叉熵的作用是 **衡量模型输出与真实值的差距**, 作为优化算法的优化对象, 还需要尽量简洁, 减少训练模型的开销.\n\n## 为什么交叉熵可以衡量输出与真实值的差别?\n- 我们由[KL_Divergence](notes/2022/2022.2/KL_Divergence-KL散度.md)一节知道, KL散度可以作为衡量两个分布差距的指标: KL散度越接近0, 两个分布的相似度越高. \n\n- 我们可以证明: 最小化 $D_{KL}(P^*||P)$ 的过程, 就是最小化 $H(P^*|P)$ 的过程: \n\n证明视频阐述得非常好 `(2:39)` : \n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Pwgpl9mKars\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write;  gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n- Reference: [Intuitively Understanding the Cross Entropy Loss - YouTube](https://www.youtube.com/watch?v=Pwgpl9mKars)\n\n- **重点:** \n\t- 交叉熵和KL散度有这样的关系: \n\t\t$$H(p, q) = H(p) + D_{\\mathrm{KL}}(p \\parallel q)$$\n\t\t也就是说: KL散度 (相对熵) = 交叉熵 - 信息熵\t\n\t\t- 由于信息熵描述的是消除 $p$ (即真实分布) 的不确定性所需信息量的度量，所以其值应该是最小的、固定的。那么：**优化减小相对熵也就是优化交叉熵，所以在机器学习中使用交叉熵就可以了。**\n\n## Cross Entropy与Softmax\n**为什么经常看到Cross Entropy和Softmax在一起?**\n- 利用交叉熵作为损失函数的时候, 要求模型的输出 $P$ 是一个概率分布, 而Softmax的作用就是将模型的\"直接输出\"转化为\"概率输出\".\n\n\t- 模型的直接输出又称为 \"[Logit](notes/2022/2022.2/Logit.md)\"(至少在深度学习里面)\n\n## 详细推导\n- 假设一共有 $K$ 个类别, 令 $\\mathbf{y}$ 为真实标签([One-hot编码](notes/2022/2022.1/One-hot_Encoding-独热编码.md), 只有 ${y}_k=1$), $\\mathbf{\\hat y}$ 为 Softmax 模型预测的输出[^2], $\\mathbf{o}$ 表示 logits(Softmax 层的输入).\n- 先参考一下交叉熵的定义:\n\t[Cross Entropy - 交叉熵](notes/2022/2022.2/Cross_Entropy-交叉熵.md#Cross%20Entropy%20-%20交叉熵)\n- 损失函数是交叉熵 $H(\\mathbf{y}, \\mathbf{\\hat y})$ :\n$$\\begin{aligned}\nL(\\mathbf{y},\\hat{\\mathbf{y}}) \u0026=\\sum_{j=1}^K {y}_j \\log \\frac{1}{\\hat{y}_j} \\\\ \u0026=-\\sum_{j=1}^K {y}_j \\log{\\hat{y}_j}\\\\\n\u0026=-\\sum_{j=1}^K {y}_j \\log{\\frac{\\exp \\left(o_{j}\\right)}{\\sum_{i=1}^{K} \\exp \\left(o_{i}\\right)}}\\\\\n(\\text{独热编码,只有}{y}_k\\text{为}1)\u0026=-\\log{\\frac{\\exp \\left(o_{k}\\right)}{\\sum_{i=1}^{K} \\exp \\left(o_{i}\\right)}}\\\\\n\u0026=-\\log \\hat{y}_k\n\\end{aligned}$$\n- 其实就是预测概率 $\\hat{y}_k$ 的负对数, 其中 $k$ 对应真实标签 ${y}_k$ \n\n- 接下来求 $L(\\mathbf{y}, \\hat{\\mathbf{y}})$ 对于Softmax层输入Logit $o_p$ 的导数: \n$$\\begin{aligned}\\frac{\\partial}{\\partial o_p}L(\\mathbf{y}, \\hat{\\mathbf{y}})\u0026=-\\frac{\\partial}{\\partial o_p}\\left(\\log{\\frac{\\exp \\left(o_{k}\\right)}{\\sum_{i=1}^{K} \\exp \\left(o_{i}\\right)}}\\right)\\\\\n\u0026=-\\frac{\\partial}{\\partial o_p}\\left(o_{k}-{\\sum_{i=1}^{K} \\exp \\left(o_{i}\\right)}\\right)\n\\\\\n\u0026=\\begin{cases}\n  \\frac{\\partial}{\\partial o_p}\\sum_{i=1}^{K} \\exp \\left(o_{i}\\right)-1 \u0026 \\text{when }p=k\\\\    \n  \\frac{\\partial}{\\partial o_p}\\sum_{i=1}^{K} \\exp \\left(o_{i}\\right)-0 \u0026 \\text{when }p\\neq k\n\\end{cases}\\\\\n\u0026=\\frac{\\partial}{\\partial o_p}\\sum_{i=1}^{K} \\exp \\left(o_{i}\\right)-{y}_p\\\\   \n\u0026=\\frac{\\exp{o_p}}{\\sum_{i=1}^{K} \\exp (o_{i})}-{y}_p\\\\\u0026=\\operatorname{Softmax}(\\mathbf{o})_p-{y}_p\n\\end{aligned}$$\n\n其中 $\\mathbf{o=W^T X+b}$\n\n- 可以看到这个导数十分简单, 就是模型输出的概率值 $\\operatorname{Softmax}(\\mathbf{o})_p$ 减去真实的概率值 ${y}_p$.[^1]\n\n### 另一种推导\n这里给出另一种推导方式:\n$$\\begin{aligned}\n\nL(\\mathbf{y}, \\hat{\\mathbf{y}}) \u0026=-\\sum_{j=1}^{K} {y}_{j} \\log \\frac{\\exp \\left(o_{j}\\right)}{\\sum_{i=1}^{K} \\exp \\left(o_{i}\\right)} \\\\\n\n(\\text{将负号移进去})\u0026=\\quad\\sum_{j=1}^{K} {y}_{j} \\log \\frac{\\sum_{i=1}^{K} \\exp \\left(o_{i}\\right)}{\\exp \\left(o_{j}\\right)} \\\\\n\n\u0026=\\sum_{j=1}^{K} {y}_{j} \\log \\sum_{i=1}^{K} \\exp \\left(o_{i}\\right)-\\sum_{j=1}^{K} {y}_{j} o_{j} \\\\\n\n(\\text{独热编码,只有一个}{y}_j\\text{为}1)\u0026=\\log \\sum_{i=1}^{K} \\exp \\left(o_{i}\\right)-\\sum_{j=1}^{K} {y}_{j} o_{j} .\n\n\\end{aligned}$$\n- 上面的推导里面, 并没有把后半部分的${y}_j$消去, 这使得下面求偏导不用分类讨论:\n\n- 我们求 $L(\\mathbf{y}, \\hat{\\mathbf{y}})$ 对于Softmax层输入 $o_p$ 的导数:\n$$\\begin{aligned}\\frac{\\partial}{\\partial o_p}L(\\mathbf{y}, \\hat{\\mathbf{y}})\u0026=\\frac{\\exp{o_p}}{\\sum_{i=1}^{K} \\exp (o_{i})}-{y}_p\\\\\u0026=\\operatorname{Softmax}(\\mathbf{o})_p-{y}_p\n\\end{aligned}$$\n\n## 对比交叉熵与均方误差\n[为什么Softmax回归不用MSE](notes/2022/2022.2/为什么Softmax回归不用MSE.md)\n\n## PyTorch的Cross Entropy Loss包括Softmax\n\u003e - `class torch.nn.CrossEntropyLoss(weight=None, size_average=True)`\n\u003e - 此标准将`LogSoftMax`和`NLLLoss`集成到一个类中。当训练一个多类分类器的时候，这个方法是十分有用的。\n\n- [torch.nn - PyTorch中文文档](https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/)\n- [CrossEntropyLoss — PyTorch 1.10 documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n\n- PyTorch 的实现还需要考虑一个 Batch 里面的多组数据, 所以比较难懂.\n- 假设一个 Batch 由 $N$ 组数据组成, 则我们可以对一个 Batch 的 $N$ 个损失进行三种操作:\n\t- 取平均\n\t- 求和\n\t- 什么也不做\n- 上面三种操作分别对应 `reduction='mean', 'sum', 'none'`, 默认 `reduction='mean'`\n\n### `reduction='none'`\n$$\\ell(x, y)=L=\\left\\{l_{1}, \\ldots, l_{N}\\right\\}^{\\top}, \\quad l_{n}=-w_{y_{n}} \\log \\frac{\\exp \\left(x_{n, y_{n}}\\right)}{\\sum_{c=1}^{C} \\exp \\left(x_{n, c}\\right)} \\cdot 1\\left\\{y_{n} \\neq \\text { ignore\\_index }\\right\\}$$\n- 在上面的式子里面, $\\ell(x, y)=L=\\left\\{l_{1}, \\ldots, l_{N}\\right\\}^{\\top}$ 的意思是: 一个 Batch 里面有 $N$ 组数据, 而输出是这 $N$ 组损失组成的一个向量.\n- $w_{y_{n}}$ 用于对不同类别的损失进行加权, 这常常在类别数据不平衡的时候使用.\n- $\\cdot 1\\left\\{y_{n} \\neq \\text { ignore\\_index }\\right\\}$ 表示忽略 $y_{n} = \\text { ignore\\_index }$ 的类别 \n\n### `reduction='sum', 'mean'`\n$$\\ell(x, y)= \\begin{cases}\\sum_{n=1}^{N} \\frac{1}{\\sum_{n=1}^{N} w_{y_{n}} \\cdot 1\\left\\{y_{n} \\neq \\text { ignore\\_index }\\right\\}}l_{n}, \u0026 \\text { if reduction }=\\text { 'mean'; } \\\\ \\sum_{n=1}^{N} l_{n}, \u0026 \\text { if reduction }=\\text { 'sum' }\\end{cases}$$\n\n## 需要把标签 one-hot 之后再输入到损失函数里面吗?\n[Cross_Entropy_Loss_Input_Format-交叉熵损失函数输入格式](notes/2022/2022.5/Cross_Entropy_Loss_Input_Format-交叉熵损失函数输入格式.md)\n\n\n\n[^1]:教材里面说: 这不是巧合，在任何指数族分布模型中 （参见[本书附录中关于数学分布的一节](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/distributions.html) ）， 对数似然的梯度正是由此得出的。 这使梯度计算在实践中变得容易很多。 但是我还是不是很理解这里什么意思. https://zh-v2.d2l.ai/chapter_linear-networks/softmax-regression.html#subsec-softmax-and-derivatives\n[^2]: Y hat (written $ŷ$ ) is **the predicted value of y** (the [dependent variable](https://www.statisticshowto.com/dependent-variable-definition/)) in a [regression equation](https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/what-is-a-regression-equation/). [Y Hat: Definition - Statistics How To](https://www.statisticshowto.com/y-hat-definition/)","lastmodified":"2022-10-15T14:06:29.510502668Z","tags":null},"/notes/2022/2022.2/D2L-15-Perceptron":{"title":"D2L-15-Perceptron","content":"# Perceptron - 感知机\n\n\u003cdiv align=\"right\"\u003e 2022-02-12\u003c/div\u003e\n\nTags: #MachineLearning #Perceptron\n\n![Perceptron](notes/2022/2022.2/assets/Perceptron.pdf)\n\n","lastmodified":"2022-10-15T14:06:29.510502668Z","tags":null},"/notes/2022/2022.2/D2L-16-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%97%AE%E9%A2%98":{"title":"D2L-16-线性模型的问题","content":"# 线性模型存在的问题\n\n\u003cdiv align=\"right\"\u003e 2022-02-12\u003c/div\u003e\n\nTags: #DeepLearning \n\n- 线性意味着 _\"单调性\"_ 假设： 输出和输入以相同的速度变化.\n\t- 但是有很多问题虽然是单调的, 但是并不是线性\"匀速\"变化的\n\t- 对策: 对数据进行预处理，使线性变得更合理，如进行对数变换。\n\n- 但是很多情况也不是单调的。 例如，我们想要根据体温预测死亡率。 对于体温高于37摄氏度的人来说，温度越高风险越大。 然而，对于体温低于37摄氏度的人来说，温度越高风险就越低。\n\t- 对策: 在这种情况下，我们也可以通过一些巧妙的预处理来解决问题。 例如，我们可以使用与37摄氏度的距离作为特征。\n\n- 但是，如何对猫和狗的图像进行分类呢？ 增加位置(13,17)处像素的强度是否总是增加（或降低）图像描绘狗的似然？ \n- 与前面的例子相比，这里任何像素的重要性都以复杂的方式取决于周围像素的值。数据需要考虑到特征之间的相互作用。从而, 在基于相互作用的表示的基础上建立一个线性模型可能会是合适的，但我们不知道如何手动计算这么一种表示。 \n\t- **对策:** 深度神经网络. 我们将观测数据输入**隐藏层**, 在此基础上建立线性预测器。\n\n![](notes/2022/2022.2/assets/mlp.svg)","lastmodified":"2022-10-15T14:06:29.510502668Z","tags":null},"/notes/2022/2022.2/D2L-17-MLP-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA":{"title":"D2L-17-MLP-多层感知机","content":"# Multilayer Perceptron  \n\n\u003cdiv align=\"right\"\u003e 2022-02-12\u003c/div\u003e\n\nTags: #MultilayerPerceptron #DeepLearning #Perceptron \n\n## 隐藏层\n![mlp](notes/2022/2022.2/assets/mlp.svg)\n \n ## 从线性到非线性\n- 用$\\mathbf{X, H, O}$ 分别代表输入层, 隐藏层和输出层, 带偏置的模型可以表示如下:\n$$\\begin{aligned}\n\u0026\\mathbf{H}=\\mathbf{X} \\mathbf{W}^{(1)}+\\mathbf{b}^{(1)} \\\\\n\u0026\\mathbf{O}=\\mathbf{H} \\mathbf{W}^{(2)}+\\mathbf{b}^{(2)}\n\\end{aligned}$$\n \n - 但是线性代数告诉我们, 线性组合的线性组合依然是一个线性组合, 所以仅仅加入一个隐藏层并没有增加模型的表达能力😞.\n\t - 证明: \n\t \t$$\\begin{aligned}\n\t\\mathbf{O}\u0026=\\left(\\mathbf{X W}^{(1)}+\\mathbf{b}^{(1)}\\right) \\mathbf{W}^{(2)}+\\mathbf{b}^{(2)}\\\\\n\t\u0026=\\mathbf{X} \\mathbf{W}^{(1)} \\mathbf{W}^{(2)}+\\mathbf{b}^{(1)} \\mathbf{W}^{(2)}+\\mathbf{b}^{(2)}\\\\\n\t\u0026=\\mathbf{X} \\mathbf{W'}+\\mathbf{b'}\\end{aligned}$$\n\n- 为了发挥多层架构的潜力， 我们还需要一个额外的关键要素： **激活函数**（**activation function**）$σ$。 \n\t- 在我们的单隐藏层模型里面, 输入之后的权重对数据进行了[仿射变换](notes/2022/2022.2/仿射变换-Affine_Transformation.md), 此后通过对每个隐藏层单元应用非线性的**激活函数**, 我们的多层感知机不再退化成线性模型.\n\t\t$$\\begin{aligned}\u0026\\mathbf{H}=\\sigma\\left(\\mathbf{X W}^{(1)}+\\mathbf{b}^{(1)}\\right) \\\\\n\t\u0026\\mathbf{O}=\\mathbf{H W}^{(2)}+\\mathbf{b}^{(2)}\n\t\\end{aligned}$$\n\t- 激活函数的输出 $σ(⋅)$ 被称为**活性值**（**activation**）。 \n\t- 注意, 通常我们不在输出层上作用激活函数, 输出层上面常常是[Softmax](notes/2022/2022.2/D2L-13-Softmax_Regression.md)或者其他变换.\n\n\t- 为了构建更通用的多层感知机， 我们可以继续堆叠这样的隐藏层， 例如$\\mathbf{H^{(1)}=σ_1(XW^{(1)}+b^{(1)})}$ 和 $\\mathbf{H^{(2)}=σ_2(H^{(1)}W^{(2)}+b^{(2)})}$， 从而产生更有表达能力的模型。\n\n## 搞清楚矩阵的形状\n- 为了使思路清晰， 我们来理一下D2L里面MLP各个矩阵的形状：\n- **输入**: $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ 表示 $n$ 个样本的小批量, 其中每个样本具有 $d$ 个输入特征。\n\t\t![X的维度](notes/2022/2022.2/assets/X的维度.svg)\n- **权重矩阵**: 以第一层权重为例：  $\\mathbf{W}^{(1)} \\in \\mathbb{R}^{d \\times h}$ \n\t![W的维度](notes/2022/2022.2/assets/W的维度.svg)\n\n- **隐藏层偏置** $\\mathbf{b}^{(1)} \\in \\mathbb{R}^{1 \\times h}$\n\t![Bias的维度](notes/2022/2022.2/assets/Bias的维度.svg)\n","lastmodified":"2022-10-15T14:06:29.510502668Z","tags":null},"/notes/2022/2022.2/D2L-18-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-Activation_Functions":{"title":"D2L-18-激活函数-Activation_Functions","content":"# 常用激活函数\n\n\u003cdiv align=\"right\"\u003e 2022-02-12\u003c/div\u003e\n\nTags: #DeepLearning #ActivationFunction \n\n![](notes/2022/2022.2/assets/img_2022-10-15-2.gif)[^2]\n\n## ReLU\n**修正线性单元**（Rectified Linear Unit，_ReLU_）\n![](notes/2022/2022.2/assets/ReLU.svg)\n- ReLU就是一个 $max(0,x)$ 函数.\n- ReLU是分段线性的\n- ReLU的变体通过设置一个线性项, 使得负轴的一些信息得到保留(Parameterized ReLU)\n\t$\\mathbf{pReLU}(x)=max(0,x)+α\\ min(0,x).$\n\t![](notes/2022/2022.2/assets/Pasted%20image%2020220212160706.png)\n\n### 导数\n![](notes/2022/2022.2/assets/Derivative_of_ReLU.svg)\n- 使用ReLU的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过。\n- 注意，当输入值精确等于0时，ReLU函数不可导。 在此时，我们默认使用左侧的导数，即当输入为0时导数为0。[^1]\n\n## Sigmoid 函数\n![Sigmoid_Function](notes/2021/2021.8/Sigmoid_Function.md)\n\n## Tanh 函数\n- 与sigmoid函数类似， tanh(双曲正切)函数将其输入压缩转换到区间$(-1, 1)$上。\n\t- 与Sigmoid的区别是压缩的区间不一样(Sigmoid: $(0,1)$, Tanh: $(-1,1)$). 同时, tanh函数是奇函数.\n\n$$\\tanh (x)=\\frac{1-\\exp (-2 x)}{1+\\exp (-2 x)}$$\n\n![](notes/2022/2022.2/assets/Tanh.svg)\n### 导数\n$$\\frac{d}{d x} \\tanh (x)=1-\\tanh ^{2}(x)$$\n\n![](notes/2022/2022.2/assets/Derivative-of-Tanh.svg)\n\n\n\n\n\n\n[^1]: 我们可以忽略这种情况，因为输入可能永远都不会是0。 这里引用一句古老的谚语，“如果微妙的边界条件很重要，我们很可能是在研究数学而非工程”， 这个观点正好适用于这里。\n[^2]: [Activation Functions Explained - GELU, SELU, ELU, ReLU and more](https://mlfromscratch.com/activation-functions-explained/#/)","lastmodified":"2022-10-15T14:06:29.510502668Z","tags":null},"/notes/2022/2022.2/D2L-19-%E9%AA%8C%E8%AF%81%E9%9B%86_or_%E6%B5%8B%E8%AF%95%E9%9B%86":{"title":"D2L-19-验证集_or_测试集","content":"- **验证** 数据集: **Validation** set, 是训练的时候调整参数的依据\n\t- 训练时被多次使用\n\n- **测试** 数据集: **Test** set, 是最终测试模型性能的数据\n\t- 训练完后只测试一次\n\n这两个词常常被混淆, 但是有着很重要的区别: \n- 测试数据集不被用来调整模型参数\n\n**一个类比:**\n- 验证集是高考模考试卷\n\t- 平时测试自己的水平用模考卷子, 根据成绩调整学习策略\n- 测试集是高考试卷\n\t- 高考只能参加一次, 不能说看了高考卷之后再去准备准备重新考一次.\n\n","lastmodified":"2022-10-15T14:06:29.510502668Z","tags":null},"/notes/2022/2022.2/D2L-20-%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE%E4%B8%8E%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE":{"title":"D2L-20-训练误差与泛化误差","content":"# 训练误差与泛化误差\n\n\u003cdiv align=\"right\"\u003e 2022-02-12\u003c/div\u003e\n\nTags: #DeepLearning \n\n- 这一小节写的挺好的: [训练误差和泛化误差¶](https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/underfit-overfit.html#id2 \"Permalink to this headline\")\n\n## 定义\n- _训练误差_（training error）是指， 模型在训练数据集上计算得到的误差。 \n- _泛化误差_（generalization error）是指， 模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。\n\n- 因为我们不能得到无限多的样本, 所以我们只能估计泛化误差.\n\n### 设置测试集的意义\n- 如果我们将所有数据都用来训练, 我们就没法监控模型的泛化能力\n\n\t- 我们的目标是发现某些模式， 这些模式捕捉到了我们训练集潜在总体的规律。 如果成功做到了这点，即使是对以前从未遇到过的个体， 模型也可以成功地评估风险。 **如何发现可以泛化的模式是机器学习的根本问题**。\n\n\n### 很多时候, 数据是不够用的\n我们可以通过[Cross Validation](notes/2021/2021.12/Cross%20Validation.md), 来尽量增加验证集的大小","lastmodified":"2022-10-15T14:06:29.510502668Z","tags":null},"/notes/2022/2022.2/D2L-21-%E6%A8%A1%E5%9E%8B%E5%AE%B9%E9%87%8F":{"title":"D2L-21-模型容量","content":"# 模型容量(复杂度)\n\n\u003cdiv align=\"right\"\u003e 2022-02-12\u003c/div\u003e\n\nTags: #DeepLearning \n\nLink: [Part.17_Overfitting_Underfitting(ML_Andrew.Ng.)](notes/2021/2021.8/Part.17_Overfitting_Underfitting(ML_Andrew.Ng.).md)\n\n## 概念\n\n- 模型容量就是模型的复杂度, 也就是一个模型的Variance.\n- 模型容量和数据的复杂度应该对应, 两者的不匹配容易导致过拟合与欠拟合问题\n![](notes/2022/2022.2/assets/img_2022-10-15-11.png)\n- 下图是两种损失随着模型复杂度的变化(注意横轴代表的是不同复杂度的模型, 不要和训练的Loss曲线搞混).\n\t![](notes/2022/2022.2/assets/capacity-vs-error.svg)\n\t- 可以看到对于同一个数据集, 过于复杂的模型虽然有着更小的训练损失, 但是有了更大的泛化损失, 则说明**模型过分契合训练集了**, 学习到了训练集的一些误差等等, 太过于灵活(High variance), 出现了过拟合\n\t\t- 而如果模型过于简单, 则训练损失和泛化损失都很高, 说明模型能力不够, 学习不到足够的知识, 模型对于数据的偏见(Bias)太高了\n\n## 模型容量\n### 估计模型容量\n- 我们很难比较不同类型模型的容量差别: 比如树模型和神经网络.\n- **影响模型容量的两个因素:**\n\t- 参数的**数量**\n\t- 参数**值的变化范围**\n\n### 模型容量的评价标准\n- [VC维](notes/2022/2022.2/VC维-VC_Dimension.md)是对一个可学习分类函数空间的能力（复杂度，表示能力等）的衡量。\n- 它定义为算法能“打散”的点集的势的最大值。\n\n## 数据复杂度\n评价数据复杂度的一些方面: \n- 样本的个数\n- 一个样本元素的个数\n- 时间, 空间结构\n- 样本分类的多样性\n\n\n\n","lastmodified":"2022-10-15T14:06:29.510502668Z","tags":null},"/notes/2022/2022.2/D2L-22-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F":{"title":"D2L-22-权重衰减","content":"# 权重衰减\n\n\u003cdiv align=\"right\"\u003e 2022-02-12\u003c/div\u003e\n\nTags: #Regularization #DeepLearning \n\n- **权重衰减**就是利用 $\\ell_{2}$ 范数进行 [正则化](notes/2022/2022.2/Regularization-正则化.md), 避免过拟合\n\t- 权重衰减是通过减小目标参数(weights)的大小来实现正则化的, 这也是其名称的由来.\n\t- 参数的范数代表了一种有用的简单性度量。[^5]\n\n\u003e Links:\n\u003e -  [Part.18_Regularization_Intuition(ML_Andrew.Ng.)](notes/2021/2021.9/Part.18_Regularization_Intuition(ML_Andrew.Ng.).md)\n\u003e -  [Part.19_Regularized_Linear_Regression(ML_Andrew.Ng.)](notes/2021/2021.9/Part.19_Regularized_Linear_Regression(ML_Andrew.Ng.).md)\n\n## Intuition\n![Norm in Regularization - Intuition](notes/2022/2022.2/Norm%20in%20Regularization%20-%20Intuition.md)\n\n---\n- **这项技术通过函数与零的距离来衡量函数的复杂度**， 因为在所有函数 $f$ 中，函数 $f=0$ （所有输入都得到值 $0$） 在某种意义上是最简单的。 但是我们应该如何精确地测量一个函数和零之间的距离呢？ 没有一个正确的答案。 事实上，函数分析和巴拿赫空间理论的研究，都在致力于回答这个问题。[^1]\n\n## 推导[^2]\n对于以下损失函数: \n$$L(\\mathbf{w}, b)=\\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{2}\\left(\\mathbf{w}^{\\top} \\mathbf{x}^{(i)}+b-y^{(i)}\\right)^{2}$$\n\n- 为了惩罚权重向量的大小， 我们必须以某种方式在损失函数中添加$‖\\mathbf w‖^2$， 但是模型应该如何平衡这个新的额外惩罚的损失？ 实际上，我们通过 **正则化常数** $λ$ 来描述这种权衡， 这是一个非负超参数，我们使用验证数据拟合：\n\n\t$$L(\\mathbf{w}, b)+\\frac{\\lambda}{2}\\|\\mathbf{w}\\|^{2}$$\n\n\t- 其中 $1/2$ 是为了求导后公式的简洁.\n\n- 你可能会想知道为什么我们使用平方范数 $\\mathbf{\\|w\\|}_2^2$ 而不是标准范数 $\\mathbf{\\|w\\|}_2$ （即欧几里得距离）？ \n\t- 我们这样做是为了便于计算。 通过平方 $L_2$ 范数，我们去掉平方根，留下权重向量每个分量的平方和。 这使得惩罚的导数很容易计算：导数的和等于和的导数。\n\t\n- $L_2$正则化回归的小批量随机梯度下降更新如下式：\n\n$$\\mathbf{w} \\leftarrow(1-\\eta \\lambda) \\mathbf{w}-\\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{x}^{(i)}\\left(\\mathbf{w}^{\\top} \\mathbf{x}^{(i)}+b-y^{(i)}\\right)$$\n\n- 我们根据估计值与观测值之间的差异来更新$\\mathbf w$。 然而，我们同时也在试图将 $\\mathbf w$ 的大小缩小到零。 **这就是为什么这种方法有时被称为 _权重衰减_** 。 我们仅考虑惩罚项，优化算法在训练的每一步_衰减_ 权重。 \n\t- 与特征选择相比，权重衰减为我们提供了一种**连续的**机制来调整函数的复杂度。 较小的 $λ$ 值对应较少约束的 $\\mathbf w$， 而较大的 $λ$ 值对 $\\mathbf w$ 的约束更大。\n\n- 是否对相应的偏置 $b^2$ 进行惩罚在不同的实践中会有所不同， 在神经网络的不同层中也会有所不同。 **通常，网络输出层的偏置项不会被正则化**。\n\n## 两种限制方式的等价性\n![](notes/2022/2022.2/assets/Pasted%20image%2020220214194009.png)\n![](notes/2022/2022.2/assets/Pasted%20image%2020220214194022.png)\n\n## Tikhonov Regularization\n- L2 正则化还有一个名字叫\"**吉洪诺夫正则化**\"[^3]\n\n\t- 1995 年，克里斯托弗·毕晓普[^4]证明了: 具有输入噪声的训练等价于 Tikhonov 正则化 [Bishop, 1995](https://zh-v2.d2l.ai/chapter_references/zreferences.html#bishop-1995)。 ^c88d1b\n\n## 实现\n- 优化算法里面的 `weight_decay` 就是实现的权重衰减\n\t- 在深度学习里面权重衰减是整合到了优化函数里面的, 所以对于任意损失我们都可以进行衰减.\n\t- 例: PyTorch里面的随机梯度下降:  [SGD — PyTorch 1.10 documentation](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)\n\t\t- ![](notes/2022/2022.2/assets/Pasted%20image%2020220302193549.png)\n- weight_decay相对于Dropout应用面更广[^6]\n\n[^1]: [4.5. 权重衰减 — 动手学深度学习 2.0.0-beta0 documentation](https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/weight-decay.html#id2)\n[^2]: [4.5. 权重衰减 — 动手学深度学习 2.0.0-beta0 documentation](https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/weight-decay.html#id2)\n[^3]: [吉洪诺夫正则化 - 维基百科，自由的百科全书](https://zh.wikipedia.org/zh-hans/%E5%90%89%E6%B4%AA%E8%AF%BA%E5%A4%AB%E6%AD%A3%E5%88%99%E5%8C%96) English:  [Tikhonov regularization - Wikipedia](https://en.wikipedia.org/wiki/Tikhonov_regularization)\n[^4]: 就是写 PRML 那本书 的 Bishop\n[^5]: [4.6. 暂退法（Dropout） — 动手学深度学习 2.0.0-beta0 documentation](https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/dropout.html#id2)\n[^6]: [13 丢弃法【动手学深度学习v2】_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1Y5411c7aY?p=3\u0026t=1196.3)","lastmodified":"2022-10-15T14:06:29.510502668Z","tags":null},"/notes/2022/2022.2/D2L-23-Dropout-%E4%B8%A2%E5%BC%83%E6%B3%95":{"title":"D2L-23-Dropout-丢弃法","content":"# Dropout - 丢弃法(暂退法)\n\n\u003cdiv align=\"right\"\u003e 2022-02-14\u003c/div\u003e\n\nTags: #Dropout #Regularization #DeepLearning \n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/drT5_1TCYrk\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n[^1]\n\n- Dropout就是在前向传播过程计算每一内部层的同时注入噪声, 从而提高模型的平滑性, 减少过拟合.\n\n![](notes/2022/2022.2/assets/dropout2.svg)\n\n## 实现方式\n- 实现的关键是要以一种无偏(不改变期望)的方式注入噪声.\n\n- 在毕晓普的工作中，他将**高斯噪声**添加到线性模型的输入中。 在每次训练迭代中，他将从均值为零的分布 $ϵ∼\\mathcal{N}(0,σ^2)$ 采样噪声添加到输入 $\\mathbf{x}$， 从而产生扰动点 $\\mathbf{x^′=x}+ϵ$， 预期是 $E[\\mathbf{x^′}]=\\mathbf{x}$ 。\n\n- 但是我们更常用的方式是按一定概念 $p$ 对隐藏层的输出置零, 换言之，每个中间活性值 $h$ 以 _暂退概率_ $p$ 由随机变量 $h^′$ 替换，如下所示：\n$$h^{\\prime}= \\begin{cases}0 \u0026 \\text { 概率为 } p \\\\ \\frac{h}{1-p} \u0026 \\text { 其他情况 }\\end{cases}$$\n根据此模型的设计, 其期望值保持不变, 即 $$E\\left[h^{\\prime}\\right]=h$$\n\n- 前向传播时, 输出的计算不再依赖于被丢弃的变量; 在反向传播时, 被丢弃的变量梯度为0, 不参与参数的优化更新. 这使得输出层的计算不能过分的依赖于隐藏层中的任何一个元素.\n\n### 测试时\n- 通常，我们在测试时不用暂退法。 给定一个训练好的模型和一个新的样本，我们不会丢弃任何节点，因此不需要标准化。 \n\n- 然而也有一些例外：一些研究人员在测试时使用暂退法， 用于估计神经网络预测的“不确定性”： 如果通过许多不同的暂退法遮盖后得到的预测结果都是一致的，那么我们可以说网络发挥更稳定。(换言之, 各个部分都是一样重要的, 丢弃了任何一部分影响都是一样的)[^3]\n\n### 适用对象\n- Dropout主要应用于全连接层\n\n## Bonus\n- 既然是随机丢弃, 那么有没有可能某一层全部被丢完了呢? \n[neural networks - What if all the nodes are dropped when using dropout? - Cross Validated](https://stats.stackexchange.com/questions/302452/what-if-all-the-nodes-are-dropped-when-using-dropout)\n\n- Loss map with drop out - Visualized[^2]\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/2PqTW_p1fIs\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n- Dropout被Google申请了专利🤐.\n\n\n\n[^1]: [Dropout (DEF 0075) - YouTube](https://www.youtube.com/watch?v=drT5_1TCYrk)\n[^2]: [DROP | Dropout variations in weight space | Loss landscape visualization | Deep Learning - YouTube](https://www.youtube.com/watch?v=2PqTW_p1fIs)\n[^3]: [4.6. 暂退法（Dropout） — 动手学深度学习 2.0.0-beta0 documentation](https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/dropout.html#id5)","lastmodified":"2022-10-15T14:06:29.510502668Z","tags":null},"/notes/2022/2022.2/D2L-24-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7":{"title":"D2L-24-数值稳定性","content":"# 深度学习里面的数值稳定性\n\n\u003cdiv align=\"right\"\u003e 2022-02-17\u003c/div\u003e\n\nTags: #DeepLearning #NumericalComputing\n\n## 问题的由来\n- 数值稳定性的问题发生在反向传播的时候. \n- 对于一个很深的模型, 计算在损失 $\\ell$ 关于第 $t$ 层权重 $\\mathbf{W_t}$ 的梯度的时候, 如果第 $t$ 层关于输出较远, 则结果由许多矩阵乘法构成, 这会导致梯度爆炸或者梯度消失.\n\t- 考虑如下有 $\\mathrm{d}$ 层的神经网络\n\t$$\\mathbf{h}^{t}=f_{t}\\left(\\mathbf{h}^{t-1}\\right) \\quad \\text { and } \\quad y=\\ell \\circ f_{d} \\circ \\ldots \\circ f_{1}(\\mathbf{x})$$\n\t\t![400](notes/2022/2022.2/assets/img_2022-10-15-12.png)\n\t- 对于一个一百层的模型: \n\t$$1.5^{100} \\approx 4 \\times 10^{17} \\quad 0.8^{100} \\approx 2 \\times 10^{-10}$$\n\t- 两者都远远超出了常用的16位浮点数能表示的最大值 $65504$ 和最小精度 $2^{-24}\\approx 6e(-8)$\n\n\n## 矩阵连乘导致的问题\n- 这是导致数值不稳定的根本问题\n- 例子: **MLP** (为了简单省略了偏移)\n\t$$\\mathbf{h}^{t}=f_{t}\\left(\\mathbf{h}^{t-1}\\right)=\\sigma\\left(\\mathbf{W}^{t} \\mathbf{h}^{t-1}\\right)$$\n\t- $\\sigma$ 是激活函数\n\t$$\\frac{\\partial \\mathbf{h}^{t}}{\\partial \\mathbf{h}^{t-1}}=\\operatorname{diag}\\left(\\sigma^{\\prime}\\left(\\mathbf{W}^{t} \\mathbf{h}^{t-1}\\right)\\right)\\left(\\mathbf{W}^{t}\\right)^{T}$$[^1]\n\t- $\\sigma^{\\prime}$ 是 $\\sigma$ 的导数函数, 则\n\t$$\\prod_{i=t}^{d-1} \\frac{\\partial \\mathbf{h}^{i+1}}{\\partial \\mathbf{h}^{i}}=\\prod_{i=t}^{d-1} \\operatorname{diag}\\left(\\sigma^{\\prime}\\left(\\mathbf{W}^{i} \\mathbf{h}^{i-1}\\right)\\right)\\left(\\mathbf{W}^{i}\\right)^{T}$$\n- 我们选择ReLU作为激活函数, 则其导数为: \n\t- $$\\sigma(x)=\\max (0, x) \\quad \\text { and } \\quad \\sigma^{\\prime}(x)= \\begin{cases}1 \u0026 \\text { if } x\u003e0 \\\\ 0 \u0026 \\text { otherwise }\\end{cases}$$\n\t- 则前面的梯度为$$\\prod_{i=t}^{d-1} \\frac{\\partial \\mathbf{h}^{i+1}}{\\partial \\mathbf{h}^{i}}=\\prod_{i=t}^{d-1} \\operatorname{diag}\\left(\\sigma^{\\prime}\\left(\\mathbf{W}^{i} \\mathbf{h}^{i-1}\\right)\\right)\\left(\\mathbf{W}^{i}\\right)^{T}$$\n\t- 其中 $\\operatorname{diag}\\left(\\sigma^{\\prime}\\left(\\mathbf{W}^{i} \\mathbf{h}^{i-1}\\right)\\right)$ 是全部由0和1构成的对角矩阵. 这使得结果的一些元素来自于 \n\t\t$$\\prod_{i=t}^{d-1} \\left(W^{i} \\right)^{T}$$\n\t\t\n\t- 在 $d-t$ 较大的时候, 如果其中出现大于1的数连乘, 则很可能出现梯度爆炸. 同样, 如果其中出现小于1的数连乘, 则很可能出现梯度消失.\n\n## 激活函数导致的问题\n- 除了矩阵连乘, 激活函数的导数值也可能导致数值问题\n\t- 例: [$Sigmoid$ Function](notes/2021/2021.8/Sigmoid_Function.md)\n\t\t它的导函数是: $$\\sigma'(x)=\\sigma(x)(1-\\sigma(x))$$\n\t\t![](notes/2022/2022.2/assets/img_2022-10-15-13.png)\n\t- 当输入较大的时候, $\\operatorname{diag}\\left(\\sigma^{\\prime}\\left(\\mathbf{W}^{i} \\mathbf{h}^{i-1}\\right)\\right)$ 会变得很小 ,则 $$\\prod_{i=t}^{d-1} \\frac{\\partial \\mathbf{h}^{i+1}}{\\partial \\mathbf{h}^{i}}=\\prod_{i=t}^{d-1} \\operatorname{diag}\\left(\\sigma^{\\prime}\\left(\\mathbf{W}^{i} \\mathbf{h}^{i-1}\\right)\\right)\\left(\\mathbf{W}^{i}\\right)^{T}$$ 是 $d-t$ 个小数值的乘积. 这很容易造成梯度消失.\n\n## 后果\n- 最直接的后果是**梯度值超出范围**\n\t- 梯度爆炸: `inf`, 梯度消失: `0`\n\n- 而这会导致**训练的时候很难选择学习率**\n\t- 梯度爆炸: 学习率被限制在一个很小的范围内. 太大了会导致梯度爆炸, 太小了则会导致训练缓慢. \n\t\t- 我们可能需要在训练时不断调整学习率\n\t- 梯度消失: 学习率无论怎么调整, 训练都没有进展\n\n- 对于**网络结构**, 梯度消失会使得模型仅仅在靠近输出的一端训练的很好, 而靠近输入的一端则由于梯度消失, 完全得不到优化.\n\n[^1]: #todo  这里面的Diag是证明来的呀","lastmodified":"2022-10-15T14:06:29.510502668Z","tags":null},"/notes/2022/2022.2/D2L-25-%E8%AE%A9%E8%AE%AD%E7%BB%83%E6%9B%B4%E5%8A%A0%E7%A8%B3%E5%AE%9A-Xavier%E5%88%9D%E5%A7%8B%E5%8C%96":{"title":"D2L-25-让训练更加稳定-Xavier初始化","content":"# 让训练更加稳定\n\n\u003cdiv align=\"right\"\u003e 2022-02-18\u003c/div\u003e\n\nTags: #DeepLearning \n\n## 要点\n- 因为我们无法改变[梯度问题的根本原因](notes/2022/2022.2/D2L-24-数值稳定性.md#问题的由来), 所以我们的目标是**将梯度的值控制在一个合理的范围内**.  \n\n### 改进方向\n- **将乘法变加法**: 如ResNet,  LSTM\n\n- **归一化**: 梯度归一化,  [Gradient Clipping-梯度剪裁](notes/2022/2022.4/D2L-54-Gradient%20Clipping-梯度剪裁.md)\n\n- **合理地初始化权重并选择合适的激活函数**: \n\t- 初始化方案的选择在神经网络学习中起着举足轻重的作用， 它对保持数值稳定性至关重要。\n\t- 此外，这些初始化方案的选择可以与非线性激活函数的选择有趣的结合在一起。\n\n## 梯度归一化\n**目标**: 我们可以将每层的输出 $h_i^s$ 和梯度 $\\frac{\\partial \\ell}{\\partial h_{i}^{s}}$ 都看做随机变量, 通过让它们的均值和方差都保持一致, 实现对梯度的归一化\n\t$$\\begin{gathered}\n\t\\text{正向:}\\quad\\mathbb{E}\\left[h_{i}^{s}\\right]=0 \\quad \n\t\\operatorname{Var}\\left[h_{i}^{s}\\right]=a \\\\\\\\\n\t\\text{反向:}\\quad\\mathbb{E}\\left[\\frac{\\partial \\ell}{\\partial h_{i}^{s}}\\right]=0 \\quad \\operatorname{Var}\n\t\\left[\\frac{\\partial \\ell}{\\partial h_{i}^{s}}\\right]=b \\quad \\forall i, s\\end{gathered}$$\n\t其中 $a,b$ 均为常数.\n\n### 权重初始化\n- 我们可以通过合理的权重初始化来达到上面的目标\n![](notes/2022/2022.2/assets/img_2022-10-15-14.png)\n- 对于一个中等难度的问题, 框架默认的随机初始化通常很有效(例如 $\\mathcal{N}(0,0.01)$ ), 但是对于很深的神经网络则难以保证效果.\n- 现在标准且实用的一种方法叫做**Xavier初始化**\n\n#### Xavier初始化[^1]\n![Xavier初始化的详细例子](notes/2022/2022.2/Xavier初始化的详细例子.md)\n\n- 从例子中可以看出, 如果正向反向都要保证方差一致, 那么需要同时满足 $n_{s}\\gamma_{s}=1$ 和 $n_{s-1} \\gamma_{s}=1$, 其中 $\\gamma_s$ 是第 $s$ 层权重的方差, $n_{s-1}, n_{s}$ 是输入和输出的维度. \n\t- 因为隐藏层的输入和输出维度很可能不一致, 所以这其实是很难满足的.\n\n- **Xavier初始化**则令 $$\\frac{\\gamma_{s}\\left(n_{s-1}+n_{s}\\right)}{2} =1 \\quad \\rightarrow\\quad \\gamma_{s}=\\frac{2}{n_{s-1}+n_{s}}$$\n\t- 尽管在上述数学推理中，“不存在非线性”的假设在神经网络中很容易被违反， 但Xavier初始化方法在实践中被证明是有效的。\n\n- **正态分布** $$\\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{s-1}+n_{s}}}\\right)$$\n- **均匀分布** $$\\mathscr{U}\\left(-\\sqrt{\\frac{6}{n_{s-1}+n_{s}}}, \\sqrt{\\frac{6}{n_{s-1}+n_{s}}}\\right)$$\n\t- 分布 $\\mathscr{U}[-a, a]$ 的方差是 $a^{2} / 3$\n\n- **Bonus Question:** [为什么参数不能初始化为同一个常数?](notes/2022/2022.2/为什么参数不能初始化为同一个常数.md) \n\n### 损失函数的选择\n- 现在我们先假设损失函数是一个线性的函数 $\\sigma(x)=\\alpha x+\\beta$, 来看看输入和输出的均值和方差经过 $\\sigma(x)$ 后会有什么变化.\n\t- 令 $\\mathbf{h}^{\\prime}=\\mathbf{W}^{t} \\mathbf{h}^{t-1} \\quad \\text { and } \\quad \\mathbf{h}^{t}=\\sigma\\left(\\mathbf{h}^{\\prime}\\right)$\n\t- 前面我们已经知道 $\\mathbf{h}^{s}=\\mathbf{W}^{s} \\mathbf{h}^{s-1}$ 时, $\\mathbb{E}(\\mathbf{h}^{s})=\\mathbb{E}(\\mathbf{h}^{s-1})=0$. 且反向也一样, 所以我们根据方差的性质可以知道: \n\t\t$$\\begin{aligned}\n\t\\mathbb{E}\\left[h_{i}^{t}\\right] \u0026=\\mathbb{E}\\left[\\alpha h_{i}^{\\prime}+\\beta\\right]=\\beta \\\\\n\t\\operatorname{Var}\\left[h_{i}^{t}\\right] \u0026=\\mathbb{E}\\left[\\left(h_{i}^{t}\\right)^{2}\\right]-\\mathbb{E}\\left[h_{i}^{t}\\right]^{2} \\\\\n\t\u0026=\\mathbb{E}\\left[\\left(\\alpha h_{i}^{\\prime}+\\beta\\right)^{2}\\right]-\\beta^{2} \\\\\n\t\u0026=\\mathbb{E}\\left[\\alpha^{2}\\left(h_{i}^{\\prime}\\right)^{2}+2 \\alpha \\beta h_{i}^{\\prime}+\\beta^{2}\\right]-\\beta^{2} \\\\\n\t\u0026=\\alpha^{2} \\operatorname{Var}\\left[h_{i}^{\\prime}\\right]\n\t\\end{aligned}$$\n\t如果要保持均值和方差不变, 则要求 $\\beta=0, \\alpha=\\pm1$\n\t- 反向也类似\n\t\t![500](notes/2022/2022.2/assets/Pasted%20image%2020220219125913.png)\n\n- 现在我们来检查各个激活函数的形状:\n\t![](notes/2022/2022.2/assets/Pasted%20image%2020220219125956.png)\n- 使用泰勒展开\n$$\\operatorname{sigmoid}(x)=\\frac{1}{2}+\\frac{x}{4}-\\frac{x^{3}}{48}+O\\left(x^{5}\\right)$$\n$$\\tanh (x)=0+x-\\frac{x^{3}}{3}+O\\left(x^{5}\\right)$$\n$$\\operatorname{ReLU}(x)=0+x \\quad for\\quad  x \\geq 0$$\n可以看到, 除了Sigmoid以外, 其他函数在原点附近都基本满足 $\\alpha=\\pm1, \\beta=0$ 的条件.\n- 我们可以规范化Sigmoid函数: \n\t$$\\text{scaled Sigmoid: }=\\quad 4\\times \\operatorname{Sigmoid}(x)-2$$\n\t减 $2$ 是为了让函数关于原点对称\n\n\n\n\n[^1]: 它以其提出者 [Glorot \u0026 Bengio, 2010](https://zh-v2.d2l.ai/chapter_references/zreferences.html#glorot-bengio-2010) 第一作者的名字命名","lastmodified":"2022-10-15T14:06:29.510502668Z","tags":null},"/notes/2022/2022.2/D2L-26-%E7%8E%AF%E5%A2%83%E5%92%8C%E5%88%86%E5%B8%83%E5%81%8F%E7%A7%BB":{"title":"D2L-26-环境和分布偏移","content":"# Environment and Distribution Shift\n\n\u003cdiv align=\"right\"\u003e 2022-02-19\u003c/div\u003e\n\nTags: #DeepLearning #DistributionShift #CovariateShift\n\n- 环境是变化的，数据也是。有时通过将基于模型的决策引入环境，我们可能会破坏模型[^1]。我们需要合理地调整模型来适应这种可能的变化。\n\n## 分布偏移的类型\n### Covariate Shift - 协变量偏移\n- 顾名思义，就是输入数据（特征、协变量）的分布发生了偏移。也就是说，输入变得不一样了，原来是真实的猫猫狗狗， 现在变成了卡通的猫猫狗狗！\n![covariate Shift](notes/2022/2022.2/assets/covariate%20-Shift.png)\n- 形式化地说, 就是输入数据的分布 $P(X)$ 发生了变化, 但是在输入确定之后, 最终输出的标签分布 $P(y\\ |\\ X)$ 不变.\n\n### Label Shift - 标签偏移\n- 协变量偏移是输入的原始数据分布发生了偏移, 而**标签偏移**则是原始数据对应的标签分布发生了偏移. \n- 比如在鼠疫的诊断上, 鼠疫最初的症状有头痛、双眼充血、咳嗽、以及怠倦感，与普通呼吸道疾病相似。虽然其症状在十三世纪时和现在都是差不多的, 但是随着医学的进步, 鼠疫的发病率已经大大降低了. 所以现在面对相同的症状, 诊断为呼吸道疾病的概率是要大于鼠疫的. 也就是说, 在鼠疫的判断问题上发生了标签偏移.\n![200](notes/2022/2022.2/assets/450px-Yersinia_pestis_fluorescent.jpeg) [^2]\n- 形式化地来说, 就是一个标签对应的特征分布 $P(X\\ |\\ y)$ 不变, 而标签的边缘概率分布 $P(y)$ 发生了变化.\n\n### Concept Shift - 概念偏移\n- 直观的理解, 概念分布就是一个事物的定义(概念)发生了变化, 比如下图表示了美国不同地区对于Soft Drink的不同定义:\n![](notes/2022/2022.2/assets/popvssoda.png)\n- 形式化地来说, Concept Shift就是 $X$ 与 $y$ 的**相互关系**发生了变化[^5], 而它们的概率分布可能不变. \n\n- 更多的例子: [Environment and Distribution Shift — Examples of Distribution Shift](https://d2l.ai/chapter_multilayer-perceptrons/environment.html#examples-of-distribution-shift)\n## 分布偏移: 纠正\n### Empirical Risk\n对于训练数据 $\\left\\{\\left(\\mathbf{x}_{1}, y_{1}\\right), \\ldots,\\left(\\mathbf{x}_{n}, y_{n}\\right)\\right\\}$, 我们最小化损失函数的过程可以表示为: $$\\operatorname{minimize} \\frac{1}{n} \\sum_{i=1}^{n} l\\left(f\\left(\\mathbf{x}_{i}\\right), y_{i}\\right)$$\n在统计的语境里面, 上面的损失也称为 **经验损失** (*Empirical Risk*). 也就是损失 $l(f(\\mathbf{x}), y)$ 在整个数据的真实分布 $p(\\mathbf{x}, y)$ 上面的数学期望:\n$$E_{p(\\mathbf{x}, y)}[l(f(\\mathbf{x}), y)]=\\iint l(f(\\mathbf{x}), y) p(\\mathbf{x}, y) d \\mathbf{x} d y$$\n但是在实际过程中我们不知道数据的真实分布 $p(\\mathbf{x}, y)$, 所以我们只能近似地去最小化Empirical risk.\n\n### Covariate Shift Correction\n- 假设由于Covariate Shift, 特征的分布由 $q(\\mathbf{x})$ 偏移到了 $p(\\mathbf{x})$, 并且标签的分布没有发生变化: $q(y \\mid \\mathbf{x})=p(y \\mid \\mathbf{x})$. 则我们可以用如下等式来对原来的模型进行修正:\n$$\\iint l(f(\\mathbf{x}), y) p(y \\mid \\mathbf{x}) p(\\mathbf{x}) d \\mathbf{x} d y=\\iint l(f(\\mathbf{x}), y) q(y \\mid \\mathbf{x}) q(\\mathbf{x}) \\textcolor{red}{\\frac{p(\\mathbf{x})}{q(\\mathbf{x})}} d \\mathbf{x} d y$$\n- 可以看到关键便是利用系数 $\\beta_{i}$ 来对每一个样本 $\\mathbf{x}_{i}$ 进行修正: $$\\beta_{i} \\stackrel{\\text { def }}{=}\\frac{p\\left(\\mathbf{x}_{i}\\right)}{q\\left(\\mathbf{x}_{i}\\right)}$$\n- 最小化损失函数的过程变为了 $$\\underset{f}{\\operatorname{minimize}} \\frac{1}{n} \\sum_{i=1}^{n} \\textcolor{red}{\\beta_{i}} l\\left(f\\left(\\mathbf{x}_{i}\\right), y_{i}\\right)$$\n\n#### 那么怎么估计 $\\beta_{i}$ 呢?\n- 因为训练样本是已知的, 所以 $q(\\mathbf{x})$ 很好计算, 但是偏移之后的 $p(\\mathbf{x})$ 则不是很好计算. 尽管有一些花哨的方法[^3]可以用来估计 $\\beta_{i}$ , 但我们也可以用简单的 [Logistic_Regression](notes/2021/2021.8/Part.12_Logistic_Regression(ML_Andrew.Ng.).md)来解决这个问题:\n- 我们的想法是: 利用Logistic Regression来训练一个分类器, 用来区分 $p(\\mathbf{x})$ 和 $q(\\mathbf{x})$. \n\t- 如果分类器不能区分的话, 说明这个样本没有发生偏移; \n\t- 对应的, 要是分类器能够区分的话, 我们便利用分类器的输出对样本进行相应的加权(利用Logistic回归的输出来生成权重 $\\beta_{i}$)\n\n- 为了简单起见, 我们假设 $p(\\mathbf{x})$ 和 $q(\\mathbf{x})$ 有同样数量的样本 $X=\\mathbf{\\{x_1,\\cdots, x_n\\}}$ 和 $U=\\mathbf{\\{u_1,\\cdots, u_n\\}}$, 对于从新的分布 $p$ 里面采样的样本, 我们令标签 $z=1$, 从旧的分布 $q$ 里面采样的样本, 我们令标签 $z=-1$, 则得到样本集合: $$\\left\\{\\left(\\mathbf{x}_{1},-1\\right), \\ldots,\\left(\\mathbf{x}_{n},-1\\right),\\left(\\mathbf{u}_{1}, 1\\right), \\ldots,\\left(\\mathbf{u}_{m}, 1\\right)\\right\\}$$\n\t- 因为有:\n\t$$P(z=1 \\mid \\mathbf{x})=\\frac{p(\\mathbf{x})}{p(\\mathbf{x})+q(\\mathbf{x})} \\text{ and }P(z=-1 \\mid \\mathbf{x})=\\frac{q(\\mathbf{x})}{p(\\mathbf{x})+q(\\mathbf{x})}$$\n\t- 两式相除: $$ \\frac{P(z=1 \\mid \\mathbf{x})}{P(z=-1 \\mid \\mathbf{x})}=\\frac{p(\\mathbf{x})}{q(\\mathbf{x})}$$\n\t- 我们又知道 $$P(z=1 \\mid \\mathbf{x})=\\frac{1}{1+\\exp (-h(\\mathbf{x}))}$$\n\t- 所以有: $$\\beta_{i}=\\frac{1 /\\left(1+\\exp \\left(-h\\left(\\mathbf{x}_{i}\\right)\\right)\\right)}{\\exp \\left(-h\\left(\\mathbf{x}_{i}\\right)\\right) /\\left(1+\\exp \\left(-h\\left(\\mathbf{x}_{i}\\right)\\right)\\right)}=\\exp \\left(h\\left(\\mathbf{x}_{i}\\right)\\right)$$\n\t- 这便是权重的估计值, 如果考虑到范围还可以约束为 $\\beta_{i}=\\min \\left(\\exp \\left(h\\left(\\mathbf{x}_{i}\\right)\\right), c\\right)$, $c$ 为常量.\n\t- 注意上面的算法依赖于一个重要的假设： 需要目标分布中的每个数据样本在训练时出现的概率非零(就是说偏移后的数据也得在原来的分布里面存在: $q(\\mathbf{x})\\neq0$ )。 如果我们找到 $p(\\mathbf{x})\u003e0$ 但 $q(\\mathbf{x})=0$ 的点， 那么相应的权重 $\\beta_{i}$ 会是无穷大。\n\n### Label Shift Correction\n- 标签偏移的纠正方法和上面很相似. 我们假设标签的分布从 $q(y)$ 偏移到了 $p(y)$, 同时每一个类的特征分布是不变的: $q(\\mathbf{x} \\mid y)=p(\\mathbf{x} \\mid y)$. 那么我们可以根据下面的等式来纠正标签偏移:\n$$\\iint l(f(\\mathbf{x}), y) p(\\mathbf{x} \\mid y) p(y) d \\mathbf{x} d y=\\iint l(f(\\mathbf{x}), y) q(\\mathbf{x} \\mid y) q(y) \\textcolor{red}{\\frac{p(y)}{q(y)}} d \\mathbf{x} d y$$\n- 同样, 修正的权重可以定义为: $$\\beta_{i} \\stackrel{\\text { def }}{=} \\frac{p\\left(y_{i}\\right)}{q\\left(y_{i}\\right)}$$\n- 训练数据里面的标签分布 $q(y_{i})$ 是已知的, 所以我们只需要估计 $p(y_{i})$\n#### 怎么估计 $p(y_{i})$ 呢\n- 如果标签偏移的程度不是特别大, 我们其实可以将原始的标签 $q(\\mathbf{y})$ 进行线性变换来得到新的输出: $p(\\mathbf{y})$\n- 详细的来说, 我们可以构造一个 $k\\times k$ 的混淆矩阵[^4] $C$ ($k$ 是类别的数目). 其中列索引对应Validation set数据的真实标签, 行索引则对应模型的输出标签. 每一个元素 $c_{ij}$ 的值则是整个Validation Set里面, 模型把类别 $j$ 预测为类别 $i$ 的概率.\n\t- 举个例子, 要是数据没有发生偏移, 那么C就是一个单位矩阵. 模型和单位矩阵差的越多, 标签偏移就越严重.\n\n- 我们可以将模型实际预测时的结果记录下来, 得到输出 $\\mu(\\hat{\\mathbf{y}}) \\in \\mathbb{R}^{k}$, 其中第 $i$ 个分量 $\\mu\\left(\\hat{y}_{i}\\right)$ 是模型预测值为 $i$ 的概率.\n- 将所有模型输出为 $i$ 的情况统计起来,  有 $$\\sum_{j=1}^{k} c_{i j} p\\left(y_{j}\\right)=\\mu\\left(\\hat{y}_{i}\\right)$$\n- 所以 $$\\mathbf{C} p(\\mathbf{y})=\\mu(\\hat{\\mathbf{y}})$$\n- 只要这个分类器在一开始是足够准确的, 并且偏移的程度不太大, 那么矩阵C就是可逆的, 我们便能够根据 $C$ 和 $\\mu(\\hat{\\mathbf{y}})$ 近似得到新的标签分布 $p(\\mathbf{y})=C^{-1}\\mu(\\hat{\\mathbf{y}})$\n\n### Concept Shift Correction\n- 概念偏移通常是很难修正的, 因为\"X和y相互关系的变化\"通常很难衡量. 举个极端的例子, 将分类猫猫狗狗的分类器拿去区分老虎和大象, 效果肯定不好. 这时候通常我们只能重新收集数据进行训练.\n\n\n\n\n\n\n[^1]: Say, for example, that we trained a model to predict who will repay vs. default on a loan, finding that an applicant’s choice of footwear was associated with the risk of default (Oxfords indicate repayment, sneakers indicate default). We might be inclined to thereafter grant loans to all applicants wearing Oxfords and to deny all applicants wearing sneakers.   In this case, our ill-considered leap from pattern recognition to decision-making and our failure to critically consider the environment might have disastrous consequences. For starters, as soon as we began making decisions based on footwear, customers would catch on and change their behavior. Before long, all applicants would be wearing Oxfords, without any coinciding improvement in credit-worthiness. Similar issues abound in many applications of machine learning: by introducing our model-based decisions to the environment, we might break the model. [4.9. Environment and Distribution Shift — Dive into Deep Learning 0.17.5 documentation](https://d2l.ai/chapter_multilayer-perceptrons/environment.html#environment-and-distribution-shift)\n\n[^2]: [鼠疫 - 維基百科，自由的百科全書](https://zh.wikipedia.org/zh-tw/%E9%BC%A0%E7%96%AB)\n[^3]: [4.9. Environment and Distribution Shift — Dive into Deep Learning 0.17.5 documentation](https://d2l.ai/chapter_multilayer-perceptrons/environment.html#covariate-shift-correction)\n[^4]: Confusion Matrix\n[^5]: [machine learning - Difference between distribution shift and data shift, concept drift and model drift - Cross Validated](https://stats.stackexchange.com/questions/548405/difference-between-distribution-shift-and-data-shift-concept-drift-and-model-dr)","lastmodified":"2022-10-15T14:06:29.510502668Z","tags":null},"/notes/2022/2022.2/D2L-27-Computation-%E5%B1%82%E5%92%8C%E5%9D%97":{"title":"D2L-27-Computation-层和块","content":"# 深度学习计算: 使用*层* 和*块*\n\n\u003cdiv align=\"right\"\u003e 2022-02-19\u003c/div\u003e\n\nTags: #DeepLearning #Computation #PyTorch \n\n- 这一章主要介绍框架的使用细节, 最好的方法就是结合代码示例, 边运行边理解. 这里我们记录一些容易忽略的要点.\n\t- [在线代码实例: 5.1. 层和块](https://zh-v2.d2l.ai/chapter_deep-learning-computation/model-construction.html#sec-model-construction \"Permalink to this headline\")\n---\n\n## 层和块: 定义\n- **\"层\"具有三个特征:** \n\t1. 接受一组输入， \n\t2. 生成相应的输出，\n\t3. 由一组可调整参数描述。\n\n- 事实证明，**研究讨论“比单个层大”但“比整个模型小”的组件更有价值**。 例如，在计算机视觉中广泛流行的ResNet-152架构就有数百层， 这些层是由 _**层组**_（groups of layers）的重复模式组成。\n\t- [我们可以自定义一个层](notes/2022/2022.2/D2L-29-Computation-自定义层.md)\n\n\n- 为了实现这些复杂的网络，我们引入了神经网络 _**块**_ 的概念。 _块_（block）可以描述单个层、由多个层组成的组件或整个模型本身。\n![](notes/2022/2022.2/assets/blocks.svg)\n## 层和块: 实现\n\n- 从编程的角度来看，块由 _**类**_ （class）表示。 \n\t- 它的任何子类都必须定义一个将其输入转换为输出的**前向传播函数**`forward()`， 并且必须存储任何必需的参数。\n\t- 反向传播与自动求导通常由框架自动完成.\n\n\n\n\n\n\n\n","lastmodified":"2022-10-15T14:06:29.510502668Z","tags":null},"/notes/2022/2022.2/D2L-28-Computation-%E5%8F%82%E6%95%B0%E7%AE%A1%E7%90%86":{"title":"D2L-28-Computation-参数管理","content":"# 深度学习计算: 参数管理\n\n\u003cdiv align=\"right\"\u003e 2022-02-19\u003c/div\u003e\n\nTags: #DeepLearning #Computation #Parameter #PyTorch \n\n- [在线代码实例: 5.2. 参数管理](https://zh-v2.d2l.ai/chapter_deep-learning-computation/parameters.html#id1 \"Permalink to this headline\")\n---\n本节主要有以下内容：\n-   访问参数，用于调试、诊断和可视化。\n-   参数初始化。\n-   在不同模型组件间共享参数。(保持某几个层的参数是同步的)\n\n## 延后初始化[¶](https://zh-v2.d2l.ai/chapter_deep-learning-computation/deferred-init.html#sec-deferred-init \"Permalink to this headline\")\n- 深度学习框架无法判断网络的输入维度是什么。 这里的诀窍是框架的 _延后初始化_（defers initialization）， 即直到数据第一次通过模型传递时，框架才会动态地推断出每个层的大小。\n- 这个在MXNET 和 Tensorflow 里面有, PyTorch还不太完善, 不过LazyLinear可以达到类似的功能","lastmodified":"2022-10-15T14:06:29.510502668Z","tags":null},"/notes/2022/2022.2/D2L-29-Computation-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B1%82":{"title":"D2L-29-Computation-自定义层","content":"# 深度学习计算: 自定义一个层\n\n\u003cdiv align=\"right\"\u003e 2022-02-19\u003c/div\u003e\n\nTags: #DeepLearning #Computation #PyTorch \n\n - [在线代码实例: 5.4. 自定义层](https://zh-v2.d2l.ai/chapter_deep-learning-computation/custom-layer.html#id1 \"Permalink to this headline\")\n---\n-   我们可以通过基本层类设计自定义层。这允许我们定义灵活的新层。\n-   在自定义层定义完成后，我们就可以在任意环境和网络架构中调用该自定义层。\n-   层可以有局部参数，这些参数可以通过内置函数创建。","lastmodified":"2022-10-15T14:06:29.510502668Z","tags":null},"/notes/2022/2022.2/D2L-30-Computation-%E8%AF%BB%E5%86%99%E6%96%87%E4%BB%B6":{"title":"D2L-30-Computation-读写文件","content":"# 深度学习计算: 读写文件\n\n\u003cdiv align=\"right\"\u003e 2022-02-19\u003c/div\u003e\n\nTags: #DeepLearning #Computation #PyTorch \n\n - [在线代码实例: 5.5. 读写文件](https://zh-v2.d2l.ai/chapter_deep-learning-computation/read-write.html#id1 \"Permalink to this headline\")\n---\n- 我们可以保存一个张量, 或者张量的字典和列表\n- 我们可以通过**参数字典**保存和加载网络的全部参数, 但是Pytorch中, 模型的定义需要用其他方法来保存.\n","lastmodified":"2022-10-15T14:06:29.510502668Z","tags":null},"/notes/2022/2022.2/D2L-31-Computation-%E8%B4%AD%E4%B9%B0%E4%B8%8E%E4%BD%BF%E7%94%A8GPU":{"title":"D2L-31-Computation-购买与使用GPU","content":"# 深度学习计算: 购买与使用GPU\n\n\u003cdiv align=\"right\"\u003e 2022-02-19\u003c/div\u003e\n\nTags: #DeepLearning #Computation #PyTorch #GPU\n\n- 购买与搭建计算平台: [16.4. 选择服务器和GPU](https://zh-v2.d2l.ai/chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html#gpu \"Permalink to this headline\")\n\n- Pytorch使用GPU: [5.6. GPU — 动手学深度学习](https://zh-v2.d2l.ai/chapter_deep-learning-computation/use-gpu.html)\n\t- Tesla GPU 和马斯克没啥关系: \n\t\u003e NVIDIA提供两种类型的GPU，针对个人用户（例如，通过GTX和RTX系列）和**企业用户（通过其Tesla系列）**","lastmodified":"2022-10-15T14:06:29.510502668Z","tags":null},"/notes/2022/2022.2/D2L-32-Convolution-%E5%8D%B7%E7%A7%AF":{"title":"D2L-32-Convolution-卷积","content":"# 卷积 - Convolution\n\n\u003cdiv align=\"right\"\u003e 2022-02-26\u003c/div\u003e\n\nTags: #DeepLearning #Convolution \n\n**关键点:**\n- Convolution Determines the Output of a System for any Input[^2]\n\n- 从数学的角度理解, 卷积是通过两个函数  ($f$ 和 $g$ ) 生成第三个函数 ($f*g$) 的一种数学算子, 卷积函数 $f*g$ 表示了两个函数相互影响的结果. \n\n## 卷积: 直观理解\n### 卷积可以衡量许多瞬时输入的累计影响\n- [The Motivation of Convolution 这是一个交互的例子, 强烈建议点进去](https://lpsa.swarthmore.edu/Convolution/sbh.html)\n\n\t- 以室外温度对室内温度的影响为例, 假设某一天的气温对于以后几天的室内温度的影响呈如下指数分布: \n\t![impulse-response-ht-vs-t|350](notes/2022/2022.2/assets/impulse-response-ht-vs-t.svg)\n\t- 我们可以看到影响随着时间的增长衰减的很快. \n\t- 显然室外温度变化是一个连续的函数, 如下面蓝色曲线所示. 而这个蓝色曲线上每一点对室内气温的影响的变化, 都是上方的红色曲线. \n\t\t![flambda-ext-t-and-ht-lam|350](notes/2022/2022.2/assets/flambda-ext-t-and-ht-lam.svg)\n\t- 我们怎样衡量今天以前**所有的室外气温**对今日气温的总影响呢? 这就要求我们计算无数个瞬时输入的累计影响, 而卷积可以做到这一点: \n\t\t- 我们计算 $f(\\lambda)\\cdot h(t-\\lambda)$ 相对于 $\\lambda$ 的变化曲线: \n\t\t![flambdamiddotht-lambda-v|350](notes/2022/2022.2/assets/flambdamiddotht-lambda-v.svg)\n\t\t- 这就是 $t$ 时刻, 所有影响在时间上的分布, 为了计算累计影响, 我们需要对这个函数关于 $\\lambda$ 进行积分: $$y(t)=\\int_{-\\infty}^{+\\infty} h(t-\\lambda) \\cdot f(\\lambda) \\cdot d \\lambda$$\n\t\t- 上式就是卷积 $y(t)=h*f$ 的定义, 通过衡量输入 $f$ 在 $t$ 时刻以前的所有影响 $h$, 我们得到了累计影响随着时间的变化 $y(t)$\n\t\t![external-and-internal-te|350](notes/2022/2022.2/assets/external-and-internal-te.svg)\n\n- **Some minor points**\n\t- 因为衡量的是 $t$ 时刻以前的累计影响, 所以我们需要\"翻转\"影响函数 $h$ 再进行\"滑动\".\n\t- 对于每一个时刻 $t$, 我们都需要计算整个坐标轴上面所有点对于 $t$ 时刻的影响, 这也是为什么 $y(t)$ 的定义里面有一个积分符号.\n\t- [Live Demo](https://lpsa.swarthmore.edu/Convolution/CI.html)\n\n### 其他理解\n- 下面是两个从离散变量角度来理解的例子\n\u003ciframe width=\"560\" height=\"315\" src=\" https://www.youtube.com/embed/MQm6ZP1F6ms?start=32\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n\u003ciframe width=\"560\" height=\"315\" src=\" https://www.youtube.com/embed/aEGboJxmq-w\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n- 这是卷积在图像处理时的应用.\n\u003ciframe width=\"560\" height=\"315\" src=\" https://www.youtube.com/embed/8rrHTtUzyZA\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n## 卷积的数学定义\n$$(f * g)(t):=\\int_{-\\infty}^{\\infty} f (\\tau) g (t-\\tau) d \\tau$$\n\n### 一些性质[^3]\n- **交换律Commutativity:** 既然可以 $g$ 滑动 $f$ 不动, 那么肯定也可以 $f$ 滑动 $g$ 不动, 它们重叠的面积肯定是不会变的, 这就有了交换律:\n\t$$(f * g)(t)=\\int_{-\\infty}^{\\infty} f (\\tau) g (t-\\tau) d \\tau=\\int_{-\\infty}^{\\infty} f (t-\\tau) g (\\tau) d \\tau$$\n\tin short: $$f * g=g * f$$\n- **结合律Associativity:** 假设有函数 $f,g,h$, 那么可以先叠加 $f$ 和 $g$ 的影响, 再叠加 $h$ 的影响, 也可以相叠加 $g$ 和 $h$ 的影响, 再叠加 $f$ 的影响, 结果肯定是不变的. \n\t$$f *(g * h)=(f * g) * h$$\n\t\n- **分配律Distributivity:**\n\t$$f *(g+h)=(f * g)+(f * h)$$\n\n## 卷积与交叉相关 Convolution \u0026 Cross-Correlation\n![](notes/2022/2022.2/assets/Comparison_convolution_correlation.svg)来源: [^1]\n\n### 啥是交叉相关\n- [Cross-correlation - Wikipedia](https://en.wikipedia.org/wiki/Cross-correlation)\n\t实数域上, 交叉相关是\n\t$$(f \\star g)(t) := \\int_{-\\infty}^{\\infty} f(\\tau) g(t+\\tau) d \\tau$$\n\t交叉相关和卷积就差了一个正负号\n\t- In signal processing, cross-correlation is **a measure of similarity of two series (序列)** as a function of the displacement of one relative to the other.\n\n- 交叉相关用于**衡量图片相似度**的一个例子\n\u003ciframe width=\"560\" height=\"315\" src=\" https://www.youtube.com/embed/MQm6ZP1F6ms?start=424\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n- 两者的差别不大, 含义都是类似的.\n\n- 在计算卷积核的时候, 实际上我们计算的是**交叉相关**, 这是深度学习里面的一个术语误用.\n\t- [19 卷积层【动手学深度学习v2】](https://www.bilibili.com/video/BV1L64y1m7Nh?p=2\u0026t=415.1)\n\t- [Convolution vs Cross Correlation - YouTube](https://youtu.be/C3EEy8adxvc)\n\t\t![](notes/2022/2022.2/assets/Pasted%20image%2020220226172550.png)\n\n## 数值计算: 卷积与傅里叶变换\n- 卷积的计算复杂度是很高的, 所以我们想要找到一个高效的方法来计算卷积, 幸运的是, 卷积与傅里叶变换有以下联系 (Convolution Theorem): \n\t$$\\mathcal{F}(f*g)=\\mathcal{F}(f)\\mathcal{F}(g)$$\n\t也就是说\n\t$$f*g=\\mathcal{F}^{-1}\\left(\\mathcal{F}(f)\\mathcal{F}(g)\\right)$$\n\t我们可以用傅里叶变换来代替卷积运算.\n\t\n- 推导: \n\t\u003ciframe width=\"560\" height=\"315\" src=\" https://www.youtube.com/embed/mOiY1fOROOg\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n- 对于Cross-Correlation也是一样的, 只不过是在推导的时候把 $t-\\tau$ 变成 $t+\\tau$\n\n- Fast Fourier transfer (FFT) reduces the complexity of convolution from  $𝑂(𝑛^2)$ to $𝑂(𝑛\\log𝑛)$\n- GPU-accelerated FFT implementations perform up to 10 times faster than CPU only alternatives. (e.g. NVIDIA CUDA libraries) .\n\n- 一个更详细的实例解释\n\t\u003ciframe width=\"560\" height=\"315\" src=\" https://www.youtube.com/embed/8rrHTtUzyZA?start=1568\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n---\n下一部分谈论卷积神经网络\n\n\n[^1]: [Convolution - Wikipedia](https://en.wikipedia.org/wiki/Convolution#/media/File:Comparison_convolution_correlation.svg)\n[^2]: [The Convolution as A Sum of Impulse Responses](https://lpsa.swarthmore.edu/Convolution/Convolution.html)\n[^3]: [Convolution - Wikipedia](https://en.wikipedia.org/wiki/Convolution#Properties)","lastmodified":"2022-10-15T14:06:29.510502668Z","tags":null},"/notes/2022/2022.2/D2L-33-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN":{"title":"D2L-33-卷积神经网络CNN","content":"# Convolutional Neural Network - 卷积神经网络\n\n\u003cdiv align=\"right\"\u003e 2022-02-26\u003c/div\u003e\n\nTags: #CNN #DeepLearning #Convolution \n\n## MLP的不足\n随着图像分辨率的提高, MLP显露出以下不足: \n- 假设我们的图像分辨率为 $1920\\times 1080$, 那么一张图片就有 $2,073,600$ 个像素点, 假设和输入层相连的隐藏层有 $1000$ 个单元, 那么光是第一个全连接层就有大约 $2\\times10^9$ ($20$ 亿) 个参数, 训练这样的网络是难以想象的, 况且这还只是网络的第一层.\n\n- 为了减少网络参数的大小, 我们需要想办法缩减参数的大小.\n\n## 从全连接层到卷积 - From Fully-Connected Layers to Convolutions\n### MLP推广到二维：权重变成了四维张量\n之前 MLP 的输入输出都是一维的：\n![mlp](notes/2022/2022.2/assets/mlp.svg)\n图像可以看作像素点构成的二维矩阵。如果网络输入输出的形状都是二维的矩阵的话, 那么网络每一层的权重会变成一个四维张量:\n![四维权重](notes/2022/2022.2/assets/四维权重.svg)\n规范地说, 就是: $$h_{i, j}=\\sum_{k, l} w_{i, j, k, l} x_{k, l}$$\n其中索引 $k,l$ 的作用是对于每一个隐藏单元, 遍历所有输入 $X$; 索引 $i,j$ 的作用是遍历所有的隐藏单元（$H_h\\times H_w$）.\n\n### 平移不变性  Translation Invariance\n![400](notes/2022/2022.2/assets/Translation_of_a_set.svg)[^1]\n\n- 在一张图片里面平移一个物体, 这个物体的形状应该是不变的. 所以我们想, 能不能利用这种相似性来减少参数的大小呢?\n![400](notes/2022/2022.2/assets/Pasted%20image%2020220227105222.png)\n- **Parameter Sharing :** 既然物体在图像哪里的形状都一样, 那么在图像任意位置识别这个物体的权重也大致相似. 也就是说, 我们可以在各个隐藏单元之间\"分享权重\".\n- 数学上说, 就是: $$h_{i, j}=\\sum_{k, l} w_{k, l} x_{k, l}$$ 权重不再和隐藏单元的索引 $i,j$ 相关了, 所有隐藏单元的权重都是同一个矩阵. 这是一个很大的进步!\n![减少了很多参数](notes/2022/2022.2/assets/减少了很多参数.svg)\n\n- 平移不变性还意味着输入中\"物体的平移\"将导致输出中\"激活的平移\"\n\n### 局部性 Local Connectivity\n![](notes/2022/2022.2/assets/Pasted%20image%2020220227114101.png)\n- 一张图像可以看作一团像素点, 而图像中相隔较远的像素点往往联系不大. 我们可以利用这种\"重要性的衰减\"来进一步缩减参数的大小.\n- 要缩减我们关注像素点的范围, 自然需要减小权重矩阵的大小, 为了方便数学上的表示, 我们对索引 $k,l$ 进行以下变换:\n\t$$\\begin{aligned}h_{i, j}\u0026=\\sum_{k, l} w_{k, l} x_{k, l}\\\\\n\t\u0026=\\sum_{a, b} v_{a, b} x_{i+a, j+b}\n\t\\end{aligned}$$\n\t- 其中 $w_{k,l}=v_{i+a, j+b}$ , 如果 $x$ 下标的范围不变, 这就意味着 $a\\in[-i,k-i],$  $b\\in[-j,l-j]$, 其实就相当于\"以 $(i, j)$ 为中心开始计算 $h_{i, j}$ \"\n- 缩减参数的范围意味着要丢弃距离 $(i, j)$ 太远的权重, 也就是说：\n\t当 $|a|,|b|\u003e\\Delta$ 时, 使得 $v_{a, b}=0$\n\t$$h_{i, j}=\\sum_{a=-\\Delta}^{\\Delta} \\sum_{b=-\\Delta}^{\\Delta} v_{a, b} x_{i+a, j+b}$$\n\t\n### 卷积是一种特殊的全连接\n- 平移不变性和局部性是我们对全连接层的进一步约束, 也就是说: **卷积是一种特殊的全连接层**.\n\n- 相比全连接层 (FC Layer), 卷积层有更少的参数, 自然有着更快的运算速度与更小的存储开销\n\t![400](notes/2022/2022.2/assets/Pasted%20image%2020220227120249.png)\n#### 感受野的增长\n![400](notes/2022/2022.2/assets/Pasted%20image%2020220227120443.png)\n- 局部性是否意味着深度神经网络只注意细节而\"不观大局\"呢?\n- 其实深度神经网络里面较深的层通常能间接地 (Indirectly) 关联更多的输入, 这使得神经网络能够从简单而稀疏的关系中组合出复杂的结构.\n- 例如, 上图中第二层的 $h_i$ 只关联了3个输入, 而 $g_3$ 关联了所有五个输入. \n\n### 卷积层的 [归纳偏置 Inductive bias](notes/2022/2022.2/归纳偏置-Inductive%20bias%20-%20learning%20bias.md)\n- 卷积层中所有的权重学习都将依赖于归纳偏置(即局部性和平移不变性)。当这种偏置与现实相符时，我们就能得到样本有效的模型，并且这些模型能很好地泛化到未知数据中。\n- 但如果这偏置与现实不符时，比如当图像不满足平移不变时，我们的模型可能难以拟合我们的训练数据。\n\u003e **一个例子**: We build a face detector. It works well on all benchmarks. Unfortunately it fails on test data—the offending examples are close-ups where the face fills the entire image (no such data were in the training set).[^4]\n\n## 进一步拓展: Channels - 通道\n- 上面我们只是单纯地将图像看作像素点的二维矩阵, 但是实际上彩色图像有RGB三个通道 (Channel). 也就是说: 图像不是二维张量，而是一个由高度、宽度和颜色组成的三维张量.\n- 因此，我们将 $X$ 索引为 $x_{i, j, k}$ . 对于图像的不同通道, 我们单独设置一个权重矩阵 $v_{a,\\ b,\\ i}$, 由此卷积相应地调整为 $v_{a,\\ b,\\ c}$ , 最后我们将不同输入通道的卷积结果相加:\n\t$$h_{i, j}=\\sum_{a=-\\Delta}^{\\Delta} \\sum_{b=-\\Delta}^{\\Delta}\\sum_c v_{a,\\ b,\\ c}\\ x_{i+a,\\ j+b,\\ c}$$\n\t- 现在我们将一个三维的图像 $X_{ijk}$ 变换为了一个二维的图像 $H_{ij}$, 权重变成了一个三维的张量 $v_{a,\\ b,\\ c}$\n\t\t![](notes/2022/2022.2/assets/conv-multi-in.svg)\n\n- 类比图像的颜色通道, 我们的隐藏表示 $H$ 能否也采用三维张量呢?  \n\t- 实践表明这样是可以增强网络的表现能力的[^2], 这些通道有时也被称为_特征映射_（feature maps），因为每个通道都向后续层提供一组空间化的学习特征。 \n\t- 直观上你可以想象在靠近输入的底层，一些通道专门识别边缘，而一些通道专门识别纹理。换句话说，对于每一个空间位置，我们采用一组而不是一个隐藏表示。\n\t ![300](notes/2022/2022.2/assets/Pasted%20image%2020220227122429.png)[^3]\n\n- 为了支持输入 $X$ 和隐藏表示 $H$ 中的多个通道，我们可以在权重 $V$ 中添加第四个坐标，即 $v_{a,\\ b,\\ c,\\ d}$  , 综上所述:\n\t$$h_{i,\\ j,\\ d}=\\sum_{a=-\\Delta}^{\\Delta} \\sum_{b=-\\Delta}^{\\Delta}\\sum_c v_{a,\\ b,\\ c,\\ d}\\ x_{i+a,\\ j+b,\\ c}$$\n\t- 多个隐藏层通道意味着每一层有许多\"块\"不同的卷积核.\n\t\t![卷积层权重大小的计算](notes/2022/2022.2/assets/卷积层权重大小的计算.svg)\n---\n- CNN就是Kernel学习机器:\n\t- CNN学习到Kernel, 用这些Kernel处理输入, 得到隐藏层里面不同的特征:\n\t\t![](notes/2022/2022.2/assets/Pasted%20image%2020220227152750.png)\n\t[✂️ From Kernel to CNN - YouTube](https://youtube.com/clip/UgkxpFJvCStGs-5uA7ay8H6_LLSE7Z8HzX_a)\n\n\n\n[^1]: [Translational symmetry - Wikipedia](https://en.wikipedia.org/wiki/Translational_symmetry#/media/File:Translation_of_a_set.svg)\n[^2]: 我自己编的 Reference Needed\n[^3]: [CNN Explainer](https://poloclub.github.io/cnn-explainer/)\n[^4]: [4.9. Environment and Distribution Shift — Dive into Deep Learning 0.17.5 documentation](https://d2l.ai/chapter_multilayer-perceptrons/environment.html#more-anecdotes)","lastmodified":"2022-10-15T14:06:29.510502668Z","tags":null},"/notes/2022/2022.2/D2L-34-%E5%8D%B7%E7%A7%AF%E5%B1%82-%E5%A1%AB%E5%85%85-Padding":{"title":"D2L-34-卷积层 - 填充 - Padding","content":"# Padding - 填充\n\n\u003cdiv align=\"right\"\u003e 2022-02-27\u003c/div\u003e\n\nTags: #DeepLearning #CNN #Padding\n\n- It's always nice to have an interactive example: \n\t- [Convolution Visualizer](https://ezyang.github.io/convolution-visualizer/index.html)\n\t- [CNN Explainer](https://poloclub.github.io/cnn-explainer/)\n\n- 卷积操作会使图像尺寸变小, 填充 (Padding) 可以**减缓**这个过程.\n\n![300](notes/2022/2022.2/assets/img_2022-10-15-3.gif)\n## 尺寸变化: 定量计算\n- 我们假设输入的大小为 $x_h\\times x_w$, 卷积核的大小为 $k_h\\times k_w$\n\n- 不带Padding的卷积: $$x_h\\times x_w \\quad\\stackrel{conv}{\\longrightarrow}\\quad \\left (x_{h}-k_{h}+1\\right) \\times\\left (x_{w}-k_{w}+1\\right)$$\n\n- 假设Padding大小为 $p_h\\times p_w$, 其中 $p_h$ 代表对height的填充（大约一半在顶部，一半在底部）, $p_w$ 代表对width的填充（大约一半在左侧，一半在右侧）\n\t- 注意这里的填充和Pytorch里面的padding有一点不同: Pytorch里面的 `Padding=(4, 2)` 是指height中, 上面填充4排, 下面也填充四排, width同理.\n\t\n\t- 填充后图像大小增加相应的值:\n\t$$x_h\\times x_w \\quad\\stackrel{conv}{\\longrightarrow}\\quad \\left (x_{h}-k_{h}+1+p_{h}\\right) \\times\\left (x_{w}-k_{w}+1+p_{w}\\right)$$\n\n## 填充大小的选取\n- 在许多情况下，我们需要设置 $p_h=k_h−1$ 和 $p_w=k_w−1$，使输入和输出具有相同的高度和宽度 ($x_{h}-k_{h}+1+k_h−1=x_h$)。\n\n- 假设 $k_h$ 是奇数，我们将在高度的两侧填充 $p_h/2$ 行。 但是如果 $k_h$ 是偶数，则一种可能性是在输入顶部填充 $⌈p_h/2⌉$ 行，在底部填充 $⌊p_h/2⌋$ 行,宽度同理。\n\t- 所以卷积神经网络中卷积核的高度和宽度通常为奇数，例如1、3、5或7。这样保持空间维度的同时，我们可以在顶部和底部填充**相同数量**的行，在左侧和右侧填充相同数量的列。\n\n\n## [最全面的大小计算公式](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)\n![](notes/2022/2022.2/assets/img_2022-10-15-15.png)\n","lastmodified":"2022-10-15T14:06:29.510502668Z","tags":null},"/notes/2022/2022.2/D2L-35-%E5%8D%B7%E7%A7%AF%E5%B1%82-%E6%AD%A5%E5%B9%85-Stride":{"title":"D2L-35-卷积层 - 步幅 - Stride","content":"# Stride - 步幅\n\n\u003cdiv align=\"right\"\u003e 2022-02-27\u003c/div\u003e\n\nTags: #DeepLearning #CNN #Stride\n\n- It's always nice to have an interactive example: \n\t- [Convolution Visualizer](https://ezyang.github.io/convolution-visualizer/index.html)\n\t- [CNN Explainer](https://poloclub.github.io/cnn-explainer/)\n\n- 卷积操作会使图像尺寸变小, 增大步幅 (Stride) 可以**加快**这个过程.\n\n![](notes/2022/2022.2/assets/img_2022-10-15-4.gif)\n- 为了高效计算或是缩减采样次数，卷积窗口可以跳过中间位置，每次滑动多个元素。而每次滑动元素的数量就称为_步幅_（*stride*）\n\n## 尺寸变化: 定量计算\n- 通常, 当垂直步幅为 $s_{h}$  (height) 、水平步幅为 $s_{w}$ 时 (width) , 输出形状为\n$$\n\\left\\lfloor\\frac{x_{h}-k_{h}+p_{h}+s_{h}}{s_{h}}\\right\\rfloor \\times\\left\\lfloor\\frac{x_{w}-k_{w}+p_{w}+s_{w}}{s_{w}}\\right\\rfloor\n$$\n\n- 如果我们设置了 $p_{h}=k_{h}-1$ 和 $p_{w}=k_{w}-1$, 则输出形状将简化为 $$\\left\\lfloor\\frac{x_{h}+s_{h}-1}{s_{h}}\\right\\rfloor \\times\\left\\lfloor\\frac{x_{w}+s_{w}-1}{s_{w}}\\right\\rfloor$$ \n\n- 更进一步, 如果输入的高度和宽度可以被垂直和水平步幅整除, 则输出形状将为 $$\\left(\\frac{x_{h}}{s_{h}}\\right) \\times\\left(\\frac{x_{w}} {s_{w}}\\right)$$ ","lastmodified":"2022-10-15T14:06:29.510502668Z","tags":null},"/notes/2022/2022.2/D2L-36-1x1%E5%8D%B7%E7%A7%AF%E5%B1%82":{"title":"D2L-36-1x1卷积层","content":"# $1×1$ 卷积层\n\n\u003cdiv align=\"right\"\u003e 2022-02-27\u003c/div\u003e\n\nTags: #CNN #DeepLearning #Convolution \n\n- $1×1$ 卷积，即 $k_h=k_w=1$，它虽然不能提取相关特征, 但是却能**融合图像的不同通道**, 也是一种很受欢迎的网络结构.\n\n- 它相当于输入形状为 $n_{h} n_{w} \\times c_{i}$ , 权重为 $c_{o} \\times c_{i}$ 的**全连接层** \n\n\t![](notes/2022/2022.2/assets/conv-1x1.svg)\n\n- 在上面这张图里面, 核函数的颜色代表输出的通道, 可以看到这个卷积将3个通道的输入变成了两个通道的输出, 图像的大小不变.\n\n-  $1×1$ 卷积层相当于对于单个像素做MLP\n\t![NiN_PixelMLP](notes/2022/2022.3/assets/NiN_PixelMLP.svg)","lastmodified":"2022-10-15T14:06:29.510502668Z","tags":null},"/notes/2022/2022.2/D2L-37-CNN%E7%9A%84%E8%AE%A1%E7%AE%97%E5%A4%8D%E6%9D%82%E5%BA%A6":{"title":"D2L-37-CNN的计算复杂度","content":"# CNN的计算复杂度\n\n\u003cdiv align=\"right\"\u003e 2022-02-27\u003c/div\u003e\n\nTags: #DeepLearning #CNN #ComputationalComplexity \n\n- 前面我们提到过利用傅里叶变换可以快速地计算卷积: [数值计算 卷积与傅里叶变换](notes/2022/2022.2/D2L-32-Convolution-卷积.md#数值计算%20卷积与傅里叶变换)\n\n- 动手学深度学习里面给出了一个例子, 说明卷积的计算复杂度其实还是很高的, 只是参数的存储开销较小.\n![](notes/2022/2022.2/assets/img_2022-10-15-16.png)","lastmodified":"2022-10-15T14:06:29.510502668Z","tags":null},"/notes/2022/2022.2/D2L-38-%E6%B1%A0%E5%8C%96%E5%B1%82-Pooling_Layer":{"title":"D2L-38-池化层-Pooling_Layer","content":"# 池化层/汇聚层 - Pooling Layer\n\n\u003cdiv align=\"right\"\u003e 2022-02-27\u003c/div\u003e\n\nTags: #PoolingLayer #DeepLearning #CNN \n\n## 首先，池化层为什么叫“池化层”\n\u003e  [Collins Dictionary - Pool](https://www.collinsdictionary.com/dictionary/english/pool)   7. verb\n\u003e - If a group of people or organizations *pool* their money, knowledge, or equipment, they share it or put it together so that it can be used for a particular purpose.\n\u003e\t -\tWe *pooled* ideas and information. \n\u003e\t - Philip and I *pooled* our savings to start up my business. \n\u003e\n\u003e - **Synonyms:** combine, share, merge, put together \n\n- 从字面上理解, Pool作为动词的时候有\"汇集许多个体\"的含义, 这和池化层的操作比较相似\n\n![](notes/2022/2022.2/assets/img_2022-10-15-5.gif)\n[^1]\n\n## Pooling的作用\n- CNN对于物体的位置实际上是很敏感的, 目标物体一点点的变化就会导致卷积层的输出发生对应的变化. \n- 而CNN最终解决的问题其实是和整个图像相关的 (例如: 图像里面有没有小企鹅?) 这就要求网络的最后一层的感受野要覆盖整个输入图像. 整个过程相当于是在逐渐聚合信息 (aggregating information), 生成越来越粗糙的映射 (coarser and coarser maps) , 既在网络的中段充分利用卷积层的优势 (平移不变性, 局部性等), 又能输出我们想要的全局信息. \n\n- Pooling层的作用就是\"让映射具有粗糙性\", 或者说, 让Short-sighted的卷积层有\"全局观\", 对目标的位置不那么敏感.\n- 此外, 池化层还有**下采样**的作用 (让图像变小), 这可以加快运算.\n\t- _Pooling layers_ serve the dual purposes of mitigating the sensitivity of convolutional layers to location and of spatially downsampling representations.[^2]\n\n- 因为池化层的作用和卷积紧密相关, 所以池化层一般是跟在卷积之后的.\n\n- 吴恩达说池化层的作用是实践出来的, 应该没有人能说清楚其中的门道.\n\t[✂️ Andrew Ng Explain Max Pooling - YouTube](https://youtube.com/clip/UgkxKiNbvqiUlN91gNNwPUcFR-0oCOwmR2uk)\n\t\n### 举几个例子加深理解\n![](notes/2022/2022.2/assets/img_2022-10-15-17.png)\n- 如果上图中的输出值代表检测到目标的可能性, 那么池化层综合考虑了四个卷积操作的结果, 用\"分治\"的思想来判断哪个区域存在目标.\n- 下图也是这个意思, 无论三个filter里面的哪一个识别到了数字五, max pool都会输出正确的结果.  这也和Pool的字面意思不谋而合: 综合许多人的结果/资源.\n![](notes/2022/2022.2/assets/img_2022-10-15-18.png)\n- 下图演示了池化层可以增强卷积对微小平移的鲁棒性. 因为我们往往更关注图中是否存在目标, 而不是目标出现在了哪个精确的位置.\n![](notes/2022/2022.2/assets/img_2022-10-15-19.png)\n- 从上图可以看出, 尽管所有的输入都发生了变化, 但只有一半的Pool输出发生了变化. 这是因为Max Pool只关心它能看到的最大值, 而不是最大值出现的位置.\n\n## 两种池化\n- 和卷积层类似, 池化层也是通过一个滑动的窗口来进行池化运算的, 自然也可以通过进行[填充](notes/2022/2022.2/D2L-34-卷积层%20-%20填充%20-%20Padding.md) 和设置[步幅](notes/2022/2022.2/D2L-35-卷积层%20-%20步幅%20-%20Stride.md) 来调整输出的尺寸. \n- 和卷积不同的是, 池化层没有Kernel, 池化层只能进行固定的两种运算: Max 或 Avg. 这也意味着池化层是没有参数可以进行学习的.\n\n\t- 池化层还能进行p-norm运算, 具体作用我没有深入了解\n\n![400](notes/2022/2022.2/assets/img_2022-10-15-20.png)\n\n![400](notes/2022/2022.2/assets/img_2022-10-15-21.png)\n## 多通道的情况: 逐个Pooling\n- 池化层和卷积层的另一个不同在于: **池化层不改变输入的通道数, 对每一个通道单独进行池化操作**. \n\t- 相反, 卷积将输入的多个通道根据卷积核转换为一个或多个输出通道.\n\n![Pooling-NoChangeInChannels](notes/2022/2022.2/assets/Pooling-NoChangeInChannels.svg)\n\n\n\n\n[^1]: [Comprehensive Guide to Different Pooling Layers in Deep Learning](https://analyticsindiamag.com/comprehensive-guide-to-different-pooling-layers-in-deep-learning/)\n[^2]: [6.5. Pooling — Dive into Deep Learning 0.17.2 documentation](https://d2l.ai/chapter_convolutional-neural-networks/pooling.html#pooling)","lastmodified":"2022-10-15T14:06:29.510502668Z","tags":null},"/notes/2022/2022.2/D2L-39-LeNet":{"title":"D2L-39-LeNet","content":"# LeNet\n\n\u003cdiv align=\"right\"\u003e 2022-03-01\u003c/div\u003e\n\nTags: #LeNet #DeepLearning #CNN #NeuralNetwork \n\n## 架构\n![](notes/2022/2022.2/assets/lenet.svg)[^1]\n- 总体来看，LeNet（LeNet-5）由两个部分组成：\n\t-   **卷积编码器**：由两个卷积层组成;\n\t-   **全连接层密集块**：由三个全连接层组成。\n\n- 每个卷积块中的基本单元是**一个卷积层**(含**一个sigmoid激活函数**) 和一个**Avg Pooling 层**。\n\t- 请注意，虽然ReLU和最大汇聚层(Max Pooling)更有效，但它们在20世纪90年代还没有出现。\n\n- 每个卷积层使用 $5×5$ 卷积核[^1]和一个sigmoid激活函数。这些层将输入映射到多个二维特征输出，通常同时增加通道的数量。\n\t- 第一卷积层有6个输出通道，而第二个卷积层有16个输出通道。每个 $2×2$ 池操作（步骤2, Stride=2）通过空间下采样将维数减少4倍 (指面积, 相当于边长减半)。\n\n- 为了将卷积块的输出传递给稠密块，我们必须在第二个Pooling后中展平每个样本。(16x5x5=400), LeNet的稠密块有三个全连接层，分别有120、84和10个输出。(所以是 $400\\rightarrow 120\\rightarrow 84\\rightarrow 10$ 因为我们在执行分类任务，所以输出层的10维对应于最后输出结果的数量。\n\n- 相比原始模型，我们去掉了最后一层的高斯激活。除此之外，这个网络与最初的LeNet-5一致。\n![](notes/2022/2022.2/assets/lenet-vert.svg)\n\n## Origin\n- LeNet是**最早发布的卷积神经网络之一**，因其在计算机视觉任务中的高效性能而受到广泛关注。 这个模型是由AT\u0026T贝尔实验室的研究员**Yann LeCun在1989年提出**的（并**以其命名**），目的是识别图像中的**手写数字(MNIST)**。 \n- 一睹大佬真容:\n\tYann LeCun(杨立昆) \n\t![150](notes/2022/2022.2/assets/Pasted%20image%2020220301203532.png)\n- Paper: PDF(zotero://select/items/@lecun1998gradient) Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning applied to document recognition,” _Proceedings of the IEEE_, vol. 86, no. 11, pp. 2278–2324, 1998, doi: [10.1109/5.726791](https://doi.org/10.1109/5.726791) .\n\n\n[^1]: 图里面的像素大小有点不对, 卷积核是5x5的","lastmodified":"2022-10-15T14:06:29.510502668Z","tags":null},"/notes/2022/2022.2/D2L-40-AlexNet":{"title":"D2L-40-AlexNet","content":"# AlexNet\n\n\u003cdiv align=\"right\"\u003e 2022-03-02\u003c/div\u003e\n\nTags: #DeepLearning #AlexNet #CNN #ImageNet \n\n![](notes/2022/2022.2/assets/alexnet.svg)\n\n## 模型解析\n\n### 对比LeNet\n- 最重要的是, AlexNet导致了**计算机视觉方法论的改变**: 从核方法到深度神经网络, 开启了神经网络的第二次热潮\n\t- ![](notes/2022/2022.2/assets/Pasted%20image%2020220302193907.png)[^5]\n- 对比LeNet, AlexNet的主要特点有: \n\t- 输入图片更 **\"大\"**, 网络结构更 **\"深\"**, 每层通道更 **\"多\"**, 滑动窗口更 **\"大\"**(核函数和池化层) \n\t- 使用了**ReLU**作为激活函数\n\t- 池化层采用了**Max Pooling**\n\t- 使用了**丢弃法(Dropout)** 作为正则化方法[^4], 而 LeNet只采用了 [权重衰减](notes/2022/2022.2/D2L-22-权重衰减.md)\n\t- AlexNet在训练前进行了**数据增强**\n\n\n![](notes/2022/2022.2/assets/AlexNet.png) [^3]\n### 网络结构的改进\n- ImageNet数据集的尺寸更大了, 硬件条件也更好了, 这也推动AlexNet的输入增大到了 $224\\times224$.[^6] (相比之下 [LeNet](notes/2022/2022.2/D2L-39-LeNet.md) 的MNIST只有 $28\\times28$)\n- 图像大了, 需要捕获的目标也变大了, 卷积核($11\\times11$)和池化层的窗口($3\\times3$)自然也增大了, Stride也相应增加.\n- 图像大了, 数据集更大了, 信息也更多了, 这使得AlexNet的通道数几乎是LeNet的十倍\n- 新增加的三个卷积层让AlexNet比LeNet更深\n- 在最后一个卷积层后有两个全连接层，分别有4096个输出。 这两个巨大的全连接层拥有将近1GB的模型参数。\n![](notes/2022/2022.2/assets/AlexNet2.png) [^3]\n### 激活函数\n- 此外，AlexNet将Sigmoid激活函数改为更简单的ReLU激活函数。 \n\t- 一方面，ReLU激活函数的计算更简单，它不需要如sigmoid激活函数那般复杂的求幂运算。 \n\t- 另一方面，当使用不同的参数初始化方法时，ReLU激活函数使训练模型更加容易。 \n\t\t- 当sigmoid激活函数的输出非常接近于0或1时，这些区域的梯度几乎为0，因此反向传播无法继续更新一些模型参数。 相反，ReLU激活函数在正区间的梯度总是1。 因此，如果模型参数没有正确初始化，sigmoid函数可能在正区间内得到几乎为0的梯度，从而使模型无法得到有效的训练。[^7]\n\n![](notes/2022/2022.2/assets/Pasted%20image%2020220302145051.png) [^1]\n\n### 容量控制和预处理\n- AlexNet通过Dropout控制全连接层的模型复杂度，而LeNet只使用了权重衰减。 \n```py\nnet = nn.Sequential(\n...\n    # 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合\n    nn.Linear(6400, 4096), nn.ReLU(),\n    nn.Dropout(p=0.5),\n    nn.Linear(4096, 4096), nn.ReLU(),\n    nn.Dropout(p=0.5),\n...\n)\n```\n- 同时为了进一步扩充数据，AlexNet在训练时增加了大量的图像增强数据，如翻转、裁切和变色。 这使得模型更健壮，更大的样本量有效地减少了过拟合。\n![](notes/2022/2022.2/assets/Pasted%20image%2020220302200454.png)\n\n## Side Notes\n- 受限于当时的硬件条件, AlexNet的原始模型使用了两个GPU同时计算, 在网络架构图里表示为双数据流设计: \n\t![](notes/2022/2022.2/assets/Pasted%20image%2020220302143910.png)[^2]\n\t- 我们现在可以将两条路合并, 精简为只使用一个GPU的模型.\n\n## 不足\n- 虽然AlexNet证明深层神经网络卓有成效，但它没有提供一个通用的模板来指导后续的研究人员设计新的网络。 \n- 在后面的一些研究里面, 我们逐渐总结出一些常用于设计深层神经网络的启发式概念。\n\t- 模块化: [VGG](notes/2022/2022.2/D2L-41-VGG.md)\n\t- 去除全连接层: [NiN](notes/2022/2022.3/D2L-42-NiN.md)\n\t- 并行连接与稀疏性: [GoogLeNet(Inception)](notes/2022/2022.3/D2L-43-GoogLeNet(Inception).md)\n\t- 残差连接: [ResNet](notes/2022/2022.3/D2L-45-ResNet.md)\n\n\n## Origin\n- AlexNet is named after Alex Krizhevsky\n![250](notes/2022/2022.2/assets/Pasted%20image%2020220302135107.png)\n- **Paper**: PDF(zotero://select/items/@krizhevsky2012imagenet) A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification with deep convolutional neural networks (AlexNet),” _Advances in neural information processing systems_, vol. 25, 2012.\n\n[^1]: [GitHub - ishanExtreme/AlexNet-Visualization: Visualizing layer output of AlexNet model trained on cifar-10 dataset](https://github.com/ishanExtreme/AlexNet-Visualization)\n[^2]: [CNN 모델](https://chacha95.github.io/2019-10-05-Efficient-DL1/)\n[^3]: [5 Advanced CNN architectures · Deep Learning for Vision Systems](https://livebook.manning.com/book/grokking-deep-learning-for-computer-vision/chapter-5/115) \n[^4]: Dropout并不是在这篇文章里面提出来的, 但是的确是同一帮人提出来的\n[^5]: [24 深度卷积神经网络 AlexNet【动手学深度学习v2】哔哩哔哩bilibili](https://www.bilibili.com/video/BV1h54y1L7oe?p=1)\n[^6]:AlexNet的输入大小好像有点不统一, 在论文里面是 $224\\times224$ , 但是如果Padding=1, Stride=4的话, 下一个卷积层的大小是 $54.5$, 不是一个整数, 需要向下取整得到54. 所以有的地方就把输入改成了227, 这样算出来就刚好是54.\n[^7]: [7.1. 深度卷积神经网络（AlexNet） — 动手学深度学习 2.0.0-beta0 documentation](https://zh-v2.d2l.ai/chapter_convolutional-modern/alexnet.html#id13)","lastmodified":"2022-10-15T14:06:29.514502711Z","tags":null},"/notes/2022/2022.2/D2L-41-VGG":{"title":"D2L-41-VGG","content":"# VGG\n\n\u003cdiv align=\"right\"\u003e 2022-03-03\u003c/div\u003e\n\nTags: #DeepLearning #VGG #CNN \n\n![](notes/2022/2022.2/assets/img_2022-10-15-22.png)\n- **模块化**是VGG网络最重要的思想.\n- 模块化进一步带来了**自由性**, 不同的块配置可以带来不同的模型表现.\n\n![](notes/2022/2022.2/assets/img_2022-10-15-23.png)\n\n## 规范化 - 模块化\n- 与芯片设计中工程师从放置晶体管到逻辑元件再到逻辑块的过程类似，神经网络架构的设计也逐渐变得更加抽象。研究人员开始从单个神经元的角度思考问题，发展到整个层，现在又转向块，重复层的模式。\n- 使用块的设计同样让模型更加简洁.\n\n### VGG块\n![](notes/2022/2022.2/assets/img_2022-10-15-24.png)\n- VGG将 [AlexNet](notes/2022/2022.2/D2L-40-AlexNet.md) 里面三层连续的卷积拿出来, 抽象成VGG块, 作为构建网络的基础模式.\n\n- VGG块是可变的, 超参数变量 `conv_arch` 指定了每个VGG块里**卷积层个数**和**输出通道数**。\n\t- ![一个VGG块](notes/2022/2022.2/assets/一个VGG块.svg)\n\t方块模型解释: \n\t![方块模型表示](notes/2022/2022.2/assets/方块模型表示.svg)\n\t- 不要想当然地把同一个大小的层作为一个块, VGG块在增大Channel数地同时减半输出尺寸. 下图是VGG 16的示意图, 16是指网络里面有13个卷积层加3个全连接层.\n\t![](notes/2022/2022.2/assets/VGG16.png)\nVGG19: [^2]\n![](notes/2022/2022.2/assets/vgg19.jpg)\n- **VGG块的参数**\n\t![](notes/2022/2022.2/assets/img_2022-10-15-24.png)\n\t- 为了保持卷积输入输出**尺寸不变**, 卷积层采用了 $3\\times3$ 的核加上 $1$ 的Padding.\n\t- VGG的Pooling窗口大小为 $2\\times2$, Stride=2, 所以输出图像的尺寸减半. (下采样到1/4的分辨率)\n\n- **为什么不添加全连接层来加深网络呢?**\n\t- 为了使网络更深, 我们可以添加更多的全连接层(FC)或者卷积层(Conv), 但是因为全连接层过于昂贵(参数数量庞大), 我们通常选择添加更多的卷积层.\n\n- **为什么不用大一点的卷积核?**\n\t- 在VGG论文中，Simonyan和Ziserman尝试了各种架构。特别是他们发现深层且窄的卷积（即 $3×3$ ）比较浅层且宽的卷积更有效。[^1]\n\t- 但是为什么更大的卷积核, 更深的网络, 效果更好?\n\t\t- 一个可能的原因是: 具有相同感知野的条件下，小的卷积核提升了网络的深度\n\t\t\t- [[notes/2022/2022.2/2个3x3卷积核堆叠后等价于一个5x5卷积核]]\n\t\t- 那为什么深的网络效果好? #todo \n\n## 自由性 - 模型的多样性\n![](notes/2022/2022.2/assets/VGG%20Variations.png)\n## Origin\n- VGG由牛津大学的 [视觉几何组（visualgeometry group）](http://www.robots.ox.ac.uk/~vgg/) 提出.\n![](notes/2022/2022.2/assets/Pasted%20image%2020220303152941.png)\n\n\n[^1]: [7.2. 使用块的网络（VGG） — 动手学深度学习 2.0.0-beta0 documentation](https://zh-v2.d2l.ai/chapter_convolutional-modern/vgg.html#id5)\n[^2]: [Image Classification on CIFAR-10 Dataset · Image Classification](https://rishabhjain.xyz/ml-class-project/)","lastmodified":"2022-10-15T14:06:29.514502711Z","tags":null},"/notes/2022/2022.2/Entropy-%E7%86%B5":{"title":"Entropy-熵","content":"# Entropy - 熵\n\n2022-02-11\u003c/div\u003e\n\nTags: #InformationTheory \n\n## 理解\n- 熵是Surprise的期望\n[Entropy (for data science) Clearly Explained!!! - YouTube](https://www.youtube.com/watch?v=YtebGVx-Fxw)\n\n- 熵是对事件复杂度的衡量, 即我们最少需要多少信息才能完整地描述这个事件\n[Intuitively Understanding the Shannon Entropy - YouTube](https://www.youtube.com/watch?v=0GCGaw0QOhA)\n\n- 换句话说, 熵的大小是编码一个随机事件所需要的**最短平均编码长度**\n\n**联系:**  一个事件越复杂, 那么就需要更多的信息来描述这个事件, 这个事件的平均\"惊讶程度\"就越高, 这个事件的熵就越高.\n\n## 公式\n$$\\text { Entropy }=-\\sum p(x) \\log (p(x))$$\nor\n$$\\text { Entropy }=\\sum p(x) \\log \\left(\\frac{1}{p(x)}\\right)$$\n(The Average *Surprise*)\n\n\n\n","lastmodified":"2022-10-15T14:06:29.514502711Z","tags":null},"/notes/2022/2022.2/Inanimate-Whose":{"title":"Inanimate Whose","content":"# Inanimate Whose\n\n\u003cdiv align=\"right\"\u003e 2022-03-04\u003c/div\u003e\n\nTags: #English \n\n- **我们可以用Whose指代不是人的动物, 也可以用于非生命的物体(Inanimate Whose[^1]就是这种情况).**\n\t- 例句: \n\t\tThat's the car _whose_ alarm keeps waking us up at night.\n\n- **如果不想用Whose, 也有以下方法来等价代换:** \n\t- 使用 *of which*\n\t\t\u003e - He was watching the movie *whose* title I couldn’t remember earlier.\n\t\t\u003e - He was watching the movie, the title *of which* I couldn’t remember earlier.  \n\t\t\u003e - The car *whose* windshield is cracked is his.  \n\t\t\u003e - The car, the windshield *of which* is cracked, is his.\n\t\t- 然而这样的句子读起来clumsy or stilted.\n\t\n\t- 也可以避免使用所有格, 比如使用*with*\n\t\t\u003e - He was watching the movie with the title that I couldn't remember earlier. \n\t\t\u003e - The car with the cracked windshield is his.\n\n- 然而在**疑问句里面**, 使用Whose指代非生命的物体会带来歧义: \n\t- \"*Whose* lid is this?\"\n\t- \"*Which* container does this lid *belong to*?\"\n\n- Ref: 这篇文章写的很好: [Using 'Whose' for Objects and Things | Merriam-Webster](https://www.merriam-webster.com/words-at-play/whose-used-for-inanimate-objects)\n\n\n[^1]: [Inanimate whose - Wikipedia](https://en.wikipedia.org/wiki/Inanimate_whose) ","lastmodified":"2022-10-15T14:06:29.514502711Z","tags":null},"/notes/2022/2022.2/KL_Divergence-KL%E6%95%A3%E5%BA%A6":{"title":"KL_Divergence-KL散度","content":"# Kullback–Leibler divergence\n\n\u003cdiv align=\"right\"\u003e 2022-02-11\u003c/div\u003e\n\nTags: #Math/Probability #DeepLearning \n\n![](notes/2022/2022.2/assets/img_2022-10-15-25.png)[^3]\n- KL散度可以衡量**两个概率分布之间的相似性**\n\n- KL散度也称为相对熵\n\n\u003e - Wikipedia: In mathematical statistics, the **Kullback–Leibler divergence**, $D _{KL} ( P ∥ Q )$ (also called **relative entropy**), is a statistical distance: a measure of how one probability distribution Q is different from a second, reference probability distribution P.\n\n- 要理解相对熵是怎样衡量概率分布之间的差异的, 请看下面这个视频:\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/SxGYPqCgJWM\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n[^1]\n\n- 在机器学习里面,  $D _{KL} ( P ∥ Q )$ 也被称为从Q切换到P的**信息增益**.\n- 交叉熵越小, 说明两个概念越相似, 也就是PQ之间切换的信息增益越小.\n\n## 公式\n- 对于相同概率空间 $\\mathcal{X}$ 里面的离散概率分布 $P$ 和 $Q$,  $P$相对于$Q$的**相对熵**定义如下:\n$$\nD_{\\mathrm{KL}}(P \\| Q)=\\sum_{x \\in \\mathcal{X}} P(x) \\log \\left(\\frac{P(x)}{Q(x)}\\right) .\n$$\n也可以提一个负号出来: \n$$\nD_{\\mathrm{KL}}(P \\| Q)=-\\sum_{x \\in \\mathcal{X}} P(x) \\log \\left(\\frac{Q(x)}{P(x)}\\right)\n$$\n\n## 性质[^2]\n- 相对熵的值为非负数：\n\t$$D_{\\mathrm{KL}}(P \\| Q) \\geq 0,$$\n\t- 由吉布斯不等式可知，当且仅当 $P=Q$ 时 $D_{K L}(P \\| Q)$ 为零。这时两个分布具有相同的信息量.\n\n- **KL散度不具有对称性**: 从分布 $P$ 到 $Q$ 的距离通常并不等于从 $Q$ 到 $P$ 的距离。\n\t$$D_{\\mathrm{KL}}(P \\| Q) \\neq D_{\\mathrm{KL}}(Q \\| P)$$\n\t- 原因很简单, 一个是$P(x)$在$log$外面, 另外一个是$Q(x)$在$log$外面\n\t- 尽管从直觉上 KL 散度是个度量或距离函数, 但是它实际上并不是一个真正的度量或距离。\n\t- \n\n\n[^1]: [Intuitively Understanding the KL Divergence - YouTube](https://www.youtube.com/watch?v=SxGYPqCgJWM)\n[^2]: [相对熵 - 维基百科，自由的百科全书](https://zh.wikipedia.org/wiki/%E7%9B%B8%E5%AF%B9%E7%86%B5)\n[^3]: https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#/media/File:KL-Gauss-Example.png","lastmodified":"2022-10-15T14:06:29.514502711Z","tags":null},"/notes/2022/2022.2/Likelihood_Function-%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0":{"title":"Likelihood_Function-似然函数","content":"# Likelihood Function - 似然函数\n\n\u003cdiv align=\"right\"\u003e 2022-02-11\u003c/div\u003e\n\nTags: #Math/Statistics #MachineLearning \n\n对于某个(某组)随机变量 $X$, 我们通过采样获得了数据集 $x$ :\n- 似然函数$\\mathcal{L}(\\theta \\mid x)$就是在某个参数(parameter) $\\theta$ 下, 现有数据 $x$ 出现的概率大小, 也就是说: \n$$\\mathcal{L}(\\theta \\mid x) = P(X=x\\mid\\theta)$$\n$P(X=x\\mid\\theta)$ 也常常写作 $p_{\\theta}(x)=P_{\\theta}(X=x)=P(X=x\\ ;\\theta)$\n\n- 因为数据集有许多样本点, 所以似然函数是一个联合概率分布(Joint Probability)\n\n\u003e - **The likelihood function** (often simply called **the likelihood**) describes the *joint probability* of the observed data as *a function of the parameters* of the chosen statistical model. \n\u003e \n\u003e - For each specific parameter value $θ$ in the parameter space, the likelihood function $p ( X | θ )$ therefore assigns a probabilistic prediction to the observed data $X$. \n\u003e \n\u003e - It is essentially the product of sampling densities.[^1]\n\n- [Maximum_Likelihood_Estimation-极大似然估计](notes/2021/2021.12/Maximum_Likelihood_Estimation-极大似然估计.md)\n\n[^1]:[Likelihood function - Wikipedia](https://en.wikipedia.org/wiki/Likelihood_function) ","lastmodified":"2022-10-15T14:06:29.514502711Z","tags":null},"/notes/2022/2022.2/Logit":{"title":"Logit","content":"# Logit: a confusing term\n\n\u003cdiv align=\"right\"\u003e 2022-02-11\u003c/div\u003e\n\nTags: #Math #DeepLearning #SoftmaxRegression \n\nRef: [machine learning - What is the meaning of the word logits in TensorFlow? - Stack Overflow](https://stackoverflow.com/questions/41455101/what-is-the-meaning-of-the-word-logits-in-tensorflow)\n- Logits is an overloaded term which can mean many different things:\n\n## 理解[^1]\n- 我们可以把Logit理解成\"对数几率/概率\", 也就是概率的对数:  $\\log p_i$.\n\t- Logit输入到Softmax之后还原成原来的概率: $p_i$  : $$\\frac{\\exp(\\log p_i)}{\\sum_K \\exp(\\log p_i)}=p_i$$ \n\n## In Math\n- **In Math**, [Logit](https://en.wikipedia.org/wiki/Logit) is a function that maps probabilities (`[0, 1]`) to R (`(-inf, inf)`)\n\n\t![enter image description here](https://i.stack.imgur.com/zto5q.png)\n\n\t- Probability of 0.5 corresponds to a logit of 0. Negative logit correspond to probabilities less than 0.5, positive to \u003e 0.5.\n\n## In ML\n- **For Tensorflow**: It's a name that it is thought to imply that this Tensor is the quantity that is being mapped to probabilities by the Softmax.\n\n- **Logits Layer**: In context of deep learning the [logits layer](https://www.tensorflow.org/tutorials/estimators/cnn#logits_layer) means the layer that feeds in to Softmax (or other such normalization). \n\t- The output of the softmax are the probabilities for the classification task and its input is **logits layer**. The logits layer typically produces values from -infinity to +infinity and the softmax layer transforms it to values from 0 to 1.\n\n## Historical Context\nWhere does this term comes from? \n\n- In 1930s and 40s, several people were trying to adapt linear regression to the problem of predicting probabilities. However linear regression produces output from -infinity to +infinity while for probabilities our desired output is 0 to 1. \n\n- One way to do this is by somehow mapping the probabilities 0 to 1 to -infinity to +infinity and then use linear regression as usual. One such mapping is cumulative normal distribution that was used by Chester Ittner Bliss in 1934 and he called this \"probit\" model, short for \"probability unit\".\n\n- However this function is computationally expensive while lacking some of the desirable properties for multi-class classification. In 1944 Joseph Berkson used the function `log(p/(1-p))` to do this mapping and called it logit, short for \"logistic unit\". The term logistic regression derived from this as well.\n\n## The Confusion\nUnfortunately the term logits is abused in deep learning. \n- From pure mathematical perspective logit is a _function_ that performs above mapping. In deep learning people started calling the layer \"logits layer\" that feeds in to logit function. Then people started calling the output _values_ of this layer \"logit\" creating the confusion with logit _the function_.\n\n**TensorFlow Code**\n- Unfortunately TensorFlow code further adds in to confusion by names like `tf.nn.softmax_cross_entropy_with_logits`. \n\n- What does logits mean here? It just means the input of the function is supposed to be the output of last neuron layer as described above. The `_with_logits` suffix is [redundant, confusing and pointless](https://github.com/tensorflow/tensorflow/issues/6531). \n\n- Functions should be named without regards to such very specific contexts because they are simply mathematical operations that can be performed on values derived from many other domains. In fact TensorFlow has another similar function `sparse_softmax_cross_entropy` where they fortunately forgot to add `_with_logits` suffix creating inconsistency and adding in to confusion. \n\n- PyTorch on the other hand simply names its function without these kind of suffixes.\n\n**Reference**\n\n- [Logit/Probit lecture slides](http://www.columbia.edu/~so33/SusDev/Lecture_9.pdf)  \n- [Wikipedia article](https://en.wikipedia.org/wiki/Logit) \n\n\n\n\n\n[^1]: [7.3. 网络中的网络（NiN） — 动手学深度学习 2.0.0-beta0 documentation](https://zh-v2.d2l.ai/chapter_convolutional-modern/nin.html#id3)","lastmodified":"2022-10-15T14:06:29.514502711Z","tags":null},"/notes/2022/2022.2/Norm-in-Regularization-Intuition":{"title":"Norm in Regularization - Intuition","content":"# Norm in Regularization - Intuition\n\n\u003cdiv align=\"right\"\u003e 2022-02-14\u003c/div\u003e\n\nTags: #Norm #Regularization #DeepLearning #MachineLearning \n\n\n![](notes/2022/2022.2/assets/img_2022-10-15-31.png)\n### L2 Norm $\\ell_{2}$ in Regularization\n- L2 Norm 的等高线是圆形的\n- 使用$L_2$范数的一个原因是它对权重向量的大分量施加了巨大的惩罚。 这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。 在实践中，这可能使它们对单个变量中的观测误差更为稳定。[^2]\n- 使用L2正规化在训练时\"更稳定\".\n\t ![](notes/2022/2022.2/assets/img_2022-10-15-32.png)\n\t - 因为在进行[小批量随机梯度下降](notes/2022/2022.1/D2L-10-小批量随机梯度下降.md)的时候, 每次训练获得的数据具有一定随机性, 从上图中可以看到, L2正则项在 $J(\\theta)$ 出现变化的时候梯度变化更小, 而L1正则项变化较大[^1]\n\n- [GeoGebra的直观互动例子](https://www.geogebra.org/m/jgq2yu36)\n\t![400](notes/2022/2022.2/assets/img_2022-10-15-33.png)\n\n\n### L1 Norm $\\ell_{2}$ in Regularization\n- L1 Norm的等高线是方形的, 在正方形上面每一点的取值相同.\n- L1 范数形状更\"尖锐\", 从图形中可以看出正方形的四个角都在坐标轴上, 这使得L1范数找到的\"平衡点\"很可能将权重集中在一小部分特征上， 而将其他权重清除为零, 也就是说, [L1范数约束后的解很\"稀疏\", 这在特征选择时是很有用的](notes/2021/2021.8/Why_do_cost_functions_use_the_square_error.md#^269677)\n\n\t- 这个视频将L1范数与稀疏性的关系讲的很好: \n\t\t\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/76B5cMEZA4Y\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n- [GeoGebra的直观互动例子](https://www.geogebra.org/m/abbbqute)\n\t![400](notes/2022/2022.2/assets/img_2022-10-15-34.png)\n\n### L1 L2 范数约束下的线性回归: LASSO回归 与 岭回归\n- 线性回归加上L2范数进行正则化就是岭回归, 岭回归通常在样本特征之间相关性很高的时候使用, \"让参数的方差变小\", 获得比较稳定的解.\n\n- 线性回归加上L1范数进行正则化则叫做LASSO回归(**Least Absolute Shrinkage and Selection Operator**,  套索回归), LASSO的主要思想是构造一个一阶惩罚函数获得一个精炼的模型, 通过最终确定一些变量的系数为0进行特征筛选。[^3]\n\n下面的文章可以进一步了解这两种方法: \n- [机器学习算法实践-岭回归和LASSO - 知乎](https://zhuanlan.zhihu.com/p/30535220)\n- [【机器学习】一文读懂线性回归、岭回归和Lasso回归 - 知乎](https://zhuanlan.zhihu.com/p/88698511)\n- [岭回归和最小二乘法的区别是什么？什么时候比较适合用岭回归？ - 知乎](https://www.zhihu.com/question/28221429)\n\n[^1]: [什么是 L1 L2 正规化 正则化 Regularization (深度学习 deep learning) - YouTube](https://www.youtube.com/watch?v=TmzzQoO8mr4)\n[^2]: [4.5. 权重衰减 — 动手学深度学习 2.0.0-beta0 documentation](https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/weight-decay.html#id2)\n[^3]: [机器学习算法实践-岭回归和LASSO - 知乎](https://zhuanlan.zhihu.com/p/30535220)","lastmodified":"2022-10-15T14:06:29.514502711Z","tags":null},"/notes/2022/2022.2/Passage_Paragraph-Difference":{"title":"Passage_Paragraph-Difference","content":"# Passage Paragraph 都是\"段落\", 有什么区别吗?\n\n\u003cdiv align=\"right\"\u003e 2022-02-22\u003c/div\u003e\n\nTags: #English \n\n- *Paragraph* is a section of a document that is usually indicated by an indent on the left hand side of the paper. It ends with the last sentence before the next indent. It is very clear where a paragraph starts and where it ends.  \n  \n- A *passage* is any excerpt from a larger work. It could be a few sentences or three pages. A passage could also be a paragraph ;)[^1]\n\n\n[^1]: [\"\"paragraph\"\" 和 \"\"passage\"\" 和有什么不一样？ | HiNative](https://zh.hinative.com/questions/311732)","lastmodified":"2022-10-15T14:06:29.514502711Z","tags":null},"/notes/2022/2022.2/Regularization-%E6%AD%A3%E5%88%99%E5%8C%96":{"title":"Regularization-正则化","content":"# Regularization\n\n\u003cdiv align=\"right\"\u003e 2022-02-14\u003c/div\u003e\n\nTags: #Regularization #DeepLearning \n\n## Definition\n- **Regularization**: any modification we make to a learning algorithm that is intended to reduce its *generalization error* but **not** its *training error*.[^1]\n\t- 对学习算法的修改——旨在减少泛化误差而不是训练误差\n\t- 这是一个很宽泛的概念, \n\n- Many strategies used in machine learning are explicitly designed to reduce the test error, possibly at the expense of increased training error. These strategies are known collectively as **regularization**.[^1]\n\n- 正则化只在训练中使用. 也就是说, 在模型实际用来推理得结果的时候不使用正则化.\n\t- 对于 L2 正则化, 这个没啥影响, 但是对于 Dropout 来说, 意味着 $\\mathbf{h}=\\operatorname{dropout}(\\mathbf{h})$\n\n## Links\n- [Part.18_Regularization_Intuition(ML_Andrew.Ng.)](notes/2021/2021.9/Part.18_Regularization_Intuition(ML_Andrew.Ng.).md)\n- [D2L-22-权重衰减](notes/2022/2022.2/D2L-22-权重衰减.md)\n- [Part.19_Regularized_Linear_Regression(ML_Andrew.Ng.)](notes/2021/2021.9/Part.19_Regularized_Linear_Regression(ML_Andrew.Ng.).md)\n- [Part.20_Regularized_Logistic_Regression(ML_Andrew.Ng.)](notes/2021/2021.9/Part.20_Regularized_Logistic_Regression(ML_Andrew.Ng.).md)\n- \n\n\n\n\n\n\n\n[^1]:I. Goodfellow, Y. Bengio, and A. Courville, _Deep learning_. MIT Press, 2016.(zotero://select/items/@Goodfellow-et-al-2016)","lastmodified":"2022-10-15T14:06:29.514502711Z","tags":null},"/notes/2022/2022.2/Relation_between_Softmax_and_Logistic_Regression":{"title":"Relation_between_Softmax_and_Logistic_Regression","content":"# Softmax 与 Logistic 回归的联系\n\n\u003cdiv align=\"right\"\u003e 2022-02-11\u003c/div\u003e\n\nTags: #SoftmaxRegression #LogisticRegression #Classification #MulticlassClassification \n\nRef: [Unsupervised Feature Learning and Deep Learning Tutorial](http://deeplearning.stanford.edu/tutorial/supervised/SoftmaxRegression/)\n\n- 二分类的[Softmax回归](notes/2022/2022.2/D2L-13-Softmax_Regression.md)形式如下: \n$$h_{\\theta}(x)=\\frac{1}{\\exp \\left(\\theta^{(1) \\top} x\\right)+\\exp \\left(\\theta^{(2) \\top} x^{(i)}\\right)}\\left[\\begin{array}{c}\n\\exp \\left(\\theta^{(1) \\top} x\\right) \\\\\n\\exp \\left(\\theta^{(2) \\top} x\\right)\n\\end{array}\\right]$$\n\n- 根据: [Softmax_Regression_is_Over-parameterized](notes/2022/2022.2/Softmax_Regression_is_Over-parameterized.md), 我们可以让参数同时减去 $\\psi=\\theta^{(2)}$, 得到: \n\n$$\\begin{aligned}\nh(x) \u0026=\\frac{1}{\\exp \\left(\\left(\\theta^{(1)}-\\theta^{(2)}\\right)^{\\top} x^{(i)}\\right)+\\exp \\left(\\overrightarrow{0}^{\\top} x\\right)}\\left[\\exp \\left(\\left(\\theta^{(1)}-\\theta^{(2)}\\right)^{\\top} x\\right) \\exp \\left(\\overrightarrow{0}^{\\top} x\\right)\\right] \\\\\n\u0026=\\left[\\begin{array}{l}\n\\frac{1}{1+\\exp \\left(\\left(\\theta^{(1)}-\\theta^{(2)}\\right)^{\\top} x^{(i)}\\right)} \\\\\n\\frac{\\exp \\left(\\left(\\theta^{(1)}-\\theta^{(2)}\\right)^{\\top} x\\right)}{1+\\exp \\left(\\left(\\theta^{(1)}-\\theta^{(2)}\\right)^{\\top} x^{(i)}\\right)}\n\\end{array}\\right] \\\\\n\u0026=\\left[\\begin{array}{c}\n\\frac{1}{1+\\exp \\left(\\left(\\theta^{(1)}-\\theta^{(2)}\\right)^{\\top}\nx^{(i)}\\right)}\\\\\n{1-\\frac{1}{1+\\exp \\left(\\left(\\theta^{(1)}-\\theta^{(2)}\\right)^{\\top} x^{(i)}\\right)}}\n\\end{array}\\right]\n\\end{aligned}$$\n\n通过将 $\\theta^{(2)}-\\theta^{(1)}$ 替换为 $\\theta'$, 得到\n$$\\begin{bmatrix}\n\\frac{1}{1+\\exp \\left(-(\\theta')^{\\top}\nx^{(i)}\\right)}\\\\\n{1-\\frac{1}{1+\\exp \\left(-(\\theta')^{\\top} x^{(i)}\\right)}}\n\\end{bmatrix}$$\n我们可以看到函数预测第一个类的概率为: \n$$\\frac{1}{1+\\exp \\left(-(\\theta')^{\\top}\nx^{(i)}\\right)}$$\n就是[Logistic回归](notes/2021/2021.8/Part.12_Logistic_Regression(ML_Andrew.Ng.).md)的情形.\n\n第二个类的概率为\n$${1-\\frac{1}{1+\\exp \\left(-(\\theta')^{\\top} x^{(i)}\\right)}}$$\n也就是logistic回归没有表述出来的情况.","lastmodified":"2022-10-15T14:06:29.514502711Z","tags":null},"/notes/2022/2022.2/Softmax%E5%87%BD%E6%95%B0":{"title":"Softmax函数","content":"## Softmax函数\n### 直观理解\n\u003e Softmax函数的作用隐藏在它关于域的映射关系里: \n\u003e $$\\sigma: \\mathbb{R}^{K} \\rightarrow(0,1)^{K}$$\n\u003e 可以看到, Softmax函数将 $K$ 维向量中每一个元素的取值范围由 $\\mathbb{R}$ 压缩到 $(0,1)$ , 并且还保证了所有元素加起来等于 $1$, 这就意味着我们可以将每一个元素看作一个概率.\n\n也就是说: \n- Softmax 的作用是把 一个序列，变成概率。\n$$\\left[\\begin{array}{c}\n\\sigma(\\mathbf{z})_{1} \\\\\n\\vdots \\\\\n\\sigma(\\mathbf{z})_{K}\n\\end{array}\\right]=\\frac{1}{\\sum_{i=1}^{K} e^{z_{i}}}\\left[\\begin{array}{c}\ne^{z_{1}} \\\\\n\\vdots \\\\\ne^{z_{K}}\n\\end{array}\\right]=\n\\left[\\begin{array}{c}\nP(t=1 \\mid \\mathbf{z}) \\\\\n\\vdots \\\\\nP(t=K \\mid \\mathbf{z})\n\\end{array}\\right]$$\n\n\n### 定义\n标准Softmax函数 $\\sigma: \\mathbb{R}^{K} \\rightarrow(0,1)^{K}$ 的定义如下所示: \n- 对于 $i=1, \\ldots, K$ and $\\mathbf{z}=\\left(z_{1}, \\ldots, z_{K}\\right) \\in \\mathbb{R}^{K}$\n $$\\sigma(\\mathbf{z})_{i}=\\frac{e^{z_{i}}}{\\sum_{j=1}^{K} e^{z_{j}}}$$\n\t- 其中$K$代表类别数, 且$K\u003e1$ (多分类)\n\n另一种表示方法是:\n$$\\sigma(\\mathbf{z})_{i}=\\frac{\\exp(z_{i})}{\\sum_{j=1}^{K} \\exp(z_{j})} \\quad$$\n\n### 性质\n- 容易知道, 输出向量 $\\sigma(\\mathbf{z})_{i}$ 的所有元素均属于 $(0,1)$ 区间。\n- 输出向量中，所有分量相加之和等于1 $$\\sum_{i=1}^K\\sigma(\\mathbf{z})_{i}=\\frac{\\sum_{i=1}^K e^{z_{i}}}{\\sum_{j=1}^{K} e^{z_{j}}}=1$$\n- $\\mathbf{z}=\\left(z_{1}, \\ldots, z_{K}\\right)$, 单独改变某一个 $z_i$的值, 则 $\\sigma(\\mathbf{z})_{i}$ 的变化符合Sigmoid函数曲线.\n\t- \u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/ytbYRIN0N4g?start=157\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n- 如果 $\\mathbf{z}$ 中所有元素同时加减相同的值, 比如: $\\mathbf{z}=\\left(z_{1}, \\ldots, z_{K}\\right)\\rightarrow\\mathbf{z}=\\left(z_{1}+1, \\ldots, z_{K}+1\\right)$ , 则根据定义, 相当于分子分母同时乘以一个系数 $e^i$, 结果不变.\n\t- 这引出了一个有趣的点: Softmax回归的参数是 \"over-parameterized\", 意味着同样的结果, Softmax的参数可能不唯一: [[notes/2022/2022.2/Softmax_Regression_is_Over-parameterized]]\n- 同样, 如果所有元素乘以一个系数, 则会改变结果向量的\"突起程度\": \n\t- \u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/ytbYRIN0N4g?start=288\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\t- 如果系数大于1, 会增加结果向量内部的差异\n\t- 如果系数小于1, 则会减小结果向量内部的差异(变得更平缓)\n\n\n### 导数\n- 在反向传播时我们需要计算Softmax函数的梯度. \n- 我们定义 $\\Sigma_{K}=\\sum_{i=1}^{K} e^{z_{i}}$ , for $c=1, \\cdots, K$. 所以 $y_{i}=e^{z_{i}} / \\Sigma_{K}$. \n- 那么输出 $\\mathbf{y}$ 关于输入 $\\mathbf{z}$ 的导数 $\\partial y_{i} / \\partial z_{j}$  为: (需要分情况讨论)\n$$\\begin{aligned}\n\u0026\\text { if } i=j: \\frac{\\partial y_{i}}{\\partial z_{i}}=\\frac{\\partial \\frac{e^{z_{i}}}{\\Sigma_{K}}}{\\partial z_{i}}=\\frac{e^{z_{i}} \\Sigma_{K}-e^{z_{i}} e^{z_{i}}}{\\Sigma_{K}^{2}}=\\frac{e^{z_{i}}}{\\Sigma_{K}} \\frac{\\Sigma_{K}-e^{z_{i}}}{\\Sigma_{K}}=\\frac{e^{z_{i}}}{\\Sigma_{K}}\\left(1-\\frac{e^{z_{i}}}{\\Sigma_{K}}\\right)=y_{i}\\left(1-y_{i}\\right) \\\\\n\u0026\\text { if } i \\neq j: \\frac{\\partial y_{i}}{\\partial z_{j}}=\\frac{\\partial \\frac{e^{z_{i}}}{\\Sigma_{K}}}{\\partial z_{j}}=\\frac{0-e^{z_{i}} e^{z_{j}}}{\\Sigma_{K}^{2}}=-\\frac{e^{z_{i}}}{\\Sigma_{K}} \\frac{e^{z_{j}}}{\\Sigma_{K}}=-y_{i} y_{j}\n\\end{aligned}$$\n注意: 如果 $i=j$ 那么这个导数和[Sigmoid_Function](notes/2021/2021.8/Sigmoid_Function.md)的导数很相似.\n\n- Funny Version: [The SoftMax Derivative, Step-by-Step!!! - YouTube](https://www.youtube.com/watch?v=M59JElEPgIg)\n","lastmodified":"2022-10-15T14:06:29.514502711Z","tags":null},"/notes/2022/2022.2/Softmax_Regression_is_Over-parameterized":{"title":"Softmax_Regression_is_Over-parameterized","content":"# Softmax Regression is Over-parameterized\n\n\u003cdiv align=\"right\"\u003e 2022-02-11\u003c/div\u003e\n\nTags: #SoftmaxRegression\n\nRef: [Unsupervised Feature Learning and Deep Learning Tutorial](http://deeplearning.stanford.edu/tutorial/supervised/SoftmaxRegression/)\n\n- 将Softmax回归里面的参数全部变化一个相同的值, 结果不变:\n$$\\begin{aligned}\nP\\left(y^{(i)}=k \\mid x^{(i)} ; \\theta\\right) \u0026=\\frac{\\exp \\left(\\left(\\theta^{(k)}-\\psi\\right)^{\\top} x^{(i)}\\right)}{\\sum_{j=1}^{K} \\exp \\left(\\left(\\theta^{(j)}-\\psi\\right)^{\\top} x^{(i)}\\right)} \\\\\n\u0026=\\frac{\\exp \\left(\\theta^{(k) \\top} x^{(i)}\\right) \\exp \\left(-\\psi^{\\top} x^{(i)}\\right)}{\\sum_{j=1}^{K} \\exp \\left(\\theta^{(j) \\top} x^{(i)}\\right) \\exp \\left(-\\psi^{\\top} x^{(i)}\\right)} \\\\\n\u0026=\\frac{\\exp \\left(\\theta^{(k) \\top} x^{(i)}\\right)}{\\sum_{j=1}^{K} \\exp \\left(\\theta^{(j) \\top} x^{(i)}\\right)}\n\\end{aligned}$$\n\n- 这就意味着一个Hypothesis可能对应多个不同的参数组合 $\\theta$\n- 取 $\\psi=\\theta^{(K)}$, 我们可以消去任意参数 $\\theta^{(K)}$.\n\n","lastmodified":"2022-10-15T14:06:29.514502711Z","tags":null},"/notes/2022/2022.2/VC%E7%BB%B4-VC_Dimension":{"title":"VC维-VC_Dimension","content":"# Vapnik–Chervonenkis dimension\n\n\u003cdiv align=\"right\"\u003e 2022-02-12\u003c/div\u003e\n\nTags: #DeepLearning #StatisticalLearning\n\n- 在VC理论中，VC维是对一个可学习分类函数空间的能力（复杂度，表示能力等）的衡量。它定义为算法能“打散”的点集的势的最大值。\n\n- 对于线性分类器: \n\n![](notes/2022/2022.2/assets/img_2022-10-15-30.png)\n\n- VC维可以衡量训练误差和泛化误差的间隔, 但是在深度学习中, 我们很难计算一个模型的VC维","lastmodified":"2022-10-15T14:06:29.514502711Z","tags":null},"/notes/2022/2022.2/Xavier%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E8%AF%A6%E7%BB%86%E4%BE%8B%E5%AD%90":{"title":"Xavier初始化的详细例子","content":"以MLP为例\n- 假设:\n\t- $w_{i, j}^{s}$ 是 i.i.d, 那么 $\\mathbb{E}\\left[w_{i, j}^{s}\\right]=0, \\operatorname{Var}\\left[w_{i, j}^{s}\\right]=\\gamma_{s}$ \n\t- $h_{i}^{s-1}$ 独立于 $w_{i, j}^{s}$\n\t- 假设没有激活函数, 即: $$\\mathbf{h}^{s}=\\mathbf{W}^{s} \\mathbf{h}^{s-1}, \\text { 这里 } \\mathbf{W}^{s} \\in \\mathbb{R}^{n_{s} \\times n_{s-1}}$$\n\n##### 正向均值\n$$\\mathbb{E}\\left[h_{i}^{s}\\right]=\\mathbb{E}\\left[\\sum_{j} w_{i, j}^{s} h_{j}^{s-1}\\right]=\\sum_{j} \\mathbb{E}\\left[w_{i, j}^{s}\\right] \\mathbb{E}\\left[h_{j}^{s-1}\\right]=0$$\n\n##### 正向方差\n$$\\begin{aligned}\n\\operatorname{Var}\\left[h_{i}^{s}\\right] \u0026=\\mathbb{E}\\left[\\left(h_{i}^{s}\\right)^{2}\\right]-\\mathbb{E}\\left[h_{i}^{s}\\right]^{2}\\\\\n\u0026=\\mathbb{E}\\left[\\left(\\sum_{j} w_{i, j}^{s} h_{j}^{s-1}\\right)^{2}\\right] \\\\\n\u0026=\\mathbb{E}\\left[\\sum_{j}\\left(w_{i, j}^{s}\\right)^{2}\\left(h_{j}^{s-1}\\right)^{2}+\\sum_{j \\neq k} w_{i, j}^{s} w_{i, k}^{s} h_{j}^{s-1} h_{k}^{s-1}\\right] \\\\\n(\\text{because i.i.d, covariance is 0} )\n\u0026=\\mathbb{E}\\left[\\sum_{j}\\left(w_{i, j}^{s}\\right)^{2}\\left(h_{j}^{s-1}\\right)^{2}\\right] \\\\\n\u0026=\\sum_{j} \\mathbb{E}\\left[\\left(w_{i, j}^{s}\\right)^{2}\\right] \\mathbb{E}\\left[\\left(h_{j}^{s-1}\\right)^{2}\\right] \\\\\n(\\text{because }\\mathbb{E}\\left[h_{j}^{s-1}\\right]^{2}, \\mathbb{E}\\left[w_{i,j}^{s}\\right]^{2}\\ is\\ 0)\n\u0026=\\sum_{j} \\operatorname{Var}\\left[w_{i, j}^{s}\\right] \\operatorname{Var}\\left[h_{j}^{s-1}\\right]\\\\\n\\left(\\mathbf{W}^{s} \\in \\mathbb{R}^{n_{s} \\times n_{s-1}}\\right)\n\u0026=n_{s-1} \\gamma_{s} \\operatorname{Var}\\left[h_{j}^{s-1}\\right]\n\\end{aligned}$$\n- 这意味着如果输入层输出层要方差不变, 则必须有 $$n_{s-1} \\gamma_{s}=1$$\n\n\n##### 反向部分\n首先, 因为 $\\mathbf{h}^{s}=\\mathbf{W}^{s} \\mathbf{h}^{s-1}$, 所以 $$\\begin{aligned}\\frac{\\partial\\ell}{\\partial\\mathbf{h}^{s-1}}\n\u0026=\\frac{\\partial\\ell}{\\partial\\mathbf{h}^{s}}\n\\frac{\\partial\\mathbf{h}^{s}}{\\partial\\mathbf{h}^{s-1}}\\\\\n\u0026=\\frac{\\partial\\ell}{\\partial\\mathbf{h}^{s}}\n\\frac{\\partial\\mathbf{W}^{s} \\mathbf{h}^{s-1}}{\\partial\\mathbf{h}^{s-1}}\\\\\n\u0026=\\frac{\\partial\\ell}{\\partial\\mathbf{h}^{s}}\n\\mathbf{W}^{s}\\end{aligned}$$\n\n为了和上面正向的步骤相似, 我们同时取一个转置, 得到: \n$$\\left(\\frac{\\partial\\ell}{\\partial\\mathbf{h}^{s-1}}\\right)^T\n=\\left(\\mathbf{W}^{s}\\right)^T\\left(\\frac{\\partial\\ell}{\\partial\\mathbf{h}^{s}}\\right)^T$$\n##### 反向均值\n$$\\mathbb{E}\\left[\\left(\\frac{\\partial\\ell}{\\partial\\mathbf{h}^{s-1}}\\right)^T_i\\right]\n=\\mathbb{E}\\left[\\sum_{j} w_{j, i}^{s} \\left(\\frac{\\partial\\ell}{\\partial\\mathbf{h}^{s}}\\right)^T_{j}\\right]\n=\\sum_{j} \\mathbb{E}\\left[w_{j, i}^{s}\\right] \\mathbb{E}\\left[\\left(\\frac{\\partial\\ell}{\\partial\\mathbf{h}^{s}}\\right)^T_{j}\\right]=0$$\n\n##### 反向方差\n$$\\begin{aligned}\n\\operatorname{Var}\\left[\\left(\\frac{\\partial\\ell}{\\partial\\mathbf{h}^{s-1}}\\right)^T_i\\right]\n\u0026=\\mathbb{E}\\left[\\left(\\left(\\frac{\\partial\\ell}{\\partial\\mathbf{h}^{s-1}}\\right)^T_i\\right)^{2}\\right]-\\mathbb{E}\\left[\\left(\\frac{\\partial\\ell}{\\partial\\mathbf{h}^{s-1}}\\right)^T_i\\right]^{2}\\\\\n\u0026=\\mathbb{E}\\left[\\left(\\sum_{j} w_{j, i}^{s} \\left(\\frac{\\partial\\ell}\n{\\partial\\mathbf{h}^{s}}\\right)^T_{j}\\right)^{2}\\right] \\\\\n\u0026=\\mathbb{E}\\left[\\sum_{j}\\left(w_{j, i}^{s}\\right)^{2}\\left(\\left(\\frac{\\partial\\ell}\n{\\partial\\mathbf{h}^{s}}\\right)^T_{j}\\right)^{2}+\\sum_{j \\neq k} w_{j, i}^{s} w_{k, i}^{s} \\left(\\frac{\\partial\\ell}\n{\\partial\\mathbf{h}^{s}}\\right)^T_{j} \\left(\\frac{\\partial\\ell}\n{\\partial\\mathbf{h}^{s}}\\right)^T_{k}\\right] \\\\\n\u0026=\\mathbb{E}\\left[\\sum_{j}\\left(w_{j, i}^{s}\\right)^{2}\\left(\\left(\\frac{\\partial\\ell}\n{\\partial\\mathbf{h}^{s}}\\right)^T_{j}\\right)^{2}\\right] \\\\\n\u0026=\\sum_{j} \\mathbb{E}\\left[\\left(w_{j, i}^{s}\\right)^{2}\\right] \\mathbb{E}\\left[\\left(\\left(\\frac{\\partial\\ell}\n{\\partial\\mathbf{h}^{s}}\\right)^T_{j}\\right)^{2}\\right] \\\\\n\u0026=\\sum_{j} \\operatorname{Var}\\left[w_{j, i}^{s}\\right] \\operatorname{Var}\\left[\\left(\\frac{\\partial\\ell}\n{\\partial\\mathbf{h}^{s}}\\right)^T_{j}\\right]\\\\\n\\left(\\mathbf{W}^{s} \\in \\mathbb{R}^{n_{s} \\times n_{s-1}}\\right)\n\u0026=n_{s} \\gamma_{s} \\operatorname{Var}\\left[\\left(\\frac{\\partial\\ell}\n{\\partial\\mathbf{h}^{s}}\\right)^T_{j}\\right]\n\\end{aligned}$$\n- 这意味着如果输入层输出层要方差不变, 则必须有 $$n_{s} \\gamma_{s}=1$$\n","lastmodified":"2022-10-15T14:06:29.514502711Z","tags":null},"/notes/2022/2022.3/D2L-42-NiN":{"title":"D2L-42-NiN","content":"# Network in Network - NiN\n\n\u003cdiv align=\"right\"\u003e 2022-03-04\u003c/div\u003e\n\nTags: #DeepLearning #NiN #CNN \n\n![NiN_PixelMLP](notes/2022/2022.3/assets/NiN_PixelMLP.svg)\n## 用卷积代替全连接\n### 动机\n#### 全连接层很贵 (参数很多)\n- 一层卷积层需要的参数为: \n\t- ![卷积层权重大小的计算](notes/2022/2022.2/assets/卷积层权重大小的计算.svg) \n\t- [卷积层参数大小的计算](notes/2022/2022.2/卷积层参数大小的计算.md)\n\n- 卷积层后面第一个全连接层的参数为: $$in\\_Channel\\times in\\_Height\\times in\\_Width\\times num\\_of\\_Hidden\\_Units$$\n\n- **对比 :**\n\t- 一个 $C_{in}=512, C_{out}=4096$ 卷积核大小为3的卷积层[^2], 参数规模是 $$512\\times4096\\times3\\times3\\approx 18M$$\n\t- 一个输入通道数为512, 隐藏单元为4096, 输入尺寸为 $7\\times7$ 的全连接层的参数规模是[^3] :  $$512\\times7\\times7\\times4096\\approx 102M$$\n\n#### 全连接层损失了空间信息\n- 这一点很好理解:  由卷积层转化到全连接层, 需要将卷积输出全部Flatten为一个一维向量. 而这意味着放弃了卷积层里面的空间信息.\n\n### 1x1 卷积相当于(单像素上的)全连接\n [D2L-36-1x1卷积层](notes/2022/2022.2/D2L-36-1x1卷积层.md)\n \n![NiN_PixelMLP](notes/2022/2022.3/assets/NiN_PixelMLP.svg)\n\n## 网络结构\n- 交替使用NiN块和步幅为2的最大池化层\n\t- 逐步减小高宽和增大通道数\n![](notes/2022/2022.3/assets/nin.svg)\n### 分块\n- 和VGG一样, NiN网络也采用了分块的规范化网络结构, 一个NiN块包括一个普通卷积层与两个连续的 $1\\times1$ 卷积层.\n\t```python\n\tdef nin_block(in_channels, out_channels, kernel_size, strides, padding):\n\t\treturn nn.Sequential(\n\t\t\tnn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(),\n\t\t\tnn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU())\n\t```\n\t- 超参数主要调节的是第一个卷积层, 后面两个$1\\times1$的卷积层不改变通道数.\n\n### 全局平均池化层 global average pooling layer\n- 其他网络里面通常将最后一层的隐藏层输出作为 [Logit](notes/2022/2022.2/Logit.md) 输入到Softmax里面得到预测概率, **但是NiN没有全连接层, 又怎么得到输出标签呢?**\n- NiN网络将每一个通道的所有像素取平均值, 作为最后的输出. 这就相当于一个窗口大小是整个输入的平均池化层, 也称 **全局平均池化层(Global Average Pooling layer)**\n- 所以NiN网络最后输出的通道数等于预测的类别数, 通过一个全局平均池化层(GAP)来得到每个类别的原始输出.\n\n- 这也是为了避免使用全连接层, 减少参数数量的一个操作. 当然也同时减少了计算量, 防止过拟合\n\n### 与AlexNet相似的超参数\n- 最初的NiN网络是在AlexNet后不久提出的，显然从中得到了一些启示。 NiN使用窗口形状为11×11、5×5和3×3的卷积层，输出通道数量与AlexNet中的相同。 每个NiN块后有一个最大汇聚层，汇聚窗口形状为3×3，步幅为2。[^5]\n\n![](notes/2022/2022.3/assets/图片1.jpg)\n## 模型的特性\n- 参数少, 模型不容易过拟合, 同时也减少了计算量.\n- 但是由于增加了大量的1x1卷积, NiN的训练时间更长, 总的计算量也并没有比AlexNet少.\n\n\n[^2]: 这是一个很大的卷积层了\n[^3]: 这就是VGG的第一个全连接层\n[^5]: [7.3. 网络中的网络（NiN） — 动手学深度学习 2.0.0-beta0 documentation](https://zh-v2.d2l.ai/chapter_convolutional-modern/nin.html#id3)","lastmodified":"2022-10-15T14:06:29.606503702Z","tags":null},"/notes/2022/2022.3/D2L-43-GoogLeNetInception":{"title":"D2L-43-GoogLeNet(Inception)","content":"# GoogLeNet\n\n\u003cdiv align=\"right\"\u003e 2022-03-05\u003c/div\u003e\n\nTags: #DeepLearning #CNN #GoogLeNet-Inception\n![](notes/2022/2022.3/assets/img_2022-10-15.jpg)\n- GoogLeNet是一个**含并行连结**的网络, 其核心组成部分为\"**Inception块**\".\n- Inception块组合使用了不同大小的卷积核, 试图用现有的稠密结构(Dense Components)来构建一个\"**最佳的局部稀疏网络**\".\n\t- **局部:** 多个Inception块拼接构成最后的GoogLeNet\n\t- **稀疏:** 也就是具有随机性的结构[^1]\n\n- GoogLeNet还具有**较高的计算效率**, 这主要得益于Inception块里面不含全连接层.\n\n## Inception Block\n![](notes/2022/2022.3/assets/inception.svg)\n\n- GoogLeNet的基本组成部分称为**Inception块**（**Inception block**）。这得名于电影《盗梦空间》（Inception）[^2].\n\n- Inception块由四条并行路径组成。 前三条路径使用窗口大小为 $1×1$、$3×3$ 和 $5×5$ 的卷积层，从不同空间大小中提取信息。 中间的两条路径在输入上执行 $1×1$ 卷积，以减少通道数，从而降低模型的复杂性。\n\n### 并行连结: 不同卷积核的组合\n- Inception并行使用了不同大小的卷积层, 可以**从不同的层面抽取信息**.\n- 尽管窗口大小是不同的, 我们也可以调节Stride和Padding来使**输入和输出高宽相等**\n- 不同路径的输出在通道上进行拼接(将通道数相加).\n\t- 不同路径的通道数占比是不一样的\n\t![](notes/2022/2022.3/assets/Pasted%20image%2020220305210148.png) [^4]\n\n### Add Sparsity using Dense Building Blocks\n- 提升网络性能的一个简单方法就是增大网络的规模, 但是规模的增长会带来网络体积与计算开销的剧烈增长. 作者认为若要从根本上解决这一问题, 我们需要舍弃稠密的结构(比如全连接层和卷积), 采用**更为稀疏的网络架构**. \n\t- 一些理论性的工作[^1]也阐述了一种最优的神经网络结构: 对高度相关的输出进行逐层聚类. 这同时也和神经科学里面的Herbbian principle很类似: \"Neurons that fire together, wire together.\"\n- 为了构建稀疏的(具有随机性的)网络, 同时利用好现有硬件对于密集矩阵的高计算性能, 作者提出了Inception块.\n\n- 下图表示了Inception的原始设计: \n![](notes/2022/2022.3/assets/Inception%20Naive%20Version.jpg)\n- 同时对上图做以下说明：[^3]\n\t- 采用不同大小的卷积核意味着不同大小的感受野，最后拼接意味着不同尺度特征的融合；\n\t- 之所以卷积核大小采用1、3和5，主要是为了方便对齐。设定卷积步长stride=1之后，只要分别设定pad=0、1、2，那么卷积之后便可以得到相同维度的特征，然后这些特征就可以直接拼接在一起了；\n\t- 文章说很多地方都表明pooling挺有效，所以Inception里面也嵌入了。\n\t- 网络越到后面，特征越抽象，而且每个特征所涉及的感受野也更大了，因此随着层数的增加，3x3和5x5卷积的比例也要增加。\n\n### 1x1卷积: 减少通道的数量\n- 但是 $3×3$ 和 $5×5$ 的卷积依然是昂贵的.  为此，文章借鉴 [NiN](notes/2022/2022.3/D2L-42-NiN.md)，采用**1x1卷积来减少通道数**. \n![](notes/2022/2022.3/assets/Inception%20with%20dimension%20reduction.jpg)\n- 额外增加的1x1卷积还为网络增加了**额外的非线性性(Non-linearity)**. 作者说 [1x1卷积层](notes/2022/2022.2/D2L-36-1x1卷积层.md) 的通道融合效果也使得之后的卷积操作能够在融合后的图像上面进行特征提取操作, 这或许能够改进效果.\n\n### Computational Efficiency\n![](notes/2022/2022.3/assets/Pasted%20image%2020220305210203.png)[^4]\n\n## GoogLeNet\n![200](notes/2022/2022.3/assets/LargeGoogLeNet_1.jpg)\n- 将多个Inception块组合起来, 我们便得到了GoogLeNet, 包含5段9个Inception块\n\t- GoogLeNet并没有一开始就使用Inception块, 作者认为这样也许更好, 但也不是一定的\n\t- 网络的最后采用了类似于NiN的全局平均池化层, 但是为了便于迁移学习与参数调整, 最后还是加上了一个全连接层.\n\t- 每一个Inception块都有通道数量上的细微差别, 除了上面提到的调整方向以外, 这些参数其实很难合理解释.\n- 在不同的Inception块之间有时还有3x3池化层用于减小图像尺寸.\n\n![](notes/2022/2022.3/assets/inception-full.svg)\n\n- GoogLeNet一共有 22 层\n- Only 5 million parameters!\n\t- 12x less than AlexNet\n\t- 27x less than VGG-16\n- ILSVRC’14 classification winner (6.7% top 5 error)\n### Auxiliary Networks: Not Important\n- 原始的网络里面为了避免梯度消失，网络额外增加了2个辅助的softmax用于向前传导梯度。但是现在有了更好的训练方法，这些特性不是必要的。\n![](notes/2022/2022.3/assets/Pasted%20image%2020220305211135.png)\n\n## Future Improvements\n- Inception-BN (v2) - 使用 batch normalization\n- Inception-V3 - 修改了Inception块\n\t- 替换 $5 \\times 5$ 为多个 $3 \\times 3$ 卷积层\n\t- 替换 $5 \\times 5$ 为 $1 \\times 7$ 和 $7 \\times 1$ 卷积层\n\t- 替换 $3 \\times 3$ 为 $1 \\times 3$ 和 $3 \\times 1$ 卷积层\n\t- 更深\n- Inception-V4 - 使用残差连接\n\n![Inception变种](notes/2022/2022.3/assets/Inception变种.pdf)\n\n\n[^1]: S. Arora, A. Bhaskara, R. Ge, and T. Ma, “Provable bounds for learning some deep representations,” in _International conference on machine learning_, 2014, pp. 584–592. PDF(zotero://select/items/@arora2014provable)\n[^2]: knowyourmeme.com/memes/we-need-to-go-deeper 电影中有这么一句话: “We need to go deeper”。\n[^3]: [GoogLeNet系列解读_shuzfan的专栏-CSDN博客_googlenet](https://blog.csdn.net/shuzfan/article/details/50738394)\n[^4]: [27 含并行连结的网络 GoogLeNet / Inception V3【动手学深度学习v2】哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1b5411g7Xo?p=1)","lastmodified":"2022-10-15T14:06:29.606503702Z","tags":null},"/notes/2022/2022.3/D2L-44-Batch_Normalization-%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96":{"title":"D2L-44-Batch_Normalization-批量归一化","content":"# Batch Normalization\n\n\u003cdiv align=\"right\"\u003e 2022-03-05\u003c/div\u003e\n\nTags: #BatchNormalization #Normalization #DeepLearning #Regularization \n\n- 批量归一化是一种**加速收敛**的方法.\n- 批量归一化作用于每一个mini-Batch, 先将这个Batch归一化, 然后再做一个统一的偏移与拉伸. \n\t- 最后这个偏移和拉伸的量是一个可以学习的超参数\n\n- 对于全连接层, BN作用于每一个特征\n- 对于卷积层, BN作用于每一个通道. \n\n## Motivation\n- 虽然BatchNorm效果很好, 但其实Batch Normalization效果好的原因并不明朗, 下面的原因也都是事后的推测.\n- 原作者认为Batch Norm能够减少网络内部的\"协变量偏移\", 也就是说网络内部的特征分布可能发生变化. BatchNorm能够减弱这种变化\n- 但是且不说作者误用了\"协变量偏移\"这个词, 一些研究表明Batch Norm并没有改变网络内部的变量分布, 而是使损失函数更加平滑了.\n\n- 从权重稳定性的角度则可以这样理解: \n![](notes/2022/2022.3/assets/img_2022-10-15-1.png)\n\n\n## Batch Normalization Layer\nBN层可以作用在全连接层或者卷积层的输出之前和输入之后,  并且在激活函数之前.\n### 全连接层\n- 对于全连接层, BN作用于每一个特征\n$$\\mathbf{h}=\\phi(\\mathbf{B N}(\\mathbf{W} \\mathbf{x}+\\mathbf{b}))$$\n\n### 卷积层\n- 对于卷积层, BN作用于每一个通道. \n- 也就是说, 假设每一个Batch有 $n$ 个样本 $X_1, X_2, \\cdots, X_n$,  这些样本经过卷积之后得到 $n$ 个输出 $O_1, O_2, \\cdots, O_n$, 每一个输出有 $c$ 个通道 $O_i^{(1)}, O_i^{(2)}, \\cdots, O_i^{(c)}$, 通道的大小为 $h\\times w$\n- BatchNorm将每一个输出 $O_i$ 里面的第 $k$ 个通道 $O_i^{(k)}$ 取出来, 全部展开拉成一条向量(Flatten), 里面一共有 $n\\times h\\times w$ 个元素 , 然后计算这个长条所有元素的平均值 $\\mu_k$ 和方差 $\\sigma_k$. \n\t- 接下来用均值 $\\mu_k$ 和方差 $\\sigma_k$ 对每一个元素进行归一化. \n\t- 进一步, 利用参数 $\\beta_k$ 和 $\\gamma_k$ 对所有元素进行拉伸和平移.\n\t- 最后将所有元素拼成原来的形状, 塞回原来的位置[^2]\n\t\n\t![](notes/2022/2022.3/assets/img_2022-10-15-2.png)\n\t[^1]\n- 每一个通道对应一对 $\\beta_k$ 和 $\\gamma_k$ ,  $\\beta_k$ 和 $\\gamma_k$ 都是可以学习的参数.\n\n![Batchnorm](notes/2022/2022.3/assets/Batchnorm.svg)\n\n## 规范化定义\n- 从形式上来说, 用 $\\mathbf{x} \\in \\mathcal{B}$ 表示一个来自小批量 $\\mathcal{B}$ 的输入，批量规范化BN对 $\\mathbf{x}$ 的作用可以表示为:\n\t$$\\operatorname{BN}(\\mathbf{x})=\\gamma \\odot \\frac{\\mathbf{x}-\\hat{\\boldsymbol{\\mu}}_{\\mathcal{B}}}{\\hat{\\boldsymbol{\\sigma}}_{\\mathcal{B}}}+\\boldsymbol{\\beta}$$\n\t- 上式中 $\\hat{\\boldsymbol{\\mu}}_{\\mathcal{B}}$ 是小批量 $\\mathcal{B}$ 的样本均值, $\\hat{\\boldsymbol{\\sigma}}_{\\mathcal{B}}$ 是小批量 $\\mathcal{B}$ 的样本标准差。\n\t- 应用标准化后, 生成的小批量的平均值为 0 和单位方差为 1 。\n\t- 将输出进行这样的标准化是一个比较强的约束, 这并不总是合理的, 因此我们通常包含 拉伸参数（scale） $\\gamma$ 和偏移参数（shift） $\\boldsymbol{\\beta}$, 它们的形状与 $\\mathbf{x}$ 相同。\n\t- 请注意, $\\gamma$ 和 $\\boldsymbol{\\beta}$ 是需要与其他模型参数一起学习的参数。\n\n- 请注意，如果我们尝试使用大小为1的小批量应用批量规范化，我们将无法学到任何东西。 这是因为在减去均值之后，每个隐藏单元将为0。 所以，只有使用足够大的小批量，批量规范化这种方法才是有效且稳定的。 请注意，批量大小的选择在有BN时比没有BN时更重要。\n\n- 每一个批量的均值 $\\hat{\\boldsymbol{\\mu}}_{\\mathcal{B}}$ 和方差 $\\hat{\\boldsymbol{\\sigma}}_{\\mathcal{B}}$ 的计算如下所示:\n\t$$\\begin{aligned}\n\t\\hat{\\boldsymbol{\\mu}}_{\\mathcal{B}} \u0026=\\frac{1}{|\\mathcal{B}|} \\sum_{\\mathbf{x} \\in \\mathcal{B}} \\mathbf{x} \\\\\n\t\\hat{\\boldsymbol{\\sigma}}_{\\mathcal{B}}^{2} \u0026=\\frac{1}{|\\mathcal{B}|} \\sum_{\\mathbf{x} \\in \\mathcal{B}}\\left(\\mathbf{x}-\\hat{\\boldsymbol{\\mu}}_{\\mathcal{B}}\\right)^{2}+\\epsilon\n\t\\end{aligned}$$\n\t- 请注意, 我们在方差估计值中添加一个小的常量 $\\epsilon\u003e0$, 以确保即使方差很小, 我们也永远不会除以零。\n\t- 估计值 $\\hat{\\boldsymbol{\\mu}}_{\\mathcal{B}}$ 和 $\\hat{\\boldsymbol{\\sigma}}_{\\mathcal{B}}$ 与Batch的随机性密切相关, 也就是说, 在对输入进行归一化的同时其实是会引入噪声的, 。你可能会认为这种噪声是一个问题, 而事实上它是有益的。\n\t\t- This turns out to be a recurring theme in deep learning. For reasons that are not yet well-characterized theoretically, various sources of noise in optimization often lead to faster training and less overfitting: this variation appears to act as a form of regularization. In some preliminary research, [Teye et al., 2018](https://d2l.ai/chapter_references/zreferences.html#teye-azizpour-smith-2018 ) and [Luo et al., 2018](https://d2l.ai/chapter_references/zreferences.html#luo-wang-shao-ea-2018 ) relate the properties of batch normalization to Bayesian priors and penalties respectively. In particular, this sheds some light on the puzzle of why batch normalization works best for moderate minibatches sizes in the 50∼100 range.[^3]\n\n## 其他相似的Normalization\n- 这里面N是样本的个数, D是特征数\n\n![Stanford CS231n BN](notes/2022/2022.3/assets/Stanford%20CS231n%20BN.pdf)\n\n## 预测与训练的不同\n- 训练的时候是以Batch进行的, 而预测的时候我们通常输入的是单张图片, 那我们怎么计算均值和方差呢? \n\t- 我们使用训练样本整体的均值和方差来代替\n\n\n[^1]: 这个图居然是Drawio画的, 可惜原作者的链接没了 [calculation of mean and variance in batch normalization in convolutional neural network - Stack Overflow](https://stackoverflow.com/questions/65613694/calculation-of-mean-and-variance-in-batch-normalization-in-convolutional-neural) \n![](notes/2022/2022.3/assets/BatchNormFull.png)\n\n[^2]: 只是为了解释的清楚, 实际计算的时候并不是我说的这样的\n[^3]: [7.5. Batch Normalization — Dive into Deep Learning 0.17.2 documentation](https://d2l.ai/chapter_convolutional-modern/batch-norm.html?highlight=scaling%20issue#training-deep-networks)","lastmodified":"2022-10-15T14:06:29.606503702Z","tags":null},"/notes/2022/2022.3/D2L-45-ResNet":{"title":"D2L-45-ResNet","content":"# ResNet 残差网络\n\n\u003cdiv align=\"right\"\u003e 2022-03-06\u003c/div\u003e\n\nTags: #ResNet #CNN #DeepLearning \n\n- ResNet在网络中引入了**残差连接**的思想, 简单的改变带来了很棒的效果.\n\n- 残差连接让每一层**很容易地包含了原始函数**[^5], 这样能保证新增加的每一层都能包含原来的最优解, 进一步在原来的基础上继续改进.\n\n## Motivation\n### 函数类的角度\n- 我们定义 $\\mathcal{F}$ 是**某个模型能够拟合的所有函数**构成的函数类.  即对于 $\\forall f\\in \\mathcal{F}$,  都存在一组参数使得模型能够拟合到 $f$. \n- 假设对于某个问题存在最优的函数 $f^*$. \n\t- 如果 $f^*\\in\\mathcal{F}$, 那么网络很容易就能找到最优解. \n\t- 但是生活总是充满了艰辛, 我们通常都只能委曲求全, 在 $\\mathcal{F}$ 里面寻找一个最优近似 $f^*_{\\mathcal{F}}$.\n\n- 形式化的说, 我们训练网络的过程就是在寻找: $$f_{\\mathcal{F}}^{*} \\stackrel{\\text { def }}{=} \\underset{f}{\\operatorname{argmin}} L(\\mathbf{X}, \\mathbf{y}, f) \\text { subject to } f \\in \\mathcal{F}$$\n\t- 其中 $L$ 是损失函数.\n\n- 为了找到更好的近似解, 我们需要改进我们的模型: $\\mathcal{F}\\rightarrow \\mathcal{F}'$. 但是新的近似解 $f^*_{\\mathcal{F'}}$ 总是更好的吗? \n\t![](notes/2022/2022.3/assets/functionclasses.svg)\n\t- 要是 $\\mathcal{F}\\notin \\mathcal{F}'$, 那么新的解可能还不如原来的解. 就像上图中表示的那样: 我们不断地改进模型, 函数类却离最优解越来越远, 模型的表现越来越差. \n\n\t- 为了保证我们能够脚踏实地向着目标前进, 我们就必须要保证 $\\mathcal{F}\\in \\mathcal{F}'$, 这样每一步都是在前一次最好结果的基础上改进的. \n\t\t- 加了一层网络, 即使没有变好, 但至少不会变差.\n\n- 怎么让 $\\mathcal{F}\\in \\mathcal{F}'$ 呢? 我们可以让新的结构尽可能简单地包含原始函数, 或者说要让新的函数很容易变成恒等映射. \n\t- 最简单的方法就是让网络能够直接跳过新的结构 $f(\\mathbf{x})$\n\t- 也就是说, 让网络的输出 $g(\\mathbf x)=f(\\mathbf{x})+\\mathbf{x}$. 这样只需要让新结构 $f(\\mathbf{x})$ 的参数变成0, 就能恢复原来网络的表达能力. 这就是构建Residual Block的基本直觉.\n\t\t![](notes/2022/2022.3/assets/residual-block.svg)\n### 深层网络的训练困境\n- 现在让我们从另一个角度来考虑残差连接的有效性. \n- 一昧的加深神经网络并不能带来模型性能的提升. 这有时候是因为 [模型容量](notes/2022/2022.2/D2L-21-模型容量.md) 与数据集不匹配导致了 [过拟合](notes/2021/2021.8/Part.17_Overfitting_Underfitting(ML_Andrew.Ng.).md), 有时则是因为过深的网络造成了梯度爆炸或者梯度消失的问题. \n\t- 对于 [后一种问题](notes/2022/2022.2/D2L-24-数值稳定性.md),  谨慎地初始化参数可能会有一些帮助, 但是这并不能从根本上解决问题. 而残差连接提供了一种新的思路: 用加法代替乘法.[^1]\n\t![LargeGoogLeNet 1|200](notes/2022/2022.3/assets/LargeGoogLeNet%201.jpg)\n- 为了简单起见, 我们将靠近输入的网络抽象为 $f(\\mathbf x)$, 靠近输出的网络表示为 $g(\\mathbf x)$. \n\t- 如果我们将前一层网络的输出作为下一层网络的输入, 则模型表示为 $$g\\left(f(\\mathbf x)\\right)$$\n\t\t![SimpleNetAbstract](notes/2022/2022.3/assets/SimpleNetAbstract.svg)\n\t- 嵌套网络在 $f(\\mathbf x)$ 层的梯度为: \n\t\t$$\\begin{aligned}\n\t\t\\frac{\\partial}{\\partial\\mathbf{w}}g\\left(f(\\mathbf x)\\right)\u0026=\\frac{\\partial g\\left(f(\\mathbf x)\\right)}{\\partial f(\\mathbf x)}\\frac{\\partial f(\\mathbf x)}{\\partial \\mathbf w}\n\t\t\\end{aligned}$$\n\t\t- 因为梯度是反向传播的, 如果网络比较深, 这个梯度会因为矩阵连乘变得较小[^2] . 这进一步会导致网络深层的更新较慢, 甚至因为梯度消失而不收敛. \n\t\t- 下图是论文里面提供的一个例子. 可以看到更深的网络的训练误差和测试误差都更高, 这说明网络\"学不动了\".\n\t\t\t![](notes/2022/2022.3/assets/Pasted%20image%2020220306200938.png)\n- 我们来看看残差链接是怎样解决这个问题的, 对于含残差连接的网络, 可以形象化的表示为 $$g(\\left(f(\\mathbf x)\\right) +f(\\mathbf x)$$\n\t![ResidualNetAbstract](notes/2022/2022.3/assets/ResidualNetAbstract.svg)\n\t- 这个网络在 $f(\\mathbf x)$ 层的梯度为: \n\t\t$$\\begin{aligned}\n\t\t\\frac{\\partial}{\\partial\\mathbf{w}}\\left(g\\left(f(\\mathbf x)\\right)+f(\\mathbf x)\\right)\u0026=\\frac{\\partial g\\left(f(\\mathbf x)\\right)}{\\partial \\mathbf w}+\\frac{\\partial f(\\mathbf x)}{\\partial \\mathbf w}\n\t\t\\end{aligned}$$\n\t\t- 尽管前面那部分依然有梯度消失的问题, 但是后面的 $\\frac{\\partial f(\\mathbf x)}{\\partial \\mathbf w}$ 依然能保证训练的继续进行.\n\n## 网络架构\n### 残差块\n![](notes/2022/2022.3/assets/residual-block%201.svg)\n- 遵循上面的设计思路, ResNet残差块的基本结构如上所示\n\n- ResNet沿用了VGG完整的 $3×3$ 卷积层设计。 残差块里首先有2个有相同输出通道数的 $3×3$ 卷积层。 每个卷积层后接一个批量归一化层(BN)和ReLU激活函数。 \n- 残差通路(Shortcut) 则跳过这2个卷积运算，将输入直接加在最后的ReLU激活函数前。 \n\t- 因为需要将卷积后的输出与残差通路相加, 所以这样的设计要求卷积层的输入输出形状一样。 \n\t- 如果想改变通道数，就需要引入一个额外的 $1×1$ 卷积层来将输入变换成需要的形状后再做相加运算。 \n\t- 残差块的实现如下：\n\t![](notes/2022/2022.3/assets/resnet-block.svg)\n### 两种不同的残差块\n- ResNet遵循了VGG的模块思想, 并且进一步为不同规模的模型构建了两种不同的Building Block:\n\t![](notes/2022/2022.3/assets/Pasted%20image%2020220306204258.png) [^3]\n\t- 深层的网络输入通道会更多, 因此ResNet先利用 $1\\times1$ 的卷积将输入通道进行融合, 在卷积后再还原原来的通道数目.[^4] \n\t- 下图是不同规模ResNet的基本架构, 可以看出50层往上的ResNet都采用了第二种架构.\n![](notes/2022/2022.3/assets/Pasted%20image%2020220306204536.png) [^3]\n\n### 拼接残差块: 多阶段的模型\n![](notes/2022/2022.3/assets/resnet18.svg)\n- GoogLeNet在后面接了4个由Inception块组成的模块。 ResNet则使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。\n- 更详细的模型实现参见教材(ResNet18): \n\t[7.6. 残差网络（ResNet） ResNet模型](https://zh-v2.d2l.ai/chapter_convolutional-modern/resnet.html#id4) \n\n\n[^1]: [29.2 ResNet为什么能训练出1000层的模型【动手学深度学习v2】 哔哩哔哩 bilibili](https://www.bilibili.com/video/BV1554y157E3)\n[^2]: [D2L-24-数值稳定性](notes/2022/2022.2/D2L-24-数值稳定性.md) . 还有如果 $g(\\mathbf x)$ 已经学习的较好了, 梯度也可能变得较小. (在损失函数取得恰当的时候,  一般是这样的. 一个例子是 [为什么Softmax回归不用MSE](notes/2022/2022.2/为什么Softmax回归不用MSE.md))\n[^3]: K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2016, pp. 770–778. PDF(zotero://select/items/@he2016deep)\n[^4]: 这体现了[D2L-43-GoogLeNet(Inception)](notes/2022/2022.3/D2L-43-GoogLeNet(Inception).md) 和 [D2L-42-NiN](notes/2022/2022.3/D2L-42-NiN.md) 的思想\n[^5]: 另一个说法是很容易让一层网络变成恒等函数 Identity Function","lastmodified":"2022-10-15T14:06:29.606503702Z","tags":null},"/notes/2022/2022.3/D2L-46-DenseNet":{"title":"D2L-46-DenseNet","content":"# DenseNet\n\n\u003cdiv align=\"right\"\u003e 2022-03-07\u003c/div\u003e\n\nTags: #DenseNet #DeepLearning #CNN \n![](notes/2022/2022.3/assets/img_2022-10-15-3.png)\n\n- ResNet极大地改变了如何参数化深层网络中函数的观点。 稠密连接网络（DenseNet）在某种程度上是**ResNet的逻辑扩展**。\n\n- PDF(zotero://select/items/@huang2017densely) \n\n## 数学直觉: 从ResNet到DenseNet\n- 某个函数在 $x=0$ 处的泰勒展开为: \n\t$$f(x)=f(0)+f^{\\prime}(0) x+\\frac{f^{\\prime \\prime}(0)}{2 !} x^{2}+\\frac{f^{\\prime \\prime \\prime}(0)}{3 !} x^{3}+\\ldots$$\n\t- 这其中的思想是: 把事物分解为复杂度不断递增的项\n\t- ResNet其实也体现了这种思想: $$f(\\mathbf{x})=\\mathbf{x}+g(\\mathbf{x})$$\n\t\t它将 $f(\\mathbf{x})$ 分解为了简单的线性项 $\\mathbf{x}$ 与复杂的非线性项 $g(\\mathbf{x})$.\n\n- 我们可以怎样将这种思想进一步推广呢?  \n\t- 我们需要创建更复杂的连接\n\t![ResNet2DenseNet|300](notes/2022/2022.3/assets/ResNet2DenseNet.svg)\n\t- 进一步, 我们使用\"拼接\"(_concatenation_)代替加法.\n\t![](notes/2022/2022.3/assets/densenet-block.svg)\n\t- 新的连接使得最后的输出是各种不同复杂度的项的聚合: \n\t\t$$\\mathbf{x} \\rightarrow\\left[\n\t\\textcolor[RGB]{157, 72, 68}{\\mathbf{x}}, \n\t\\textcolor[RGB]{255, 116, 109}{f_{1}(\\mathbf{x})}, \n\t\\textcolor[RGB]{255, 154, 109}{f_{2}\\left(\\left[\\mathbf{x}, f_{1}(\\mathbf{x})\\right]\\right)}, \n\t\\textcolor[RGB]{255, 211, 109}{f_{3}\\left(\\left[\\mathbf{x}, f_{1}(\\mathbf{x}), f_{2}\\left(\\left[\\mathbf{x}, f_{1}(\\mathbf{x})\\right]\\right)\\right]\\right)},\n\t\\ldots\\right]$$\n\t![](notes/2022/2022.3/assets/densenet.svg)\n\t- 可以看到函数依赖图变得十分的\"稠密\"(Dense), 这也是DenseNet名字的由来.\n- 为了使网络不会过于复杂, 我们使用1x1卷积核池化来控制通道的数量.\n\n## 网络架构\n- DenseNet由相互制约的两个组件构成: **稠密块**(*Dense Block*)和**过渡层**(*Transition Layer*). \n\t- 前者定义如何连接输入和输出，而后者则控制通道数量，使其不会太复杂。\n\n### 稠密块\n- DenseNet使用了ResNet改良版的“批量规范化、激活和卷积”架构.[^1]\n- 一个_稠密块_由多个卷积块组成，每个卷积块使用相同数量的输出通道。 在前向传播中，我们不再将多路输出相加, 而是直接在通道上相连(Concatenate).\n\n- 详细实现参见 : [7.7. Densely Connected Networks (DenseNet) — Dense Block](https://d2l.ai/chapter_convolutional-modern/densenet.html#dense-blocks)\n### 过渡层\n- 由于每个稠密块都会带来通道数的增加，使用过多则会过于复杂化模型。 而过渡层可以用来控制模型复杂度。 它通过1×1卷积层来减小通道数，并使用步幅为2的平均汇聚层减半高和宽，从而进一步降低模型复杂度。\n\n- 详细实现参见 : [7.7. Densely Connected Networks (DenseNet) —Transition Layer](https://d2l.ai/chapter_convolutional-modern/densenet.html#transition-layers)\n\n### DenseNet\n- DenseNet首先使用同ResNet一样的单卷积层和最大汇聚层。\n- 接下来，类似于ResNet使用的4个残差块，DenseNet使用的是4个稠密块。 与ResNet类似，我们可以设置每个稠密块使用多少个卷积层。\n- 在每个模块之间，ResNet通过步幅为2的残差块减小高和宽，DenseNet则使用过渡层来减半高和宽，并减半通道数。\n\n- 与ResNet类似，最后接上全局汇聚层和全连接层来输出结果。\n\n- 详细实现参见 : [7.7. Densely Connected Networks (DenseNet) — DenseNet](https://d2l.ai/chapter_convolutional-modern/densenet.html#densenet-model)\n\n[^1]: He, K., Zhang, X., Ren, S., \u0026 Sun, J. (2016). Identity mappings in deep residual networks. _European conference on computer vision_ (pp. 630–645).","lastmodified":"2022-10-15T14:06:29.606503702Z","tags":null},"/notes/2022/2022.3/D2L-47-%E5%BA%8F%E5%88%97%E4%BF%A1%E6%81%AF":{"title":"D2L-47-序列信息","content":"# 序列信息\n\n\u003cdiv align=\"right\"\u003e 2022-03-07\u003c/div\u003e\n\nTags: #SequentialData\n\n## 数据分布的不同\n- 对于图像或者表格数据， 我们通常都假设所有样本是**独立同分布**的[^1]。 然而，大多数的数据都有序列性。 \n\t- 例如，文章中的单词是按顺序写的，如果顺序被随机地重排，就很难理解文章原始的意思。 同样，视频中的图像帧、对话中的音频信号以及网站上的浏览行为都是有顺序的。 因此，针对此类数据而设计特定模型，可能效果会更好。\n\n## 实际情景\n- 在接收一个序列作为输入的时候， 我们通常期望猜测这个序列的后续。 \n\t- 例如预测股市的波动、 患者的体温曲线或者赛车所需的加速度。 我们需要能够处理这些数据的特定模型。\n\n## 相关模型\n- 如果说卷积神经网络可以有效地处理空间信息， 那么 _循环神经网络_（recurrent neural network，RNN）则可以更好地处理序列信息。 \n\n#todo \n\n\n[^1]: 这真的合理吗？","lastmodified":"2022-10-15T14:06:29.606503702Z","tags":null},"/notes/2022/2022.3/D2L-48-%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B-Sequence_Models":{"title":"D2L-48-序列模型-Sequence_Models","content":"# 序列模型 - Sequence Models\n\n\u003cdiv align=\"right\"\u003e 2022-03-07\u003c/div\u003e\n\nTags: #SequenceModel #SequentialData\n\n- 两种流行的序列模型是**自回归模型**和**隐变量自回归模型**\n\n## 预测问题\n![](notes/2022/2022.3/assets/img_2022-10-15-4.png)\n- 假设我们想要根据前 $t-1$ 天的股票价格预测今天的股票价格 $x_t$, 这个问题可以抽象为： $$x_{t} \\sim P\\left(x_{t} \\mid x_{t-1}, \\ldots, x_{1}\\right)$$\n\t- 问题的关键在于估计 $P\\left(x_{t} \\mid x_{t-1}, \\ldots, x_{1}\\right)$\n\n- 一个简单的想法是使用**回归**模型来估计概率。但是我们首先需要解决这么一个问题： 随着时间的流逝，模型的输入 $x_1, \\cdots，x_{t-1}$ 会不断增加，计算会变得越来越困难，直到不可接受。\n- 基于不同的假设，我们得到了两种模型：**自回归模型** 和 **隐变量自回归模型**\n\n## 自回归模型 Autoregressive Models\n- 自回归模型的假设是： 只有**最近一段时间的数据**是有用的，这被称为马尔可夫条件(Markov condition)。因此，我们只需要考虑一个时间跨度 $\\tau$ 里面的所有数据: $x_{t-1}, \\ldots, x_{t-\\tau}$.\n\t- 因为输入的长度是不变的，所以模型的参数也是不变的. \n\n- 因为模型根据 $x_{t-1}, \\ldots, x_{t-\\tau}$ 预测 $x_t$,  相当于是在对变量 $x$ 自身进行回归. 故称模型为\"自回归模型\".\n\n- 如果将模型表示为 $f(x)$, 则自回归模型可以抽象为: $$p\\left(x_{t} \\mid x_{t-1}, \\ldots x_{1}\\right)=p\\left(x_{t} \\mid x_{t-1}, \\ldots, x_{t-\\tau}\\right)=p\\left(x_{t} \\mid f\\left(x_{t-1}, \\ldots, x_{t-\\tau}\\right)\\right)$$\n\n\n### 马尔可夫模型\n- 满足马尔可夫条件的模型称为**马尔可夫模型**(Markov Model). 根据 $\\tau$ 的不同取值, 我们可以得到不同\"阶数\"的模型:\n\t- $\\tau=1$ : 一阶马尔可夫模型 $$P\\left(x_{1}, \\ldots, x_{T}\\right)=\\prod_{t=1}^{T} P\\left(x_{t} \\mid x_{t-1}\\right) \\text { 当 } P\\left(x_{1} \\mid x_{0}\\right)=P\\left(x_{1}\\right)$$\n\t\t- 当 $x_i$ 取离散值的时候, 一阶马尔可夫模型可以使用**动态规划**可以沿着马尔可夫链精确地计算结果. 比如: $$\\begin{aligned}\n\t\tP\\left(x_{t+1} \\mid x_{t-1}\\right) \u0026=\\frac{\\sum_{x_{t}} P\\left(x_{t+1}, x_{t}, x_{t-1}\\right)}{P\\left(x_{t-1}\\right)} \\\\\n\t\t\u0026=\\frac{\\sum_{x_{t}} \\textcolor{red}{P\\left(x_{t+1} \\mid x_{t}, x_{t-1}\\right)} P\\left(x_{t}, x_{t-1}\\right)}{P\\left(x_{t-1}\\right)} \\\\\n\t\t\u0026=\\sum_{x_{t}} \\textcolor{red}{P\\left(x_{t+1} \\mid x_{t}\\right)} P\\left(x_{t} \\mid x_{t-1}\\right)\n\t\t\\end{aligned}$$\n\t\t- 红色的部分利用了马尔可夫条件, 我们不需要考虑 $x_{t-1}$ 即可求得结果.\n\n## 隐变量自回归模型 Latent Autoregressive Models\n- 这个模型的假设是: \"之前所有输入都可以总结为隐变量 $h_{t}$ \", 我们可以根据隐变量来预测 $x_t$ \n\t![](notes/2022/2022.3/assets/sequence-model.svg)\n\t- 如上图所示, 我们保留一些对过去观测的总结 $h_{t}$ , 同时更新预测 $\\hat{x}_{t}$ 和总结 $h_{t}$ 。也就是基于 $\\hat{x}_{t}=P\\left(x_{t} \\mid h_{t}\\right)$ 来估计 $x_{t}$ .\n\n- 有时也可以写成这样: 根据上一次输出与潜变量一起决定这一次的输出.\n![](notes/2022/2022.3/assets/Pasted%20image%2020220308193240.png)\n\n\n## Stationary Dynamics\n- 假设上面的模型能够成功地预测股价, 我们的模型到底学到了什么呢? \n\t- 模型的输出是在不断地变化的, 而我们的模型学习到的应该是这种变化背后的规律(Dynamics). 如果这个规律不会变化, 我们就称其为\"静止的\"规律 (Dynamics, 动力学).\n\n## 因果关系 Causality\n- 我们可以这样表示一个序列的总概率: \n\t$$P\\left(x_{1}, \\ldots, x_{T}\\right)=\\prod_{t=1}^{T} P\\left(x_{t} \\mid x_{t-1}, \\ldots, x_{1}\\right)$$ ^973ecf\n- 但是, 数学上我们完全也可以反过来写: \n\t$$P\\left(x_{1}, \\ldots, x_{T}\\right)=\\prod_{t=T}^{1} P\\left(x_{t} \\mid x_{t+1}, \\ldots, x_{T}\\right)$$\n\t- 但是这意味着现在的事件影响了过去的事件! 但是这有意义吗?\n\t- 在许多情况下，数据存在一个自然的方向，即在时间上是前进的。 很明显，未来的事件不能影响过去。 因此，如果我们改变 $x_t$，可能会影响未来发生的事情 $x_{t+1}$ ，但不能反过来。也就是说，如果我们改变 $x_t$ ，基于过去事件得到的分布不会改变。 因此，解释P $(x_{t+1}∣x_t)$ 应该比解释 $P(x_t∣x_{t+1})$ 更容易。  ^f8bb86\n\n\t\t- 例如，在某些情况下，对于某些可加性噪声 $ϵ$， 显然我们可以找到 $x_{t+1}=f(x_t)+ϵ$， 而反之则不行 [Hoyer et al., 2009](https://zh-v2.d2l.ai/chapter_references/zreferences.html#hoyer-janzing-mooij-ea-2009 )。 这是个好消息，因为这个前进方向通常也是我们感兴趣的方向。\n\n## k步预测\n- 对于直到时间步 $t$ 的观测序列，其在时间步 $t+k$ 的预测输出是“ $k$ 步预测”。\n- 例如5步预测可以表示如下: \n$$\\begin{aligned}\n\u0026\\textcolor{blue}{\\hat{x}_{5}}=f\\left(x_{1}, x_{2}, x_{3}, x_{4}\\right) \\\\\n\u0026\\textcolor{blue}{\\hat{x}_{6}}=f\\left(x_{2}, x_{3}, x_{4}, \\textcolor{blue}{\\hat{x}_{5}}\\right) \\\\\n\u0026\\textcolor{blue}{\\hat{x}_{7}}=f\\left(x_{3}, x_{4}, \\textcolor{blue}{\\hat{x}_{5}}, \\textcolor{blue}{\\hat{x}_{6}}\\right) \\\\\n\u0026\\textcolor{blue}{\\hat{x}_{8}}=f\\left(x_{4}, \\textcolor{blue}{\\hat{x}_{5}}, \\textcolor{blue}{\\hat{x}_{6}}, \\textcolor{blue}{\\hat{x}_{7}}\\right) \\\\\n\u0026\\textcolor{blue}{\\hat{x}_{9}}=f\\left(\\textcolor{blue}{\\hat{x}_{5}}, \\textcolor{blue}{\\hat{x}_{6}}, \\textcolor{blue}{\\hat{x}_{7}}, \\textcolor{blue}{\\hat{x}_{8}}\\right)\n\\end{aligned}$$\n- 但是随着我们对预测时间 $k$ 值的增加，会造成误差的快速累积和预测质量的急速下降。\n\t![](notes/2022/2022.3/assets/kStepPredict.svg)","lastmodified":"2022-10-15T14:06:29.606503702Z","tags":null},"/notes/2022/2022.3/D2L-49-%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86-Text-Preprocessing":{"title":"D2L-49-文本预处理-Text Preprocessing","content":"# 文本预处理\n\n\u003cdiv align=\"right\"\u003e 2022-03-08\u003c/div\u003e\n\nTags: #Preprocessing \n\n- 动手是最好的学习: [8.2. 文本预处理 — 动手学深度学习 2.0.0-beta0 documentation](https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/text-preprocessing.html)\n\n- 一些常见的操作:\n\t1.  将文本作为字符串加载到内存中。\n\t2.  将字符串拆分为词元（如单词和字符）。Tokenize\n\t3.  建立一个词表，将拆分的词元映射到数字索引。\n\t4.  将文本转换为数字索引序列，方便模型操作。\n\n- 语料 in English: corpus\n\n- 语料库中不存在或已删除的任何词元都将映射到一个特定的未知词元 `\u003cunk\u003e` 。 我们可以选择增加一个列表，用于保存那些被保留的词元， 例如：填充词元（ `\u003cpad` \u003e）； 序列开始词元（ `\u003cbos\u003e` ）； 序列结束词元（ `\u003ceos\u003e` ）。\n\t- 尖括号通常用来表示特殊字符\n\n","lastmodified":"2022-10-15T14:06:29.610503745Z","tags":null},"/notes/2022/2022.3/D2L-50-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-%E4%BC%A0%E7%BB%9F%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8D%E8%B6%B3":{"title":"D2L-50-语言模型-传统模型的不足","content":"# 语言模型\n\n\u003cdiv align=\"right\"\u003e 2022-03-08\u003c/div\u003e\n\nTags: #LanguageModel\n\n## 传统模型\n- 语言模型的输出是一个文本序列: $$x_{t},\\ x_{t-1},\\ \\ldots\\ ,\\ x_{1}$$\n\t- 为了生成有意义的序列, 我们希望模拟语料库里面的语句, 生成概率 $P\\left(x_{1}, x_{2}, \\ldots, x_{T}\\right)$ 最高的语句.\n\n![](notes/2022/2022.3/D2L-48-序列模型-Sequence_Models.md#^973ecf)\n\n- 一个例子是 $$\\begin{aligned}\u0026P(deep, learning, is, fun)=\\\\\u0026P(deep) P(learning\\mid deep ) P( is  \\mid  deep, learning ) P( fun  \\mid  deep, learning, is )\\end{aligned}$$\n\n- 为了估计这个句子的概率, 我们需要计算里面的每一个部分, 一个想法是用频率代替概率: $$\\hat{P}(\\text { learning } \\mid \\text { deep })=\\frac{n(\\text { deep }, \\text { learning })}{n(\\text { deep })}$$\n- 但是: \n\u003e 不幸的是，由于连续单词对“deep learning”的出现频率要低得多， 所以估计这类单词正确的概率要困难得多。 特别是对于一些不常见的单词组合，要想找到足够的出现次数来获得准确的估计可能都不容易。 而对于三个或者更多的单词组合，情况会变得更糟。 许多合理的三个单词组合可能是存在的，但是在数据集中却找不到。 除非我们提供某种解决方案，来将这些单词组合指定为非零计数， 否则将无法在语言模型中使用它们。 如果数据集很小，或者单词非常罕见，那么这类单词出现一次的机会可能都找不到。[^1]\n\n### 拉普拉斯平滑\n- 一种常见的策略是执行 **拉普拉斯平滑**（Laplace smoothing）， 具体方法是在所有计数中添加一个小常量。 用 $n$ 表示训练集中的单词总数，用 $m$ 表示唯一单词的数量。 此解决方案有助于处理单元素问题，例如通过：\n$$\\begin{aligned}\n\\hat{P}(x) \u0026=\\frac{n(x)+\\epsilon_{1} / m}{n+\\epsilon_{1}} \\\\\n\\hat{P}\\left(x^{\\prime} \\mid x\\right) \u0026=\\frac{n\\left(x, x^{\\prime}\\right)+\\epsilon_{2} \\hat{P}\\left(x^{\\prime}\\right)}{n(x)+\\epsilon_{2}} \\\\\n\\hat{P}\\left(x^{\\prime \\prime} \\mid x, x^{\\prime}\\right) \u0026=\\frac{n\\left(x, x^{\\prime}, x^{\\prime \\prime}\\right)+\\epsilon_{3} \\hat{P}\\left(x^{\\prime \\prime}\\right)}{n\\left(x, x^{\\prime}\\right)+\\epsilon_{3}} .\n\\end{aligned}$$\n- 其中，$ϵ_1$, $ϵ_2$ 和 $ϵ_3$ 是超参数。 以 $ϵ_1$ 为例：当 $ϵ_1=0$ 时，不应用平滑； 当 $ϵ_1$ 接近正无穷大时，$\\hat P(x)$ 接近均匀概率分布 $1/m$。 上面的公式是 [Wood et al., 2011](https://zh-v2.d2l.ai/chapter_references/zreferences.html#wood-gasthaus-archambeau-ea-2011 ) 的一个相当原始的变形。[^2]\n\n#### 缺点\n- 然而，这样的模型很容易失效，原因如下： \n\t- 首先，我们需要存储所有的计数； \n\t- 其次，这完全忽略了单词的意思。 例如，“猫”（cat）和“猫科动物”（feline）可能出现在相关的上下文中， 但是想根据上下文调整这类模型其实是相当困难的。 \n\t- 最后，长单词序列大部分是没出现过的， 因此一个模型如果只是简单地统计先前“看到”的单词序列频率， 那么模型面对这种问题肯定是表现不佳的。\n\n\n[^1]: [8.3. 语言模型和数据集 — 动手学深度学习 2.0.0-beta0 documentation](https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/language-models-and-dataset.html)\n[^2]: [8.3. 语言模型和数据集 — 学习语言模型](https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/language-models-and-dataset.html#id2)","lastmodified":"2022-10-15T14:06:29.610503745Z","tags":null},"/notes/2022/2022.3/D2L-51-%E8%AF%AD%E8%A8%80%E7%9A%84%E7%BB%9F%E8%AE%A1%E7%89%B9%E5%BE%81":{"title":"D2L-51-语言的统计特征","content":"# 语言的统计特征\n\n\u003cdiv align=\"right\"\u003e 2022-03-08\u003c/div\u003e\n\nTags: #Zipf_Law \n\n## n元语法 n-gram\n- 我们将涉及一个、两个和三个变量的概率公式的模型分别称为 “一元语法”（unigram）、“二元语法”（bigram）和“三元语法”（trigram）模型.\n\t$$\\begin{aligned}\n\t\u0026P\\left(x_{1}, x_{2}, x_{3}, x_{4}\\right)=P\\left(x_{4}\\right) P\\left(x_{3}\\right) P\\left(x_{2}\\right) P\\left(x_{1}\\right) \\\\\n\t\u0026P\\left(x_{1}, x_{2}, x_{3}, x_{4}\\right)=P\\left(x_{4} \\mid x_{3}\\right) P\\left(x_{3} \\mid x_{2}\\right) P\\left(x_{2} \\mid x_{1}\\right) P\\left(x_{1}\\right) \\\\\n\t\u0026P\\left(x_{1}, x_{2}, x_{3}, x_{4}\\right)=P\\left(x_{4} \\mid x_{2}, x_{3}\\right)P\\left(x_{3} \\mid x_{1}, x_{2}\\right) P\\left(x_{2} \\mid x_{1}\\right) P\\left(x_{1}\\right) \\end{aligned}$$\n\t\n\t- 例如, 一阶马尔可夫模型的依赖关系为 $P\\left(x_{t} \\mid x_{t-1}\\right)$, 对应二元语法.\n\n\n## 齐普夫定律\n- 在自然语言的语料库里，一个单词出现的频率与它在频率表里的排名成反比。[^1]\n- 第 $i$ 个最常用单词的频率 $n_{i}$ 为：\n$$n_{i} \\propto \\frac{1}{i^\\alpha}$$ 等价于$$\\log n_{i}=-\\alpha \\log i+c$$\n- 在双对数曲线上可以表示为: \n\t![](notes/2022/2022.3/assets/Zipf.svg)\n- 有趣的是, 即使是多元语法的词序列也符合Zipf's Law: \n\t![](notes/2022/2022.3/assets/ZipfALL.svg)\n\t- 这张图非常令人振奋！首先，除了一元语法词，**单词序列**似乎也遵循齐普夫定律， 并且公式中的指数 $α$ 更小 （指数的大小受序列长度的影响）。 \n\t- 其次，词表中 $n$ 元组的数量并没有那么大，**这说明语言中存在相当多的结构**， 这些结构给了我们应用模型的希望。 (要是语言中没有太多规律, 则n元组会更随机, 种类也会更多)\n\n- 第三，很多n元组很少出现，这使得 [拉普拉斯平滑](notes/2022/2022.3/D2L-50-语言模型-传统模型的不足.md#拉普拉斯平滑) 非常不适合语言建模。 作为代替，我们将使用基于深度学习的模型。\n\n[^1]: [齐普夫定律 Zipf's law - 集智百科 - 复杂系统|人工智能|复杂科学|复杂网络|自组织](https://wiki.swarma.org/index.php/%E9%BD%90%E6%99%AE%E5%A4%AB%E5%AE%9A%E5%BE%8B_Zipf%27s_law)","lastmodified":"2022-10-15T14:06:29.610503745Z","tags":null},"/notes/2022/2022.3/D2L-52-%E8%AF%BB%E5%8F%96%E9%95%BF%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95":{"title":"D2L-52-读取长序列数据的两种方法","content":"# 读取长序列数据的两种方法\n\n\u003cdiv align=\"right\"\u003e 2022-03-08\u003c/div\u003e\n\nTags:  #SequentialData #DataPreprocessing\n\n- 尽管序列数据本质上是连续的, 我们在处理数据的时候也希望将其分为小批量, 方便模型读取。\n- 设我们将使用神经网络来训练语言模型， 模型中的网络一次处理具有预定义长度 （例如n个时间步）的一个小批量序列。\n![](notes/2022/2022.3/assets/timemachine-5gram.svg)\n- 我们应该从上图中选择哪一个序列呢？ 事实上，他们都一样好。 \n\t- 然而，如果我们只选择一个偏移量， 那么用于训练网络的、所有可能的子序列的覆盖范围将是有限的。 \n\t- 因此，我们可以从随机偏移量开始划分序列， 以同时获得覆盖性（coverage）和随机性（randomness）。 下面，我们将描述如何实现随机采样（random sampling）和 顺序分区（sequential partitioning）策略。\n\n## 随机采样\n下图中 `Batchsize=2`, 也就是说每个Batch里面有两个序列, 一个红色的, 一个蓝色的.\n\n![随机选取](notes/2022/2022.3/assets/随机选取.svg)\n- 每个样本都是在原始的长序列上任意捕获的子序列。 \n- 在迭代过程中，来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻。 \n\t- 注意, 即使采用了随机采样, 我们也同样覆盖了整个文本序列.\n- 对于语言建模，目标是基于到目前为止我们看到的词元来预测下一个词元， 因此标签是移位了一个词元的原始序列。\n\n\n## 顺序分区\n\n![顺序选取](notes/2022/2022.3/assets/顺序选取.svg)\n- 两个相邻的小批量中的子序列在原始序列上也是相邻的","lastmodified":"2022-10-15T14:06:29.610503745Z","tags":null},"/notes/2022/2022.3/Latex-Colored-Text":{"title":"Latex Colored Text","content":"![](notes/2022/2022.3/assets/img_2022-10-15-5.png)\n- 使用颜色名\n$$\\textcolor{red}{Sample}$$\n```latex\n\\textcolor{red}{Sample}\n```\n- 使用 `rgb`\n$$\\textcolor[rgb]{0.5,0.2,0.8}{text}$$\n```latex\n\\textcolor[rgb]{r,g,b}{text}\n```\n其中{r,g,b}代表red、green和blue三种颜色的组合，取值范围为[0-1]\n\n- 使用 `RGB`\n$$\\textcolor[RGB]{123,234,099}{text}$$\n```latex\n\\textcolor[RGB]{R,G,B}{text}\n```\n其中{R,G,B}代表red、green和blue三种颜色的组合，取值范围为[0-255]\n\n#### 比较好看的颜色: \n$\\textcolor{forestgreen}{forestgreen}$\n$\\textcolor{royalblue}{royalblue}$\n$\\textcolor{darkorchid}{darkorchid}$\n$\\textcolor{orangered}{orangered}$\n$\\textcolor{salmon}{salmon}$\n$\\textcolor{darkorange}{darkorange}$\n\n","lastmodified":"2022-10-15T14:06:29.610503745Z","tags":null},"/notes/2022/2022.3/cmd_powershell_bash-Comparison":{"title":"cmd_powershell_bash-Comparison","content":"# CMD / Powershell / bash: Comparison\n\n\u003cdiv align=\"right\"\u003e 2022-03-18\u003c/div\u003e\n\nTags: #Terminal #OperatingSystem \n\n![](notes/2022/2022.3/assets/img_2022-10-15.png)\n\n- Bash是Unix系统的, 而前两个是Windows的\n- Powershell是CMD的升级版\n\n\n","lastmodified":"2022-10-15T14:06:29.64250409Z","tags":null},"/notes/2022/2022.4/%E5%87%B8%E7%BB%84%E5%90%88-Convex-Combination":{"title":"凸组合 - Convex Combination","content":"# Convex Combination \n\n\u003cdiv align=\"right\"\u003e 2022-04-18\u003c/div\u003e\n\nTags: #NonlinearProgreamming #Math #ConvexCombination\n\n![](notes/2022/2022.4/assets/img_2022-10-15-1.png)\n\n- A *convex combination* of points $\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, \\ldots, \\mathbf{x}^{(k)} \\in \\mathbb{R}^{n}$ is a \"weighted average\": a linear combination\n$$\n\\lambda_{1} \\mathbf{x}^{(1)}+\\lambda_{2} \\mathbf{x}^{(2)}+\\cdots+\\lambda_{k} \\mathbf{x}^{(k)}\n$$\n\twhere $\\lambda_{1}+\\lambda_{2}+\\cdots+\\lambda_{k}=1$ and $\\lambda_{1}, \\ldots, \\lambda_{k} \\geq 0$\n\n- The *convex hull* $\\operatorname{conv}(S)$ of a set of points $S$ is sometimes defined as the set of all convex combinations of points from $S$.\n\n- In the plane, you can visualize $\\operatorname{conv}(S)$ as the interior of a rubber band stretched around points in $S$.\n\n![](notes/2022/2022.4/assets/img_2022-10-15-2.png)\n\nRef: Original File(@ConvexCombination)\n\n","lastmodified":"2022-10-15T14:06:29.770505469Z","tags":null},"/notes/2022/2022.4/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CFeedforward-neural-network":{"title":"前馈神经网络(Feedforward neural network)","content":"# 前馈神经网络: Feedforward Neural Network\n\n\u003cdiv align=\"right\"\u003e 2022-04-12\u003c/div\u003e\n\nTags: #FeedforwardNeuralNetwork\n\n- 多层前馈神经网络的结构如下图所示： \n\t- 每层神经元与下一层神经元全部互连 \n\t- 同层神经元之间不存在连接 \n\t- 跨层神经元之间也不存在连接\n\n![](notes/2022/2022.4/assets/img_2022-10-15.jpg)\n\n","lastmodified":"2022-10-15T14:06:29.770505469Z","tags":null},"/notes/2022/2022.4/%E6%B1%89%E5%AD%97%E6%8E%92%E6%A3%80%E6%B3%95":{"title":"汉字排检法","content":"# 汉字排检法\n\n\u003cdiv align=\"right\"\u003e 2022-04-02\u003c/div\u003e\n\nTags: #ChineseCharacters\n\n- 汉字作为象形文字, 在信息化的过程中面临着先天的困难. \n\n- 其实除了拼音, 汉字是有许多索引方式的, 这些方法统称汉字排检法. 比如过去的四角号码检字法.\n\t- 其实相比**基于语音**的拼音, 四角号码就像五笔一样, 你即使认不到这个字, 也可以输入, 这是**基于字形**的检索方法的优势.\n\n\n\n- [汉字排检法_百度百科](https://baike.baidu.com/item/%E6%B1%89%E5%AD%97%E6%8E%92%E6%A3%80%E6%B3%95/9979589)\n- [四角号码查字法_百度百科](https://baike.baidu.com/item/%E5%9B%9B%E8%A7%92%E5%8F%B7%E7%A0%81%E6%9F%A5%E5%AD%97%E6%B3%95/144844?fromtitle=%E5%9B%9B%E8%A7%92%E5%8F%B7%E7%A0%81%E6%A3%80%E5%AD%97%E6%B3%95\u0026fromid=2783996)\n\n\n","lastmodified":"2022-10-15T14:06:29.770505469Z","tags":null},"/notes/2022/2022.4/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E5%8D%8F%E8%AE%AE%E4%BF%A1%E9%81%93%E5%88%A9%E7%94%A8%E7%8E%87%E5%88%86%E6%9E%90":{"title":"滑动窗口协议信道利用率分析","content":"# 滑动窗口协议信道利用率分析 \n## Link Utilization of Sliding Window Protocols\n\u003cdiv align=\"right\"\u003e 1953184 傅驰原\u003c/div\u003e\n\u003cdiv align=\"right\"\u003e 2022-04-25\u003c/div\u003e\n\nTags: #ComputerNetwork #Course \n\n- 我们假设某一帧出错的概率相等且独立, 并且都为 $P$\n\n## 1Bit滑动窗口\n- 这其实是一种窗口大小为 $1$, 帧序号位数也为 $1$ 的特殊回退 $N$ 协议.\n- 假设发送一个数据帧花费的时间为 $t_{frame}$, 数据传输时延为 $t_{trans}$, 则信道利用率可以表示为: $$U_1=(1-P)\\frac{t_{frame}}{t_{frame}+2t_{trans}}$$\n\t- 如果令 $$\\alpha=\\frac{t_{trans}}{t_{frame}}$$ 则有: $$U_1=(1-P)\\frac{1}{1+2\\alpha}$$\n## 选择重传协议\n- 在之前的基础上, 进一步考虑发送 $W$ 个数据帧的时间, 就得到选择重传协议的信道利用率为:\n\t- $W\u003c1+2\\alpha$ 时:\n\t\t$$U_2=\\frac{W(1-P)}{1+2\\alpha}$$\n\t- $W\\geq1+2\\alpha$ 时\n\t$$U_2=1-P$$\n\n## 回退 $N$ 滑动窗口\n- 在之前的基础上, 再进一步考虑平均重传 $W\\times P$ 个损坏数据帧的时间, 就得到回退 $N$ 滑动窗口协议的信道利用率为:\n\t- $W\u003c1+2\\alpha$ 时:\n\t$$U_2=\\frac{W(1-P)}{(1+2\\alpha)(1-P+WP)}$$\n\t- $W\\geq1+2\\alpha$ 时\n\t$$U_2=\\frac{1-P}{1+2\\alpha P}$$\n## 性能分析\n- 作图分析, 可以形象的看出:\n\t- 窗口更大更有利于提高信道利用率\n\t- 在窗口更大的时候, 选择重传要更有优势(这时候回退 $N$ 步的代价太大了)\n\t![](notes/2022/2022.4/assets/img_2022-10-15.png)\n\n","lastmodified":"2022-10-15T14:06:29.770505469Z","tags":null},"/notes/2022/2022.4/%E9%80%92%E6%8E%A8%E5%85%AC%E5%BC%8F-a_tb_t+c_ta_t-1-%E8%BD%AC%E9%80%9A%E9%A1%B9%E5%85%AC%E5%BC%8F":{"title":"递推公式 $a_{t}=b_{t}+c_{t}a_{t-1}$ 转通项公式","content":"# 递推公式 $a_{t}=b_{t}+c_{t}a_{t-1}$ 转通项公式\n\n\u003cdiv align=\"right\"\u003e 2022-04-02\u003c/div\u003e\n\nTags: #Math \n\n\n$$\\begin{aligned}\na_{t}=b_{t} \u0026+c_{t} a_{t-1} \\\\\n\u0026+ c_{t}\\left(b_{t-1}+c_{t-1} a_{t-2}\\right) \\\\\n\u0026\\hspace{4.25em}+c_{t-1}\\left(b_{t-2}+c_{t-2} a_{t-3}\\right) \\\\\n\u0026\\hspace{13em}\\vdots \\\\\n\u0026\\hspace{12.5em}+c_{2}\\left(b_{1}+c_{1} a_{0}\\right) \\\\\n\u0026\\hspace{17.5em}\\uparrow\\\\\n\u0026\\hspace{17.8em}0\\\\\n\\end{aligned}$$\n右边:\n$$\\begin{aligned}\n\u0026\\textcolor{blue}{b_{t}}+\\textcolor{darkorange}{c_{t}}(\\textcolor{blue}{b_{t-1}}+\\cdots \\textcolor{darkorange}{c_{4}}(\\textcolor{blue}{b_{3}}+\\textcolor{darkorange}{c_{3}}(\\textcolor{blue}{b_{2}}+\\textcolor{darkorange}{c_{2}} \\textcolor{blue}{b_{1}})))\\\\\n=\u0026\\textcolor{darkorange}{c_{t}c_{t-1}\\cdots c_{4}c_{3}c_{2}}\\textcolor{blue}{b_{1}}+\\textcolor{darkorange}{c_{t}c_{t-1}\\cdots c_{4}c_{3}}\\textcolor{blue}{b_{2}}+\\textcolor{darkorange}{c_{t}c_{t-1}\\cdots c_{4}}\\textcolor{blue}{b_{3}}+\\cdots+\\textcolor{darkorange}{c_{t}}\\textcolor{blue}{b_{t-1}}+\\textcolor{blue}{b_{t}}\n\\\\=\u0026\\textcolor{blue}{b_{t}}+\\textcolor{blue}{\\sum_{i=1}^{t-1}}\\left(\\textcolor{darkorange}{\\prod_{j=i+1}^{t}c_{j}}\\right)\\textcolor{blue}{b_{i}}\n\\\\=\u0026b_{t}+\\sum_{i=1}^{t-1}\\left(\\prod_{j=i+1}^{t}c_{j}\\right)b_{i}\n\\end{aligned}$$\n故\n$$a_{t}=b_{t}+\\sum_{i=1}^{t-1}\\left(\\prod_{j=i+1}^{t}c_{j}\\right)b_{i}$$\n\n","lastmodified":"2022-10-15T14:06:29.770505469Z","tags":null},"/notes/2022/2022.4/D2L-53-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN":{"title":"D2L-53-循环神经网络RNN","content":"# Recurrent Neural Networks\n\n\u003cdiv align=\"right\"\u003e 2022-04-01\u003c/div\u003e\n\nTags: #RNN #DeepLearning #NeuralNetwork \n\n## Motivation\n- 基于马尔可夫假设的N元语法（*n-gram*）需要存储大量的参数。在 $n$ 逐渐增大的过程中，*n-gram*模型的参数大小 $|W|$ 与序列长度 $n$ 是指数关系：$$|W|=|\\mathcal{V}|^n $$ ($|\\mathcal{V}|$ 是单词的数目)\n- 因此, 我们将目光转向了 [隐变量自回归模型](notes/2022/2022.3/D2L-48-序列模型-Sequence_Models.md#隐变量自回归模型%20Latent%20Autoregressive%20Models). 隐状态 $h_{t-1}$ 能够(近似地)存储当前时间步之前所有输入的综合影响:\n\t$$P\\left(x_{t} \\mid x_{t-1}, \\ldots, x_{1}\\right) \\approx P\\left(x_{t} \\mid h_{t-1}\\right)$$\n\t- 进而, 我们可以使用当前的输入和上一个隐状态来生成当前的隐状态:\n\t$$h_{t}=f\\left(x_{t}, h_{t-1}\\right)$$\n\t只要函数 $f$ 足够强大, 就能将所有信息都综合到 $h_t$ 里面, 不丢失任何信息. 当然, 这样的开销也是很大的.\n\t\n![](notes/2022/2022.3/assets/img_2022-10-15-6.png)\n- _Recurrent neural networks_ (RNNs)就是带有隐状态的神经网络.\n\n## 从MLP到RNN\n- RNN和 [MLP](notes/2022/2022.2/D2L-17-MLP-多层感知机.md) 其实是非常相似的, 它们唯一的区别就是有没有隐状态 $h_{t}$\n- **MLP**\n\t$$\\begin{aligned}\n\t\\mathbf{H}\u0026=\\phi\\left(\\mathbf{X} \\mathbf{W}_{x h}+\\mathbf{b}_{h}\\right)\\\\\n\t\\mathbf{O}\u0026=\\mathbf{H} \\mathbf{W}_{h q}+\\mathbf{b}_{q}\\end{aligned}$$\n- **RNN**\n$$\\begin{aligned}\n\t\\mathbf{H}_t\u0026=\\phi\\left(\\textcolor[RGB]{255, 83, 61}{\\mathbf{H}_{t-1}\\mathbf{W}_{h h}}+\\mathbf{X} \\mathbf{W}_{x h}+\\mathbf{b}_{h}\\right)\\\\\n\t \\mathbf{O}\u0026=\\mathbf{H} \\mathbf{W}_{h q}+\\mathbf{b}_{q}\\end{aligned}$$\n- 注意在加上偏置的时候触发了PyTorch的广播机制(Broadcasting, see [Section 2.1.3](https://d2l.ai/chapter_preliminaries/ndarray.html#subsec-broadcasting)) \n- 下面是一些图例:\n![](notes/2022/2022.3/assets/img_2022-10-15-7.png)\n\n![](notes/2022/2022.3/assets/img_2022-10-15-8.png)\n\n- 值得注意的一点是, 因为我们在每一个时间步都使用的是同一个参数矩阵 $\\mathbf{W}_{hh}$, 所以RNN的参数大小并不会随着时间步的增长而增长.\n### Hidden States\n![Hidden States](notes/2022/2022.4/Hidden%20States.svg)\n\n### 为什么叫\"Recurrent\" Neural Network\n- 尽管每一次计算都需要利用上一次的 $\\mathbf H_{t-1}$, 但是使用的参数 $\\mathbf{W}_{hh}$ 都是一样的. 也就是说, 每一层的隐状态的计算方式是完全一样的, 这一次的输出就是下一次的输入, 是一个*递归*的关系, 这也是Recurrent的含义.\n\n\t- 递归计算隐状态的层自然就叫做\"**递归层**(*recurrent layer*)\" \n\n![](notes/2022/2022.4/assets/R5nRD.jpg)\n\n### $\\mathbf{H}_{t-1}\\mathbf{W}_{h h}+\\mathbf{X} \\mathbf{W}_{x h}$ 实际的计算方式\n- 公式里面是:\n![Matrix Mutiplication Trick A](notes/2022/2022.4/assets/Matrix%20Mutiplication%20Trick%20A.svg)\n\n- 实际上可以简化成:\n![Matrix Mutiplication Trick B](notes/2022/2022.4/assets/Matrix%20Mutiplication%20Trick%20B.svg)\n\n- RNN的计算可以表示为:\n![](notes/2022/2022.4/assets/rnn.svg)\n ^ba5f9a\n- 因为RNN的递归层存在环路, 所以RNN不属于[[notes/2022/2022.4/前馈神经网络(Feedforward neural network)]].\n\n## RNN的评估指标: 困惑度 Perplexity\n- Review:  [Cross_Entropy-交叉熵](notes/2022/2022.2/Cross_Entropy-交叉熵.md)\n\n- 虽然我们可以使用整个句子的概率 $P\\left(x_{1}, \\ldots, x_{T}\\right)$ 来评价模型的质量, 但由于短序列的概率很可能比长序列大许多, 我们很难直观的评价模型的好坏.\n\n- 一个自然的想法就是将整个句子的概率\"除以\"句子的长度, 用\"每个字符的概率\"作为评价指标. 利用信息论的知识, 我们可以结合交叉熵的概念: 一个好的预测模型应该输出和真实句子尽可能相似的结果. 我们可以用交叉熵来衡量模型的输出概率 $q(x_i)=P\\left(x_{i} \\mid x_{t-1}, \\ldots, x_{1}\\right)$ 和真实序列分布 $p(x_i)$ 的差距: \n\t$$\\begin{aligned}\n\tCE(p,q)\u0026=-\\sum_i^{|\\mathcal{V}|}p(x_i)\\log q(x_i)\\\\\n\t(\\text{独热编码})\u0026=-\\log q(x_t)\\\\\u0026=-\\log P\\left(x_{t} \\mid x_{t-1}, \\ldots, x_{1}\\right)\\end{aligned}$$\n\t- 其中 $|\\mathcal{V}|$ 是所有可能的 $x_i$ 个数, 因为真实序列是已知的, 相当于独热编码, 所以只剩下 $x_i=x_t$ 的一项, 其中 $x_t$ 是在时间步 $t$ 从该序列中观察到的实际词元\n- 将整个序列里面所有位置的交叉熵综合起来, 再取平均得到: $$-\\frac{1}{n} \\sum_{t=1}^{n} \\log P\\left(x_{t} \\mid x_{t-1}, \\ldots, x_{1}\\right)$$\n- 由于历史原因，自然语言处理的科学家更喜欢使用一个叫做_困惑度_（perplexity）的量。 简而言之，它是上式的指数：\n\t$$\\exp \\left(-\\frac{1}{n} \\sum_{t=1}^{n} \\log P\\left(x_{t} \\mid x_{t-1}, \\ldots, x_{1}\\right)\\right)$$\n\t\n\u003e 困惑度的最好的理解是“下一个词元的实际选择数的[调和平均数(*Harmonic* mean)](notes/2022/2022.5/Harmonic_Mean-调和平均数.md)”。 我们看看一些案例：\n\u003e - 在最好的情况下，模型总是完美地估计标签词元的概率为 $1$。 在这种情况下，模型的困惑度为 $1$。\n\u003e -   在最坏的情况下，模型总是预测标签词元的概率为 $0$。 在这种情况下，困惑度是正无穷大。\n\u003e -   在基线上，该模型的预测是词表的所有可用词元上的均匀分布。 在这种情况下，困惑度等于词表中唯一词元的数量 $|\\mathcal{V}|$。 事实上，如果我们在没有任何压缩的情况下存储序列， 这将是我们能做的最好的编码方式。 因此，这种方式提供了一个重要的上限， 而任何实际模型都必须超越这个上限。[^1]\n\n## RNN中output和hidden_state的区别\n![RNN中output和hidden_state的区别](notes/2022/2022.4/RNN中output和hidden_state的区别.md)\n\n\n[^1]: [8.4. 循环神经网络 — 动手学深度学习 2.0.0-beta0 documentation](https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/rnn.html)","lastmodified":"2022-10-15T14:06:29.64250409Z","tags":null},"/notes/2022/2022.4/D2L-54-Gradient-Clipping-%E6%A2%AF%E5%BA%A6%E5%89%AA%E8%A3%81":{"title":"D2L-54-Gradient Clipping-梯度剪裁","content":"# Gradient Clipping\n\n\u003cdiv align=\"right\"\u003e 2022-04-02\u003c/div\u003e\n\nTags: #GradientClipping\n\n- 梯度剪裁是预防梯度爆炸的一种方法, 它直接给梯度设置一个上限.\n\n$$\\mathbf{g} \\leftarrow \\min \\left(1, \\frac{\\theta}{\\|\\mathbf{g}\\|}\\right) \\mathbf{g}$$\n- 上面的写法有点绕, 因为为了保持梯度 $\\mathbf{g}$ 的方向不变, 剪裁时需要作用于 $\\mathbf{g}$ 的每一个分量, 整体上来说其实就是:\n$$\\mathbf{g} \\leftarrow \\min \\left(\\|\\mathbf{g}\\|, \\theta \\frac{\\mathbf{g}}{\\|\\mathbf{g}\\|}\\right)$$\n\n![400](notes/2022/2022.4/assets/img_2022-10-15-3.png)\n- 相比直接减小学习率，Clipping是分段的, 可以只在梯度较大时加以限制.\n\n- 抽象版的描述见：　[8.5. Implementation of Recurrent Neural Networks from Scratch — Dive into Deep Learning 0.17.5 documentation](https://d2l.ai/chapter_recurrent-neural-networks/rnn-scratch.html#gradient-clipping)\n\n## PyTorch\n[torch.nn.utils.clip_grad_norm_ — PyTorch 1.11.0 documentation](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)\n```python\nnn.utils.clip_grad_norm_(model.parameters(), max_norm=CLIP_GRAD)\n```\n\n## D2l 里面的简易实现\n```py\ndef grad_clipping(net, theta):  #@save\n    \"\"\"裁剪梯度\"\"\"\n    if isinstance(net, nn.Module):\n        params = [p for p in net.parameters() if p.requires_grad]\n    else:\n        params = net.params\n    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n    if norm \u003e theta:\n        for param in params:\n            param.grad[:] *= theta / norm #注意这里是*=\n```","lastmodified":"2022-10-15T14:06:29.64250409Z","tags":null},"/notes/2022/2022.4/D2L-55-%E5%9C%A8%E6%97%B6%E9%97%B4%E4%B8%8A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD":{"title":"D2L-55-在时间上反向传播","content":"# Backpropagation Through Time\n\n\u003cdiv align=\"right\"\u003e 2022-04-02\u003c/div\u003e\n\nTags: #Backpropagation #RNN \n\n- 和正向传播的时候一样, RNN在反向传播的时候需要在时间步上面进行迭代, 这可能导致梯度问题. \n- 下面我们先大概分析在\"时间上\"反向传播的不同之处, 然后简要介绍一些缓解梯度问题的训练方法, 最后, 我们详细的分析一下在时间上反向传播的细节问题.\n- 这篇笔记以 [8.7. Backpropagation Through Time — Dive into Deep Learning](https://d2l.ai/chapter_recurrent-neural-networks/bptt.html) 为基础. \n\n## 在时间上反向传播/RNN的反向传播\n### 模型\n - 我们可以把RNN的计算简化为两个函数: $f$ 更新隐状态, $o_t$ 根据隐状态生成输出:\n$$\\begin{aligned}h_t \u0026= f(x_t, h_{t-1}, w_h),\\\\o_t \u0026= g(h_t, w_o),\\end{aligned}$$\n- 其中:\n\t- $x_t$ 是 $t$ 时刻的输入, 注意下标是时间步而不是在序列里的位置\n\t- $h_t$ 是 $t$ 时刻的隐状态\n\t- $w_h$ 是隐藏层的权重\n\t- $w_o$ 是输出层的权重\n\n![简化版RNN](notes/2022/2022.4/assets/img_2022-10-15-4.png)\n### 损失函数\n- 损失函数里面, 我们将 $x_1, \\ldots, x_T$ 输入模型, 得到输出 $o_1, \\ldots, o_T$, 将 $y_1, \\ldots, y_T$ 与每一个输出进行比较, 再取平均. 其中 $w_h, w_o$ 是我们需要优化的目标.\n$$L(x_1, \\ldots, x_T, y_1, \\ldots, y_T, w_h, w_o) = \\frac{1}{T}\\sum_{t=1}^T l(y_t, o_t)$$\n\n### 求梯度: $\\frac{\\partial L}{\\partial w_o}$\n- 对于 $w_o$ 的梯度很好计算, 因为每一个输出仅依赖于当前时刻的隐状态 $h_t$, 不存在递归关系.\n$$\\begin{aligned}\\frac{\\partial L}{\\partial w_o}\u0026 = \\frac{1}{T}\\sum_{t=1}^T \\frac{\\partial l(y_t, o_t)}{\\partial w_o}\\\\\u0026= \\frac{1}{T}\\sum_{t=1}^T \\frac{\\partial l(y_t, o_t)}{\\partial o_t} \\frac{\\partial g(h_t, w_o)}{\\partial w_o}\\end{aligned}$$\n\n### 求梯度: $\\frac{\\partial L}{\\partial w_h}$\n- 在下面的式子里面, 前两个因子都很好计算, 而递归关系隐藏在 ${\\partial h_t}/{\\partial w_h}$ 里面.\n$$\\begin{aligned}\\frac{\\partial L}{\\partial w_h}\u0026 = \\frac{1}{T}\\sum_{t=1}^T \\frac{\\partial l(y_t, o_t)}{\\partial w_h}\\\\\u0026= \\frac{1}{T}\\sum_{t=1}^T \\frac{\\partial l(y_t, o_t)}{\\partial o_t} \\frac{\\partial g(h_t, w_o)}{\\partial h_t} \\frac{\\partial h_t}{\\partial w_h}\\end{aligned}$$\n- $h_t = f(x_t, h_{t-1}, w_h)$ 而 $h_{t-1}$ 同样依赖于 $w_h$, 所以根据求导法则有:\n$$\\frac{\\partial h_t}{\\partial w_h}= \\frac{\\partial f(x_{t},h_{t-1},w_h)}{\\partial w_h} +\\frac{\\partial f(x_{t},h_{t-1},w_h)}{\\partial h_{t-1}} \\frac{\\partial h_{t-1}}{\\partial w_h}$$\n用颜色标识出每部分的求导对象:\n$$\\textcolor{red}{\\frac{\\partial h_t}{\\partial w_h}}= \\frac{\\partial f(x_{t},h_{t-1},\\textcolor{blue}{w_h})}{\\textcolor{blue}{\\partial w_h}} +\\frac{\\partial f(x_{t},\\textcolor{green}{h_{t-1}},w_h)}{\\textcolor{green}{\\partial h_{t-1}}} \\textcolor{red}{\\frac{\\partial h_{t-1}}{\\partial w_h}}$$\n其中红色的部分构成了递归.\n\n- 对于递推公式 $a_{t}=b_{t}+c_{t}a_{t-1}$, $a_{0}=0$. 我们可以求出通项公式为[^1]:\n$$a_{t}=b_{t}+\\sum_{i=1}^{t-1}\\left(\\prod_{j=i+1}^{t}c_{j}\\right)b_{i}$$\n\n- 再进行替换:\n$$\\begin{aligned}a_t \u0026= \\textcolor{red}{\\frac{\\partial h_t}{\\partial w_h}},\\\\\nb_t \u0026= \\frac{\\partial f(x_{t},h_{t-1},\\textcolor{blue}{w_h})}{\\textcolor{blue}{\\partial w_h}}, \\\\\nc_t \u0026= \\frac{\\partial f(x_{t},\\textcolor{green}{h_{t-1}},w_h)}{\\textcolor{green}{\\partial h_{t-1}}},\\end{aligned}$$\n得到:\n$$\\textcolor{red}{\\frac{\\partial h_t}{\\partial w_h}}\n=\\frac{\\partial f(x_{t},h_{t-1},\\textcolor{blue}{w_h})}{\\textcolor{blue}{\\partial w_h}}+\n\\textcolor{blue}{\\sum_{i=1}^{t-1}}\n\\left(\\textcolor{green}{\\prod_{j=i+1}^{t}} \\frac{\\partial f(x_{j},\\textcolor{green}{h_{j-1}},w_h)}{\\textcolor{green}{\\partial h_{j-1}}} \\right) \n\\frac{\\partial f(x_{i},h_{i-1},\\textcolor{blue}{w_h})}{\\textcolor{blue}{\\partial w_h}}.$$\n- 在上面的式子里面, $b_t$ 可以理解为 $w_h$ 对于 $t$ 时刻的隐状态 $h_t$ 的影响强度, $c_t$ 可以理解为上一时刻隐状态 $h_{t-1}$ 对于当前 $h_t$ 的影响强度.\n\n## RNN缓解梯度问题的一些策略\n- Reveiw:\n\t- [D2L-24-数值稳定性](notes/2022/2022.2/D2L-24-数值稳定性.md)\n\t- [D2L-25-让训练更加稳定](notes/2022/2022.2/D2L-25-让训练更加稳定-Xavier初始化.md)\n\n- 可以看到在上一节的结论里, 绿色的部分和蓝色的部分都会导致梯度问题. 因而我们很少直接利用上式计算RNN的梯度.\n\n### Truncating Time Steps\n- 为了避免梯度爆炸, 我们可以只计算从当前时间步往前 $\\tau$ 个时间步的一小部分. 这会使梯度的传导距离变短, 让我们只关注当前时间步附近的一段序列. 实践表明这种方法还有一定 [正则化](notes/2022/2022.2/Regularization-正则化.md) 的作用在里面, 它倾向于让模型变得更简单稳定.\n- 通常我们可以在一定时间步后detach掉一些部分, 就像[下面这段代码](https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/rnn-scratch.html#id7)里面一样:\n```python\ndef train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):\n\n...\n\n\tfor X, Y in train_iter:\n        if state is None or use_random_iter:\n            # 在第一次迭代或使用随机抽样时初始化state\n            state = net.begin_state(batch_size=X.shape[0], device=device)\n        else:\n            if isinstance(net, nn.Module) and not isinstance(state, tuple):\n                # state对于nn.GRU是个张量\n                state.detach_()\n            else:\n            # state对于nn.LSTM或对于我们从零开始实现的模型是个张量\n                for s in state:\n                    s.detach_()\n \n ...\n\n    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()\n```\n\n### Randomized Truncation\n- 我们可以人为地设定 $\\tau$, 自然也可以让 $\\tau$ 随机变化梯度传播距离. \n- 定义随机变量 $\\xi_t$ , 有 $P(\\xi_t = 0) = 1-\\pi_t$ 和 $P(\\xi_t = \\pi_t^{-1}) = \\pi_t$, 其中 $\\pi_t$ 是人为设定的参数且 $0 \\leq \\pi_t \\leq 1$. 上面的规定是为了保证 $E[\\xi_t] = 1$, 进一步保证数值稳定性[^2]\n\t- 从而有:\n\t$$z_t= \\frac{\\partial f(x_{t},h_{t-1},w_h)}{\\partial w_h} +\\xi_t \\frac{\\partial f(x_{t},h_{t-1},w_h)}{\\partial h_{t-1}} \\frac{\\partial h_{t-1}}{\\partial w_h}$$\n\t- 在$\\xi_t=0$时梯度停止传播\n\n- 在实际过程中, 随机化截断的效果并没有定长截断的效果好.[^3]\n\n### 截断方式可视化\n![](notes/2022/2022.4/assets/truncated-bptt.svg)\n不同的截断方式代表了梯度不同的传播距离, 上面的图表示了每一个位置的隐状态可能的影响范围. \n\n## RNN梯度传播的细节问题\n- 下面我们把 $h_t=f(x_t, h_{t-1}, w_h)$ 和 $o_t= g(h_t, w_o)$ 展开, 讨论RNN梯度传播的实际情况.\n- 将 $f,g$ 展开后得到(我们先忽略激活函数和偏置):\n$$\\begin{aligned}\\mathbf{h}_t \u0026= \\mathbf{W}_{hx} \\mathbf{x}_t + \\mathbf{W}_{hh} \\mathbf{h}_{t-1},\\\\\\mathbf{o}_t \u0026= \\mathbf{W}_{qh} \\mathbf{h}_{t},\\end{aligned}$$\n- 根据上式可以画出三个时间步内的计算图:\n\n![](notes/2022/2022.4/assets/rnn-bptt.svg)\n\n其中圆圈代表运算, 方框代表变量或参数\n\n- 在反向传播时, 我们需要计算损失函数关于参数 $\\mathbf{W}_{hx}$, $\\mathbf{W}_{hh}$, 和 $\\mathbf{W}_{qh}$ 的导数, 即: $\\partial L/\\partial \\mathbf{W}_{hx}$, $\\partial L/\\partial \\mathbf{W}_{hh}$, 和 $\\partial L/\\partial \\mathbf{W}_{qh}$. 计算图里面逆箭头指向参数的路径也就是反向传播的路径.\n- 为了简化细节, 我们使用 $\\text{prod}$ 运算符来代表任意张量,向量或者标量之间的\"乘\"运算.[^4]\n\n### Step 1: $\\frac{\\partial L}{\\partial \\mathbf{o}_t}$\n$$L = \\frac{1}{T} \\sum_{t=1}^T l(\\mathbf{o}_t, y_t).$$\n![](notes/2022/2022.4/assets/Pasted%20image%2020220403114646.png)\n$$\\frac{\\partial L}{\\partial \\mathbf{o}_t} =\\frac{1}{T}\\frac{\\partial l (\\mathbf{o}_t, y_t)}{\\partial \\mathbf{o}_t} \\in \\mathbb{R}^q$$\n\n### Step 2: $\\frac{\\partial L}{\\partial \\mathbf{W}_{qh}}$\n根据计算图, 损失函数对 $\\mathbf{W}_{qh}$ 的梯度依赖于 $\\mathbf{o}_1, \\ldots, \\mathbf{o}_T$, 利用链式法则有:\n$$\\frac{\\partial L}{\\partial \\mathbf{W}_{qh}}\n= \\sum_{t=1}^T \\text{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{o}_t}, \\frac{\\partial \\mathbf{o}_t}{\\partial \\mathbf{W}_{qh}}\\right)\n= \\sum_{t=1}^T \\frac{\\partial L}{\\partial \\mathbf{o}_t} \\mathbf{h}_t^\\top$$\n![](notes/2022/2022.4/assets/Pasted%20image%2020220403133206.png)\n### Step 3: $\\frac{\\partial L}{\\partial \\mathbf{h}_t}$\n我们先来看看对于最后一个时间步 $T$ 来说, 梯度 $\\frac{\\partial L}{\\partial \\mathbf{h}_T}$ 的计算:\n$$\\frac{\\partial L}{\\partial \\mathbf{h}_T} = \\text{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{o}_T}, \\frac{\\partial \\mathbf{o}_T}{\\partial \\mathbf{h}_T} \\right) = \\mathbf{W}_{qh}^\\top \\frac{\\partial L}{\\partial \\mathbf{o}_T}$$\n![](notes/2022/2022.4/assets/Pasted%20image%2020220403133243.png)\n在 $t\u003cT$ 的时候计算变得复杂起来, 因为 $h_t$ 的梯度同时依赖于 $o_t$ 和 $h_{t+1}$\n![](notes/2022/2022.4/assets/Pasted%20image%2020220403134004.png)\n根据链式法则有:\n$$\\frac{\\partial L}{\\partial \\mathbf{h}_t} = \\text{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{h}_{t+1}}, \\frac{\\partial \\mathbf{h}_{t+1}}{\\partial \\mathbf{h}_t} \\right) + \\text{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{o}_t}, \\frac{\\partial \\mathbf{o}_t}{\\partial \\mathbf{h}_t} \\right) = \\mathbf{W}_{hh}^\\top \\frac{\\partial L}{\\partial \\mathbf{h}_{t+1}} + \\mathbf{W}_{qh}^\\top \\frac{\\partial L}{\\partial \\mathbf{o}_t}$$\n转化为通项公式:\n$$\\frac{\\partial L}{\\partial \\mathbf{h}_t}= \\sum_{i=t}^T {\\left(\\mathbf{W}_{hh}^\\top\\right)}^{T-i} \\mathbf{W}_{qh}^\\top \\frac{\\partial L}{\\partial \\mathbf{o}_{T+t-i}}.$$\n即使我们省略了激活函数, 从中我们已经能够看到一些问题:  表达式里面 $\\mathbf{W}_{hh}^\\top$ 的指数部分可能会很大, 在 $\\mathbf{W}_{hh}^\\top$ 里面特征值大于 $1$ 的部分会梯度爆炸, 而特征值小于 $1$ 的部分会梯度消失.\n在多次矩阵连乘以后, 一个向量会越来越靠近特征值最大的特征向量的方向.\n![EigenvalueMatrixPower](notes/2022/2022.4/assets/EigenvalueMatrixPower.gif)[^5]\n### Step 4: $\\partial L / \\partial \\mathbf{W}_{hx}$ and $\\partial L / \\partial \\mathbf{W}_{hh}$,\n最后我们基于$\\frac{\\partial L}{\\partial \\mathbf{h}_t}$计算隐藏层参数的梯度: $\\partial L / \\partial \\mathbf{W}_{hx} \\in \\mathbb{R}^{h \\times d}$ 和 $\\partial L / \\partial \\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$,\n$$\n\\begin{aligned}\n\\frac{\\partial L}{\\partial \\mathbf{W}_{hx}}\n\u0026= \\sum_{t=1}^T \\text{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{h}_t}, \\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{W}_{hx}}\\right)\n= \\sum_{t=1}^T \\frac{\\partial L}{\\partial \\mathbf{h}_t} \\mathbf{x}_t^\\top,\\\\\n\\frac{\\partial L}{\\partial \\mathbf{W}_{hh}}\n\u0026= \\sum_{t=1}^T \\text{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{h}_t}, \\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{W}_{hh}}\\right)\n= \\sum_{t=1}^T \\frac{\\partial L}{\\partial \\mathbf{h}_t} \\mathbf{h}_{t-1}^\\top,\n\\end{aligned}\n$$\n- 其中公共的部分 $\\frac{\\partial L}{\\partial \\mathbf{h}_t}$ 可以存储起来, 避免重复计算.\n\t![](notes/2022/2022.4/assets/Pasted%20image%2020220403135844.png)\n- 影响梯度稳定性的部分主要是Step3里面的 $\\frac{\\partial L}{\\partial \\mathbf{h}_t}$\n\n\n\n\n[^1]: [[notes/2022/2022.4/递推公式 $a_{t}=b_{t}+c_{t}a_{t-1}$ 转通项公式]]\n[^2]: [梯度归一化](notes/2022/2022.2/D2L-25-让训练更加稳定-Xavier初始化.md#梯度归一化)\n[^3]: [8.7. Backpropagation Through Time — Dive into Deep Learning 0.17.5 documentation](https://d2l.ai/chapter_recurrent-neural-networks/bptt.html#comparing-strategies)\n[^4]: 具体看看这一节: [D2L-5-拓展链式法则](notes/2022/2022.1/D2L-5-拓展链式法则.md) 利用抽象的符号可以省略掉很多繁杂的细节\n[^5]: [如何理解矩阵特征值？ - 知乎](https://www.zhihu.com/question/21874816/answer/181864044)","lastmodified":"2022-10-15T14:06:29.64250409Z","tags":null},"/notes/2022/2022.4/D2L-56-%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83GRU":{"title":"D2L-56-门控循环单元GRU","content":"# Gated Recurrent Units (GRU)\n\n\u003cdiv align=\"right\"\u003e 2022-04-03\u003c/div\u003e\n\nTags: #GRU #RNN #DeepLearning \n\n- GRU在RNN的基础上添加了\"门\"(*Gate*), 针对性地解决了RNN里面存在的以下问题:\n\t- **长期依赖问题**: 序列早期的部分可能对未来所有观测值都有非常重要的影响, 我们需要能够保留序列早期信息的网络结构. \n\t\t- GRU里面体现在: **重置门**减少重置, **更新门**更多地保留上一个隐状态\n\t- 序列里面可能有**干扰信息**, 我们需要能够跳过(遗忘)这些信息的机制\n\t\t- GRU里面体现在: **更新门**更多地保留上一个隐状态\n\t- 序列里面可能有**逻辑中断**, 比如一本书里面章节的变化往往会导致主题的变化. 我们需要有重置网络状态的机制.\n\t\t- GRU里面体现在: **重置门**屏蔽掉上一个隐状态\n\n\n![GRU](notes/2022/2022.4/assets/GRU.svg)\n## Gated Hidden State\n- 门控循环单元与普通的循环神经网络之间的关键区别在于： 后者用Gate对Hidden state进行了进一步的控制。\n\t- 这意味着模型有专门的机制来确定应该何时更新隐状态， 以及应该何时重置隐状态。 \n\t- 这些机制是可学习的，并且能够解决了上面列出的问题。 \n\t\t- 例如，如果第一个词元非常重要， 模型将学会在第一次观测之后不更新隐状态。 同样，模型也可以学会跳过不相关的临时观测。 最后，模型还将学会在需要的时候重置隐状态。 \n\n- 下面我们详细讨论两个门控:\n\n### Reset Gate and Update Gate\n![](notes/2022/2022.4/assets/gru-1.svg)\n这两个门的公式都是一样的, 使用input $\\mathbf{X}_{t}$ 和前一个隐状态 $\\mathbf{H}_{t-1}$ 作为输入, 利用Sigmoid映射到 $(0,1)$ 区间:\n$$\\begin{aligned}\n\u0026\\mathbf{R}_{t}=\\sigma\\left(\\mathbf{X}_{t} \\mathbf{W}_{x r}+\\mathbf{H}_{t-1} \\mathbf{W}_{h r}+\\mathbf{b}_{r}\\right) \\\\\n\u0026\\mathbf{Z}_{t}=\\sigma\\left(\\mathbf{X}_{t} \\mathbf{W}_{x z}+\\mathbf{H}_{t-1} \\mathbf{W}_{h z}+\\mathbf{b}_{z}\\right)\n\\end{aligned}$$\n不同的是使用它们的方式: \n- 重置门 $\\mathbf{R}_{t}$ 将和上一个隐状态 $\\mathbf{H}_{t-1}$ 按元素相乘 $\\odot$, 相当于一个Soft的Mask, 可以控制有多少 $\\mathbf{H}_{t-1}$ 用于生成候选隐状态 $\\tilde{\\mathbf{H}}_{t}$, 也就是控制有多少 $\\mathbf{H}_{t-1}$ 被\"重置\"掉了(*Reset*).\n\t- $$\\tilde{\\mathbf{H}}_{t}=\\tanh \\left(\\mathbf{X}_{t} \\mathbf{W}_{x h}+\\left(\\mathbf{R}_{t} \\odot \\mathbf{H}_{t-1}\\right) \\mathbf{W}_{h h}+\\mathbf{b}_{h}\\right)$$\n- 更新门$\\mathbf{Z}_{t}$则被用于混合候选隐状态 $\\tilde{\\mathbf{H}}_{t}$和上一个隐状态 $\\mathbf{H}_{t-1}$ :\n$$\\mathbf{H}_{t}=\\mathbf{Z}_{t} \\odot \\mathbf{H}_{t-1}+\\left(1-\\mathbf{Z}_{t}\\right) \\odot \\tilde{\\mathbf{H}}_{t}$$\n\t- 注意最后的结果 $\\mathbf{H}_{t}$ 是候选隐状态和上一个隐状态的 [凸组合 - Convex Combination](notes/2022/2022.4/凸组合%20-%20Convex%20Combination.md), 这也是为什么 $\\mathbf{Z}_{t}$ 要用Sigmoid来把范围控制到 $(0,1)$ 的原因\n\n### 候选隐状态\n顾名思义, 就是用来生成结果的隐状态, 使用重置门 $\\mathbf{R}_{t}$, input $\\mathbf{X}_{t}$ 和前一个隐状态 $\\mathbf{H}_{t-1}$ 作为输入, 使用tanh激活函数将结果映射到 $(-1,1)$ 区间:\n$$\\tilde{\\mathbf{H}}_{t}=\\tanh \\left(\\mathbf{X}_{t} \\mathbf{W}_{x h}+\\left(\\mathbf{R}_{t} \\odot \\mathbf{H}_{t-1}\\right) \\mathbf{W}_{h h}+\\mathbf{b}_{h}\\right)$$\n![](notes/2022/2022.4/assets/gru-2.svg)\n\n### 输出隐状态\n![](notes/2022/2022.4/assets/gru-3.svg)\n\n总之，门控循环单元具有以下两个显著特征：\n\n-   **重置门** $\\mathbf{R}_{t}$ 有助于捕获序列中的**短期**依赖关系。\n-   **更新门** $\\mathbf{Z}_{t}$ 有助于捕获序列中的**长期**依赖关系。\n\n","lastmodified":"2022-10-15T14:06:29.64250409Z","tags":null},"/notes/2022/2022.4/D2L-57-LSTM-%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C":{"title":"D2L-57-LSTM-长短期记忆网络","content":"# Long Short-Term Memory\n\n\u003cdiv align=\"right\"\u003e 2022-04-18\u003c/div\u003e\n\nTags: #LSTM #DeepLearning #RNN \n\n- LSTM是最早用于解决长期依赖问题的一种RNN. 它比GRU复杂, 但是设计思想是一样的. 有趣的是, LSTM(1997)比GRU(2014)早出现近20年.\n\n- LSTM和GRU一样, 使用了不同的门(Gate)来控制上一个隐状态在下一个隐状态里面的占比, 也就是有选择地来混合\"长期记忆\"和\"短期记忆\", 这也是其名称的由来.\n\n- 相比GRU(一种隐状态, 2个门), LSTM有两种隐状态: Cell State $\\mathbf{C}_{t}$ 和Hidden State $\\mathbf{H}_{t}$, 并且LSTM多一个门(3个).\n\n## Cell State\n- 相比传统的RNN, LSTM解决长期依赖的关键部分是新加入的Cell State:\n![](notes/2022/2022.4/assets/img_2022-10-15-5.png)\n- Cell State类似于一个传送带(Conveyor Belt), 信息在其中能够顺畅地流动. 在穿过每一个Cell的时候, 信息只有微小的改变, 这使得网络拥有了长期记忆的能力.[^1]\n\n- 不过注意只有隐状态才会传递到输出层， 而记忆元 $\\mathbf{C}_{t}$ 不直接参与输出计算。\n## 3 Gates \u0026 1 Candidate State\n### 3 Gates\n![](notes/2022/2022.4/assets/lstm-0.svg)\n- 和GRU一样, 前一个隐状态 $\\mathbf{H}_{t-1}$ 和输入 $\\mathbf{X}_{t}$ Concatenate起来, 一起作为三个门的输入, 激活函数为Sigmoid:\n\t$$\\begin{aligned}\n\\mathbf{I}_{t} \u0026=\\sigma\\left(\\mathbf{X}_{t} \\mathbf{W}_{x i}+\\mathbf{H}_{t-1} \\mathbf{W}_{h i}+\\mathbf{b}_{i}\\right) \\\\\n\\mathbf{F}_{t} \u0026=\\sigma\\left(\\mathbf{X}_{t} \\mathbf{W}_{x f}+\\mathbf{H}_{t-1} \\mathbf{W}_{h f}+\\mathbf{b}_{f}\\right) \\\\\n\\mathbf{O}_{t} \u0026=\\sigma\\left(\\mathbf{X}_{t} \\mathbf{W}_{x o}+\\mathbf{H}_{t-1} \\mathbf{W}_{h o}+\\mathbf{b}_{o}\\right)\n\\end{aligned}$$\n### 1 Candidate State\n- 和GRU不一样的是: Candidate Cell State直接接受前一个隐状态 $\\mathbf{H}_{t-1}$ 和输入 $\\mathbf{X}_{t}$ 作为输入, 而不是先进行\"Reset/遗忘\"操作:\n![](notes/2022/2022.4/assets/lstm-1.svg)\n$$\\tilde{\\mathbf{C}}_{t}=\\tanh \\left(\\mathbf{X}_{t} \\mathbf{W}_{x c}+\\mathbf{H}_{t-1} \\mathbf{W}_{h c}+\\mathbf{b}_{c}\\right)$$\n### Update Cell State\n![](notes/2022/2022.4/assets/Pasted%20image%2020220418183027.png)\n对上一个Cell State $\\mathbf{C}_{t-1}$ 的更新分为两步:\n\n#### Forget using Forget Gate\n$$\\mathbf{C}_{t}=\\textcolor{darkorchid}{\\mathbf{F}_{t} \\odot \\mathbf{C}_{t-1}}+\\mathbf{I}_{t} \\odot \\tilde{\\mathbf{C}}_{t}$$\n与Forget Gate按元素相乘，屏蔽掉需要忘记的元素。\n\n#### Merge new Candidate State\n$$\\mathbf{C}_{t}=\\mathbf{F}_{t} \\odot \\mathbf{C}_{t-1}\\textcolor{orangered}{+\\mathbf{I}_{t} \\odot \\tilde{\\mathbf{C}}_{t}}$$\n先将候选Cell State与Input Gate按元素相乘得到需要更新的位置，再和遗忘后的结果相加。\n\n### Output new Hidden State\n其实 $\\mathbf{H}_{t}$ 只是 $\\mathbf{C}_{t}$ 的门控版本: 先利用tanh调整大小范围到 $(-1,1)$, 再使用Output Gate Mask一遍:\n$$\\mathbf{H}_{t}=\\mathbf{O}_{t} \\odot \\tanh \\left(\\mathbf{C}_{t}\\right)$$\n![](notes/2022/2022.4/assets/lstm-3.svg)\n\n## Variants of LSTM\n### Peepholes\nAll gates can have a peep at the cell state $\\mathbf{C}_{t-1}$.\n![](notes/2022/2022.4/assets/LSTM3-var-peepholes.png)\n\n### Convex Combination[^2] (coupled forget and input gates) \nForget to remember, remember to forget. The total amount of information stays the same.\n![](notes/2022/2022.4/assets/LSTM3-var-tied.png)\n### GRU\n[D2L-56-门控循环单元GRU](notes/2022/2022.4/D2L-56-门控循环单元GRU.md)\n\n\n\n\n[^1]: [Understanding LSTM Networks -- colah's blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) Many pics in this article is from colah's blog.\n[^2]: [凸组合 - Convex Combination](notes/2022/2022.4/凸组合%20-%20Convex%20Combination.md)","lastmodified":"2022-10-15T14:06:29.64250409Z","tags":null},"/notes/2022/2022.4/D2L-58-%E6%B7%B1%E5%BA%A6%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C":{"title":"D2L-58-深度循环神经网络","content":"# Deep Recurrent Neural Networks\n\n\u003cdiv align=\"right\"\u003e 2022-04-18\u003c/div\u003e\n\nTags: #RNN #DeepLearning \n\n![](notes/2022/2022.4/assets/deep-rnn.svg)\n\n- 和MLP与CNN中一样, 我们可以通过添加更多的层来增强网络的表达能力. 但不同的是, 增加的每一层都需要在时间步上展开, 就像上图一样.\n![](notes/2022/2022.4/assets/Pasted%20image%2020220418224642.png)\n- 具体的来说, 除了边缘部分外, 每一个隐状态 $H^{(l)}_t$ 同时接受上一层同一时间步的 $\\textcolor{red}{H^{(l-1)}_t}$ 和同一层上一时间步的 $\\textcolor{red}{H^{(l)}_{t-1}}$ 作为输入, 并且输出到下一层同一时间步的 $\\textcolor{royalblue}{H^{(l+1)}_t}$ 和同一层下一时间步的 $\\textcolor{royalblue}{H^{(l)}_{t+1}}$\n- 用GRU或LSTM的隐状态代替上图中的隐状态，便得到深度GRU或深度LSTM。\n\n## 形式化定义\n- 假设在时间步 $t$ 有一个小批量的输入数据 $\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$（样本数：$n$，每个样本中的输入数：$d$）。\n- 同时，将 $l^\\mathrm{th}$ 隐藏层（$l=1,\\ldots,L$）的隐状态设为 $\\mathbf{H}_t^{(l)}  \\in \\mathbb{R}^{n \\times h}$（隐藏单元数：$h$），输出层变量设为 $\\mathbf{O}_t \\in \\mathbb{R}^{n \\times q}$（输出数：$q$）。\n- 设置 $\\mathbf{H}_t^{(0)} = \\mathbf{X}_t$，第 $l$ 个隐藏层使用激活函数 $\\phi_l$，则：\n\t$$\\mathbf{H}_t^{(l)} = \\phi_l(\\mathbf{H}_t^{(l-1)} \\mathbf{W}_{xh}^{(l)} + \\mathbf{H}_{t-1}^{(l)} \\mathbf{W}_{hh}^{(l)}  + \\mathbf{b}_h^{(l)})$$\n\t- 其中，权重 $\\mathbf{W}_{xh}^{(l)} \\in \\mathbb{R}^{h \\times h}$，$\\mathbf{W}_{hh}^{(l)} \\in \\mathbb{R}^{h \\times h}$ 和偏置 $\\mathbf{b}_h^{(l)} \\in \\mathbb{R}^{1 \\times h}$ 都是第 $l$ 个隐藏层的参数。\n\n- 输出层的计算仅基于最终第 $l$ 个隐藏层的隐状态：\n$$\\mathbf{O}_t = \\mathbf{H}_t^{(L)} \\mathbf{W}_{hq} + \\mathbf{b}_q$$\n\n## 实现\n在框架里面可以很容易的实现多层RNN, 比如多层LSTM在 `PyTorch` 里面可以定义为: \n```python\nnn.LSTM(num_inputs, num_hiddens, num_layers)\n```\n","lastmodified":"2022-10-15T14:06:29.64250409Z","tags":null},"/notes/2022/2022.4/D2L-59-%E5%8F%8C%E5%90%91%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C":{"title":"D2L-59-双向循环神经网络","content":"# Bidirectional Recurrent Neural Networks\n\n\u003cdiv align=\"right\"\u003e 2022-04-18\u003c/div\u003e\n\nTags: #RNN #DeepLearning #BidirectionalRNN\n\n\n![](notes/2022/2022.4/assets/birnn%202.svg)\n\n- 双向神经网络增加了反向扫描的隐藏层, 使网络拥有了\"前瞻能力\"\n- 正向层和反向层的输入是相同的, 是并行进行的, 最后正向和反向的结果一起生成输出.\n- 在D2L教程里面将正向反向扫描的过程和隐马尔科夫模型动态规划的正向与反向传递[^1]进行了类比: \n\u003e \t这种转变集中体现了现代深度网络的设计原则： 首先使用经典统计模型的函数依赖类型，然后将其参数化为通用形式。\n\n## 形式化定义\n- 对于任意时间步 $t$，对于一个批量的输入 $\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$（$n$ 为样本数，$d$ 为每个示例中的输入数），并令隐藏层激活函数为 $\\phi$。\n\t- 在双向架构中，我们设该时间步的前向和反向隐状态分别为 $\\overrightarrow{\\mathbf{H}}_t  \\in \\mathbb{R}^{n \\times h}$ 和 $\\overleftarrow{\\mathbf{H}}_t  \\in \\mathbb{R}^{n \\times h}$，其中 $h$ 是隐藏单元的数目。\n\t- 则Bidirectional RNN的更新规则可以定义如下：\n\t$$\n\t\\begin{aligned}\n\t\\overrightarrow{\\mathbf{H}}_t \u0026= \\phi(\\mathbf{X}_t \\mathbf{W}_{xh}^{(f)} + \\overrightarrow{\\mathbf{H}}_{t-1} \\mathbf{W}_{hh}^{(f)}  + \\mathbf{b}_h^{(f)}),\\\\\n\t\\overleftarrow{\\mathbf{H}}_t \u0026= \\phi(\\mathbf{X}_t \\mathbf{W}_{xh}^{(b)} + \\overleftarrow{\\mathbf{H}}_{t+1} \\mathbf{W}_{hh}^{(b)}  + \\mathbf{b}_h^{(b)}),\n\t\\end{aligned}\n\t$$\n- 将前向隐状态 $\\overrightarrow{\\mathbf{H}}_t$ 和反向隐状态 $\\overleftarrow{\\mathbf{H}}_t$ 拼接起来，我们得到输出层的输入 $\\mathbf{H}_t \\in \\mathbb{R}^{n \\times 2h}$。\n\t- 输出 $\\mathbf{O}_t \\in \\mathbb{R}^{n \\times q}$ 的计算公式如下:（$q$ 是输出单元的数目）\n\n$$\\mathbf{O}_t = \\mathbf{H}_t \\mathbf{W}_{hq} + \\mathbf{b}_q.$$\n\n- 值得注意的是, 两个方向其实可以拥有不同数量的隐藏单元\n\n## 双向模型的局限性\n- 因为双向模型需要同时使用来自序列两端的信息来估计输出, 所以它**完全不能用于单向的预测**(即给一个句子开头, 让模型进行续写)\n\t- [一个失败的例子](https://zh-v2.d2l.ai/chapter_recurrent-modern/bi-rnn.html#id10)\n\t![](notes/2022/2022.4/assets/Pasted%20image%2020220419003512.png)\n\n- 另一个严重问题是，双向循环神经网络的**计算速度非常慢**。 \n\t- 其主要原因是网络的前向传播需要在双向层中进行前向和后向递归， 并且网络的反向传播还依赖于前向传播的结果。 因此，梯度求解将有一个非常长的链。\n\n## 应用\n- 双向层的使用在实践中非常少，并且仅仅应用于部分场合。 \n\t- 例如，filling in missing words, annotating tokens (e.g., for named entity recognition), and encoding sequences wholesale as a step in a sequence processing pipeline (e.g., for machine translation)\n\n\n[^1]: [之前的笔记 - 序列模型-Sequence_Models](notes/2022/2022.3/D2L-48-序列模型-Sequence_Models.md#^f8bb86)","lastmodified":"2022-10-15T14:06:29.64250409Z","tags":null},"/notes/2022/2022.4/D2L-60-Encoder-Decoder":{"title":"D2L-60-Encoder-Decoder","content":"# 编码器-解码器架构\n\n\u003cdiv align=\"right\"\u003e 2022-04-19\u003c/div\u003e\n\nTags: #EncoderDecoder #RNN #DeepLearning \n\n![](notes/2022/2022.4/assets/encoder-decoder.svg)\n\n- Encoder-Decoder将模型分为两部分, 使得我们可以先用编码器处理不规则的输入, 然后再将输出送入Decoder得到最终结果.\n- Encoder-Decoder是一种抽象的模型架构, 可以有许多不同的实现方式.\n\n- 有的时候Decoder也需要Input, 所以上图也可以表示成下面的样子:\n\n![](notes/2022/2022.4/assets/encoder-decoder-cn.svg)\nIntuition:\n![](notes/2022/2022.4/assets/Encode-decoder-Intuition.svg)\n\n- Encoder-Decoder模型适用于机器翻译等序列转换问题。\n\n\n","lastmodified":"2022-10-15T14:06:29.64250409Z","tags":null},"/notes/2022/2022.4/D2L-61-Sequence-to-Sequence-Learning-Seq2Seq":{"title":"D2L-61-Sequence to Sequence Learning - Seq2Seq","content":"# Seq2Seq: 序列到序列模型\n\n\u003cdiv align=\"right\"\u003e 2022-04-19\u003c/div\u003e\n\nTags: #Seq2Seq #EncoderDecoder #DeepLearning #RNN \n\n- Seq2Seq也就是Sequence to Sequence, 顾名思义, 它实现的是一种序列到另一种序列的转换(比如从英语到中文).\n- Seq2Seq符合[Encoder-Decoder](notes/2022/2022.4/D2L-60-Encoder-Decoder.md)架构\n\n![](notes/2022/2022.4/assets/seq2seq.svg)\n## 总览\n- 如上图所示, 首先Encoder输入长度可变的序列， 并将其转换为固定形状的隐状态。然后隐状态输入Decoder, 解码器根据隐状态和输入来生成最后的输出.\n![](notes/2022/2022.4/assets/seq2seq-details.svg)\n- 我们如何将Encoder的结果输入到Decoder呢? 其实有两个方式:\n\t1. 我们可以利用Encoder最后的隐状态来初始化Decoder的隐状态.\n\t\t- 这要求Encoder和Decoder的隐藏层大小是一样的.\n\t\n\t2. 我们可以把Encoder最后的隐状态作为Decoder输入的一部分. \n\t\t- 也就是说, Decoder每一次的输入既包括前一个词, 又包括Encoder的隐状态.\n\t\n\t- 这两种方式可以同时使用.\n\n### Encoder\n#### 上下文变量: Encoder的输出\n- Encoder不仅将长度不定的序列转换为固定长度的上下文变量 $\\mathbf c$, 也将序列所含语义特征也提取到了 $\\mathbf c$ 里面.\n\n- 形式化地说, \n\t- 隐状态同时取决于输入和上一个隐状态 \n\t$$\\mathbf{h}_t = f(\\mathbf{x}_t, \\mathbf{h}_{t-1}) $$\n\t- 而上下文变量则是所有隐状态的综合:\n\t$$\\mathbf{c} =  q(\\mathbf{h}_1, \\ldots, \\mathbf{h}_T)$$\n\t- 在上面的叙述中做了简化: 上下文变量就是最后一个隐状态: $\\mathbf c= \\mathbf{h}_T$\n\n#### Encoder的模型选择\n- 我们可以使用普通的RNN作为Encoder, 当然也可以使用GRU, LSTM... \n\t- 使用LSTM的时候, Cell State也是隐状态的一部分\n\n- 我们可以使用Bi-directional的RNN来作为编码器, 其实双向RNN经常被用在Encoder里面.\n\n### Decoder\n解码器需要根据之前所有的预测 $y_1, \\ldots, y_{t'-1}$ 和context variable $\\mathbf{c}$ 来预测下一个元素 $y_{t'}$, 即:\n$$P(y_{t'} \\mid y_1, \\ldots, y_{t'-1}, \\mathbf{c})$$\n- 根据RNN的计算方式, Decoder首先根据 $y_{t^\\prime-1}, \\mathbf{c}, \\mathbf{s}_{t^\\prime-1}$ 来更新当前时间步的隐变量:\n$$\\mathbf{s}_{t^\\prime} = g(y_{t^\\prime-1}, \\mathbf{c}, \\mathbf{s}_{t^\\prime-1})$$\n- 然后根据隐变量用输出层的MLP和Softmax来生成对应的输出:\n\t$$\\mathbf{o_{t^\\prime}}=p(\\mathbf{s}_{t^\\prime})$$\n\t- 也就是计算条件概率分布 $P(y_{t'} \\mid y_1, \\ldots, y_{t'-1}, \\mathbf{c})$\n\n![](notes/2022/2022.4/D2L-53-循环神经网络RNN.md#^ba5f9a)\n\n## Loss Function\n- 对于softmax的输出， 我们可以通过计算[交叉熵损失函数](notes/2022/2022.2/D2L-14-Cross%20Entropy%20as%20Loss.md)来进行优化。\n- 但是为了进行批量训练, 我们对序列进行了pad, 在计算损失的时候需要将pad排除在外.\n\n![](notes/2022/2022.4/assets/seq2seq.ipynb)\n\n## Train\n- 在训练的时候, `\u003cbos\u003e` + 正确序列会被输入到Decoder里面, 这被称为*Teacher Forcing*\n![](notes/2022/2022.4/assets/seq2seq.svg)\n## Predict\n- 在预测的时候, Decoder的输出被用作下一个时间步的输入. \n\t- 在输出为`\u003ceos\u003e`或者长度大于num_steps的时候停止输出:\n![](notes/2022/2022.4/assets/seq2seq-predict.svg)\n## Evaluate\n- 一个序列越长, 成功翻译它的难度就更大. 为了平衡不同句子长度的预测难度, 我们使用BLEU来作为输出效果的评价标准. \n\t- 之前我们使用的都是[困惑度 Perplexity](notes/2022/2022.4/D2L-53-循环神经网络RNN.md#RNN的评估指标%20困惑度%20Perplexity)\n\n[D2L-62-BLEU (Bilingual Evaluation Understudy)](notes/2022/2022.4/D2L-62-BLEU%20(Bilingual%20Evaluation%20Understudy).md)","lastmodified":"2022-10-15T14:06:29.64250409Z","tags":null},"/notes/2022/2022.4/D2L-62-BLEU-Bilingual-Evaluation-Understudy":{"title":"D2L-62-BLEU (Bilingual Evaluation Understudy)","content":"# BLEU (Bilingual Evaluation Understudy)\n\n\u003cdiv align=\"right\"\u003e 2022-04-20\u003c/div\u003e\n\nTags: #BLEU #DeepLearning \n\n- **BLEU** 是一种用于评价输出序列质量的评价指标, 其特点在于它考虑到了序列长度和预测难度的关系.\n\n- **BLEU** 通过综合\"不同*n-gram*在结果中的成功次数\"来评价最终质量的好坏.\n\n## 定义\n$$ \\exp\\left(\\min\\left(0, 1 - \\frac{\\mathrm{len}_{\\text{label}}}{\\mathrm{len}_{\\text{pred}}}\\right)\\right) \\prod_{n=1}^k p_n^{1/2^n}$$\n\n其中: \n- $\\mathrm{len}_{\\text{label}}$ 表示标签序列中的词元数\n- $\\mathrm{len}_{\\text{pred}}$ 表示预测序列中的词元数\n- $k$ 是最长的 $n$ 元语法的长度。\n\n另外，用 $p_n$ 表示 $n$ 元语法的精确度，它是两个数量的比值：\n$$p_n=\\frac{\\operatorname{Num}^{(n)}_{match}}{\\operatorname{Num}^{(n)}_{total}}$$\n\n- 分子是预测序列与标签序列中成功匹配的 $n$ 元语法个数，\n- 分母是预测序列里面n-gram的总个数。\n\n举个例子，给定标签序列 $A$、$B$、$C$、$D$、$E$、$F$\n和 预测序列                     $A$、$B$、$B$、$C$、$D$，\n我们有 $p_1 = 4/5$、$p_2 = 3/4$、$p_3 = 1/3$ 和 $p_4 = 0$。\n\n### 解释\n#### 上限: 完美情况\n当两个序列完全相同的时候, BLEU=1. \n\n#### 系数: 惩罚短的预测\n$$\\exp\\left(\\min\\left(0, 1 - \\frac{\\mathrm{len}_{\\text{label}}}{\\mathrm{len}_{\\text{pred}}}\\right)\\right)$$\n- 这个系数的作用是: 为更长的$n$元语法的精确度分配更大的权重。\n- 例如，当$k=2$时，给定标签序列$A$、$B$、$C$、$D$、$E$、$F$ 和预测序列$A$、$B$，尽管$p_1 = p_2 = 1$，但是惩罚因子$\\exp(1-6/2) \\approx 0.14$会降低BLEU。\n\n#### 底数: 也在惩罚短的预测\n- 当$p_n$固定时(也就是假设所有n-gram的预测成功比率都是一样的)，$p_n^{1/2^n}$会随着$n$的增长而增加（原始论文使用$p_n^{1/n}$）, 所以n越大占比越大.\n![BLEU](notes/2022/2022.4/assets/img_2022-10-15.gif)\n[^1]\n\n## 代码实现\n```python\ndef bleu(pred_seq, label_seq, k):  #@save\n    \"\"\"计算BLEU\"\"\"\n    pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')\n    len_pred, len_label = len(pred_tokens), len(label_tokens)\n    score = math.exp(min(0, 1 - len_label / len_pred))\n    for n in range(1, k + 1):\n        num_matches, label_subs = 0, collections.defaultdict(int)\n        for i in range(len_label - n + 1):\n            label_subs[' '.join(label_tokens[i: i + n])] += 1\n        for i in range(len_pred - n + 1):\n            if label_subs[' '.join(pred_tokens[i: i + n])] \u003e 0:\n                num_matches += 1\n                label_subs[' '.join(pred_tokens[i: i + n])] -= 1\n        score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))\n    return score\n```\n\n\n\n[^1]: [计算器套件 - GeoGebra](https://www.geogebra.org/calculator/x9gpkywp)","lastmodified":"2022-10-15T14:06:29.64250409Z","tags":null},"/notes/2022/2022.4/D2L-63-Beam-Search":{"title":"D2L-63-Beam Search","content":"#  束搜索\n\n\u003cdiv  align=\"right\"\u003e  2022-04-20\u003c/div\u003e\n\nTags:  #BeamSearch  #DynamicProgramming\n\n-  在[Seq2Seq](notes/2022/2022.4/D2L-61-Sequence%20to%20Sequence%20Learning%20-%20Seq2Seq.md)里面预测的时候,  我们直接就将上一步预测概率最大的选项输入到下一个时间步,  其实这是一种贪心策略:  最大化当前时间步的预测概率.  而贪心算法常常不能找到全局的最优解,  我们能怎样改进呢?\n\n![](notes/2022/2022.4/assets/img_2022-10-15-6.png)\n## 贪心 Greedy Search\n我们先来评估一下贪心算法的时间复杂度, 我们需要计算 $T$ 个时间步的 $|\\mathcal{Y}|$ 个概率, 总的时间复杂度为: $$\\mathcal{O}({T}\\cdot\\left|\\mathcal{Y}\\right|)$$\n##  穷举算法  Exhaustive  Search\n如果搜索空间不大,  我们可以直接穷举所有可能的序列  $y_1,  \\cdots,  y_{t-1},  y_{t}$,  这时的时间复杂度为:$$\\mathcal{O}(\\left|\\mathcal{Y}\\right|^{T})$$\n其中$|\\mathcal{Y}|$ 表示输出词表的大小(包括`\u003ceos\u003e`), 由于词表或者时间步常常较大, 所以穷举在计算复杂度上是不可行的\n\n##  Viterbi\n求解最优序列也是HMM模型里面的一个问题, 而最常用的算法就是[Viterbi Algorithm](notes/2022/2022.4/Viterbi%20Algorithm.md)了.\n在这个问题里面, Viterbi的复杂度为:$$\\mathcal{O}({T}\\cdot\\left|\\mathcal{Y}\\right|^2)$$\n比穷举的指数复杂度好多了, 但是$|\\mathcal{Y}|^2$依然是一个很大的项, 我们希望开销更小一点.\n\n## Beam Search\n束搜索就是开销介于Viterbi和贪心之间的那个算法了: 通过选定一个 _束宽_（beam size）$k$, 我们在每一个时间步只选择概率最高的 $k$ 条路径, 进一步减少了计算开销:\n$$\\mathcal{O}(k\\cdot{T}\\cdot\\left|\\mathcal{Y}\\right|)$$\n在时间步$1$，我们选择具有最高条件概率的$k$个词元。这$k$个词元将分别是$k$个候选输出序列的第一个词元。在随后的每个时间步，基于上一时间步的$k$个候选输出序列，我们将继续从$k\\left|\\mathcal{Y}\\right|$个可能的选择中挑出具有最高条件概率的$k$个候选输出序列。\n![](notes/2022/2022.4/assets/beam-search.svg)\n### 平衡序列长度\n- 束搜索在最后找出的$k$个序列长度可能是不等的, 而长序列的概率本来就较低. 为了公平的选出最优序列, 我们对序列概率进行了加权:\n$$ \\frac{1}{L^\\alpha} \\log P(y_1, \\ldots, y_{L}\\mid \\mathbf{c}) = \\frac{1}{L^\\alpha} \\sum_{t'=1}^L \\log P(y_{t'} \\mid y_1, \\ldots, y_{t'-1}, \\mathbf{c})$$\n- 其中$L$是最终候选序列的长度，$\\alpha$通常设置为$0.75$。\n- 这种\"平衡序列长度对结果影响\"的思路和[BLEU](notes/2022/2022.4/D2L-62-BLEU%20(Bilingual%20Evaluation%20Understudy).md)很相似\n\n## 总结\n-  贪心算法是记忆力为 $1$ 的小傻瓜\n-  而Viterbi的记忆力为每一个时间步的所有选项大小(*vocab_size:* $|\\mathcal{Y}|$ ),  所以能够找到最优的序列.\n-  Beam Search是介于两者之间的,  不一定能够找到最优的方法,  但是没有贪心那么傻.\n- 后面两者都是动态规划算法.\n- 实际上，贪心搜索可以看作是一种束宽为 $1$ 的特殊的束搜索, Viterbi是束宽为 $|\\mathcal{Y}|$ 的束搜索。 通过灵活地选择束宽，束搜索可以在正确率和计算代价之间进行权衡。\n\n---\nRef:\n[natural  language  -  What  is  the  difference  between  the  Viterbi  algorithm  and  beam  search?  -  Cross  Validated](https://stats.stackexchange.com/questions/536249/what-is-the-difference-between-the-viterbi-algorithm-and-beam-search)\n\n\n\n\n\n\n\n\n\n","lastmodified":"2022-10-15T14:06:29.64250409Z","tags":null},"/notes/2022/2022.4/D2L-64-Kernel-Regression":{"title":"D2L-64-Kernel Regression","content":"# Nadaraya-Watson Kernel Regression\n\n\u003cdiv align=\"right\"\u003e 2022-04-20\u003c/div\u003e\n\nTags: #KernelRegression #Nonparametric #Attention #MachineLearning \n\n## Intuition\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/t9-RDKyOU3o\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n## Definition\n$$f(x) = \\sum_{i=1}^n \\frac{K(x - x_i)}{\\sum_{j=1}^n K(x - x_j)} y_i$$\n- 其中是$K$ 是 _核_（kernel）， 上面的估计器（Estimator）也被称为 _Nadaraya-Watson核回归_（*Nadaraya-Watson kernel regression*）\n\n- 在核为高斯核 (Gaussian Kernel) 的时候：\n\t$$K(u) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{u^2}{2})$$\n$$\\begin{split}\\begin{aligned} f(x) \u0026=\\sum_{i=1}^n \\frac{K(x - x_i)}{\\sum_{j=1}^n K(x - x_j)} y_i\\\\ \u0026= \\sum_{i=1}^n \\frac{\\exp\\left(-\\frac{1}{2}(x - x_i)^2\\right)}{\\sum_{j=1}^n \\exp\\left(-\\frac{1}{2}(x - x_j)^2\\right)} y_i \\\\\u0026= \\sum_{i=1}^n \\mathrm{softmax}\\left(-\\frac{1}{2}(x - x_i)^2\\right) y_i. \\end{aligned}\\end{split}$$\n ^33f5b4\n- 值得注意的是，Nadaraya-Watson核回归是一个非参数模型. 可以看到上式里面没有任何参数, 模型的拟合结果完全由样本点决定.\n\n### Example of Nadaraya–Watson Estimator in practice\n![](notes/2022/2022.4/assets/output_nadaraya-waston_736177_41_0.svg)[^2]\n![400](notes/2022/2022.4/assets/nw-1-1.png)[^1]\n\n## Kernel Regression and Attention \n[[notes/2022/2022.4/D2L-66-Kernel Regression and Attention]]\n\n\n\n\n[^1]: [6.2 Kernel regression estimation | Notes for Predictive Modeling](https://bookdown.org/egarpor/PM-UC3M/npreg-kre.html)\n[^2]: [10.2. 注意力汇聚：Nadaraya-Watson 核回归 — 动手学深度学习 2.0.0-beta0 documentation](https://zh-v2.d2l.ai/chapter_attention-mechanisms/nadaraya-waston.html)\n\n\n","lastmodified":"2022-10-15T14:06:29.64250409Z","tags":null},"/notes/2022/2022.4/D2L-65-Attention-Cues-Attention-Mechanisms":{"title":"D2L-65-Attention Cues \u0026 Attention Mechanisms","content":"# 注意力信号 与 注意力机制\n\n\u003cdiv align=\"right\"\u003e 2022-04-20\u003c/div\u003e\n\nTags: #Attention #DeepLearning \n\n## Attention Cue\n- Attention Cue分为两种: _nonvolitional cue_ 和 _volitional cue_.\n\n\u003e Your **volition** is the power you have to decide something for yourself. \n\n- 我在这里翻译为\"自发的\"(_volitional_)和\"非自发的\"(_nonvolitional_)信号\n\n- Volitional的信号本身就是醒目的. (intrinsically salient and conspicuous)\n![300](notes/2022/2022.4/assets/eye-coffee.svg)\n- Nonvolitional的信号是你主动去关注的. 比如你想要看书, 所以会去寻找书本的信号.\n![300](notes/2022/2022.4/assets/eye-book.svg)\n\n## Attention Mechanism\n- 尽管上面的注意力信号分类是粗浅的, 但我们也可以按照这个思路去构造深度学习里面的**注意力机制**(Attention Mechanism).\n![](notes/2022/2022.4/assets/qkv.svg)\n### New Jargons\n- 我们先给两种动机不同的信号起个新名字:\n\t- **Query**: Volitional Cue\n\t- **Key**: Nonvolitional Cue\n\n- 并且我们称 intermediate feature representations, 也就是 Sensory Inputs 为 **Value**[^1]\n\n### Incorporate 2 Cues\n- 前面我们提到, Query和Key分别代表了两种不同的Attention Cue. 在Attention Mechanism里面, 我们可以将两者通过Attention Pooling结合到一起. 然后与Values一起生成最后的Output.\n![](notes/2022/2022.4/assets/qkv.svg)\n- One Query + Multiple Key-Value Pairs $\\rightarrow$ One Output\n\n- 其实还有其他构造Attention Mechanism的方式.\n\n\n\n[^1]: 其实我感觉这个名字起的不是很好, 很多时候并没有value里\"结果\"的意思","lastmodified":"2022-10-15T14:06:29.64250409Z","tags":null},"/notes/2022/2022.4/D2L-66-Kernel-Regression-and-Attention":{"title":"D2L-66-Kernel Regression and Attention","content":"# Kernel Regression And Attention\n\n\u003cdiv align=\"right\"\u003e 2022-04-20\u003c/div\u003e\n\nTags: #KernelRegression #Attention #DeepLearning \n\n- Nadaraya-Watson kernel regression is an example of machine learning with attention mechanisms.\n\n## 更一般化的Attention Pooling\n- 在Kernel Regression里面, Estimator有如下形式:\n$$f(x) = \\sum_{i=1}^n \\frac{K(x - x_i)}{\\sum_{j=1}^n K(x - x_j)} y_i$$\n其中 $x$ 对应query, 而 $(x_i, y_i)$ 是key-value pair.\n\n- 其中attention pooling的部分其实只包括: $$\\frac{K(x - x_i)}{\\sum_{j=1}^n K(x - x_j)}$$\n它综合query里面的volitional cue和key里面的nonvolitional cue, 和value $y_i$ 一起生成最后的结果.\n![](notes/2022/2022.4/assets/qkv.svg)\n- Kernel Regression只是注意力机制一个简单的特例. 我们可以将Attention Pooling抽象成更一般的形式: $$f(x) = \\sum_{i=1}^n \\alpha(x, x_i) y_i$$\n其中 $\\alpha(x, x_i)$ 代表 _Attention Weight_ , 表示query $x$ 和 key $x_i$ 一起生成的注意力权重, 对应着value $y_i$.\n\n- 在[Kernel Regression 里面的采用高斯核的时候](notes/2022/2022.4/D2L-64-Kernel%20Regression.md#^33f5b4), 我们可以可视化拟合后的注意力权重$\\alpha(x, x_i)$如下所示:\n![](notes/2022/2022.4/assets/output_nadaraya-watson_61a333_56_0.svg)\n- 其中testing inputs代表query $x$, training inputs代表keys $x_i$. \n- 可以看到两者之前的注意力权重和两者之间的距离呈正相关, 距离越近, 权重越大.\n\n### 总结\n- Kernel Regression里面的Attention Pooling是所有value $y_i$ (训练输出)的加权平均. 其中权重就是 $\\alpha(x, x_i)$ , 其值取决于query $x$ 和 key $x_i$\n\n## Parameterized Kernel Regression\n- Attention Pooling可以是**nonparametric**的, 也可以是**parametric**的, 下面便是一个例子:\n- 我们可以很容易在Kernel Regression里面加入可学习的参数, 提高模型的性能. 当然这同时也会改变Attention weight的分布.\n- 参数化的Kernel Regression可以表示如下(依然采用高斯函数作为核函数)\n$$\\begin{split}\\begin{aligned}f(x) \u0026= \\sum_{i=1}^n \\alpha(x, x_i) y_i \\\\\u0026= \\sum_{i=1}^n \\frac{\\exp\\left(-\\frac{1}{2}((x - x_i)w)^2\\right)}{\\sum_{j=1}^n \\exp\\left(-\\frac{1}{2}((x - x_j)w)^2\\right)} y_i \\\\\u0026= \\sum_{i=1}^n \\mathrm{softmax}\\left(-\\frac{1}{2}((x - x_i)w)^2\\right) y_i.\\end{aligned}\\end{split}$$\n- 训练的结果如下:\n![](notes/2022/2022.4/assets/output_nadaraya-watson_61a333_128_0.svg)\n- 可以看到, 加入了weight的Attention Weight变得更集中了:\n\n![](notes/2022/2022.4/assets/output_nadaraya-watson_61a333_140_0.svg)","lastmodified":"2022-10-15T14:06:29.64250409Z","tags":null},"/notes/2022/2022.4/D2L-67-Attention-Scoring-Function":{"title":"D2L-67-Attention Scoring Function","content":"# 注意力评分函数\n\n\u003cdiv align=\"right\"\u003e 2022-04-21\u003c/div\u003e\n\nTags: #Attention #DeepLearning \n\n- 抽取出Attention Pooling里面都有的Softmax部分, 我们可以将注意力机制的设计简化为Attention Scoring Function的设计.\n\n![](notes/2022/2022.4/assets/attention-output%201.svg)\n\n形式化的表达如下: \n- query $\\mathbf{q} \\in \\mathbb{R}^q$,  \n- $m$ 个key-value pairs $(\\mathbf{k}_1, \\mathbf{v}_1), \\ldots, (\\mathbf{k}_m, \\mathbf{v}_m)$, 其中 $\\mathbf{k}_i \\in \\mathbb{R}^k$ 且 $\\mathbf{v}_i \\in \\mathbb{R}^v$.\n则Attention Pooling $f$ 可以表达为value的如下加权平均: \n$$f(\\mathbf{q}, (\\mathbf{k}_1, \\mathbf{v}_1), \\ldots, (\\mathbf{k}_m, \\mathbf{v}_m)) = \\sum_{i=1}^m \\alpha(\\mathbf{q}, \\mathbf{k}_i) \\mathbf{v}_i \\in \\mathbb{R}^v$$\n- 进一步将attention weight里面的softmax提取出来:\n$$\\alpha(\\mathbf{q}, \\mathbf{k}_i) = \\mathrm{softmax}(a(\\mathbf{q}, \\mathbf{k}_i)) = \\frac{\\exp(a(\\mathbf{q}, \\mathbf{k}_i))}{\\sum_{j=1}^m \\exp(a(\\mathbf{q}, \\mathbf{k}_j))} \\in \\mathbb{R}$$\n其中 $a(\\mathbf{q}, \\mathbf{k}_i)$ 便是注意力评分函数的一般形式.\n\n- One Query + Multiple Keys $\\rightarrow$ Multiple Scores $\\rightarrow$ Multiple Weights + Corresponding Values $\\rightarrow$ One Output \n\n- 更一般的情况会同时存在多个Query $q_1,\\cdots, q_i$, 我们需要计算每一个key和每一个query对应的注意力分数 $a(\\mathbf{q}_i, \\mathbf{k}_j)$. 也就是说, 注意力分数是一个二维的矩阵, 而**注意力Pooling的输出的个数等于Query的个数, 输出的维度等于Value的维度**\n\n## Masked SoftMax\n- 因为batch训练的时候需要对长度进行Pad, 所以不仅需要对Cost Function进行Mask, 还需要对Softmax进行Mask, 在概率计算的时候排除掉Pad的位置.\n- 相比每次根据序列长度控制计算的元素个数, 一个技巧是直接先把pad的位置替换成一个很大的负数(e.g. -1e6), 再计算Softmax. 这样在计算的时候无关的项就会变为0, 简化了计算的逻辑.\n- [代码示例](https://d2l.ai/chapter_attention-mechanisms/attention-scoring-functions.html#masked-softmax-operation)\n\n## 两种注意力评分函数\n### Additive Attention\n![D2L-68-Additive Attention](notes/2022/2022.4/D2L-68-Additive%20Attention.md)\n\n### Scaled Dot-Product Attention\n![D2L-69-Scaled Dot-Product Attention](notes/2022/2022.4/D2L-69-Scaled%20Dot-Product%20Attention.md)\n\n\n","lastmodified":"2022-10-15T14:06:29.64250409Z","tags":null},"/notes/2022/2022.4/D2L-68-Additive-Attention":{"title":"D2L-68-Additive Attention","content":"# 加性注意力\n\n\u003cdiv align=\"right\"\u003e 2022-04-21\u003c/div\u003e\n\nTags: #Attention #DeepLearning \n\n- 一般来说，当Query和Key是**不同长度**的矢量时，我们可以使用Additive Attention来作为Scoring Function。\n- 给定查询 $\\mathbf{q} \\in \\mathbb{R}^q$ 和键 $\\mathbf{k} \\in \\mathbb{R}^k$，*加性注意力*（additive attention）的评分函数(Scoring Function)为\n$$a(\\mathbf q, \\mathbf k) = \\mathbf w_v^\\top \\text{tanh}(\\mathbf W_q\\mathbf q + \\mathbf W_k \\mathbf k) \\in \\mathbb{R},$$\n- 函数输出的是一个标量. 其中可学习的参数是 $\\mathbf W_q\\in\\mathbb R^{h\\times q}$、$\\mathbf W_k\\in\\mathbb R^{h\\times k}$ 和 $\\mathbf w_v\\in\\mathbb R^{h}$. 前两个权重分别将 Query 和 Key 映射为长度为 $h$ 的向量, 然后 $\\mathbf w_v$ 将向量映射为value 对应的注意力权重.\n\n- 下图展示了Additive Attention是怎样等价于一个单隐藏层的MLP的. \n\t- (图中简化为一个 $k\\times1$ 的key和一个 $q\\times1$ 的query)\n![Additive Attention](notes/2022/2022.4/assets/Additive%20Attention.svg)\n- 注意上面的MLP和下图的对应关系, 一个Query需要和所有的Keys分别计算一次Score. \n![](notes/2022/2022.4/assets/Pasted%20image%2020220427164928.png)\n\n\n## 通用形式\n- 在实际应用中, Scoring function需要面对Batch形式输入的Keys and Queries, 并且每一个Batch里面都有多个Keys and Queries.\n```\nqueries的形状：(batch_size，num_of_queries，query_length)\nkey的形状：(batch_size，num_of_key－value_pair，key_length)\n```\n- 在一个Batch里面, 我们需要计算每一个Key和每一个Query的注意力分数, 这可以通过PyTorch里面的Broadcasting机制来巧妙地实现:\n```python\nclass AdditiveAttention(nn.Module):\n    \"\"\"加性注意力\"\"\"\n    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):\n        super(AdditiveAttention, self).__init__(**kwargs)\n        self.W_k = nn.Linear(key_size, num_hiddens, bias=False)\n        self.W_q = nn.Linear(query_size, num_hiddens, bias=False)\n        self.w_v = nn.Linear(num_hiddens, 1, bias=False)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, queries, keys, values, valid_lens):\n\t    # 先和权重相乘\n        queries, keys = self.W_q(queries), self.W_k(keys)\n        # 在维度扩展后，\n        # queries的形状：(batch_size，查询的个数，1，num_hidden)\n        # key的形状：(batch_size，1，“键－值”对的个数，num_hiddens)\n        # 使用广播方式进行求和\n        # 现在feature的形状为: \n        # (batch_size，查询的个数，“键－值”对的个数，num_hiddens)\n        features = queries.unsqueeze(2) + keys.unsqueeze(1)\n        features = torch.tanh(features)\n        # w_v(features)的形状为:(batch_size，查询的个数，“键－值”对的个数，1)\n        # self.w_v仅有一个输出，因此从形状中移除最后那个维度。\n        scores = self.w_v(features).squeeze(-1)\n        # scores的形状：(batch_size，查询的个数，“键-值”对的个数)\n        self.attention_weights = masked_softmax(scores, valid_lens)\n        # attention_weights的形状：(batch_size，查询的个数，“键-值”对的个数)\n        # values的形状：(batch_size，“键－值”对的个数，值的维度)\n        return torch.bmm(self.dropout(self.attention_weights), values)\n        # 返回值的形状：(batch_size，查询的个数，值的维度)\n        \n```\n- 其中`masked_softmax`对于每一个Query取前`valid_lens`个 \"key-value\" pair\n\n可视化Softmax的结果: attention_weights如下:\n```python\nqueries, keys = torch.normal(0, 1, (2, 1, 20)), torch.ones((2, 10, 2))\n# values的小批量，两个值矩阵是相同的\nvalues = torch.arange(40, dtype=torch.float32).reshape(1, 10, 4).repeat(\n    2, 1, 1)\nvalid_lens = torch.tensor([2, 6])\n\nattention = AdditiveAttention(key_size=2, query_size=20, num_hiddens=8,\n                              dropout=0.1)\nattention.eval()\nattention(queries, keys, values, valid_lens)\n```\n![](notes/2022/2022.4/assets/Pasted%20image%2020220421153308.png)\n![](notes/2022/2022.4/assets/output_attention-scoring-functions_2a8fdc_78_0.svg)\n","lastmodified":"2022-10-15T14:06:29.64250409Z","tags":null},"/notes/2022/2022.4/D2L-69-Scaled-Dot-Product-Attention":{"title":"D2L-69-Scaled Dot-Product Attention","content":"# 缩放的点积注意力\n\n\u003cdiv align=\"right\"\u003e 2022-04-21\u003c/div\u003e\n\nTags: #Attention #DeepLearning \n\n![](notes/2022/2022.4/assets/img_2022-10-15-7.png)[^1]\n\n- 相比Additive Attention, 使用点积可以得到计算效率更高的Scoring Function. 但是点积操作要求查询和键具有相同的长度 $d$。\n\n- 我们知道[内积可以衡量两个向量之间的相似程度](notes/2021/2021.11/内积和相关性的联系-Dot(Inner)_Product_\u0026_Correlation.md), 所以我们可以这样解读缩放点积注意力: \n\t- 注意力机制是Values的一个加权平均, 而缩放点积注意力会赋予和 Query 更相似的 Key-Value Pair 更高的权重.\n\n- 假设Query $\\mathbf q$ 和Key $\\mathbf k$ 的所有元素都是独立的随机变量，并且均值为 $0$, 方差为 $1$. 则它们的点积 $\\mathbf q^\\top\\mathbf k$ 内元素的均值为 $0$，方差为 $d$。\n\t- 为确保结果的方差仍为 $1$，我们将点积除以 $\\sqrt{d}$，就得到了*缩放点积注意力*（*scaled dot-product attention*）Scoring Function：\n$$a(\\mathbf q, \\mathbf k) = \\frac{\\mathbf{q}^\\top \\mathbf{k}}  {\\sqrt{d}}$$\n- 缩放点积注意力里面是没有任何参数的, 这是它和加性注意力的一个区别.\n\n## 通用形式\n- 在实践中，我们通常从小批量的角度来考虑提高效率\n- 例如一个Batch需要基于 $n$ 个 Query 和 $m$ 个 Key-value pair 计算注意力，其中Query和Key的长度为 $d$，Value的长度为 $v$, 查询 $\\mathbf Q\\in\\mathbb R^{n\\times d}$、键 $\\mathbf K\\in\\mathbb R^{m\\times d}$ , 值 $\\mathbf V\\in\\mathbb R^{m\\times v}$ . \n\t- 则缩放点积注意力为：\n$$ \\mathrm{softmax}\\left(\\frac{\\mathbf Q \\mathbf K^\\top }{\\sqrt{d}}\\right) \\mathbf V \\in \\mathbb{R}^{n\\times v}.$$\n\n- **代码实现:**\n```python\nclass DotProductAttention(nn.Module):\n    \"\"\"缩放点积注意力\"\"\"\n    def __init__(self, dropout, **kwargs):\n        super(DotProductAttention, self).__init__(**kwargs)\n        self.dropout = nn.Dropout(dropout)\n\n    # queries的形状：(batch_size，查询的个数，d)\n    # keys的形状：(batch_size，“键－值”对的个数，d)\n    # values的形状：(batch_size，“键－值”对的个数，值的维度)\n    # valid_lens的形状:(batch_size，)或者(batch_size，查询的个数)\n    def forward(self, queries, keys, values, valid_lens=None):\n        # 获取query的长度(key的长度)\n        d = queries.shape[-1]\n        # 设置transpose_b=True为了交换keys的最后两个维度\n        scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d)\n        self.attention_weights = masked_softmax(scores, valid_lens)\n        return torch.bmm(self.dropout(self.attention_weights), values)\n```\n- 在代码里面我们还使用了[Dropout](notes/2022/2022.2/D2L-23-Dropout-丢弃法.md)来进行正则化\n\n\n[^1]: 不是图源, 但是我是在这里看到的[Attention机制详解（二）——Self-Attention与Transformer - 知乎](https://zhuanlan.zhihu.com/p/47282410)","lastmodified":"2022-10-15T14:06:29.64250409Z","tags":null},"/notes/2022/2022.4/D2L-70-Seq2Seq-with-Attention-Bahdanau-Attention":{"title":"D2L-70-Seq2Seq with Attention - Bahdanau Attention","content":"# 含注意力机制的Seq2Seq\n\n\u003cdiv align=\"right\"\u003e 2022-04-22\u003c/div\u003e\n\nTags: #Seq2Seq #Attention #DeepLearning #RNN \n\n## Motivation\n- 在[Seq2Seq](notes/2022/2022.4/D2L-61-Sequence%20to%20Sequence%20Learning%20-%20Seq2Seq.md)模型里面, Encoder向Decoder传递的仅仅是最后一个时间步的隐状态, 也就是上下文变量 $\\mathbf c= \\mathbf{h}_T$, 我们假设里面已经包含了输入序列的所有信息:\n![](notes/2022/2022.4/assets/img_2022-10-15-8.png)\n- 但这样每一步Decoder的输入都是原序列的一个\"全局, 笼统的总结\", 这是不太合理的: 在下图中, 在翻译\"Knowledge\"的的时候, 显然\"力量\"这个词是不太重要的. \n\t![](notes/2022/2022.4/assets/img_2022-10-15-1.gif)[^1]\n\t- 在原始的Seq2Seq模型里面, 输入序列里面的所有元素都是同等重要的, 我们可以引入注意力机制来解决这一点.\n\t- 在Seq2Seq里面引入注意力机制有不同的方法，下面要介绍的称为**Bahdanau注意力**\n\t\t- [✂️ How to pronounce Bahdanau - YouTube](https://youtube.com/clip/UgkxQ5GU8jcvH5w14GCdZ3RypfWVN5Q1PeV6)\n\n## 模型构建\n- 我们将注意力模块整合到Decoder里面去, 这样只需要重写Decoder模块就好啦.\n\t- 其实新增注意力模块也等价于采用新的context variable $\\mathbf c$, 这个新的上下文变量会随着时间步的变化而变化.\n\n\t![](notes/2022/2022.4/assets/img_2022-10-15-9.png)\n\t\n**选择注意力评分函数**\n- 我们选择[Additive Attention](notes/2022/2022.4/D2L-68-Additive%20Attention.md), 因为加性注意力里面有可以学习的参数, 可以在一定程度上增加模型的表达能力.\n\n**Decoder隐状态的初始化**\n- 和之前一样, 我们使用Encoder最后一个时间步的`hidden_state`来初始化decoder的hidden state\n\n### 注意力模块\n![](notes/2022/2022.4/assets/seq2seq-attention-details.svg)\n假设输入序列长度为 $T$, 则Decoder在时间步 $t^\\prime$ 的上下文变量 $\\mathbf{c}_{t'}$ 为: (这也是Attention Pooling在时间步 $t^\\prime$ 的输出)\n$$\\mathbf{c}_{t'} = \\sum_{t=1}^T \\alpha(\\mathbf{s}_{t' - 1}, \\mathbf{h}_t) \\mathbf{h}_t$$\n其中: \n- $\\alpha$ 是**Attention weight**\n- $\\mathbf{s}_{t' - 1}$ 是上一个时间步Decoder的**hidden state**, 对应Query\n- $\\mathbf{h}_t$ 是 **Encoder** `output` 里面第 $t$ 个时间步hidden state的输出. 既对应Key, 又对应 Value.\n- 需要注意的是每一个时间步 $t^\\prime$ 的上下文变量 $\\mathbf{c}_{t'}$ 是Encoder**每一个时间步**的Attention**总和**.\n\n**输入**\n- **Query:** $\\mathbf{s}_{t' - 1}$ \n\t- 来自Decoder循环层的`output`\n- **Key \u0026 Value:** $\\mathbf{h}_t$\n\t- 来自Encoder的 `output`\n\n## 模型实现\n![](notes/2022/2022.4/assets/bahdanau_attention.ipynb)\n\n\n\n[^1]: [Overview - seq2seq](https://google.github.io/seq2seq/)","lastmodified":"2022-10-15T14:06:29.64250409Z","tags":null},"/notes/2022/2022.4/D2L-71-Multi-Head_Attention":{"title":"D2L-71-Multi-Head_Attention","content":"# 多头注意力\n\n\u003cdiv align=\"right\"\u003e 2022-04-27\u003c/div\u003e\n\nTags: #Attention #Multi-headAttention #DeepLearning \n\n- 多头注意力就是对 Query, Key, Value 进行一些线性变换, 并行地计算多个注意力, 期望模型能学习到多样化的依赖关系.\n\n![](notes/2022/2022.4/assets/multi-head-attention.svg)\n- Another way of seeing it:\n![350](notes/2022/2022.4/assets/Pasted%20image%2020220429225229.png)[^1]\n\n## 模型构建\n下面我们给出 Multi-Head Attention 的形象化表示:\n### Part 1\n![](notes/2022/2022.4/assets/Pasted%20image%2020220427151440.png)\n- 给定 Query $\\mathbf{q} \\in \\mathbb{R}^{d_q}$、Key $\\mathbf{k} \\in \\mathbb{R}^{d_k}$ 和 Value $\\mathbf{v} \\in \\mathbb{R}^{d_v}$，则每个注意力头 $\\mathbf{h}_i$（$i = 1, \\ldots, h$）的计算方法为：\n\t$$\\mathbf{h}_i = f(\\mathbf W_i^{(q)}\\mathbf q, \\mathbf W_i^{(k)}\\mathbf k,\\mathbf W_i^{(v)}\\mathbf v) \\in \\mathbb R^{p_v}$$\n\t- 其中，可学习的参数包括\n\t\t- $\\mathbf W_i^{(q)}\\in\\mathbb R^{p_q\\times d_q}$、 $\\mathbf W_i^{(k)}\\in\\mathbb R^{p_k\\times d_k}$ 和 $\\mathbf W_i^{(v)}\\in\\mathbb R^{p_v\\times d_v}$，\n\t\t- 以及代表注意力汇聚的函数 $f$。$f$ 可以是 [Attention Scoring Function](notes/2022/2022.4/D2L-67-Attention%20Scoring%20Function.md) 中的加性注意力或缩放点积注意力。\n\n### Part 2\n![](notes/2022/2022.4/assets/Pasted%20image%2020220427151520.png)\n- 然后我们需要汇聚 $h$ 个注意力头的结果. 我们使用一个 FC 来进行汇聚 (也就是先进行 Concatenation, 再进行一个线性变换).\n\t$$\\mathbf W_o \\begin{bmatrix}\\mathbf h_1\\\\\\vdots\\\\\\mathbf h_h\\end{bmatrix} \\in \\mathbb{R}^{p_o}$$\n\t- 其中可学习的参数为 $\\mathbf W_o\\in\\mathbb R^{p_o\\times h p_v}$ \n\n- 基于这种设计，每个头都可能会关注输入的不同部分，可以表示比简单加权平均值更复杂的函数。\n\n## 模型实现\n- 模型实现的关键在于: **并行地**计算 $h$ 个头的注意力.\n\n### Attention Pooling 参数规模的问题\n- 首先, 因为多头注意力引入了大量的全连接层, 这会极大地增加 Attention Pooling 的参数大小和计算复杂度. \n- 为了避免这个问题, 我们令 $p_q = p_k = p_v = p_o / h$, 也就是说, 现在每一个头里面的 Query, Key, Value 都只有原来的 $1/h$ 大.\n- 因为 Value 大小变味了原来的 $1/h$, 所以 Attention 的输出长度也只有原来的 $1/h$. 而拼接以后的 $\\begin{bmatrix}\\mathbf h_1\\\\\\vdots\\\\\\mathbf h_h\\end{bmatrix}$ 长度和原来一样.\n- 最后的 $\\mathbf W_o$ 输入输出大小一样.\n```python\nclass MultiHeadAttention(nn.Module):\n    \"\"\"多头注意力\"\"\"\n    def __init__(self, key_size, query_size, value_size, num_hiddens,\n                 num_heads, dropout, bias=False, **kwargs):\n        super(MultiHeadAttention, self).__init__(**kwargs)\n        self.num_heads = num_heads\n        self.attention = d2l.DotProductAttention(dropout)\n        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)\n        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)\n        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)\n        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)\n\n######初始化时############################\nnum_hiddens, num_heads = 100, 5\nattention = MultiHeadAttention(\n    key_size    =   num_hiddens,\n    query_size  =   num_hiddens,\n    value_size  =   num_hiddens,\n    num_hiddens =   num_hiddens,\n    num_heads   =   num_heads,\n    dropout     =   0.5 )\n```\n- 提问: 既然 Attention 前的线性映射缩小了 Query, Key 和 Value 的长度, 那为什么上面初始化时 `key_size`, `query_size`, `value_size` 还是等于 `num_hiddens` 呢?\n\t- 其实 $W_q, W_k, W_v$ 表示的是将 $h$ 个小全连接层拼起来, 得到的一个\"大号全连接层\"的参数.\n\t\t![Multihead Query](notes/2022/2022.4/assets/Multihead%20Query.svg)\n\n### 并行化思路\n- 为了实现并行计算, 我们先将线性映射之后的 Query, Key, Value 的按 $Batches\\times heads$ 的方式拼接在一起:\n![](notes/2022/2022.4/assets/Pasted%20image%2020220427202107.png)\n- 然后将拼接后的张量一起送进 Attention, 得到未融合的注意力输出,\n\n- 再重新变换形状, 得到 $\\begin{bmatrix}\\mathbf h_1\\\\\\vdots\\\\\\mathbf h_h\\end{bmatrix}$\n\n- 最后再经过一个全连接层 $\\mathbf W_o$, 得到融合后的注意力输出\n\n```python\ndef forward(self, queries, keys, values, valid_lens):\n        # queries，keys，values的形状:\n        # (batch_size，查询或者“键－值”对的个数，num_hiddens)\n        # valid_lens　的形状:\n        # (batch_size，)或(batch_size，查询的个数)\n        # 经过变换后，输出的queries，keys，values　的形状:\n        # (batch_size*num_heads，查询或者“键－值”对的个数，\n        # num_hiddens/num_heads)\n        queries = transpose_qkv(self.W_q(queries), self.num_heads)\n        keys = transpose_qkv(self.W_k(keys), self.num_heads)\n        values = transpose_qkv(self.W_v(values), self.num_heads)\n\n        if valid_lens is not None:\n            # 在轴0，将第一项（标量或者矢量）复制num_heads次，\n            # 然后如此复制第二项，然后诸如此类。\n            valid_lens = torch.repeat_interleave(\n                valid_lens, repeats=self.num_heads, dim=0)\n\n        # output的形状:(batch_size*num_heads，查询的个数，\n        # num_hiddens/num_heads)\n        output = self.attention(queries, keys, values, valid_lens)\n\n        # output_concat的形状:(batch_size，查询的个数，num_hiddens)\n        output_concat = transpose_output(output, self.num_heads)\n        return self.W_o(output_concat)\n```\n\n### 并行化细节\n- 详细地说, 张量形状的变化如下图所示:\n![Tensor Shape in Multihead Attention](notes/2022/2022.4/assets/Tensor%20Shape%20in%20Multihead%20Attention.svg)\n\n\n![](notes/2022/2022.4/assets/multihead_attention.ipynb)\n\n[^1]: From Attention is All You Need","lastmodified":"2022-10-15T14:06:29.64250409Z","tags":null},"/notes/2022/2022.4/D2L-72-Self-Attention":{"title":"D2L-72-Self-Attention","content":"# 自注意力\n\n\u003cdiv align=\"right\"\u003e 2022-04-26\u003c/div\u003e\n\nTags: #Self-Attention #Attention #DeepLearning \n\n- Attention 机制可以抽象为:[^1]\n$$\\begin{align}\n\\textit{Attention}(Q,K,V) = V\\cdot\\textit{softmax}\\ (\\textit{score}(Q, K))\n\\end{align}$$\n自注意力就是 $Q = K = V$ , 也就是同一个序列同时作为 Query, Key 和 Value.\n\n![450](notes/2022/2022.4/assets/img_2022-10-15-10.png)\n- 因为 Query, Key, Value 都是同一个序列 $X$, 所以**自注意力的输出就是输入 $X$ 自身的一个加权平均**, 只不过是一种具备 Attention 的, \"动态的\"加权平均.\n\n![Self-Attention](notes/2022/2022.4/assets/Self-Attention.svg)\n\n## 实现\n- 自注意力只是一种思想, 任何一种 Attention Mechanism 都可以通过把 $Q,K,V$ 变成一样的来实现自注意力.\n\n- 下面是一个利用 Multihead Attention 来实现自注意力的例子:[^2]\n```python\nnum_hiddens, num_heads = 100, 5\nattention = d2l.MultiHeadAttention(num_hiddens, \n\t\t\t\t\t\t\t\t   num_hiddens, \n\t\t\t\t\t\t\t\t   num_hiddens, \n\t\t\t\t\t\t\t\t   num_hiddens, \n\t\t\t\t\t\t\t\t   num_heads, 0.5)\nattention.eval()\n```\n\n## 优缺点\n- 我们可以通过比较**CNN**, **RNN**, **自注意力**三种结构, 来分析自注意力机制的优缺点:\n![](notes/2022/2022.4/assets/cnn-rnn-self-attention.svg)\n\n- $k$ : 卷积核大小\n- $n$ : 序列长度\n- $d$ : 输入和输出的通道数量, 隐状态数量, QKV 的长度\n\n|            | CNN                  | RNN                 | 自注意力            |\n| ---------- | -------------------- | ------------------- | ------------------- |\n| **计算复杂度** | $\\mathcal{O}(knd^2)$ | $\\mathcal{O}(nd^2)$ | $\\mathcal{O}(n^2d)$ |\n| **并行度**     | $\\mathcal{O}(n)$     | $\\mathcal{O}(1)$    | $\\mathcal{O}(n)$    |\n| **最长路径**   | $\\mathcal{O}(n/k)$   | $\\mathcal{O}(n)$    | $\\mathcal{O}(1)$    |\n- CNN 里面**最长路径**的意思是: 如果 $x_1$ 需要看到 $x_5$, 那么需要有 $n/k=5/3\\rightarrow 2$ 个卷积层才能看到.\n\n\n- **优点:** 可以看到自注意力的并行度好, 最长路径短(信息流通容易) 适合用于处理长序列. \n- **缺点:** 但是自注意力的计算复杂度在序列较长的时候也增长的很快\n\n- 总而言之，CNN 和自注意力都拥有并行计算的优势， 而且自注意力的最大路径长度最短。 但是因为其计算复杂度是关于序列长度的二次方，所以在很长的序列中计算会非常慢。\n\n\n## 位置编码\n- 在处理词元序列时，RNN是逐个的重复地处理词元的， 而**自注意力则因为并行计算而放弃了顺序操作**。 \n![D2L-73-Positional_Encoding](notes/2022/2022.4/D2L-73-Positional_Encoding.md)\n\n\n[^1]:  回看我之前的笔记, 我当时记录的并没有这么清晰. 这说明在第一遍\"学懂\"以后, 第二遍的梳理往往能获得新的, 更凝练的理解. [What is Attention, Self Attention, Multi-Head Attention? | Aditya Agrawal](https://www.adityaagrawal.net/blog/deep_learning/attention)\n[^2]: [10.6. 自注意力和位置编码 — 动手学深度学习 2.0.0-beta0 documentation](https://zh-v2.d2l.ai/chapter_attention-mechanisms/self-attention-and-positional-encoding.html#id7)","lastmodified":"2022-10-15T14:06:29.64250409Z","tags":null},"/notes/2022/2022.4/D2L-73-Positional_Encoding":{"title":"D2L-73-Positional_Encoding","content":"# 位置编码: 将位置信息加入数据\n\n\u003cdiv align=\"right\"\u003e 2022-04-27\u003c/div\u003e\n\nTags: #PositionalEncoding #Self-Attention #DeepLearning \n\n\n- **为了使用序列的顺序信息**，我们通过在输入表示中添加 **位置编码**（*positional encoding*）来注入**绝对的**或**相对的**位置信息。\n\n- 我觉得D2L讲的很深入很好了: [10.6. Self-Attention and Positional Encoding](https://d2l.ai/chapter_attention-mechanisms/self-attention-and-positional-encoding.html#positional-encoding)\n\n**Highlights:**\n![Positional Encoding|300](notes/2022/2022.4/assets/Positional%20Encoding.svg)\n- In binary representations, a higher bit has a lower frequency than a lower bit. Similarly, as demonstrated in the heat map below, the positional encoding decreases frequencies along the encoding dimension by using trigonometric functions. Since the outputs are float numbers, such continuous representations are more space-efficient than binary representations.\n![](notes/2022/2022.4/assets/output_self-attention-and-positional-encoding_d76d5a_67_0.svg)\n\n\n\n","lastmodified":"2022-10-15T14:06:29.64250409Z","tags":null},"/notes/2022/2022.4/D2L-74-Transformer":{"title":"D2L-74-Transformer","content":"# Transformer\n\n\u003cdiv align=\"right\"\u003e 2022-04-27\u003c/div\u003e\n\nTags: #Transformer #Attention #DeepLearning \n\n![](notes/2022/2022.4/assets/transformer.svg)[^2]\n- Transformer 是一个纯基于 Attention 的 [Encoder Decoder](notes/2022/2022.4/D2L-60-Encoder-Decoder.md) 架构模型\n\n- **Hugging Face Explorable Transformer**: [exBERT](https://huggingface.co/exbert/?model=bert-base-uncased\u0026modelKind=bidirectional\u0026sentence=The%20girl%20ran%20to%20a%20local%20pub%20to%20escape%20the%20din%20of%20her%20city.\u0026layer=2\u0026heads=..0,1,2,3,4,5,6,7,8,9,10,11\u0026threshold=0.32\u0026tokenInd=11\u0026tokenSide=right\u0026maskInds=..\u0026hideClsSep=true)\n\n## Motivation\n- 我们在 [Self-Attention](notes/2022/2022.4/D2L-72-Self-Attention.md#优缺点) 中比较了卷积神经网络（CNN）、循环神经网络（RNN）和自注意力（self-attention）。值得注意的是，自注意力同时具有**并行计算**和**最短的最大路径长度**这两个优势。因此，**使用自注意力来设计深度架构是很有吸引力的**.[^1]\n\n## 整体架构\n- 其实整个 Transformer 的结构是很清晰的：总的来说，Transformer 分为 Encoder 和 Decoder 两个部分，并且每个部分都由 $n$ 个块构成。\n### Encoder Block\n![300](notes/2022/2022.4/assets/Pasted%20image%2020220429160435.png)\n- 每个 Encoder 块里面包含一个注意力层和一个 FFN 层(其实就是两层全连接)\n\t- Attention 层使用**多头自注意力**, 用来从整个输入序列里面提炼信息\n\t- FFN层对每一个Position进行相同的变换\n\n- 在每一次变换以后, 都进行一次Layer Norm和残差连接\n\n### Decoder Block\n![300](notes/2022/2022.4/assets/Pasted%20image%2020220429161346.png)\n- 每个 Decoder 块里面包含 2 个注意力层和 1 个 FFN 层\n\t- 第一个注意力层也使用**多头自注意力**, 但是由于预测是逐步完成的, 所以需要Mask掉未知的项\n\t- 第二个注意力接受 Encoder 的输出, 使用多头注意力, 接收上一层的输出作为 Query.\n\t- 最后一层使用 FFN 对每一个 Position 进行单独变换.\n\n- 和 Encoder Block 一样 , 每一层后面都跟着一次 Layer Norm 和 一次残差连接.\n\n- 下面我们详细的介绍每一个 Building Block:\n\n## Attention: 3 different kinds\n![499](notes/2022/2022.4/assets/Pasted%20image%2020220429162339.png)\n- 在 Transformer 里面, 三个注意力层有着细微的差别:\n\n- **Encoder 里面的注意力层: 多头自注意力**\n\t- 并行地提取输出序列里面的信息\n\n- **Decoder 里面的注意力层 A: Mask 后的多头自注意力**\n\t- 因为预测是一步一步地进行的, 我们不能参考当前时间步以后的序列. 所以需要进行 Mask.\n\t- 但是它仍然是自注意力, 只不过每一次自注意力能使用的长度都在增加.\n\n- **Decoder 里面的注意力层 B: 常规的多头注意力**\n\t- 这一层接收 Encoder 的输出作为 Key-Value Pair, 而 Query 是上一层的输出. 所以它不属于自注意力了.\n\n- 可以看到所有的Attention层都使用了[多头注意力](notes/2022/2022.4/D2L-71-Multi-Head_Attention.md), 并且我们在能使用自注意力的地方都使用了自注意力, 与构建整个网络的Motivation相一致.\n\n## Position-wise Feed-Forward Networks\n- Position-wise FFN 就是一个两层的 MLP. \n- 之所以称它为\"Position-wise\" FFN, 是因为它只对输入的最后一个维度进行变换(`num_hiddens`, 每一个样本的特征维度). 也就是说它只在每一个样本内部进行特征转换, 不同位置的样本不会相互影响.\n![FFN|500](notes/2022/2022.4/assets/FFN.svg)\n```python\nclass PositionWiseFFN(nn.Module):\n    \"\"\"基于位置的前馈网络\"\"\"\n    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,\n                 **kwargs):\n        super(PositionWiseFFN, self).__init__(**kwargs)\n        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)\n        self.relu = nn.ReLU()\n        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)\n\n    def forward(self, X):\n        return self.dense2(self.relu(self.dense1(X)))\n```\n- 在 Transformer 里面, Position-wise FFN 通常先将特征维放大, 然后再转换回去:\n```python\n# D2L里面的例子\nffn_num_input, ffn_num_hiddens = 32, 64\n```\n原论文的说明\n![](notes/2022/2022.4/assets/Pasted%20image%2020220429170217.png)\n![FFN dimensions](notes/2022/2022.4/assets/FFN%20dimensions.svg)\n\n\n## Add \u0026 Norm\n- 我们之所以将这两个步骤抽象为一个组件, 是因为它们都是构建有效的深度架构的关键。\n### Residual Connection\n- 残差连接的思想来自于 [ResNet](notes/2022/2022.3/D2L-45-ResNet.md#Motivation), 它在保证网络的性能的同时使训练更加容易了.\n- 但是残差连接需要两个输入的形状相同, 这就要求 Attention Layer, FFN 都不能改变张量的形状.\n\n### Layer Norm\n- 我们之前学习过 [Batch_Normalization-批量归一化](notes/2022/2022.3/D2L-44-Batch_Normalization-批量归一化.md) 可以用来加速收敛, 我们自然也想把它用在 Transformer 里面.\n- 但是 Transformer 的输入是**长度变化**的序列: 不仅训练网络的时候长度是变化的, 预测的时候长度更是不确定的, 这会给 Batch Normalization 带来一些问题:\n\t- Batch Norm是按feature来进行归一化的, 也就是说, 它会将一个Batch里面的所有Sequence的同一个Feature抽出来进行归一化.\n\t- 但因为序列长度是变化的, 分配给每一个Sequence的\"份额\", 会受其他Sequence的影响: 要是有一个Sequence很短, 而其他Sequence都很长, 那么短的Sequence所有Features都会比较小, 这是不太合理的，如下图所示：\n\t![Layer Norn and Batch Norm](notes/2022/2022.4/assets/Layer%20Norn%20and%20Batch%20Norm.svg)\n- 而Layer Norm只对一个\"Layer\"里面的一个样本的进行归一化, 不同长度的样本之间不会相互影响.\n\n- 讲解见视频 `25:38` [Transformer论文逐段精读](https://www.bilibili.com/video/BV1pu411o7BE?t=1538.0)\n\n- 总之， 尽管Batch Norm在计算机视觉中被广泛应用，但在自然语言处理任务中（输入通常是变长序列）Batch Norm通常不如Layer Norm的效果好。[^3]\n\n### 实现\n- 值得注意的是, 我们还在最后进行了[Dropout](notes/2022/2022.2/D2L-23-Dropout-丢弃法.md)\n```python\nclass AddNorm(nn.Module):\n    \"\"\"残差连接后进行层规范化\"\"\"\n    def __init__(self, normalized_shape, dropout, **kwargs):\n        super(AddNorm, self).__init__(**kwargs)\n        self.dropout = nn.Dropout(dropout)\n        self.ln = nn.LayerNorm(normalized_shape)\n\n    def forward(self, X, Y):\n        return self.ln(self.dropout(Y) + X)\n```\n\n## Input Preprocessing: Positional Encoding \u0026 Embedding\n对于输入我们需要做两件事: \n- 首先需要把序列转化为向量: **Embedding**\n- 然后因为 [自注意力忽略了原始序列的位置信息](notes/2022/2022.4/D2L-72-Self-Attention.md#位置编码), 我们需要再把位置信息加回去: **Positional Encoding**\n\t- 在Positional Encoding里面还加入了Dropout\n\n### 实现\n- 因为利用三角函数实现的位置编码输出范围在-1 和 1 之间， 为了平衡 Positional Encoding 和 Embedding 的数量级， 我们需要先将 embedding 的结构乘上 $\\sqrt{d}$, 其中 $d$ 是 embedding 的维数.\n```python\n# Since positional encoding values are between -1 and 1, the embedding\n# values are multiplied by the square root of the embedding dimension\n# to rescale before they are summed up\nX = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n```\n\n- 注意 Positional Encoding \u0026 Embedding 在 Encoder 和 Decoder 里面都是有的。\n\n## Putting it all together\n![](notes/2022/2022.4/assets/transformer.ipynb)\n\n- 有了所有的 Building Blocks 之后， 我们便可以构建起整个 Transformer 模型了：\n- 下面的视频形象而直观的展现了 Transformer 模型的工作过程：\n\t- 输入序列先进入 Encoder，并行地进行 $n$ 次处理\n\t- 然后在 Decoder 中, 按照时间步顺序依次输出模型结果\n\t\t- 每一个 Decoder 块的输入都包括 Decoder 先前的输出和 Encoder 的输出两部分:![](notes/2022/2022.4/assets/The_transformer_encoder_decoder_stack.png)[^5]\n![](notes/2022/2022.4/assets/Attention%20Visualized.mp4) [^4]\n\nMore illustrations:[^5]\n![](notes/2022/2022.4/assets/transformer_resideual_layer_norm_3.png)\n![](notes/2022/2022.4/assets/transformer_decoding_2.gif)\n\n- 下面这个笔记本里面有一个很好的多头注意力可视化:\n\t- [Tensor2Tensor Intro - Colaboratory](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb)\n![](notes/2022/2022.4/assets/transformer_self-attention_visualization.png)\n![](notes/2022/2022.4/assets/Pasted%20image%2020220602115027.png)\n\n\n## Further Development\n- 尽管 Transformer 架构是为了“序列到序列”的学习而提出的，但在后续基于 Transformer 的模型中(比如 BERT)，Transformer **Encoder** 或 Transformer **Decoder** 通常被**单独**用于不同的深度学习任务中。\n\n\n\n\n[^1]: [10.7. Transformer — 动手学深度学习 2.0.0-beta0 documentation](https://zh-v2.d2l.ai/chapter_attention-mechanisms/transformer.html)\n[^2]: [10.7. Transformer — Dive into Deep Learning 0.17.5 documentation](https://d2l.ai/chapter_attention-mechanisms/transformer.html)\n[^3]: [10.7. Transformer — 动手学深度学习 2.0.0-beta0 documentation](https://zh-v2.d2l.ai/chapter_attention-mechanisms/transformer.html#id8)\n[^4]: [Google AI Blog: Transformer: A Novel Neural Network Architecture for Language Understanding](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)\n[^5]: [The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.](https://jalammar.github.io/illustrated-transformer/)","lastmodified":"2022-10-15T14:06:29.64250409Z","tags":null},"/notes/2022/2022.4/D2L-75-BERT":{"title":"D2L-75-BERT","content":"# Bidirectional Encoder Representations from Transformers (BERT)\n\u003cdiv align=\"right\"\u003e 2022-04-30\u003c/div\u003e\n\nTags: #BERT #Transformer #DeepLearning \n\n![](notes/2022/2022.4/assets/img_2022-10-15-2.gif)\n\n## Motivation\n### 构建一个通用的语言模型\n- 在计算机视觉领域中, 我们能对一个已经训练好的大型网络进行微调(Fine-tune), 以较小的计算成本和网络改动就能获得很好的模型.\n- BERT就是期望能够构建一个足够强大的预训练模型(Pretrained), 来适配各种各样的任务.\n![450](notes/2022/2022.4/assets/img_2022-10-15-11.png)\n### 结合两个现有架构的优点: ELMo \u0026 GPT\n### GPT: task-agnostic\n- 其实在 BERT 以前, OpenAI 已经提出了 GPT （Generative Pre-Training，生成式预训练）模型, 试图提供一种既考虑上下文语意(context-sensitive), 又能适配多任务(task-agnostic[^2])的模型\n- 但是 GPT 是基于 Transformer Decoder 的, 这就意味着它具备自回归模型(Auto-regressive)的性质: 只能\"向前看\"(从左到右).\n\n- 在 Fine-tune 的时候, GPT 不冻结任何参数, 所有原来的所有参数会跟着新的输出层一起训练\n### ELMo: Bi-directional\n - 尽管基于 BiLSTM 的 ELMo 模型能够很好的综合两个方向的语义信息, 但是 ELMo 是 task-specific 的, 无法适配多种任务.\n\n- 这就意味着我们 Pretrained ELMo 之后, 还需要为不同的 NLP 任务设计不同的后续架构, 这是很累的\n\n### BERT: Combining the Best of Both Worlds\n- BERT 基于 Transformer 的 Bidirectional Encoder, 既可以综合 Bidirectional 的上下文信息, 又可以适配多种模型(task-agnostic).\n![](notes/2022/2022.4/assets/elmo-gpt-bert.svg)\n- 并且 BERT 在 Fine-tune 的时候和 GPT 类似: 所有原来的所有参数会跟着新的输出层一起训练.\n\n![](notes/2022/2022.4/assets/BERT%20hang%20out.gif)\n\n## Model - Overview\n- 作为一个预训练模型, 训练 BERT 分为 Pre-train 和 Fine-tune 两部分.\n\t- 而且我们需要为 Pretrain 设计一个通用的训练任务, 适配多种应用场景.\n\t![](notes/2022/2022.4/assets/Pasted%20image%2020220430172119.png)\n\n- BERT 相当于一个只有 Encoder 的 Transformer.\n\t- 为了适配设计的\"通用任务\", 我们还需要对模型进行相应的改进 ,后面详述.\n\n- 作为预训练模型, BERT提供了不同规格的两个版本.\n```\nBase: \n\t#blocks = 12, hidden size = 768, #heads = 12, #parameters = 110M\nLarge: \n\t#blocks = 24, hidden size = 1024, #heads = 16, #parameter = 340M\n```\n\n## 预训练任务\n![Pretrain Tasks](notes/2022/2022.4/D2L-76-BERT%20-%20Pretrain.md#Pretrain%20Tasks)\n\n## Model - Detail\n![](notes/2022/2022.4/assets/bert.ipynb)\n### Masked Language Model 任务\n- 我们使用一个单隐层的 MLP 来将 Encoder 的输出转化为预测的单词标签(Vocab 里面的序号)\n```python\nclass MaskLM(nn.Module):\n    \"\"\"BERT的掩蔽语言模型任务\"\"\"\n    def __init__(self, vocab_size, num_hiddens, num_inputs=768, **kwargs):\n        super(MaskLM, self).__init__(**kwargs)\n        self.mlp = nn.Sequential(\n\t     nn.Linear(num_inputs, num_hiddens),\n\t\t nn.ReLU(),\n\t\t nn.LayerNorm(num_hiddens),\n\t\t nn.Linear(num_hiddens, vocab_size))\n\n    def forward(self, X, pred_positions):\n        num_pred_positions = pred_positions.shape[1] # 一共有多少个位置需要预测\n        pred_positions = pred_positions.reshape(-1)  # 全部排成一列\n        batch_size = X.shape[0] \n        batch_idx = torch.arange(0, batch_size) # batch的序号\n        # 假设batch_size=4，num_pred_positions=2\n        # 那么batch_idx是np.array（[0,0,1,1,2,2,3,3]）\n        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)\n        # 取出X里面需要预测的位置\n        masked_X = X[batch_idx, pred_positions]\n        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))\n        # 进行预测\n        mlm_Y_hat = self.mlp(masked_X)\n        return mlm_Y_hat\n```\n\n### Next Sentence Prediction 任务\n- 我们依然使用一个单隐层的 MLP 来处理 NSP 任务\n- 但是和 MLM 任务不同, BERT 在这一步只使用了每一个序列的 `\u003ccls\u003e` 标签, 这个标签在每个句子的开头, 可以使用 `encoded_X[:, 0, :]` 来提取出来\n![](notes/2022/2022.4/assets/bert-output-tensor%201.png)\n![](notes/2022/2022.4/assets/bert-output-tensor-selection.png)[^1]\n- `\u003ccls\u003e` 标签表示 Classification, 是 BERT 中专门用于句子分类的一个标签.\n\t- [What is purpose of the [CLS] token and why is its encoding output important? - Data Science Stack Exchange](https://datascience.stackexchange.com/questions/66207/what-is-purpose-of-the-cls-token-and-why-is-its-encoding-output-important )\n\t- [Why Bert transformer uses [CLS] token for classification instead of average over all tokens? - Data Science Stack Exchange](https://datascience.stackexchange.com/questions/77044/bert-transformer-why-bert-transformer-uses-cls-token-for-classification-inst?rq=1)\n\n```python\nclass NextSentencePred(nn.Module):\n    \"\"\"BERT的下一句预测任务\"\"\"\n\n    def __init__(self, num_inputs,  num_hiddens, **kwargs):\n        super(NextSentencePred, self).__init__(**kwargs)\n        print(\"num_inputs:\", num_inputs)\n\n        self.output = nn.Sequential(\n            nn.Linear(num_inputs, num_hiddens),\n            nn.Tanh(),\n            nn.Linear(num_hiddens, 2))\n\n    def forward(self, X):\n        # X的形状：(batchsize,num_hiddens)\n        return self.output(X)\n```\n\n### 完整的模型\n```python\nclass BERTModel(nn.Module):\n    \"\"\"BERT模型\"\"\"\n\n    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input, \n                 ffn_num_hiddens, num_heads, num_layers, dropout,\n                 max_len=1000,\n                 #  这里设置默认大小 768 有点误导, \n                 #  768是BERT base版本的hidden_size\n                 #  其实这里的维度要和前面的hidden_size对应起来,\n                 key_size=768,\n                 query_size=768,\n                 value_size=768,\n                 mlm_in_features=768,\n                 nsp_in_features=768):\n        super(BERTModel, self).__init__()\n        self.encoder = BERTEncoder(vocab_size, \n\t        num_hiddens, norm_shape,  ffn_num_input, ffn_num_hiddens,\n\t        num_heads, num_layers, dropout, max_len=max_len,\n\t        key_size=key_size, query_size=query_size, \n\t        value_size=value_size)\n        # mlm的hidden_size是768, 但是可以取不一样的\n        self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)\n        self.nsp = NextSentencePred(nsp_in_features, num_hiddens)\n\n    def forward(self, tokens, segments, \n\t\t    valid_lens=None, pred_positions=None):\n        encoded_X = self.encoder(tokens, segments, valid_lens)\n        if pred_positions is not None:\n            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n        else:\n            mlm_Y_hat = None\n        # 用于下一句预测的多层感知机分类器的隐藏层，0是“\u003ccls\u003e”标记的索引\n        nsp_Y_hat = self.nsp(encoded_X[:, 0, :])\n        return encoded_X, mlm_Y_hat, nsp_Y_hat\n```\n\n## Fine-tune\n[D2L-77-BERT - Fine-tune](notes/2022/2022.4/D2L-77-BERT%20-%20Fine-tune.md)\n\n![](notes/2022/2022.4/assets/BERT%20and%20ERNIE.gif)\n\n[^1]: [A Visual Guide to Using BERT for the First Time – Jay Alammar – Visualizing machine learning one concept at a time.](https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/)\n[^2]: Agonistic: someone who does not know, or believes that it is impossible to know, if a god exists 不可知论者（对神存在与否不能肯定或认为不可知）,","lastmodified":"2022-10-15T14:06:29.64250409Z","tags":null},"/notes/2022/2022.4/D2L-76-BERT-Pretrain":{"title":"D2L-76-BERT - Pretrain","content":"# BERT: Pretrain\n\n\u003cdiv align=\"right\"\u003e 2022-04-30\u003c/div\u003e\n\nTags: #BERT #Pretrain #DeepLearning #Transformer \n\n\n## Pretrain Tasks\n### Task 1 - Masked Language Modeling\n#### Motivation\n- [语言模型(Language Model)](notes/2022/2022.3/D2L-50-语言模型-传统模型的不足.md#语言模型) 在输出时是从左到右进行的, 使用左侧的上下文来预测未知词元。\n\n- 而 BERT 是双向地对文本进行编码的, 我们需要为模型保留两个方向的上下文. \n\t- 所以我们用一种类似\"完形填空\"的方式来训练模型: 随机 mask 掉序列里面的一些词元, 让模型使用双向上下文以类似自监督的方式预测掩蔽词元. \n\t- 这类任务称为**掩蔽语言建模 (模型)** : _masked language model_, MLM.\n\n#### 任务细节\n\n**预测哪些词?  预测多少词?**\n- 在 BERT 的训练任务中, 我们 **随机** 选择 **15%** 的词元进行预测\n\n**怎么处理输入?**\n- 对于随机选择的 15%:\n\t- **80%** 的概率: 替换为 `\u003cmask\u003e` \n\t\t- （例如，“this movie is **great**” 变为 “this movie is `\u003cmask\u003e` \")\n\t- **10%** 的概率: 替换为 `随机词元`\n\t\t- （例如，“this movie is **great**” 变为 “this movie is **drink**” ）\n\t- **10%** 的概率: `不做任何改动`\n\t\t- （例如，“this movie is **great**” 变为 “this movie is **great**” ）。\n\n**为什么我们要加入随机词元的 10%和不做任何改动的 10%?**\n- 因为在 Finetuning 的时候, 训练数据都是没有 `\u003cmask\u003e` 的语句, 这会导致 Pretrain 和 Finetune 的 mismatch.\n- 我们加入 **10%** 和 Finetune 里面一样的数据\n- 加入 **10%** 随机数据作为噪声, 让模型不会过多关注 `\u003cmask\u003e` 符号.\n\n### Task 2 - Next Sentence Prediction\n#### Motivation\n- 尽管 Masked Language Modeling 能够编码双向上下文来表示单个词元，但它不能显式地建模文本对(text pairs)之间的逻辑关系。[^1]\n\t- 为了帮助BERT理解两个文本序列之间的关系，我们设计了一个二分类任务:  **下一句预测** *(Next Sentence Prediction)* \n\n#### 任务细节\n- 在生成句子对时: \n\t- **50%** 的概率它们的确是连续的, 标签为\"True\"\n\t- **50%** 的概率第二个句子是从语料库中随机抽取的，标签为\"False\"\n\n## Data Representation\n详见[14.9. 用于预训练BERT的数据集 — 动手学深度学习 2.0.0-beta0 documentation](https://zh-v2.d2l.ai/chapter_natural-language-processing-pretraining/bert-dataset.html)\n## Start Pretrain\n详细的过程可以参见: [14.10. 预训练BERT — 动手学深度学习 2.0.0-beta0 documentation](https://zh-v2.d2l.ai/chapter_natural-language-processing-pretraining/bert-pretraining.html) , 这里说明几个需要注意的点:\n\n- BERT 预训练的最终损失是遮蔽语言模型损失和下一句预测损失的和。当然也可以是加权了的和.\n- 因为BERT模型是很大的, 用来训练BERT的数据集也是很大的, 所以我们通常指定入`num_steps`指定了训练的迭代步数，指定`num_epoch`\n-  在预训练BERT之后，我们可以用它来表示单个文本、文本对或其中的任何词元。\n-  在实验中，同一个词元在不同的上下文中具有不同的BERT表示。这支持BERT表示是**上下文敏感**的。\n\n![](notes/2022/2022.4/assets/img_2022-10-15-3.gif)\n\n[^1]: [14.8. Bidirectional Encoder Representations from Transformers (BERT) — Dive into Deep Learning 0.17.5 documentation](https://d2l.ai/chapter_natural-language-processing-pretraining/bert.html#next-sentence-prediction)","lastmodified":"2022-10-15T14:06:29.64250409Z","tags":null},"/notes/2022/2022.4/D2L-77-BERT-Fine-tune":{"title":"D2L-77-BERT - Fine-tune","content":"# BERT: Fine-tune\n\n\u003cdiv align=\"right\"\u003e 2022-04-30\u003c/div\u003e\n\nTags: #BERT #Fine-tune #DeepLearning #Transformer \n\n\n![](notes/2022/2022.4/assets/img_2022-10-15-4.gif)\n\n- 预训练好 BERT 以后, 我们只需要对模型进行很小的改动即可适配很多任务.\n- 在 Finetuning 的时候, 新增的输出部分是从头开始训练的, 而 BERT 主体部分是在 pre-train 的基础上进行训练的.\n\n## Single Text Classification\n![](notes/2022/2022.4/assets/bert-one-seq.svg)\n- 我们可以利用 `\u003ccls\u003e` tag来对一个语句序列进行分类, 只需要添加一个新的全连接层就好了\n\n## Text Pair Classification or Regression\n![](notes/2022/2022.4/assets/bert-two-seqs.svg)\n- 语句对的分类/回归问题依然只需要添加一个全连接层来对 `\u003ccls\u003e` 标签进行分类. 它和上一个应用的唯一区别就是输入序列的形式不同.\n- 有时我们还会对损失函数进行更改, 比如对于回归任务, 我们可以改用[Mean_Squared_Error_均方误差](notes/2021/2021.8/Mean_Squared_Error_均方误差.md)\n\n## Text Tagging\n![](notes/2022/2022.4/assets/bert-tagging.svg)\n- 文本标注则将Token的输出作为全连接的输入, 尽管有多个Token, 我们仍使用同一个全连接层.\n\n## Question Answering\n![](notes/2022/2022.4/assets/bert-qa.svg)\n- 问答的输入由 Question 和 Passage 组成\n- 在问答任务中, 模型需要在 Passage 里面找到答案的起始位置. 我们的做法是用两个全连接层分别进行\"答案起始位置\"和\"答案结束位置\"的预测:  评估 Passage 里面每一个词元作为起始/结束位置的概率, 选择概率最高的片段作为输出.\n\n## 实例\n[15.7. 自然语言推断：微调BERT — 动手学深度学习 2.0.0-beta0 documentation](https://zh-v2.d2l.ai/chapter_natural-language-processing-applications/natural-language-inference-bert.html)","lastmodified":"2022-10-15T14:06:29.64250409Z","tags":null},"/notes/2022/2022.4/Difference-between-Purple-and-Violet":{"title":"Difference between Purple and Violet","content":"# Difference between Purple and Violet\n\n\u003cdiv align=\"right\"\u003e 2022-04-23\u003c/div\u003e\n\nTags: #Purple #Violet #English #Color\n\n- Purple是复色光(Blue and Red 1:1)激发的颜色, 而Violet是单色光激发的颜色 \n- 因而看起来Purple要Red一点, 饱和度要高一点.\n \n![](notes/2022/2022.4/assets/img_2022-10-15-12.png)\n\n- **In depth reading**: [Difference between ‘violet’ and ‘purple’](https://jakubmarian.com/difference-between-violet-and-purple/)\n\n\n\n\n","lastmodified":"2022-10-15T14:06:29.64250409Z","tags":null},"/notes/2022/2022.4/Get-ahead-of-oneself":{"title":"Get ahead of oneself","content":"## **get ahead of yourself**\n\nto do something too early, or before you are ready or prepared\n\n- That last game suggests that we have been getting ahead of ourselves in praising the team's progress.\n- She didn't want to get ahead of herself and risk losing what she had achieved so far.\n","lastmodified":"2022-10-15T14:06:29.64250409Z","tags":null},"/notes/2022/2022.4/Indexing-a-tensor-or-ndarray-with-None":{"title":"Indexing a tensor or ndarray with `None`","content":"#  `None` as index \n\n\u003cdiv align=\"right\"\u003e 2022-04-20\u003c/div\u003e\n\nTags: #Numpy #PyTorch \n\n## `None` in index is equivalent to `unsqueeze()`\nSimilar to NumPy you can insert a singleton dimension (_\"unsqueeze\"_ a dimension) by indexing this dimension with `None`. In turn `n[:, None]` will have the effect of inserting a new dimension on `dim=1`. This is equivalent to `n.unsqueeze(dim=1)`:\n\n```\n\u003e\u003e\u003e n = torch.rand(3, 100, 100)\n\n\u003e\u003e\u003e n[:, None].shape\n(3, 1, 100, 100)\n\n\u003e\u003e\u003e n.unsqueeze(1).shape\n(3, 1, 100, 100)\n```\n\n## Some other types of _`None` indexings_.\nIn the example above `:` is was used as a placeholder to designate the first dimension `dim=0`. If you want to insert a dimension on `dim=2`, you can add a second `:` as `n[:, :, None]`.\n\nYou can also _place_ `None` with respect to the last dimension instead. To do so you can use the [ellipsis](https://python-reference.readthedocs.io/en/latest/docs/brackets/ellipsis.html) syntax `...`:\n\n-   `n[..., None]` will insert a dimension last, _i.e._ `n.unsqueeze(dim=-1)`.\n    \n-   `n[..., None, :]` on the before last dimension, _i.e._ `n.unsqueeze(dim=-2)`.\n\n## `None` is slower than `unsqueeze()`\n`None` is a version with advanced indexing , which might be a bit slower because it has more checking to do to find out exactly what you want to do.\n\n---\nSources:\n- [syntax - Indexing a tensor with None in PyTorch - Stack Overflow](https://stackoverflow.com/a/69797906/15893958)\n- [What is the difference between [None, ...] and unsqueeze? - PyTorch Forums](https://discuss.pytorch.org/t/what-is-the-difference-between-none-and-unsqueeze/28451)","lastmodified":"2022-10-15T14:06:29.64250409Z","tags":null},"/notes/2022/2022.4/Latex-White-Spaces":{"title":"Latex White Spaces","content":"# Latex 里面的空格\n\n\u003cdiv align=\"right\"\u003e 2022-04-03\u003c/div\u003e\n\nTags: #Latex \n\n[There are a number of horizontal spacing macros for LaTeX](https://tex.stackexchange.com/a/74354/267634) :\n1.  `\\,` inserts a `.16667em` space in text mode, or `\\thinmuskip` (equivalent to `3mu`) in math mode; there's an equivalent `\\thinspace` macro;\n\n3.  `\\\u003e` (or `\\:`) inserts a `.2222em` space in text mode, or `\\medmuskip` (equivalent to `4.0mu plus 2.0mu minus 4.0mu`) in math mode; there's an equivalent `\\medspace`;\n4.  `\\negmedspace` is the _negative_ equivalent to `\\medspace`;\n5.  `\\;` inserts a `.2777em` space in text mode, or `\\thickmuskip` (equivalent to `5.0mu plus 5.0mu`) in math mode; there's an equivalent `\\thickspace`;\n\n7.  `\\enspace` inserts a space of `.5em` in text or math mode;\n8.  `\\quad` inserts a space of `1em` in text or math mode;\n9.  `\\qquad` inserts a space of `2em` in text or math mode;\n\n\n12.  `\\hspace{\u003clen\u003e}` ==inserts a space of length== `\u003clen\u003e` (may be negative) in math or text mode (a LaTeX `\\hskip`); ==得带单位==\n\n","lastmodified":"2022-10-15T14:06:29.64250409Z","tags":null},"/notes/2022/2022.4/RNN%E4%B8%ADoutput%E5%92%8Chidden_state%E7%9A%84%E5%8C%BA%E5%88%AB":{"title":"RNN中output和hidden_state的区别","content":"# Difference between `output` and `hidden_state` in RNN\n\n\u003cdiv align=\"right\"\u003e 2022-04-22\u003c/div\u003e\n\nTags: #RNN \n\n- 首先要将RNN理解为一个二维的网络, 它不仅可能有多个隐藏层, 还在时间维度上有多个时间步.\n\n![RNN in detail](notes/2022/2022.4/assets/RNN%20in%20detail.svg)\n\n- `output`是 **最后一层隐藏层** 在 *每一个时间步* 的状态\n- `hidden_state` 是 **最后一个时间步** *所有隐藏层* 的状态\n\n- `output` 常常被用作Encoder-Decoder架构里面Attention的输入\n- `hidden_state` 常常在Encoder-Decoder架构里面被用来初始化Decoder隐藏状态\n\n## Bidirectional Case\n\u003e 1.  The `output` will give you the hidden layer outputs of the network for each time-step, _but only for the final layer_. This is useful in many applications, particularly encoder-decoders using attention. (These architectures build up a 'context' layer from all the hidden outputs, and it is extremely useful to have them sitting around as a self-contained unit.)\n\u003e     \n\u003e 2.  The `h_n` will give you the hidden layer outputs for the last time-step only, but for all the layers. Therefore, if and only if you have a single layer architecture, `h_n` is a strict subset of `output`. Otherwise, `output` and `h_n` intersect, but are not strict subsets of one another. (You will often want these, in an encoder-decoder model, from the encoder in order to jumpstart the decoder.)\n\u003e     \n\u003e 3.  **If you are using a bidirectional output** and you want to actually verify that part of `h_n` is contained in `output` (and vice-versa) you need to understand what PyTorch does behind the scenes in the organization of the inputs and outputs. Specifically, it concatenates a time-reversed input with the time-forward input and runs them together. This is literal. This means that the 'forward' output at time T is in the final position of the `output` tensor sitting right next to the 'reverse' output at time 0; if you're looking for the 'reverse' output at time T, it is in the first position.[^1]\n\n\n\n\n[^1]: [machine learning - Is hidden and output the same for a GRU unit in Pytorch? - Stack Overflow](https://stackoverflow.com/a/61195982/15893958)","lastmodified":"2022-10-15T14:06:29.64250409Z","tags":null},"/notes/2022/2022.4/Variant-or-Variance":{"title":"Variant or Variance","content":"# Variant or Variance\n\n\u003cdiv align=\"right\"\u003e 2022-04-19\u003c/div\u003e\n\nTags: #English\n\nThey are pretty much the **same**.\n\n\u003e Very strictly speaking, variation is change, and a variant is one of the forms resulting from the change.\n\u003e \n\u003e The use of **variation** to mean **variant** is so common, though, that only a hardcore pedant would ever even recognize a difference in that context, much less say either one is incorrect.[^1]\n\n\n[^1]: [word choice - What is the difference between a variant and a variation? - English Language \u0026 Usage Stack Exchange](https://english.stackexchange.com/a/148805)","lastmodified":"2022-10-15T14:06:29.64250409Z","tags":null},"/notes/2022/2022.4/Viterbi-Algorithm":{"title":"Viterbi Algorithm","content":"# 维特比算法\n\n\u003cdiv align=\"right\"\u003e 2022-04-19\u003c/div\u003e\n\nTags: #Viterbi\n\n**Viterbi 算法**是一个用于求解最佳路径的动态规划算法。\n\n- Viterbi 算法常常用于HMM模型里面，用于寻找最有可能产生观测事件序列的**维特比路径**——隐含状态序列\n\n- 下面这个回答解释的很清楚了： \n\t如何通俗地讲解 Viterbi 算法？ https://www.zhihu.com/question/20136144/answer/763021768","lastmodified":"2022-10-15T14:06:29.64250409Z","tags":null},"/notes/2022/2022.5/%E4%BB%8E%E4%BA%8C%E9%A1%B9%E5%88%86%E5%B8%83%E5%88%B0%E6%B3%8A%E6%9D%BE%E5%88%86%E5%B8%83%E5%86%8D%E5%88%B0%E6%8C%87%E6%95%B0%E5%88%86%E5%B8%83-From-Binomial-Distribution-to-Poisson-Distribution-to-Exponential-Distribution":{"title":"从二项分布到泊松分布再到指数分布-From Binomial Distribution to Poisson Distribution to Exponential Distribution","content":"# From Binomial Distribution to Poisson Distribution to Exponential Distribution\n\n\u003cdiv align=\"right\"\u003e 2022-05-22\u003c/div\u003e\n\nTags: #Math/Probability #PoissonDistribution #BinomialDistribution #ExponentialDistribution\n\n- 这两个回答讲的挺好: \n\t- 泊松分布的现实意义是什么，为什么现实生活多数服从于泊松分布？ - 马同学的回答 - 知乎 https://www.zhihu.com/question/26441147/answer/429569625\n\t- 指数分布公式的含义是什么？ - 马同学的回答 - 知乎 https://www.zhihu.com/question/24796044/answer/673838656\n\n### Binomial Distribution\n- 对于概率为 $p$ 的事件, $n$ 次试验里面发生 $k$ 次的概率为:\n$$\\left(\\begin{array}{l}\nn \\\\\nk\n\\end{array}\\right) p^{k}(1-p)^{n-k}$$\n### Poisson Distribution\n- 在 $n\\rightarrow \\infty$ 的时候, *Binominal Distribution* 等价于期望相同的 *Poisson Distribution*: \n$$\\lim _{n \\rightarrow \\infty}\\left(\\begin{array}{l}\nn \\\\\nk\n\\end{array}\\right) p^{k}(1-p)^{n-k}$$\n- 二项分布的期望为: $E(X)=np$, 泊松分布的期望为: $\\lambda$. 因为期望 $E(X)$ 相同: \n\t$$p=\\frac{\\lambda}{n}$$\n- 带入上式, 得到:\n\t$$\\lim _{n \\rightarrow \\infty}\\left(\\begin{array}{l}\nn \\\\\nk\n\\end{array}\\right) p^{k}(1-p)^{n-k}=\\lim _{n \\rightarrow \\infty}\\left(\\begin{array}{l}\nn \\\\\nk\n\\end{array}\\right)\\left(\\frac{\\lambda}{n}\\right)^{k}\\left(1-\\frac{\\lambda}{n}\\right)^{n-k}$$\n- 求解这个极限:\n$$\\begin{aligned}\n\\lim _{n \\rightarrow \\infty}\\left(\\begin{array}{l}\nn \\\\\nk\n\\end{array}\\right)\\left(\\frac{\\lambda}{n}\\right)^{k}\\left(1-\\frac{\\lambda}{n}\\right)^{n-k} \u0026=\\lim _{n \\rightarrow \\infty} \\frac{n(n-1)(n-2) \\cdots(n-k+1)}{k !} \\frac{\\lambda^{k}}{n^{k}}\\left(1-\\frac{\\lambda}{n}\\right)^{n-k} \\\\\n\u0026=\\lim _{n \\rightarrow \\infty} \\frac{\\lambda^{k}}{k !} \\frac{n}{n} \\cdot \\frac{n-1}{n} \\cdots \\frac{n-k+1}{n}\\left(1-\\frac{\\lambda}{n}\\right)^{-k}\\left(1-\\frac{\\lambda}{n}\\right)^{n}\n\\end{aligned}$$\n\n- 其中:\n\t$$\\begin{gathered}\n\\lim _{n \\rightarrow \\infty} \\frac{n}{n} \\cdot \\frac{n-1}{n} \\cdots \\frac{n-k+1}{n}\\left(1-\\frac{\\lambda}{n}\\right)^{-k}=1 \\\\\n\\lim _{n \\rightarrow \\infty}\\left(1-\\frac{\\lambda}{n}\\right)^{n}=e^{-\\lambda}\n\\end{gathered}$$\n- 所以\n\t$$\\lim _{n \\rightarrow \\infty}\\left(\\begin{array}{l}\n\tn \\\\\n\tk\n\t\\end{array}\\right)\\left(\\frac{\\lambda}{n}\\right)^{k}\\left(1-\\frac{\\lambda}{n}\\right)^{n-k}=\\frac{\\lambda^{k}\\ e^{-\\lambda}}{k !} $$\n也就是 Poisson Distribution 的概率密度函数\n\n- 在 $n$ 很大, 也就是 $p$ 很小的时候, 两者大概相似:\n![](notes/2022/2022.5/assets/img_2022-10-15.jpg)\n\n### Exponential Distribution\n- The waiting times for *Poisson distribution* is an *exponential distribution* with parameter $\\lambda$.\n\t- 服从泊松分布的事件之间的时间差服从指数分布\n[Relationship between Poisson and exponential distribution - Cross Validated](https://stats.stackexchange.com/a/2094/354372)\n\nI will use the following notation to be as consistent as possible with the wiki (in case you want to go back and forth between my answer and the wiki definitions for the [poisson](http://en.wikipedia.org/wiki/Poisson_distribution) and [exponential](http://en.wikipedia.org/wiki/Exponential_distribution).)\n\n- $N_t$: the number of arrivals during time period $t$\n- $X_t$: the time it takes for one additional arrival to arrive assuming that someone arrived at time $t$\n\nBy definition, the following conditions are equivalent:\n$$(X_t\u003ex)≡(N_t=N_t+x)$$\n- The event on the left captures the event that no one has arrived in the time interval $[t,t+x]$ which implies that our count of the number of arrivals at time $t+x$ is identical to the count at time $t$which is the event on the right.\n\n- By the complement rule, we also have:\n$$P(X_t≤x)=1−P(X_t\u003ex)$$\n- Using the equivalence of the two events that we described above, we can re-write the above as:\n$$P(X_t≤x)=1−P(N_{t+x}−N_t=0)$$\n- But,\n$$P(N_{t+x}−N_t=0)=P(N_x=0)$$\n- Using the poisson pmf the above where $λ$ is the average number of arrivals per time unit and $x$ a quantity of time units, simplifies to:\n$$P(N_{t+x}−N_t=0)=\\frac{(λx)^0}{0!}e^{−λx}$$\n- i.e.\n$$P(N_{t+x}−N_t=0)=e^{−λx}$$\n- Substituting in our original eqn, we have:\n$$P(X_t≤x)=1−e^{−λx}$$\n- The above is the cdf of a exponential pdf.","lastmodified":"2022-10-15T14:06:29.774505512Z","tags":null},"/notes/2022/2022.5/%E7%A6%BB%E5%B2%B8%E4%BA%BA%E6%B0%91%E5%B8%81%E4%B8%8E%E5%9C%A8%E5%B2%B8%E4%BA%BA%E6%B0%91%E5%B8%81":{"title":"离岸人民币与在岸人民币","content":"# 汇率计算中两种不同的人民币\n\n\u003cdiv align=\"right\"\u003e 2022-05-03\u003c/div\u003e\n\nTags: #ExchangeRate #CHY\n\n其实简单来说就是，在岸价是在**国内**换汇的汇率，离岸价是在**国外**换汇的汇率。\n\n## 离岸人民币(CNH)\n- **离岸**——在中国**境外**经营人民币业务(CHY offshore)。央行开放香港以及其他国家进行人民币交易的汇率就叫离岸人民币，而2010年中国香港实施的人民币离岸交易(CNH)已经是泛指海外离岸人民币交易。\n\n- 目前主要的人民币离岸市场在香港，新加坡、伦敦、台湾也在积极发展人民币离岸市场。一般来说，国外希望人民币大幅升值，这样有利于打开中国市场，但国内希望增加出口，所以离岸汇率高于在岸汇率。\n\n## 在岸人民币(CHY)\n- 在岸——在国内经营的人民币业务。央行授权中国外汇中心于每个工作日上午对外公布当日人民币兑换美元、欧元、日元、港币汇率的中间价作为当日银行间即期外汇市场以及银行柜台交易汇率的参考价格，这就叫在岸人民币。\n\n## 区别\n- 在岸人民币市场发展的时间比较长、规模比较大、受到的管制也比较多，在岸汇率受到央妈政策影响比较大。而离岸则相反，他的汇率变化主要是受到国际因素影响比较多，通常离岸人民币市场更能充分反映市场对人民币供需。\n\n![](notes/2022/2022.5/assets/img_2022-10-15.png)\n\n[离岸人民币、在岸人民币和人民币中间价的区别，你分得清楚吗？ — 搜航网](https://www.sofreight.com/news_57103.html)\n[CNY vs CNH - Differences Between Two Types of Renminbi - Wise, formerly TransferWise](https://wise.com/us/blog/cny-vs-cnh)","lastmodified":"2022-10-15T14:06:29.774505512Z","tags":null},"/notes/2022/2022.5/%E7%AB%AF%E5%88%B0%E7%AB%AF%E5%AD%A6%E4%B9%A0-End_to_End_Learning-E2E":{"title":"端到端学习-End_to_End_Learning-E2E","content":"# 端到端学习\n\n\u003cdiv align=\"right\"\u003e 2022-05-05\u003c/div\u003e\n\nTags: #EndtoEndLearning #MachineLearning #DeepLearning \n\n- 端到端的学习就是省略中间步骤，直接从输入得到输出结果。\n\n![End2End](notes/2022/2022.5/assets/End2End.svg)\n\n## Pro \u0026 Con\n### Pro\n- 不用人为设计中间步骤, 减少了工作量, \n- 并且避免人为添加的步骤给模型带来不好的[归纳偏置](notes/2022/2022.2/归纳偏置-Inductive%20bias%20-%20learning%20bias.md).\n\n### Con\n- 需要足够多的数据才能达到较好的效果\n- 有时候人为的步骤是可以帮助模型进行学习的\n\n- [What is end-to-end deep learning? (C3W2L09) - YouTube](https://www.youtube.com/watch?v=ImUoubi_t7s)\n- [Whether to Use End-To-End Deep Learning (C3W2L10) - YouTube](https://www.youtube.com/watch?v=l_-CUyEx_x4)\n\n\n","lastmodified":"2022-10-15T14:06:29.774505512Z","tags":null},"/notes/2022/2022.5/Cross_Entropy_Loss_Input_Format-%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%BE%93%E5%85%A5%E6%A0%BC%E5%BC%8F":{"title":"Cross_Entropy_Loss_Input_Format-交叉熵损失函数输入格式","content":"## PyTorch\n标签不需要变成独热编码:\n\n![](notes/2022/2022.5/assets/img_2022-10-15-1.png)\n\n\n## Keras\nKears有两种形式:\n-   Categorical Cross Entropy [Doc](https://keras.io/api/losses/probabilistic_losses/#categoricalcrossentropy-class):\n\n\u003e Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided in a one_hot representation.\n\n```python\n\u003e\u003e\u003e y_true = [[0, 1, 0], [0, 0, 1]]\n\u003e\u003e\u003e y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n\u003e\u003e\u003e # Using 'auto'/'sum_over_batch_size' reduction type.  \n\u003e\u003e\u003e cce = tf.keras.losses.CategoricalCrossentropy()\n\u003e\u003e\u003e cce(y_true, y_pred).numpy()\n1.177\n```\n\n-   Sparse Categorical Cross Entropy [Doc](https://keras.io/api/losses/probabilistic_losses/#sparsecategoricalcrossentropy-class):\n\n\u003e Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided as integers.\n\n```python\n\u003e\u003e\u003e y_true = [1, 2]\n\u003e\u003e\u003e y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n\u003e\u003e\u003e # Using 'auto'/'sum_over_batch_size' reduction type.  \n\u003e\u003e\u003e scce = tf.keras.losses.SparseCategoricalCrossentropy()\n\u003e\u003e\u003e scce(y_true, y_pred).numpy()\n1.177\n```\n\n\n[python - What is the difference between sparse_categorical_crossentropy and categorical_crossentropy? - Stack Overflow](https://stackoverflow.com/a/68617676/15893958)","lastmodified":"2022-10-15T14:06:29.770505469Z","tags":null},"/notes/2022/2022.5/F1_Score":{"title":"F1_Score","content":"# F1 Score\n\n\u003cdiv align=\"right\"\u003e 2022-05-05\u003c/div\u003e\n\nTags: #F1Score \n\n\n- The traditional *F-measure* or *balanced F-score* (*F\u003csub\u003e1\u003c/sub\u003e score*) is the [Harmonic_Mean-调和平均数](notes/2022/2022.5/Harmonic_Mean-调和平均数.md) of [Precision Recall](notes/2022/2022.5/Precision%20Recall%20and%20Accuracy.md):\n\n$$F_{1}=\\frac{2}{\\text { recall }^{-1}+\\text { precision }^{-1}}=2 \\cdot \\frac{\\text { precision } \\cdot \\text { recall }}{\\text { precision }+\\text { recall }}=\\frac{\\text { tp }}{t p+\\frac{1}{2}(\\mathrm{fp}+\\mathrm{fn})}$$\n\n## 直观解读 F1 score\n![直观理解](notes/2022/2022.5/Harmonic_Mean-调和平均数.md#直观理解)","lastmodified":"2022-10-15T14:06:29.770505469Z","tags":null},"/notes/2022/2022.5/GeoGebra-Embed":{"title":"GeoGebra Embed","content":"```html\n\u003ciframe src=\"https://www.geogebra.org/calculator/jzuwutfr?embed\" width=\"800\" height=\"600\" allowfullscreen style=\"border: 1px solid #e4e4e4;border-radius: 4px;\" frameborder=\"0\"\u003e\u003c/iframe\u003e\n```\n\n- 只需要在链接后添加 `?embed` 即可\n\n- 修改页面效果的设置在这里:\n![](notes/2022/2022.5/assets/img_2022-10-15-2.png)\n![](notes/2022/2022.5/assets/img_2022-10-15-3.png)\n![](notes/2022/2022.5/assets/img_2022-10-15-4.png)\n\n![](notes/2022/2022.5/assets/img_2022-10-15-5.png)\n","lastmodified":"2022-10-15T14:06:29.770505469Z","tags":null},"/notes/2022/2022.5/Harmonic_Mean-%E8%B0%83%E5%92%8C%E5%B9%B3%E5%9D%87%E6%95%B0":{"title":"Harmonic_Mean-调和平均数","content":"# Harmonic Mean\n\n\u003cdiv align=\"right\"\u003e 2022-05-03\u003c/div\u003e\n\nTags: #HarmonicMean #Math \n\n用 $H$ 表示两个数的调和平均数, 则:\n$$\\frac{1}{H}=\\frac{1}{2}\\left(\\frac{1}{x_{1}}+\\frac{1}{x_{2}}\\right)$$\n显式地表示为:\n$$H=\\frac{2 x_{1} x_{2}}{x_{1}+x_{2}}$$\n\n## 直观理解\n用紫色线段 $H$ 表示 $a, b$ 的*Harmonic Mean*:\n其中: \n- $$A=\\frac{a+b}{2}$$为两数的算数平均数(*Arithmetic Mean*)\n- $$G=\\sqrt{ab}$$为两数的几何平均数(*Geometric Mean*)\n\n\n![](notes/2022/2022.5/assets/MathematicalMeans.svg)\n\n- 在 $a+b$ 保持不变的情况下变化 $a,b$ 之前的比例, 可以看到 $H$ 在 $a=b$ 的时候取得最大值\n\n- 可视化这种变化关系如下:\n$$H=\\frac{2 x_{1} x_{2}}{x_{1}+x_{2}}$$\n\u003ciframe src=\"https://www.geogebra.org/calculator/jzuwutfr?embed\" width=\"800\" height=\"600\" allowfullscreen style=\"border: 1px solid #e4e4e4;border-radius: 4px;\" frameborder=\"0\"\u003e\u003c/iframe\u003e\n\n\n![](notes/2022/2022.5/assets/Harmonic_mean_3D_plot_from_0_to_100.png)\n","lastmodified":"2022-10-15T14:06:29.770505469Z","tags":null},"/notes/2022/2022.5/Jupyter_Notebook-%E4%BD%BF%E7%94%A8%E4%BB%A3%E7%90%86":{"title":"Jupyter_Notebook-使用代理","content":"# 如何在Jupyter Notebook里面使用代理\n\n\u003cdiv align=\"right\"\u003e 2022-05-18\u003c/div\u003e\n\nTags: #Jupyter #Python #Proxy\n\n## 方案一: 使用 IPython 启动脚本\n- 根据 [这个回答](https://stackoverflow.com/a/36884552/15893958) 以及下面评论, IPython 可以在启动之前运行一些代码. 我们可以借助这个功能来为 Jupyter Notebook 设置代理\n- Windows 的启动脚本默认在以下目录:\n\t- `C:\\Users\\[用户名]\\.ipython\\profile_default\\startup`\n\n1. 阅读目录里面的 Readme 查看使用指南\n2. 创建新的 python 文件, 输入以下内容: \n\t```python\n\timport os\n\tos.environ['http_proxy'] = \"http://127.0.0.1:7890\"\n\tos.environ['https_proxy'] = \"http://127.0.0.1:7890\"\n\t```\n\t- 注意需要根据自己的配置修改相应的代理地址.\n\t- 添加后如图所示: \n\t\t![](notes/2022/2022.5/assets/img_2022-10-15-6.png)\n3. 以后在启动 Jupyter Notebook 时, 上面的脚本都会自动运行, 所有的 Jupyter Notebook 都会有代理后的网络连接.\n\n- **优点:** 不用为每一个笔记本单独设置代理, 更加方便\n- **缺点:** 所有笔记本都被代理了, 这也意味着如果不开代理软件, 笔记本的网络连接会出现问题:\n\t- ![](notes/2022/2022.5/assets/img_2022-10-15-7.png)\n\t- **对策:** 运行以下代码取消代理 \n```python\nimport os\nos.environ['http_proxy'] = \"\"\nos.environ['https_proxy'] = \"\"\n```\n\n## 方案二: 为某个笔记本单独设置代理\n如果只想为某个笔记本单独设置代理, 可以运行如下单元格:\n```python\nimport os\nos.environ['http_proxy'] = \"http://127.0.0.1:7890\"\nos.environ['https_proxy'] = \"http://127.0.0.1:7890\"\n```\n\n- 这个方案和方案一是互补的.\n\n## 方案三: 在使用了代理的终端中启动Jupyter Notebook\n- **缺点:** 只能在网页版本的Jupyter Notebook中使用代理, 无法在VScode 启动的笔记本中使用代理.\n\n\n\n\n\n\n\n\n\n","lastmodified":"2022-10-15T14:06:29.770505469Z","tags":null},"/notes/2022/2022.5/Precision-Recall-and-Accuracy":{"title":"Precision Recall and Accuracy","content":"# Precision, Recall \u0026 Accuracy\n\n\u003cdiv align=\"right\"\u003e 2022-05-05\u003c/div\u003e\n\nTags: #Precision #Recall\n\n- 圈圈是模型的预测: 圈圈里面是预测的 Positive, 圈圈外面是预测的 Negative\n- 方方是真实的情况: 左半边是真实的 Positive(实心点点), 右半边是真实的 Negative(空心点点).\n![](notes/2022/2022.5/assets/Precisionrecall.svg) [^1]\n- Precision 是预测的 Positive 有多少是对的: 即查准率.\n- Recall 是所有 Positive 里面你找出来了多少: 即查全率.\n- Accuracy 则是预测的正例负例一起的准确度\n![](notes/2022/2022.5/assets/Acc%20Pre%20Rec%20F1.png)\n\n\n\n\n\n[^1]: Wikipedia [https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg](https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg)\n","lastmodified":"2022-10-15T14:06:29.770505469Z","tags":null},"/notes/2022/2022.6/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-%E5%91%A8%E6%9C%9F%E6%80%A7%E6%95%B0%E6%8D%AE%E6%97%B6%E9%97%B4%E7%AD%89":{"title":"数据预处理-周期性数据(时间等)","content":"# 周期性数据的预处理\n\n\u003cdiv align=\"right\"\u003e 2022-06-14\u003c/div\u003e\n\nTags: #MachineLearning #DataPreprocessing #CyclicFeatureEncoding\n\nSource: [Three Approaches to Encoding Time Information as Features for ML Models | NVIDIA Technical Blog](https://developer.nvidia.com/blog/three-approaches-to-encoding-time-information-as-features-for-ml-models/)\n\n**三种方法:**\n- Dummy Variable ([One-hot_Encoding-独热编码](notes/2022/2022.1/One-hot_Encoding-独热编码.md))\n- cyclical encoding with sine/cosine transformation\n- radial basis functions (径向基函数)\n\t![](notes/2022/2022.6/assets/img_2022-10-15.png)\n","lastmodified":"2022-10-15T14:06:29.774505512Z","tags":null},"/notes/2022/2022.6/Imbalanced-Data-%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%9D%87%E8%A1%A1":{"title":"Imbalanced Data - 数据不均衡","content":"# 数据不均衡\n\n\u003cdiv align=\"right\"\u003e 2022-06-07\u003c/div\u003e\n\nTags: #DataPreprocessing \n\n## [什么是数据不均衡](https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data)\n- A classification data set with skewed class proportions is called **imbalanced**. Classes that make up a large proportion of the data set are called **majority classes**. Those that make up a smaller proportion are **minority classes**.\n\nWhat counts as imbalanced? The answer could range from mild to extreme, as the table below shows.\n\n| Degree of imbalance | Proportion of Minority Class |\n| ------ | ------ | \n| Mild |   20-40% of the data set| \n| Moderate |   1-20% of the data set| \n| Extreme |   \u003c1% of the data set| \n","lastmodified":"2022-10-15T14:06:29.774505512Z","tags":null},"/notes/2022/2022.6/JS%E6%95%A3%E5%BA%A6":{"title":"JS散度","content":"# Jensen–Shannon divergence\n\n\u003cdiv align=\"right\"\u003e 2022-06-08\u003c/div\u003e\n\nTags: #JSDivergence \n\n- KL 散度是不对称的, 这使得在训练过程中可能出现一些问题，所以我们在 KL 散度基础上引入 JS 散度\n\n## 定义\n- The Jensen–Shannon divergence (JSD) $M_+^1( A ) × M_+^1(A) → [ 0 , ∞ )$ [^1]is a **symmetrized and smoothed version** of the [Kullback–Leibler divergence](notes/2022/2022.2/KL_Divergence-KL散度.md) $D ( P ∥ Q )$ . \n- It is defined by \n$$\n\\operatorname{JSD}(P \\| Q)=\\frac{1}{2} D(P \\| M)+\\frac{1}{2} D(Q \\| M)\n$$\n\twhere $M=\\frac{1}{2}(P+Q)$.\n\n\n\n\n\n\n[^1]: 看不懂这个符号: Consider the set $M_+^1( A )$ of probability distributions where $A$ is a set provided with some σ-algebra of measurable subsets. In particular we can take $A$  to be a finite or countable set with all subsets being measurable. [Jensen–Shannon divergence - Wikipedia](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence)","lastmodified":"2022-10-15T14:06:29.774505512Z","tags":null},"/notes/2022/2022.6/Un-is-not-always-negative-Ravel-Unravel":{"title":"Un- is not always negative - Ravel \u0026 Unravel","content":"# Ravel 和 Unravel 有相同的意思\n\n\u003cdiv align=\"right\"\u003e 2022-06-13\u003c/div\u003e\n\nTags: #English \n\n[When 'Un-' Isn't Negative | Merriam-Webster](https://www.merriam-webster.com/words-at-play/when-un-isnt-negative)\n\n\u003e These are three instances in which a verb beginning with _un-_ means the same as, rather than the negative or opposite of, its stem.\n\u003e \n\u003e And while it might seem frustratingly illogical when you are used to regarding un- strictly as a negative prefix, this use of _un-_ isn't as irrational as it looks.\n\u003e \n\u003e In all three cases—_unthaw_, _unloosen_, and _unravel_—the _un-_ form of the verb came into being only after its stem was already established in English.\n\u003e \n\u003e And in all three cases, notably, the verb stem already connotes some kind of undoing, an action of removing something from a state that had existed (frozenness, knottedness, intactness).\n\u003e \n\u003e Rather than negating each action, the placement of _un-_ in front of these verb stems seems to reinforce the undoing action of each—thereby _emphasizing_ the idea of negation rather than serving to negate the verb stem itself.\n\n\n\n## 并且, 为什么 ravel 有互相矛盾的意思?\nSource: [The verb \"ravel\" has two contradicting definitions. | WordReference Forums](https://forum.wordreference.com/threads/the-verb-ravel-has-two-contradicting-definitions.556645/)\n\n\u003e Don't blame the dictionaries. This is an odd word, which does have two contradictory meanings: to tangle, and to untangle.  \n\u003e   \n\u003e Online Etymology Dictionary:  \n\u003e ravel  \n\u003e 1582, \"to untangle, unwind,\" also \"to become tangled or confused\" (1585), from Du. ravelen \"to tangle, fray, unweave,\" from rafel \"frayed thread.\" **The seemingly contradictory senses of this word (ravel and unravel are both synonyms and antonyms) are reconciled by its roots in weaving and sewing: as threads become unwoven, they get tangled.**\n\n","lastmodified":"2022-10-15T14:06:29.774505512Z","tags":null},"/notes/2022/2022.7/%E4%BA%86%E8%A7%A3%E4%BF%A1%E7%94%A8%E5%8D%A1":{"title":"了解信用卡","content":"# 了解信用卡\n\n\u003cdiv align=\"right\"\u003e 2022-07-11\u003c/div\u003e\n\nTags: #DailyLife #CreditCard\n\n## 信用卡是什么?\n- 信用卡（英语：Credit Card），是一种非现金交易付款的方式，是银行业提供的**信贷服务**。\n- 与一般的信用卡与借记卡、提款卡不同，信用卡在消费时不会直接扣除用户的资金，而是等到账单日时再进行还款。\n\n## 信用卡的历史\n根据维基百科，最早的信用支付出现于 19 世纪末的资本主义重镇英国，大约在十九世纪八十年代，针对有钱人购买昂贵的奢侈品却没有随身携带那么多钱，英国服装业发展出所谓的信用制度，利用记录卡，购物的时候可以及早带流行商品回去，旅游业与商业部门也都跟随这个潮流抢占商机。\n\n但当时的卡片仅能进行在特定场所的短期商业赊借行为，款项还是要随用随付，不能长期拖欠，也没有授信额度，完全是依赖富裕人口的资本信用而设计。\n\n20 世纪 50 年代，第一张针对大众的信用卡出现，美国曼哈顿信贷专家麦克纳马拉（Frank McNamara）在饭店用餐，由于没有带足够的钱，只能让太太送钱过来。这让他觉得很狼狈，于是组织了“食客俱乐部”（英语：Diners Club，即为大来卡），任何人获准成为会员后，带一张就餐记账卡到指定 27 间餐厅就可以记账消费，不必付现金，这就是最早的信用卡。\n\n此后随签约的合作对象越来越多，可供临时透支的服务范围也越来越大，人们也习惯了这种不必携带现金的方便交易型式，促进了银行信用卡的到来，美国富兰克林国民银行是第一家发行信用卡的银行，之后其他美国银行也跟随。\n\n详细的历史可以参考这篇文章：[信用卡的历史起源 - 阮一峰的网络日志](http://www.ruanyifeng.com/blog/2019/07/origin-of-credit-card.html)\n\n## 信用卡由谁发行\n信用卡由**国际信用卡组织**发行，国际上六大信用卡组织分别是 VISA、MasterCard（万事达卡）、American Express（美国运通）、UnionPay（银联）、JCB 和 Diners Club。除银联和 JCB 外，其余四大信用卡均起源于美国。\n\n### 信用卡组织和银行的关系是什么\n银行卡组织本身并不发卡，而是由加入银行卡组织的金融类机构会员（主要是银行）来发行各种卡片，并且会提示其会员机构们开发新的支付产品和技术。持卡人的相关关系也有各金融机构自己负责管理。[^1]\n\n\n\n\n\n\n\n[^1]: [Fetching Title#9j0f](https://zhuanlan.zhihu.com/p/32120593)","lastmodified":"2022-10-15T14:06:29.814505943Z","tags":null},"/notes/2022/2022.7/%E6%9C%89%E6%95%88%E5%9C%B0%E8%83%8C%E8%AF%B5GRE%E5%8D%95%E8%AF%8D":{"title":"有效地背诵GRE单词","content":"# 有效地背诵 GRE 单词\n\n\u003cdiv align=\"right\"\u003e 2022-07-09\u003c/div\u003e\n\n#### Bathos\n- **Bathos** is a coined word(from Greek _bathys,_ “deep”), which means unsuccessful, and therefore ludicrous, attempt to portray [pathos](https://www.merriam-webster.com/dictionary/pathos) in art, _i.e.,_ to evoke pity, sympathy, or sorrow. The term was first used in this sense by [Alexander Pope](https://www.britannica.com/biography/Alexander-Pope-English-author) in his [treatise](https://www.merriam-webster.com/dictionary/treatise) _Peri Bathous; or, The Art of Sinking in Poetry_ (1728).\n\n- **Pathos**: the power of a person, situation, piece of writing, or work of art to cause feelings of sadness, especially because people feel sympathy.（境况、文章、艺术品或人的）感染力\n\n- Pathos is sincere compassion. Bathos is ridiculous and ludicrous.\n\n- Example of Bathos: [What Writers Should Learn From Wonder Woman - YouTube](https://www.youtube.com/watch?v=w-QhdzQo66o)\n\n#### miscreant\n- someone who has done something illegal or behave badly.\n- `mis-`,  不; `cre-,cred-` believe, trust.\n\t- 不相信基督的, non-Christian.\n\n#### filibuster\n- 通过发表冗长的演讲来阻止和拖延一个法律进程\n- `flibutor` 在荷兰语里面的意思是 `pirate`, 一个 filibuster 的人就像 pirate 一样讨厌.\n\n#### robust\n- 一个坚定而强烈的观点也可以是 `robust` 的\n- A British Foreign Office minister has made **a robust defence** of the agreement.\n- He has the keen eye and **robust approach** needed.\n\n#### feeble\n一个言论也可以是 feeble 的: 站不住脚的, unconvincing, poor, weak, 干巴巴的, \n- a feeble joke/excuse\n\n\n#### fledge\n- fledge: to acquire feathers\n- fledge + ling: fledgling 正在长羽毛的 - 初出茅庐的, 没有经验的\n- fledge + ed: fledged 羽翼丰满的\n\n#### credence\n- `noun`, 相信\n\t- add/give/lend **credence** to: 证实, 相信\n\n##### `creed-`, `cred` : to believe\n- credit: 信用\n- credulous: 容易被骗的\n- credential: 证明自己可以被相信的东西 -\u003e 证书, 证件\n\n#### dolorous\n- `dole-` 悲伤, grieve, sorrow\n- dolo + rous -\u003e 悲伤的, sad\n- doleful -\u003e depressing, miserable\n\n#### override\n- over-ride\n- out-weight\n- over-come\n- super-sede\n- eclipse, 日食, 月食 -\u003e 比如日食的时候大家的注意力都在月亮上面, 引申为 the second thing gets all the attention\n\n#### demote☢☢\n- 对比 pro-mote\n- downgrade, relegate, degrade\n\n#### rhetorical\n- rhetorical question: 反问句\n- rhetoric: 修辞学, 雄辩术-\u003e华而不实的煽动性语言\n\n#### cow\neasily herded =\u003e intimidate, daunt, frighten, scare, 恐吓\n\n\n#### culpable\n`culp-` =\u003e guilt, fault\n有罪的, blameworthy, wrong, guilty, to blame\n\n\n#### burgeon☢\nto bud, to sprout - 发芽, 结花苞 =\u003e develop quickly, increase, flower, grow\n\n\n#### deign\ndescend + dignity =\u003e 屈尊\n\n#### condescend\ncon- 一起 + descend 下降 =\u003e 一起下降 =\u003e 屈尊\n\n\n#### impious\nim- + pious =\u003e 不+虔诚的 =\u003e 不虔诚的, 对神灵不敬的\n\n#### tactile\ntact- tang- tag- =\u003e touch 比如 tangible\ntact + ile =\u003e 触觉的, 爱动手动脚摸别人的, 摸起来舒服的\n\n#### prepossessing☢☢\nattractive, impressive, interesting\npre- + possess + ing =\u003e 预先占领的 (mentally) =\u003e精神上预先占领 =\u003e 给人好感的\n\n\n#### ingratiating☢☢\nin- 使... + grat- 高兴, 感激 + ate 做, 造成 + ing ... 的\n=\u003e 让人感到高兴/感激的, =\u003e 迎合的, 奉承的, \n- sycophantic\n- obsequious\n- servile\n\n#### sycophant\nsycophant（马屁精）：替领导用下流手势侮辱对手的人\n英语单词sycophant（谄媚者）来自希腊语sykophantes，由sykon（fig，无花果）+phainein（to show，显示）组成而成，字面意思就是fig-shower（显示无花果的人）。在这里，fig（无花果）隐喻女性外阴，fig-show就是用手比划成女性外阴的形状来侮辱对方。在古希腊，政治家们表面上很正派，背地里却唆使自己手下用这种下流手势来侮辱自己的对手。而那些听从大人物的吩咐，用下流手势侮辱对手的手下则被称为sycophant（fig-shower）。现在，英语sycophant用来比喻那种卑鄙无耻、奴颜婢膝的谄媚者、马屁精。\nsycophant：['sɪkəfænt] n. 谄媚者，奉承者，马屁精 adj. 谄媚的，奉承的\nsycophantic：[,sɪkə'fæntɪk] adj. 说奉承话的；阿谀的\nsycophancy：['sɪkəfænti] n. 谄媚；奉承；追随；拍马屁\n\n#### penitence\npenit- 懊悔 + ence 表状态 =\u003e 懊悔的状态, 忏悔\n- repenitence: re-表示强调, 再, =\u003e 悔过, 忏悔, regret, guilt, grief, sorrow.\n\n- penitent 忏悔的, 悔过的\n- impenitent 不知悔改的\n\n\n#### sulk\n**sulk** 生闷气, 愠怒 -\u003e back-formation from **sulky**\n- sulky -\u003e quietly sullen\n- sullen -\u003e morose 闷闷不乐的, 愠怒的\n\nsullen 还能用来形容天气 =\u003e the sullen sky =\u003e dark and unpleasant\n=\u003e 就像汉语里面\"阴沉的\"既能用来形容人的情绪, 也能用来形容天气一样.\n\n#### palatial\nlike a palace, magnificent\n\n#### incinerate\nin- 使 ➕ ciner- ash 灰 ➕ -ate 做,造成 =\u003e 使变成灰 =\u003e 焚烧, 焚毁 / burn to death\n\n\n#### sporadic\nspor- spore, 孢子, 种子, 引申为 scattered 分散的, 再引申为 \"时间上分散的\" =\u003e 间断的, 不连续的, 零星的 \n- intermittent\n- occasional\n- scattered\n\n#### incessant\nin- 不  ➕ cess- 走开, 停止, 撤退, 停息 ➕ -ant 的 =\u003e 不停息的, 连续不断的, 没完没了的\n- all the time, constantly, continually, endlessly\n\n#### pensive\npen-：悬垂，称重，买东西称重的时候都需要思考=\u003e权衡, 沉思\npensive： ['pɛnsɪv] adj. 沉思的，忧郁的；悲伤的，哀愁的\n\npansy（三色堇）：状如沉思小脸的花卉\n单词 pansy 来自拉丁语动词 pensare（沉思），动词 pendere（权衡、称重）的反复形式，本意是“沉思的”，与单词 pensive（沉思的）、pendent（悬垂的）同源。植物学家用 pansy 一词来表示“三色堇”这种植物，因为它的花瓣近乎圆形，状如一张正在沉思的小脸，故此得了这么一个充满诗意的名称。\n![](notes/2022/2022.7/assets/1-1ZPGKA35a.jpg)\npendent：['pɛndənt] adj. 悬而未决的；下垂的；未定的；向外伸出的\n\n#### heterodox☢\nhetero-: the other, 异的\n-dox: 观点\n\n\n#### rostrum\nrostrum - 该词系拉丁语借用词，原指“鸟嘴”。公元前 338 年古罗马征服了安齐奥（Anzio，旧名 Antium），将被俘战船上取下的喙形船首作为战利品运回罗马，并用它们来装饰古罗马广场（the Roman Forum）的演讲台。这些船首因形似鸟嘴，故罗马人称喙形船首为 rostra（rōstrum 的复数形式），随后将古罗马广场的演讲台也称作 Rostra。不少罗马演说家曾经登上 Rostra 发表演说，古罗马政治家、演说家、哲学家西塞罗（Cicero, 106-43 BC）就曾在这个 Rostra 发表反安东尼演说。凯撒（Julius Caesar）被刺杀后，西塞罗逃离罗马，不久遭追捕并被杀害，首级和双手被砍下送回罗马悬于 Rostra 示众。嗣后，rostra 一词被逐渐用以泛指“演讲台”。18 世纪 rōstrum(rostra 的单数形式）进入英语作 rostrum，也用以泛指“演讲台”或“讲坛”。\n\n\n#### macerate☢\nmacer- : soften, 使变软\nate : 做, 造成 \n使什么东西变软, 许多东西在水里泡了之后都会变软, 引申为**浸泡, 腌制**\n\n\n#### tenacity\n- quality of holding firmly: `ten-` hold + `acity` 有...倾向 \ndetermination, 坚持, 固执, 坚守\n- tenacious\n\n#### recant☢☢\n`re-` back\n`cant` Latin: *cantare* , to chant, to sing\n- back chant/sing =\u003e withdraw, take back, retract, disclaim\n\n\n#### satiate\n`satis` enough + ate =\u003e 使足够, 使满足, =\u003e 饱足的, 充分满足欲望, 以至于有一点厌恶\n\nsatiating: 饱腹感强的\n\n#### plethora\npleth- full + ora 名词后缀 =\u003e noun. 过多, 过剩, 多血症\n\n\n#### ramshackle\n摇摇欲坠的\n`ram-` 树枝 branch, + `shackle` 束缚\n\n像树枝一样绑在一起的, 摇摇欲坠的\n\n#### impecunious☢\n- im-,in-,il-,ir- 表示“无，没有（not,opposite）”，来自拉丁语 in-。\n- pecu- 来自拉丁语 peculium，表示私有财产，pecu- 特指牛群或牲畜，这些都是当时最重要的财产形式，所以也引申为财产和特殊的。\n- -ous,-itious,-eous,-ious,-uous 表形容词，表示“…的”，用于化学领域表示“亚酸的，低价（金属）的”\n\n没有私人财产的, 贫穷的 \\\u003cFormal\u003e\n\n#### insular\ninsula: island\n- 比如 peninsula: 半岛 (`pen-` 近似, 差不多)\n- insular, 像岛屿一样的 =\u003e 思想狭隘的, 保守的, narrow-minded, prejudiced, provincial, closed\n\n- provincial =\u003e 外地的, 外省的 =\u003e 保守的, 狭隘的, 可以看到古人也认为地域决定思想: 地域偏僻地方的人的思想也保守狭隘.\n\n#### haughty\nhaught: 高的, =\u003e 自己以为自己很高的 =\u003e 傲慢的\n\nhauteur =\u003e 高傲, 傲慢\n\n#### verdant\n- `verd-` green + -ant =\u003e 草木苍翠的 \n- green, lush, leafy, grassy\n\n#### dirge\ndirge（挽歌）：基督教丧礼上的挽歌\n根据基督教礼仪，当基督教徒去世后，将在教堂为其举行丧礼（Office of the Dead）。丧礼通常包括若干次祷告，死者的亲朋好友在牧师的引导下一起诵念祷词或唱圣歌（antiphon）。其中，最早的一次祷告称为“晨祷”（Matins或Office of Readings）。晨祷一般在拂晓时分进行，所唱的第一首圣歌来源于《旧约•诗篇》中的一篇赞美诗，第一句话就是拉丁语“Dirige, Domine, Deus meus, in conspectu tuo viam meam”（主啊，我的上帝，您的目光引导我的道路）。其中的第一个单词dirige就是“引导”的意思，是拉丁语dirigere（引导）的祈使形态，与英语单词direct（引导）同源。因此，这一篇赞美诗就被称为dirige，进入英语后拼写演变为dirge。由于它是用来哀悼死者的，所以dirge通常被译为“挽歌”。现在，dirge一词可以用来表示哀悼死者的任何哀歌或挽歌。\ndirge：[dɜːdʒ] n.挽歌，哀悼歌\n\n\n#### Boisterous\n- 猛烈的, 喧闹的, 狂暴的\n- Full of noisy enthusiasm and energy\n\nBoisterous 词源不详, 但是和 Beast 和 Boast 很像\n\n\n#### cunning\n- cun =\u003e (g)noscere =\u003e to know, 知道\n- ing =\u003e 形容词后缀\ncunnning =\u003e learned, skillful, possessing knowledge, =\u003e 狡猾的, 机灵的, 狡诈的\n\n#### jolt\n- to move sth suddenly and violently =\u003e 猛的一下摇动, 震动, 剧烈晃动\n- =\u003e 引申为精神上的\"震动\" =\u003e 使震惊, 让某人震惊, 以至于改变了 ta 的思维方式或者促使 ta 产生某种行动(to shock someone in order to change their behaviour or way of thinking)\n\n - The charity used photos of starving children in an attempt to jolt the public conscience (= make people feel guilty and take action). 慈善机构用饥饿儿童的照片来触动公众的良知。\n **jolt sb into/out of sth**\n- The news about Sam's illness jolted her into action.\n\n#### project☢\nProject 作为动词除了\"投影\"还有:\n1. \"计划, 期望\"的意思:\n- This sector is projected to double in size over the next 12 months.\n- a projected deficit of $1.5 million\n\n2. 和中文里面一样, Project \"投射\"的含义还能引申到精神层面 =\u003e 将(情感, 观点, 感觉)投影, 投射到其他人身上\n- I suspect he's projecting his fear onto you.\n\n3. 和中文里面不太一样的是: 英语里面 Project 这个\"投射\"的意思还能引申为\"**展现**(某种品质)\"的意思(If you project sb/sth in a particular way, you try to make people see them in that way. if you project a particular feeling or quality, you show it in your behaviour.)\n- Recently the president has sought to project a much tougher image.\n- Bradly projected a natural warmth and sincerity.\n\n\n4. 最后, Project 作为动词, 还有\"伸出, 突出(某个边缘, 某个平面)\"的意思, 这可能和\"投影\"的时候光线\"伸出\"的路径有点像\n- ...the remains of a war-time defence which projected out from the shore.\n\n\n#### Compunction\nguilt, misgiving, remorse, scruples\n- have no **compunction** about doing sth\n\n- com- =\u003e 一起, 表强调\n- punct- =\u003e 点, 尖 =\u003e 刺, to prick\n- -ion =\u003e 名词后缀\n\t- 浑身像针刺一样 =\u003e 良心的不安, 内疚, 自责, 悔恨\n\n#### sedulous☢☢\n- careful and using a lot of effort/constant or persistent in use or attention\n- 勤奋的, 谨慎而刻苦的\n- sedulous =\u003e sedulo + ous \u0026\u0026 sedulo =\u003e sedolo =\u003e se + dolo\n- sedulo =\u003e sincerely, dilegently\n- sedolo =\u003e se- \"without\" + dolo \"deception, guile\"\n\t- 勤奋刻苦的 =\u003e 没有 + 欺骗 =\u003e 不会对自己的工作说谎\n\n##### sedentary\n久坐的\n\n#### rail\n- rail 除了\"铁轨\"的意思, 还有\"扶手, 挂东西的横杆\"的意思\n\t- Will spectators please stay behind the rail?\n\t- The clothes rail in her wardrobe was crammed full of dresses.\n\t- The bathroom has a heated towel rail. 挂毛巾的那个杆子就叫 rail\n\t- Hold on to the rail so you don't fall.\n\n- rail 还有一个词源完全不同的含义: to complain angrily\n\t- 来自法语\"raillier\" =\u003e to tease or joke\n\t- He railed against/at the injustice of the system.\n\n\n#### admonish☢☢\n- ad- \n\t来自拉丁介词 ad, 表示“朝、向、去，或弱化为强调”。\n- mon- \n\t= warn, 表示“警告”。源自拉丁语 monere \"to remind, warn.\"\n- -ish \n\t表动词，“造成…”。\n\n增强+警告 =\u003e too tell sb very seriously that they have done sth wrong.\n- reprimand, rebuke, caution, censure\n\n#### censor/censure☢\n- censor =\u003e 审查\n- censure =\u003e 责备, 谴责\n\n#### untoward\nun + toward =\u003e not having inclination =\u003e 和愿望不一致的 =\u003e 事与愿违的, 意外的(unexpected, not convenient, unpleasant)\n\n#### inter\n埋葬 bury, lay to rest, entomb\n- in- + -ter (terra=\u003eearth) =\u003e (将遗体)埋进土壤中 =\u003e 埋葬\n\n- disinter =\u003e 发掘, 显露\n- reinter =\u003e 改葬, 重埋\n- interment =\u003e 埋葬(noun)\n\n#### salve\n- 油膏, 软膏, 药膏(ointment) =\u003e 引申为\"宽慰, 慰藉\" \n- salve one's conscience =\u003e 使某人的良心得到宽慰, 减轻某人的内疚感\n\t- He salves his conscience by giving money to charity.\n\n- selp- =\u003e fat, butter\n\n\n#### overt\n- overt =\u003e 公开的, 显式地, 明显的, 毫不掩饰的\n- covert =\u003e 隐蔽的, 秘密的, 隐蔽的, 隐式的\n\n\n#### tractable\n- tract- =\u003e treat, handle =\u003e 处理\n- -able =\u003e 能够... 的\n能够处理的 =\u003e 容易处理的, 容易控制的, 易驯服的\n\nintractable =\u003e 棘手的, 难以控制的\n\n\n#### ripen\nripen 是动词, ripe 是形容词\n- When fruits **ripen**, they become **ripe**.\n\n\n#### anachronism\n`ana-`  -\u003e 错误的\n`chron-` -\u003e time\n`-ism` -\u003e 抽象名词后缀\n- 时代错误 / 过时的事物\n\n- adj. anachronistic\n\n#### impassive☢\n- im- \n\t表示“无，没有（not,opposite）”，源自拉丁语 in- \"not.\"\n- pass- \n\t= suffer, 表示“忍受”，引申为“感情”。源自拉丁语 pati \"to suffer.\"\n- -ive \n\t表形容词。源自拉丁语 -ivus \"adjective suffix.\"\n\n没有感情的 =\u003e 神情冷漠的, 木然的 unemotional, unmoved, emotionless\n\n- impassivity\n- passible =\u003e susceptible to emotion or suffering, able to feel, 容易被感动的, 易动情的\n- impassible =\u003e not susceptible to pain or injury, unmoved\n\n#### conflate☢☢\nto combine two or more pieces of text or ideas into a single one.\n\n- `co-, com-, con-` =\u003e with, together\n- `flate` =\u003e blow\nto blow together.\n\n#### herald\n- herald 表示旧时的传令官, 后来引申为\"预兆, 先兆\"的意思(forerunner, sign, signal, indication), \n\t- The president's speech heralds a new era in foreign policy.\n\n- 也可以作为动词, \"预示着... 的开始\"(indicate, promise, precede, pave the way)\n\t- Their discovery could herald a cure for some forms of impotence.\n\n#### demagogue\n- dem- \n\t= people，表示“人民，民众”。源自希腊语 `demos` \"people, land.\"\n- agog- \n\t= lead, 表示“引导”。源自希腊语 `agein` \"to drive, lead, weigh.\" \n- 词义贬义化，用来指暴民的领导者，蛊惑民心的政客。\n\n`dem-, demo-` =\u003e people\n- democrat =\u003e demo・crat =\u003e 民主党党员, 民主主义者\n- demos =\u003e demo・s =\u003e 人民, 民众\n- demotic =\u003e demo・tic =\u003e 人民的, 民众的\n- democracy =\u003e demo・cracy =\u003e 民主主义, 民主政治\n- demography =\u003e demo・graphy =\u003e 人口统计学\n- epidemic =\u003e epi・dem・ic =\u003e `epi-` \"在... 周围\",   `-ic` 形容词后缀 =\u003e 在人民周围的, =\u003e 传染病, 传染病的\n\n#### agog☢\nexcited and eager to see more.\n这里的 agog 和上面的 demagogue 里面的 agog 是同一个词根, 还有 fun 的意思. =\u003e 引申为 in a state of desire, imagination\n\n- We waited agog for the news.\n\n\n#### flag\n(verb) to put a mark on sth so it can be found easily among other similar things\n- Flag any file that might be useful later.\n\n#### retentive\nretent \"保持\"(retention) + ive =\u003e 保持的 引申为(mentally) =\u003e 记忆力好的\n- retentive feeling =\u003e 执着的感情\n\n\n#### indolent☢\n- in- \"没有\"\n- dol- = dole, 悲伤, grieve\n- -ent ...的\n\n没有悲伤的, 没有痛苦的, 引申为\"不用忍受痛苦的\" =\u003e 懒惰的, 懒散的, lazy, slack, slothful, \n\n- indolence =\u003e 懒惰\n\n#### commensurate\n相对应的, 相称的(corresponding in amount, degree or magnitude)\n- a salary that is commensurate with skills and experience.\n- equivalent, consistent, corresponding, comparable\n\ncom- \"一起\" + mensur=measure \"测量\" +  ate \"...的\" =\u003e 一起测量的 =\u003e 相称的, 相当的\n\n#### deplete\nuse up, reduce, drain, exhaust\nde- + -plete \n- de- =\u003e 相反\n- plet, plen =\u003e full, fill \n\n#### replenish\nre- + plen + ish\n- re- 重新\n- plen, =\u003e fill, full\n- ish 动词后缀\nto make sth full or complete again\n\n#### fallible\n会犯错的, 容易出错的\n- `fall-, fail-, fault-` =\u003e err, deceive 错误, 谬误, 欺骗\n- `-ible` 能 .. 的, 形容词后缀\n\n- infallible =\u003e in・fall・ible =\u003e 万无一失的\n- fallacy =\u003e fall・acy =\u003e 谬论, 谬误\n\n#### conservative\n- conservative 是\"保守的\"的意思. 在汉语里面, 有\"保守估计\"的说法, 意思是\"谨慎的估计\", 有趣的是英语里面 conservative 也可以这样用:\n- A conservative estimate of the bill\n\n#### axiomatic\naxiom =\u003e 公理, 在数学里面, 公理是不需要证明的\naxiomatic =\u003e 不言自明的, 显而易见的 self-evident, given, understood, obviously true\n\n#### extravagant \n- 该词始用于 14 世纪，源自拉丁语，由拉丁语的两个词 extrā 'beyond, outside'（在外）与 vagārī 'wander'（游荡）复合而成，合起来就是 wandering outside，即“游荡在外”的意思。在莎翁的著名悲剧《哈姆雷特》（Hamlet, 1602）中，丹麦王子哈姆雷特称他父王的亡魂为'the extravagant and erring spirit', extravagant 和 erring 两词正是用于此义。嗣后，extravagant 的词义逐渐发生变化，由“游荡在外”、“离开正道的”引申为“偏离合理的限制范围”、“超过限度的”，最后演变为“挥霍的”、“奢侈的”（spending much more than necessary）这一如今最常用的词义。此义是直到 18 世纪初才确立下来的。\n\n- 英语另有三个词vagabond（流浪者），vagrant（流浪者），vagary（怪异多变）跟extravagant一词有些亲缘关系。它们都源于前面提到的拉丁语vagārī 'wander'（游荡）。\n\n例:\n- It was extravagant of you to spend ￥800 on a dress. 花 800 元去买一件衣服，那太奢侈了。\n- You are too extravagant with your father's money. 你花父亲的钱太大手大脚了。\n- An extravagant person has extravagant tastes and habits. 奢侈的人有奢侈的嗜好和习惯。\n- To be perfectly frank, I don't think I deserve such extravagant praise. 坦率地说，我认为如此高的溢美之词我是不配的。\n\n#### restitution\n归还, 赔偿, 补偿 compensate, satisfaction, amends, refund\n- re- 重新, 再\n- stitu- set up, place, 建立, 放\n- tion 名词后缀\n\n重新建立, 修复(损坏的东西) =\u003e 后面引申为赔偿, 补偿, 归还被盗或者丢失的东西\n\n#### badinage\n\"light railery, playful banter,\" 1650s, from French badinage \"playfulness, jesting,\" from badiner (v.) \"to jest, joke,\" from badin \"silly, jesting\" (16c.)\n\n- banter, joking, teasing, mockery\n\n\n#### frosty\n严寒的, 覆满冰霜的 =\u003e 引申为(行为)冷淡的, 冰冷的\n- He gave me a frosty look\n- There was a certain frostiness in his smile.\n\n比较: frigid\n\n#### Planet\nThe word planet derives ultimately from the Greek verb _planan_, “*to wander*.” The planets were the heavenly bodies that appeared to wander across the sky, as opposed to the stars, which were (relatively) fixed.\n\n#### Jovial\n'Under the influence of planet Jupiter, pertaining to Jupiter'\n- (of a person) friendly and in good mood, (of a situation) enjoyable because if being friendly and pleasant.\n- cheerful, happy, jolly, animated\n\nJupiter, the largest planet, was named for the king of the Roman gods. _Jove_ is a form of Jupiter. A person born under the planet Jupiter, therefore, was believed to be **jovial**—**cheerful and friendly**. This may be perplexing if you think of Jove mostly as a thunder-god, as his Greek equivalent Zeus is often portrayed. But if Mars is ascendant in times of war, Jove is the god who rules when the the work of Mars is done, when peace and prosperity, feasting and gladness prevail.\n\n#### Saturnine\n- Of the seven planets that were visible to the ancients, Saturn was the coldest, the darkest, the most distant, and the slowest moving. Saturn was the oldest of the Roman gods, the father of Jupiter and several other of the major gods. A **_saturnine_** disposition, therefore, is **gloomy, sluggish, given to dark moods**.\n\n#### Mars\n- Mars, the red planet, was named for the Roman god of war. The word **_martial_** means “warlike” or “related to the military”—as in martial law.\n\n#### Mercury\n- Mercury, the namesake of the planet, was the messenger god, always zooming around from one place to another, never keeping still. A **_mercurial_** personality is **impulsive, unpredictable, tending to change directions and change again at a moment’s notice**. \n- If you’ve ever seen the element mercury, the way it wiggles and zips around and splits into smaller beads and re-forms into larger beads, you have a good visual image of what mercurial means. (You will also know why the element mercury’s other name is quicksilver.)\n\n#### The Moon (Luna)\n- The moon was personified by the Roman goddess Luna. The adjective _**lunar_**, of course, refers to anything moon-related. The moon is both the closest and the most changeable of the heavenly bodies. Each night’s moon looks a little different from the previous night’s moon. It was believed that bodies beyond the moon were unchanging and unchangeable. But below the moon, everything changes. That’s where the word _**sublunary_** comes from. Sublunary things—that, is, everything beneath the moon—is earthy, changeable, not perfect and immutable like the superlunary world.\n\n- The highly erratic Luna also made people insane. A _**lunatic_** is a person who has fallen under her influence. The word _**loony_** comes from the same place.\n\n![](notes/2022/2022.7/assets/Pasted%20image%2020220912014540.png)\n\n\n#### flounder☢☢\n- have many problems and may soon fail completely\n\t- 这个意思和 founder 的动词有点像\n- sb is floundering =\u003e not making decisions and not knowing what to say or do.\n\t- I know that you're floundering around, trying to grasp at any straw.\n- flounder in mud/water =\u003e move in an uncontrolled way and trying not to sink.\n\n#### friable\nfri- =\u003e 摩擦, friction\n-able =\u003e ... 的\n- 易摩的, 易碎的, crumbly\n\n#### accede (to)☢\n- ac- ad- =\u003e to \n- cede =\u003e go away, withdraw, yield\naccede =\u003e come to or arrive at (a state, position, office etc.)\n=\u003e to agree to do what people have asked you to do. \n\t(to withdraw, yield and do what was asked 同意, 应允)\n=\u003e to become king or queen, or to take a position or power. \n\t(to accept the position 登基, 即位)\n\n- He graciously **acceded to** our request. \n\t他通情达理地同意了我们的请求。\n- It is doubtful whether the government will ever **accede to** the nationalists' demands for independence. \n\t政府不太可能会同意民族主义者的独立要求。 \n\n和concede相比, 这个没有\"不情愿\"的意思\n\n#### concede☢\n- con- =\u003e with, together, 在这里表示强调\n- cede =\u003e go away, withdraw, yield\n\ngive away, yield, go away =\u003e 引申为 agree, consent(unwillingly)\n- Bess finally conceded that Nancy was right\n\n还可以 concede 物品 =\u003e give sb sth\n- The strike ended after the government concede some of their commands\n\n可以 concede 某种 rights or privileges\n- The French subsequently conceded full independence to Laos\n\nconcede 自己的失败(defeat):\n- He happily concede the election =\u003e 他开心地承认自己在选举中**失败**了\n- The company conceded defeat in its attempt to take control of its holiday industry rival.\n\n#### secede\n- se- =\u003e apart\n- cede =\u003e go away, withdraw, yield\n脱离, 退出(国家或者政府)\n- There is likely to be civil war if the region tries to secede from the south. \n\t如果这一地区想脱离南方而独立，很可能会爆发内战。 \n\n#### intercede☢\n- inter- =\u003e between, 在 ... 之间\n- cede =\u003e go away, withdraw, yield\n\nto use your influence to persuade someone in authority to forgive another person, or save this person from punishment\n（为某人）求情，说情\n\n- Several religious leaders have interceded with the authorities on behalf of the condemned prisoner. \n\t一些宗教领袖替这个被判刑的犯人向当局求情。\n\n#### connotation☢\ncon- =\u003e with, together 一起\nnotation =\u003e 含义, mark, note\n=\u003e to mark along with =\u003e 内涵意义, 隐含意义, a feeling or idea that is suggested by a particular word although it need not to be a part of the word's meaning, or something suggested by an object or situation.\n\n- The word 'lady' has connotation of refinement and excessive femininity which some women find offensive.\n\n\n#### dilettante\n一知半解的业余爱好者\n- 来自意大利语deletto, 愉快，高兴，词源同delight, delicious. 原指爱好者，后词义贬义化。\n\n\n#### frail\nfrail \u003c=\u003e fragile \n- Frail is the Frenchified version of fragile.\n\n\n#### antediluvian\n- deluge（洪水）：圣经故事中上帝毁灭人类的大洪水\n\t\n\t英语单词 deluge 在词典中的解释是“大洪水、暴雨”，人们往往不知道它和 flood（洪水）之间的区别。其实，这个词有自己独特的背景，它来自圣经，指的是上帝毁灭人类世界的那次大洪水。\n\t\n\t在《圣经•创世纪》中记载，上帝造人后，看到人在世间胡作非为，犯下各种罪恶，便后悔造人了，想毁灭人类世界，于是发下大洪水淹没了整个世界，人类遭到毁灭，只有敬神的诺亚一家因为事先按照上帝的旨意修建了诺亚方舟才躲过此难。\n\t\n\t单词 deluge 原本专指这场大洪水，但人们通常用它来比喻各种洪水一般的事物。在表示实际的洪水这一自然灾害时，人们往往还是使用 flood 一词。\n\n- 英国物理学家托马斯•布朗（Thomas Browne）在 17 世纪创造了 antediluvian 一词，字面意思为“大洪水之前的”，比喻远古的。\n\n- deluge：['deljuːdʒ] n. 洪水，暴雨，上帝毁灭人类的大洪水 vt. 使泛滥，压倒\n- antediluvian：[,æntɪdɪ'luːvɪən] adj. 大洪水前的；远古的；陈旧的；旧式的n. 大洪水以前的人；年迈的人；不合时宜的人\n\n\n#### misanthrope\nmis- miso- =\u003e to hate\nanthrop anthropos =\u003e man human being\n- one who hates mankind, 厌恶人类的人, 同misanthropist\n- cynic, sceptic, grouch, \n\n- misanthropic\n- misanthropize\n\n#### husky\nattractive rough and low voice\n- 可以用来形容男性和女性\na tall, strong and attractive man\n- 特指男性\n\n- 哈士奇, 爱斯基摩犬, 北极犬\n\n#### carnal\ncarnal feelings and desires are sexual and physical, without any spiritual element.\n\n- carn- =\u003e flesh , , 表示“肉，肉欲”，原始含义“切、砍”下来的一块肉。源自拉丁语 caro (词干 carn-) \"flesh.\"\n- -al  表形容词，“…的”，一般缀于名词后。源自拉丁语 -alis\n\ncarnival（狂欢节）：大斋期开始前的“谢肉节”\n- “狂欢节”在英语中为 carnival，来自拉丁文 carne vale，等于英语中的“flesh, farewell!”（肉，再见！），所以依其本意应译为“谢肉节”。根据基督教的教义，基督复活节前 40 天是一个为期 40 天的大斋期，即四旬斋（lent）。斋期里，人们禁止娱乐，禁食肉食，反省、忏悔以纪念复活节前 3 天遭难的耶稣，生活肃穆沉闷。因此，在四旬斋开始前三天，人们便抓紧时间开始纵情吃喝玩乐。一些主张宗教改革的人如新教徒便借此举办各种狂欢活动，挑战天主教的权威。荷兰画家 Pieter Bruegel 的名画《狂欢者与斋戒者的斗争》（Fight Between Carnival and Lent）便反映了两教派之间的斗争。\n- 因为再过三天就要跟肉食说拜拜了，这三天故此得名 carnival（谢肉节）。久而久之，纵情狂欢的 carnival 倒成了一个盛大节日，而四旬斋却逐渐被人遗忘。\n- carnival的前半部分是词根carn-，表示“肉”。该词根衍生众多与“肉”有关的单词。\ncarn-：肉\ncarnal：['kɑːn(ə)l] adj.肉体的，性欲的\ncarnival：['kɑːnɪv(ə)l] n.狂欢节，嘉年华，饮宴狂欢\ncarnivore： ['kɑːnɪvɔː] n. 食肉动物，食虫植物\ncarnage: 大屠杀, 大量灭绝\ncarnivorous：[kɑː'nɪv(ə)rəs] adj.食肉的，肉食性的\ncarcass： ['kɑrkəs] n.（人或动物的）尸体，残骸；（除脏去头备食用的）畜体\nreincarnate： [,riːɪn'kɑːneɪt] vt.使转世化身adj.转世化身的。记：re再次+in进入+carn肉体+ate动词后缀\n\n\n\n#### alacrity\nalacr- =\u003e swift\n-ity =\u003e 名词后缀, 表示某种特质\n\nliveliness, briskness, speed =\u003e 引申为 eagerness, 不仅在动作上迅速, 在心理上也很愿意, 所以\"欣然接受\"\n\n- alacrious adj.\n\n\n#### motile☢\nmotility n. =\u003e motile adj.\n特指 plants, organisms and very small forms of life, 即这些平时看起来不会动的东西, =\u003e capable of moving by themselves.\n\n- mot- mob- mov- =\u003e move, 源自拉丁语 movere \"move\", 其过去分词为 motus, 形容词分词形式 mobilis 。\n- -ity 表名词，指具备某种性质。\n\n\n#### conducive\nconduc(e) + ive\n\nproviding the right conditions for something good to happen or exist. 有利的，有助的，有益的\n\n- Such a noisy environment was not ***conducive** to* a good night's sleep. 在这样嘈杂的环境下晚上难以睡个好觉。\n- A quiet room is a more ***conducive*** atmosphere for studying. 置身一个安静的房间更有利于学习。\n\n##### conduce\n- to help make a particular situation happen or help produce a particular result\n\t有助于，导致\n- the belief that technological progress conduces to human happiness 现代科技进步可以为人类带来幸福的想法\n\n\"to lead, conduct\" (a sense now obsolete), from Latin conducere \"to lead or bring together, contribute, serve,\" from assimilated form of **com \"with, together\" (see con-)** + **ducere \"to lead\" (from PIE root *deuk- \"to lead\")**. Intransitive sense of \"aid in or contribute toward a result\" is from 1580s.\n\n#### vex\nannoy, bother, irritate, worry\n\nfrom Latin vexare 'to shake, jolt, toss violently' \n=figuratively=\u003e attack, harass, bother, trouble, annoy\n\n#### vitriolic\nfull of bitterness and hate, and so causes a lot of distress and pain.\nvitriol + ic 直接理解就是: 像vitriol一样的, 那么vitriol是什么呢?\n\nvitriol =\u003e n. 硫酸盐, 明矾, vt. 用硫酸处理 =\u003e 引申为像硫酸一样尖酸刻薄的话\n- If you refer to what someone says or writes as vitriol, you disapprove of it because it is full of bitterness and hate, and so causes a lot of distress and pain. [disapproval]\n- The vitriol he hurled at members of the press knew no bounds. \n\nvitri- 这个词根的含义是玻璃 源自拉丁语 vitrum, 可是硫酸盐和玻璃有什么关系呢? \n\n明矾是十二水合硫酸铝钾, 它的晶体是这个样子的:\n![300](notes/2022/2022.7/assets/800px-Potassium_alum_octahedral_crystal.jpg)\n和玻璃十分相像, 并且有的硫酸盐加热后会分解生成硫酸, 所以有了和硫酸相关的含义.\n\n- vitreous =\u003e adj. 玻璃的, 玻璃状的, 玻璃似的\n\t- vitreous china/enamel\n- vitrify =\u003e 玻璃化, 使...成玻璃\n\n\n#### lopsided\nlop + sided =\u003e lop : 砍去, 砍断 =\u003e 有一边被砍去了的 =\u003e 不平衡的, 向一边倾斜的\ncrooked, one-sided, tilting, warped\n\n\n#### waddle\nto walk with short steps, swaying from side to side; to walk as a duck does.\n是 wade\"涉水而行, 艰难地行走\"的反复动词 =\u003e 像鸭子那样走就像是在不停地在水里行走一样, 摇摇摆摆地.\n\n#### Waddle vs Wobble - What's the difference?\n- As **nouns** the difference between waddle and wobble is that waddle is a **swaying gait** while wobble is an **unsteady motion**.\n- As **verbs** the difference between waddle and wobble is that waddle is to **walk with short steps**, tilting the body from side to side while wobble is to **move with an uneven or rocking motion**, or unsteadily to and fro. \n\n#### inane\n*extremely* silly or with no real meaning or importance\n- He is always making inane remarks\n\nempty, void =\u003e 脑袋空空, 也就是愚蠢至极, 没有意义, 无聊\n\n- inanity =\u003e n.\n\n#### convenience☢\na public convenience =\u003e a public toilet\n\n\n#### cowardice\ncoward 是懦弱的人 cowardice 是 cowardly behaviour\n\n\n#### quixotic\n- 来自西班牙作家塞万提斯的小说 Don Quixote--堂吉诃(hē)德. 书中主人公堂吉诃德，企图用理想化的骑士精神改造社会。他痛恨专制残暴，主持正义，但耽于幻想，脱离实际，结果在现实面前到处碰壁。后成为善良、勇敢但又盲动的人物的典型。\n- 所以 quixotic 就是 Quixote 的形容词, \"像堂吉诃德一样的\", 也就是理想主义的, 不切实际的, 异想天开的, absurdly romantic, unrealistic, idealistic, absurd.\n\n\n#### naivety\nnaive 的名词, gullibility, innocence, simplicity, inexperience\n\n\n#### peevish☢\nbad-tempered, irritable, cross, crabbed, childish\n=\u003e 从这个单词衍生出了单词\"peeve\" =\u003e 令人不快的小事, 惹恼(某人)\n\n##### pet peeve\n- The term _**pet peeve_** was introduced to a wide readership in the single-panel comic strip _The Little Pet Peeve_ in the [Chicago Tribune](https://en.wikipedia.org/wiki/Chicago_Tribune \"Chicago Tribune\") during the period 1916–1920.\n\n\n#### prescription\n医生开的处方 =\u003e 因为吃了处方上面的药一般都会使病情好转, 所以引申为\"解决问题的方法, 诀窍, 解决方案\" (e.g. a prescription for success)\n\n#### cower\n退缩, 畏缩, bend your body or move backwards because of fear\ncringe, shrink, crouch\n\n#### desecrate☢\nde- =\u003e \"相反\"\nsecr = sacr =\u003e holy, 神圣的, sacred\n-ate =\u003e 动词后缀, 做, 造成...\n- 使其与神圣相反, 使其不再神圣 =\u003e 亵渎神灵, 污辱, 玷污, 亵渎, profane, dishonour, defile, violate\n\n- The mosque was desecrated by vandals.\n\ndesecration \u003c=\u003e sacrilege, debasement, blasphemy\n\n\n#### besmirch☢\n- be- \n构成动词，表示“使…成为”，来源于盎格鲁撒克逊语。\n- smirch \nvt. 沾污, 弄脏 n. 脏污, 污点\n=\u003e 引申为 玷污, 损坏某人的名誉\ntarnish, damage, stain\n\n\n#### meteoric\nmeteor + ic =\u003e 像流星一样的, 和流星相关的\n但是, 虽然流星是一闪而过的, 向下坠落的, 这个单词**并不是**昙花一现的意思.\n\n这个单词侧重于流星\"迅速, 引人注目\"的特点, 用来形容某个事物发展十分迅猛, 并且吸引了很多注意力(\"看, 流星!\"🌠🌠🌠, 出现流星的时候大家的注意力都会马上被吸引过去)\n\n - The group had a meteoric rise to fame in the 70s. \n\t 这支乐队在 20 世纪 70 年代迅速成名。\n- Her political career has been meteoric. 她的政治生涯可谓**扶摇直上**。 \n\n#### frigid\nfrig- friger- =\u003e = cold, 表示“冷”。源自拉丁语 frigere \"to be code,\" frigidus \"cold.\" 例如 refrigerator, fridge(电冰箱)\n- 严寒的，寒冷的，freezing, cold, icy\n- 引申到情感上面 =\u003e (of one's behaviour) very formal and unfriendly \n- 引申到性上面 =\u003e 性冷淡的\n\n\n#### be on good, friendly, etc. terms (with sb)\nto have a good, friendly, etc. relationship with someone\n（与某人）关系好，关系友好（等等）\n- My ex-wife and I are still on friendly terms. 我跟前妻仍然关系友好。\n- Are you on good terms with your boss? 你跟老板关系好吗？\n\n\n#### mesmerize\nmesmerize - 该词源出奥地利医师、当代催眠术的先驱Mesmer (Franz Anton Mesmer, 1734-1815)。他认定人体内有一种潜在的“动物磁力”（animal magnetism），并用它来治病。然而，维也纳的医生指控他玩弄魔术。1778年他被迫离开奥地利去巴黎定居，在那里继续行医，并再次遭到医学界的反对。1784年路易十六任命一个由科学家和医生组成，其中包括B. 富兰克林和A. 拉瓦锡在内的专门委员会去调查Mesmer的方法。委员会的报告对他持否定态度，把他称为骗子，但承认他的方法有疗效，将他的“治愈例”归因于患者自己的想像。结果，Mesmer又被迫离开巴黎，在瑞士度过了他的余生。其实，Mesmer所用的方法就是后人所谓的催眠（术）。在hypnotism和hypnotize出现之前，人们据他的姓氏创造了mesmerism及其相应动词形式mesmerize来分别表示“催眠（术）”和“对……施催眠术”。在当代英语中后两者除了作为前两者的同义语使用之外，更常用于引申义，分别表示“巨大的魅力”和“使入迷”或“惊得目瞪口呆”。\n\n- Hypnotize是Mesmer的美国追随者coin出来的一个词, 与Mermerize词义基本相同\n\n例 \n- The magician mesmerized a volunteer from the audience. (NED) 魔术师对观众里的一位志愿者施以催眠术。\n- He was mesmerized by her charm and beauty. (LDC) 他被她的魅力和美貌迷住。\n- She stood there mesmerized as he picked up the gun and turned it slowly towards her. (LLA) 当他拿起枪，慢慢把枪对着她，她站在那里惊呆了。\n\n- 这个视频将Mermer的故事讲的很清楚:\n\t[The phony health craze that inspired hypnotism - YouTube](https://youtu.be/KQyAnKjD6W4)\n\n- mesmerize -\u003e v. 催眠\n- mesmerism -\u003e n. 催眠术, 催眠状态\n- mesmeric -\u003e adj. 催眠术的, 令人着迷的, 令人陶醉的\n\t- ... music with a repetitive, slightly mesmeric quality\n\n#### aggravate\n- 1520s, \"make heavy, burden down,\" from Latin aggravatus, past participle of aggravare \"to render more troublesome,\" literally \"to make heavy or heavier, add to the weight of,\" \n- from ad \"to\" (see ad-) + gravare \"weigh down,\" from gravis \"heavy\" (from PIE root gwere- (1) \"heavy\"). The literal sense in English has become obsolete; meaning \"**to make a bad thing worse**\" is from 1590s; colloquial sense \"**exasperate, annoy**\" is from 1610s. The earlier English verb was aggrege \"make heavier or more burdensome; make more oppressive; increase, intensify\" (late 14c.), from Old French agreger.\n\n- 有意思的是, 汉语里面的\"加重\"一词和 aggravate 本义和引申义都对应的上: 从 make heavy =\u003e 引申到 使本来就糟糕的情况恶化\n- 同时\"使情况恶化\"也让我们感到\"恼怒, 不快\", 也就有了\"annoy, exasperate\" 的含义\n\n#### exodus\n- the movement of a lot of people from a place\n\t（大批人的）退出，离开\n- There has been a mass exodus of workers from the villages to the towns. 一直有大量工人源源不断从农村涌入城市。\n\n这个含义是由下面引申而来的:\n\n- the second book of the Bible telling of Moses and the journey of the Israelites out of Egypt\n\t《出埃及记》（基督教《圣经‧旧约》第二卷，记载摩西率领以色列人离开埃及之事）\n\t\n- 《出埃及记》是圣经旧约的第二书，主要是讲述以色列人如何在埃及帝国受到逼害，然后由摩西带领他们离开埃及的故事。出埃及记传统上认为是摩西在旷野完成的第二本书，因此在一些圣经译本如德文圣经中，它也简称作摩西二书（2. Mose）。 \n\n#### complacent\nfeeling so satisfied with your own abilities or situation that you feel you do not need to try any harder.  自满的，自鸣得意的\n- a complacent smile/attitude 自鸣得意的微笑／态度\n- We can't afford to become complacent about any of our products. \n\t我们不能对自己的任何一款产品沾沾自喜。\n\n- 其他表示\"满意的\"词还有 smug, self-satisfied, pleased, satisfied, gratified\n\t- smug 和 complacent 一样, 具有贬义(pejorative connotation), 对自己的成就沾沾自喜的意思, \n\t- satisfied, gratified 没有贬义\n\n- complacence\n- complacency\n\n#### don\n- If you don clothing, you put it on.\n- 例句: The crowd threw petrol bombs at the police, who responded by **donning** riot gear.\n\n\"to **put on** (articles of clothing),\" mid-14c. **contraction of do on** (compare doff). \"After 1650 retained in popular use only in north. dialect; as a literary archaism it has become very frequent in 19th c.\" \n\ndon (n.)\ntitle of respect, 1520s, **from Spanish or Portuguese Don, a title of respect prefixed to a man's Christian name**, from Latin dominus \"lord, master, owner\" (from domus \"house,\" from PIE root *dem- \"house, household\").\n![300](notes/2022/2022.7/assets/Pasted%20image%2020220717162321.png)\n\n\n#### spoof☢☢\n滑稽的模仿: spoof, parody, caricature\n\n#### be a glutton for punishment☢\n吃苦耐劳的人, 任劳任怨的人\nHe's a real glutton for punishment, taking on all that extra work.\n\n\n#### glutton\n但丁在神曲里根据恶行的严重性顺序列出了七宗罪(7 deadly sins):\n| 中文 | 拉丁文   | 英文     |\n| ---- | -------- | -------- |\n| 傲慢 | superbia | pride    |\n| 嫉妒 | invidia  | envy     |\n| 愤怒 | ira      | wrath    |\n| 怠惰 | acedia   | sloth    |\n| 贪婪 | avaritia | greed    |\n| 暴食 | gula     | gluttony |\n| 色欲 | luxuria  | lust     |\n\n其中 gluttony 是七宗罪之一, 而 glutton 就是暴食的人: \n- a person who regularly eats and drinks more than is needed\n\t好吃贪杯的人；暴饮暴食者 \n\n- 其实但丁所说的暴食并不单单指\"饮食\", 但丁的观点是“过分贪图逸乐”。\n\n- 相对于七宗罪，天主教也列出了七美德。 \n| 罪行                               | 美德               |\n| ---------------------------------- | ------------------ |\n| 色欲（Lust）                       | 贞洁（Chastity）   |\n| 暴食（Gluttony）                   | 节制（Temperance） |\n| 贪婪（Greed）                      | 慷慨（Charity）    |\n| 懒惰（Sloth）                      | 勤勉（Diligence）  |\n| 愤怒（Wrath）                      | 耐心（Patience）   |\n| 嫉妒（Envy）                       | 宽容（Kindness）   |\n| 傲慢（Pride）                      | 谦虚（Humility）   |\n\n- gluttonous -\u003e adj.\n\nglut- =\u003e to devour, 吞下, 吃\n-on =\u003e 名词后缀 \n\n- glut =\u003e 过多供应\n- deglutible =\u003e 可吞服的\n\n\n#### terror☢\n作为名词, 还可以指淘气的难管教的小孩, 熊孩子, unruly children\n- He was a terror. He had been a difficult child for as long as his parent can remember.\n\n#### atone\natone - atone是at one二词连写合成的。它是如何形成的呢？旧时at one常常出现在to bring at one accord（使一致）和to set at one assent（使一致）等一类较长的词组中。在使用中人们觉得at one二词就足以表达整个词组的意思，因此at one后面的名词往往被省略掉。早在13世纪，牧师在布道时常常恳求教徒to be at one with God（与上帝和好）。由于此语用得如此频繁，到了14世纪at one逐渐结合为一个词，并被赋予“（使）和解”、“（使）协调”、“与……完全一致”等义。当时one读如own，所以atone发成/əˈtəʊn/。这就使得后人往往注意不到atone中的-one与数词one之间的历史联系了。一般说来，和解与协调往往是在一方向另一方赎罪后出现的，而在基督教中与上帝和好则意味着通过祈祷、苦修和行善等为自己赎罪。到了17世纪末期atone作为宗教术语开始由“和解”、“和好”、“协调”等义转而表示“赎罪”、“补偿”、“弥补”等义。\n\natone的名词形式是atonement。昔日，at onement一语在基督教中常被用来表示上帝与人的和好，英国神学家威克里夫（John Wycliffe, 1330?-1384）在1382年版本的《圣经》英译本中就曾用过onement一词。嗣后，at onement二词合而为一，作atonement。因此一些辞书认为atone系由atonement逆构而生，也不无道理。\n\n例 \n- There was no way he could atone for the wrong he had done. 他无法弥补他所犯的过错。\n- He gave large sums of money to charities in an effort to atone for his sins. (CAE) 为了赎罪，他捐了一大笔钱给慈善事业。\n- His hard work atones for his lack of skill. 他的艰苦努力弥补了技术上的不熟练。\n- They're still trying to make some sort of atonement and reparation. 他们仍在尽力作出某种补偿。\n\n#### abreast\na - breast =\u003e 并肩前行, 中国是并肩, 外国是并胸\n- 引申义 =\u003e 跟上 ... 的最新进展 keep abreast of sth\n\t- I try to keep abreast if any developments.\n\n\n#### domicile☢\ndom- domin =\u003e house (e.g. domestic, domain, condominium)\n-ile =\u003e 物体\n\na place where a person lives == dwelling, home, residence, house\n\n\n#### esoteric\n- esoteric [,esə'terɪk] adj. 秘传的；限于圈内人的；难懂的 \n- exoteric [,eksə(ʊ)'terɪk] adj. 开放的；外界的；通俗的。\n- 两个单词的拼写相差一个字母，意思却截然相反，为啥哩？希腊前缀 exo-表 outer,outside，另一个希腊前缀 eso-表 within。共同点是：-ter-表比较级，后缀-ic 表 having to do with。\n\n\n\n#### excoriate☢☢\nex- =\u003e 去除, 从...离开\ncori- cort- =\u003e skin, from Latin \"corium\" =\u003e skin, hide, leather\n-ate =\u003e 做, 造成\nstrip off the skin of =\u003e 引申为严厉的指责, 痛斥\n- His latest novel received excoriating reviews. \n\t他的最新小说受到了严厉的批评。\n- The president excoriated the Western press for their biased views. \n\t这位总统痛斥西方新闻界充满偏见的观点。 \n\n\n#### indelible\nin- + del(ete) + ble =\u003e 不可以 delete 的 =\u003e 不可磨灭的\n\npermanent, lasting, enduring, ingrained\n\n\n#### benighted☢☢\novertaken by darkness =\u003e 引申为 overtaken by intellectual or moral darkness =\u003e without knowledge or morals \u003c=\u003e 愚昧的, 无知的, 未开化的 \n\n- Some of the early explorers thought of the locals people as benighted savages who could be exploited.\n\n#### halcyon☢\n![](notes/2022/2022.7/assets/White-throated_kingfisher.webp)\nhalcyon（神翠鸟）：殉情自尽的痴情夫妻\n希腊神话中，风神有一个女儿叫艾尔莎奥妮（Alcyone），嫁给了黎明女神的儿子、国王西克斯（Ceyx)。两口子非常恩爱，竟然得意地自比宙斯和赫拉，结果惹恼了宙斯和赫拉，决定要好好惩罚这两个不敬神灵的凡人。于是，有一天，西克斯在海上航行的时候溺水而亡，艾尔莎奥妮伤心愈绝，跳崖身亡。\n众神被她的痴情感动，就将西克斯与艾尔莎奥妮双双变成神翠鸟（halcyon），从此永不分离。 传说中这对恩爱的夫妻在波浪上抚育他们的孩子，而艾尔莎奥妮的父亲风神因为眷顾女儿， 每年十二月份，他就会平息海浪，便于翠鸟在海上筑窝，生育后代。所以单词 halcyon 还有“风浪平息”的含义。\nhalcyon： ['hælsɪən; -ʃ(ə)n] n.神翠鸟，翡翠鸟adj.宁静的，平稳的\nhalcyon days：太平时期\n\n#### arable\nar- = to plow, 表示“犁地”。\n-able 一般缀于动词后，构成形容词，表示“可…的，能…”。\n\n#### profane\nadj. 亵渎的，世俗的 vt. 亵渎，玷污\n\n- profane（亵渎的）：不能进入神圣区域内的俗人\n- 英语单词 profane 原本是一个宗教术语。在古罗马人的宗教仪式中，一般的“俗人”不得进入宗教场所中的“神圣区域”（fanum），如寺庙的内庭，只有受过“启蒙”（initiated）的人即神职人员才有资格进入这片区域执行各种宗教仪式。世俗人士即使要向神灵祭祀，也只能将祭祀用的牺牲放在 fanum 之前的祭坛上。因此，在拉丁语中，用 profanus 来表示“世俗的”，是 sacred（神圣的）的反义词。它由 pro（in front of）+fanum 构成，字面意思就是“在 fanum 前门外的，不得进入 fanum 的”。\n- 现在，profane一词的宗教色彩已经逐渐消失，并在“世俗的”词义之上衍生出“亵渎的”之意。\n\n#### avow / disavow\n##### vow\nto make a determined decision or promise to do something\n发誓，立誓\n\n##### avow\n- a- 表示强调\nto admit something or say something publicly\n声明；宣称；承认\n- He **avowed** that he regretted what he had done. \n\t他承认对自己的所作所为感到后悔。\n- It is a society in which homosexuality is rarely **avowed**. \n\t这样一个社会中，很少有人公开承认自己是同性恋。\n\n##### disavow\ndis・a・vow\n\nto say that you know nothing about something, or that you have no responsibility for or connection with something\n声称对…一无所知；否认对…负有任何责任；声称与…毫无关联\n\n- They were quick to **disavow** the rumour. \n\t他们迅速否认与那些谣传有任何牵连。\n\n\n#### deify\n- dei-  \n\t= god, divine, 表示“神、神的”。源自拉丁语 deus \"god,\" divus \"divine, god.\"\n- -fy \n\t缀于名词或形容词后，表示 “…化”, “作成…”等意思的动词\n\n- deity 神, 神性\n- divine 神圣的\n\n\n#### concave\ncon- =\u003e 表示强调\ncave =\u003e hole, 洞穴\n洞穴是凹下去的, =\u003e 凹的\n\n凸的 =\u003e convex\n\n#### resuscitate☢☢\nre・suscitate\n- 1530s, \"**revive, restore, revivify (a thing), restore (a person) to life**,\" from Latin resuscitatus, resuscitare \"rouse again, revive,\" from re- \"**again**\" + suscitare \"**to raise, revive**,\"\n\n- to bring someone or something back to life or wake someone or something\n\t使苏醒；使恢复知觉；使复活\n- Her heart had stopped, but the doctors successfully resuscitated her. 她的心跳都已经停了，但医生们还是成功地把她从鬼门关拉了回来。\n\n\n#### deadpan\ndead + pan 而 pan 在 slang 里面是 face 的意思\n- looking or seeming serious when you're telling a joke.\n\n- ...her natural capacity for irony and deadpan humour.\n\n\n#### platitude\nplat- =\u003e flat 平 (e.g. platform, plateau, platter)\n-itude =\u003e 名词后缀, \"状态, 性质\"\n平凡无奇的话 =\u003e 引申为(贬义的) 陈词滥调, 老生常谈\n\n- Why couldn't he say something original instead of spouting the same platitude?\n\n不要和 plentitude 搞混\n\n#### goad\n![](notes/2022/2022.7/assets/Pasted%20image%2020220717195916.png)\n古时候用来赶牛的尖尖的赶牛棒, 后来引申为: \n\n招惹, 刺激, 激怒某人 (为了让他们做某事) =\u003e 赶牛棒让牛很烦躁\n- A group of children were goading (= laughing at or pushing) another child in the school playground. 一群孩子正在学校操场上嘲弄推搡另外一个孩子。 \n-  She seemed determined to goad him into a fight. 她似乎铁了心要激怒他来打一架。\n- He refused to be goaded by their insults. 他对他们的侮辱不屑一顾。\n\n刺激, 驱使  =\u003e 赶牛棒可以刺激牛向前走\n- The runner was goaded on by his desire to keep up with the others. 这名跑者不停地努力，一心要跟上其他人。\n\n\n#### evanesce☢☢\n/ˌev.əˈnes.əns/\ne- =\u003e \"从...离开\"\nvan- =\u003e empty, 空, 源自拉丁语 vanus \n-esce =\u003e \"动作的起始\"\n\n(of smoke, mist, etc.) 转瞬即逝, 消散, 隐没\n\nevanescent =\u003e evanesce+ent =\u003e adj. 短暂的, 转瞬即逝的, 瞬息的\n\n##### -esce\nac・qui・esce   =\u003e   v. 默许, 默然同意\nadol・esce        =\u003e   v. 进入青春期\nco・al・esce     =\u003e   v. 联合\nfluor・esce        =\u003e   v. 发荧光\nin・cand・esce =\u003e   v. 使白热化\nob・sol・esce   =\u003e   v. 使过时, 使淘汰\nopal・esce         =\u003e   v. 发出乳白色的光\nphosphor・esce =\u003e  v. 发出磷光\nre・juven・esce =\u003e  v. 使重焕青春\n\n- 上面很多词将后缀 \"esce\" 换成复合后缀 \"escence\"(-esce + -ence) 表示\"动作的起始\", 可以得到名词. 比如 adol・escence\n\n\n#### importune ☢\nim- =\u003e 没有\nport- =\u003e harbor 港口\n\n没有港口的, 无法入港的, =\u003e 引申为\"麻烦的\" =\u003e 强求, 纠缠, pester, press, plague, hound\n\n\n#### court\ncourt 作为动词还有\"奉承, 讨好, 献殷勤的意思\" \n- Adams is being courted by a number of football clubs. \n\t有好几家足球俱乐部在向亚当斯献殷勤。 \n\n=\u003e 进一步引申为\"求取, 设法取得\"的意思 \n - She courts publicity by inviting journalists to extravagant parties. 她邀请新闻记者参加奢华聚会来谋求曝光度。\n\n=\u003e 同时还有招致, 招惹麻烦的意思\n- Drinking and driving is simply courting disaster. \n\t酒后驾车纯粹是在招惹祸端。\n\n\n#### -sent\nsens-, sent- =\u003e feel, \"感觉\"\n\n##### dissent \ndis- =\u003e 相反, differ\n=\u003ediffer in sentiments =\u003e 不同意, 反对, 异议\n\n##### assent\nas- =\u003e 朝, 向, to, 一样\nfeel the same =\u003e 赞成, 同意\n\n##### resent\nre- =\u003e 相反, 相对, 也表示强调\nhave a strong feeling against =\u003e 憎恨, 愤恨\n\n##### consent\ncon- =\u003e with, together \nfeel together =\u003e 同意, 答应, 认可\n\n#### stipple☢\n点彩画法, 比如化妆时候用美妆蛋按压的动作, to apply paint with light dabs\n\n- 不要和 dabble 搞混了\n\n#### discredit☢\ndis + credit =\u003e 抹黑, 使名誉受损, 否认, 驳斥\ndisgrace, blame, shame, smear, dispute, \n\n#### pluck\n- 拔\npluck your eyebrows =\u003e 修眉\npluck the chicken =\u003e 给鸡拔毛\n- 摘\npluck a lemon\n- 拨(弦)\npluck the strings of a guitar\n\n引申为 =\u003e\n- to remove someone suddenly from a situation that is ordinary\n\t使脱颖而出；提升\n\tHe was plucked from obscurity to star in the film. 他脱颖而出成为这部电影的主演。\n\n- to remove someone quickly from a dangerous or difficult situation   使.... 脱离险境 \nThe last passengers were plucked from the ship just seconds before it sank. 最后一批游客在船沉没前几秒钟被救起。\n=\u003e \n(noun.)\n- courage and a strong wish to succeed  胆识，勇气\nShe showed a lot of pluck in standing up to her boss. \n她顶撞老板，显示出了很大的勇气。\n\n#### abjure☢☢\nab- =\u003e opposite, not, \njur- =\u003e swear, law, 发誓，法律\n- 背离自己的誓言 =\u003e 公开放弃\n\n#### adhere\nadhere 是黏附, 附着的意思, 可以引申为=\u003e \n- (黏附于某种信念) =\u003e 坚持某种观点或者信念\nIf you **adhere to an opinion or belief**, you support or hold it.\n- If you can't **adhere to** my values, then you have to find another place to live. \n\n- (黏附于某种规则) =\u003e 遵守某个规则, 准则\nIf you **adhere to a rule or agreement**, you act in the way that it says you should.\n- All members of the association **adhere to** a strict code of practice.\n- It is only when safety procedures are not strictly **adhered to** that catastrophes occur. \n\n#### antagonize\n- ant- \n表示“反对，相反”。源自希腊语 anti \"against, opposite.\"\n- agon- \n= struggle, 表示“挣扎，斗争”。源自希腊语 agein \"to drive, lead, weigh.\"\n- -ise \n动词后缀，一般缀于形容词后。-ise 是英式英语，-ize 是美式英语。源自希腊语 -izein, verbal suffix.\n\ncompete against =\u003e 引申为\"使敌对, 激怒\"\n\n#### antic☢☢\nantic - 这个词的特殊意义产生于意大利。文艺复兴时代，意大利人在古罗马遗迹发掘出大量奇形怪状的假面具和雕像。在罗马皇帝提图斯（Titus, 39-81）的豪华浴厅墙上雕刻的人物、动物、花草等更以风格奇异、形状怪诞而引人注目。意大利人把这些稀奇古怪的雕刻品称作antico。16世纪英语借用了该词，作antic。最初antic也指“奇形怪状的雕塑”，以后引申为“丑角”，但这两个词义今已不用。现在antic常指“滑稽动作”或“古怪行为”，且多作antics。在当形容词用时则是“古怪的”、“滑稽的”之意。实际上antico这一意大利词还可以追溯到拉丁语antīquus 'ancient'（古老的），英语里意指“古玩”、“古物”的antique一词也源出于此。\n\n例 \n- The antics of the clowns amused the children.  小丑们的滑稽动作把孩子们逗得发笑。\n- The audience were/was entertained by the antics of the magician. 观众被魔术师的滑稽动作逗乐了。\n\n##### attic\n阁楼\n\n#### auspicious☢\n/ɑːˈspɪʃ.əs/\nauspice + -ous\n\nauspice（吉兆）：古罗马人的鸟卜法\n古罗马的伊特鲁利亚人有通过观看飞鸟来占卜凶吉的做法。巫师用一个魔杖（lituus）在天空中画出一个区域，观察鸟在该区域的飞行，通过鸟的飞行来判断吉凶。这种观鸟占卜法称为 auspice，源自拉丁语 avis（bird，鸟）+specere（look，看）。后来，auspice 的含义演变为占卜时取得的吉兆，并逐渐衍生出“赞助、主办”的意思，因为吉兆不就是老天爷的支持和赞助吗？执行观鸟占卜的巫师叫做 augur 或 auspex，所以观鸟占卜法也被称为 augury。\n- auspice：['ɔːspɪs] n. 吉兆，赞助，主办\n- auspicious： [ɔ'spɪʃəs] adj. 吉兆的，吉利的；幸运的\n\n- augury：['ɔːgjʊrɪ] n.占卜，预言，预兆\n- augur：['ɔːgə] n. 占卜师，预言者 v. 预言，占卜\n- auspex： ['ɔːspeks] n. 鸟卜者；占卜者\n\n\n#### belie☢\nbe- =\u003e 使 ... 成为\nlie =\u003e 掩盖\n使 ... 被掩盖 =\u003e disguise, misrepresent, conceal, distort\n\n- To belie means to contradict. If you are 93 but look like you are 53, then your young looks belie your age.\n\n\n#### atrocious☢\natroc- =\u003e cruel, 残酷\n\n穷凶极恶的, 骇人听闻的, 残暴的\n=\u003e atrocity n. 暴行\n\n\n#### clog\n原来的意思是 a lump of wood(一大块木头) =\u003e 因为把一大块木头绑在上面东西上就会阻碍 ta 的行动, 所以后来有了 hinder, impede the movement of 的意思, 也有阻塞, 堵塞的意思\n- 因为有'一大块木头'的意思, 所以clog也代表鞋底(sole)是用一块木头做成的鞋子 =\u003e 木屐, 木底鞋\n\n#### culpable\nculp- =\u003e fault, 过错, 罪\n- 英语从拉丁文直接借用了不少词语，并且保留了原拉丁文拼写形式，mea culpa 即为其中之一，相当于英语 my fault（我的过失），是个较正式的用语。它源出罗马天主教礼拜仪式的祷文：“mea culpa, mea culpa, mea maxima culpa.”（这是我的过失，我的过失，我的极大过失。）教徒以此向上帝表示忏悔认罪。形容词 culpable 表面上是借自古法语 coulpable，但却是源于 culpa 'fault'（过失）一词，其古义为“有罪的”、“犯了罪的”，但今天多释义为“应受责备的”或“该受惩处的”。\n\n例 \n- They held him culpable for the offence. \n\t他们认为他有罪，应受惩罚。\n- The prime minister is highly culpable in this affair. \n\t这件事首相难辞其咎。\n\n##### culp-\n- exculpate =\u003e ex・culp・ate =\u003e 开脱, 使无罪\n- disculpate =\u003e dis・culp・pate =\u003e 开脱, 使无罪\n- inculpate =\u003e in・culp・ate =\u003e 归罪于, 加罪于\n- inculpable =\u003e in・culp・able =\u003e 无罪的\n\n\n#### defray\n- 来自古法语 defrayer, 成本，支付，来自 de-,向下，支出，-frai,破碎，花费，词源同 break. 原义为破坏和平的罚金。后词义通用化。\n\n\n\n#### delicacy\n- 类似于 fragility, 形容美丽优雅物品的脆弱与精美\n\t- the delicacy of a rose\n- 形容一个情况, 问题的\"微妙\", 需要很细致地处理才不会出错的情况\n\t- He sensed the delicacy of the situation\n\t- Both countries are behaving with rare delicacy.\n- 珍馐, 佳肴\n\t- Smoked salmon was considered an expensive delicacy.\n\n#### demolish☢\nto destroy (a large structure) completely \n- 引申到观点, 论点 =\u003e 推翻论点, 颠覆观点(形容很彻底)\n\t- Our intention was to demolish the rumours that surrounded him.\n- 引申到比赛, 对手 =\u003e 完败对手, 彻底击败对手\n\t- Millwall demolished Notts Country 6-0 on Saturday.\n- 引申到食物 =\u003e 狼吞虎咽\n\t- Joe demolished an enormous plateful of chicken and fries.\n\n#### accrete\nac- =\u003e 来, 向, 去, 也表示强调\ncre- creas- =\u003e grow, make, 增长\n\n=\u003e accrete 通常表示自然物质的积聚, 堆积, 吸积, 累积, 沉积\n\n##### cre- creas- \n- increase\n- increment\n- crescendo =\u003e n. (声音, 乐曲) 渐强, 最高点\n- procreate =\u003e v. 生育, 生殖, pro- 表示很多, 向前(在这里可以理解为时间上的向前)\n\n\n#### adulterate\nad- =\u003e 朝, 向, 去, 或者为强调\nulter- alter- =\u003e 改变, 其他的, other, to change, from Latin \"alius\"\n-ate =\u003e 动词后缀, 造成, 使...\n=\u003e to other, to change, =\u003e debase by mixing with foreign or inferior material, 掺假, 兑假货\n\n - There were complaints that the beer had been **adulterated with** water. 有多人投诉说这啤酒里面兑了水。 \n\n- adulterator\n- adulteration\n- adulterated\n\n=\u003e 引申到情感变质 =\u003e adultery n. 通奸, 婚外性行为\n- adulterer 奸夫, 通奸者\n- adulteress 奸夫\n- adulterous adj. 通奸的, \n\n\n#### allure☢☢\n鹰猎（falconry）在英国和欧洲大陆一度极为盛行。所谓鹰猎乃是放鹰狩猎，即使用鹰、隼等来狩猎的一种活动。这种鹰，即猎鹰（falcon），须先经过耐心而得法的驯化和训练，方能用于鹰猎。狩猎人往往用伴有肉食并系着长绳的一束鲜艳羽毛作引诱物召回猎鹰。这种引诱物英语就称lure。lure的终极词源为日耳曼语之古法语词loirre 'bait'（诱饵）。在现代英语中它通常用于贬义，泛指任何“引诱物”或“诱饵”，并引申为“诱惑力”或“吸引力”，也常作动词用，表示“诱惑”或“引诱”。作为lure之同义词的allure产生于同一背景。它源自古法语alurer，由a 'to'加lure 'falcon's lure'（狩猎人之诱饵）构成。虽然allure和lure基本同义，也表示“诱惑”、“引诱”和“诱惑力”，但一般不含贬义。\n\n例 \n- Rewards **allure** men to brave danger. 重赏之下必有勇夫。\n- Even in her fifties she had lost none of her seductive **allure**. 她虽年过半百，但风韵犹存。\n- Most newspaper journalists find it hard to resist the **allure** of working in television. 大多数报纸记者发现很难抗拒在电视台工作的吸引力。\n- He was **lured** into the job by the offer of a high salary. 他受高薪诱惑才接受了那份工作。\n\n- alluring =\u003e attractive or exciting\n\t- She was wearing a most alluring dress at Sam's dinner party.\n- allurement\n\n不要和 allude 弄混了\n\n\n#### menace☢\n威吓, 以喊叫威胁的方式驱赶牲畜\n古代人放牧、驱赶牲畜时比较粗暴简单，通常通过喊叫、威胁或鞭打的方式来驱赶牲畜，这种方式在拉丁语中叫做minare，进入法语后拼写为mener。英语单词menace（威胁）就来源于此，本意指的是驱赶牲畜时人所发出的叫喊声、威胁声，兼有“威胁”和“驱策”的含义。后来“驱策”的含义逐渐消失，仅仅表示“威胁、恐吓”，变成了threaten的同义词。\nmenace：['menəs] n.v.威胁，恐吓\nminacious： [mɪ'neʃəs] adj. 威吓的\nminatory：['mɪnə,təri] adj. 恐吓的，威胁的\n\n#### amenable☢☢\n还有一个单词也来源于此，那就是amenable，由a（=to，面对）+mener（驱赶、领导）+able（能够），字面意思就是“能够面对领导”，引申为“顺从的、易于管教、易于屈服、容易被说服的”，还可以表示“负有责任的、经得起检验的”。要注意这个词与amend（改善）并没有关系。\namenable：[ə'miːnəb(ə)l] adj.顺从的，肯接受的；负有责任的；经得起检验的\npromenade：[,prɑmə'ned] v. 散步，漫步；骑马 n. 散步；舞会；骑马 adj. 散步的\n\n\n#### ancillary\nancilla + ary\nancilla =\u003e Latin \"handmaid\" 女佣的, 也就是勤杂人员的意思, 引申为\"附属的, 附加的, 辅助的\"\n\n#### apparition\napparition（幽灵）：耶稣基督以婴儿形态诞生\n英语单词 apparition 来自拉丁语 apparitionem，是动词 apparere（appear，显现）的过去分词动名词形式，本意就是“显现”。这个词最初用于宗教场合，表示“基督显现”，即耶稣基督以婴儿的形态诞生。据《圣经•新约》记载，耶稣诞生时，东方三博士在观星时发现异象，便一路向西前来拜见“犹太人的王”，最后见到了刚出世的耶稣，向他献上了三件宝物为礼。为了纪念耶稣基督的显现，基督教徒将耶稣诞生之日定为“主显节”（Epiphany）。单词 epiphany 的本意和 apparition 一样，都表示“基督显现”，但常常用来表示“主显节”。而 apparition 一词的宗教色彩逐渐消退，常常用来表示幽灵鬼怪的显现。\n- apparition：[,æpə'rɪʃən] n. 幽灵，幻影，离奇出现的东西\n- epiphany：[ɪ'pɪfəni] n. 主显节，显现\n\nappearance 和 apparition 的词根相同, 也都表示\"出现\", 但是 apparition 表示令人惊讶的, unexpected 的显现, 所以通常用来指鬼灵的显现, 而 appearance 则用来指一般的出现了\n\n#### attest\nat- =\u003e to\ntest =\u003e 证明 \nto test, to bear witness to =\u003e prove, affirm, confirm, testify\n\n##### test-\n- 英语词根 testi-表示“作证”，如 testify（作证、证明）、testament（圣约）、testimony（证言、证据）。这个词根来自拉丁语 testis（睾丸），与单词 testicle（睾丸）同源。睾丸和作证有什么关系呢？有种说法认为，睾丸是男性特征的证明，所以原本表示睾丸的 testis 便衍生出“证明”之意。还有人认为，古代罗马人在法庭上作证时，会用手捧着睾丸以表示自己所说的千真万确、绝无虚言。\n- 但是，不少词源学家认为这一理由缺乏根据，属于传说。不管怎样，词根 testi-与 testicle（睾丸）同源，这个故事有助于我们记忆这些单词。\ntestify：['testɪfaɪ] v. 作证，证明，证实\ntestimony：['testɪmənɪ] n. 证词，证言，证据\ntestament：['tɛstəmənt] n. 圣约，确实的证明\ntesticle：['testɪk(ə)l] n. 睾丸\ntesticular：[tɛs'tɪkjʊlɚ] adj. [解剖] 睾丸的；睾丸状的\n\n\n#### awry\na-  =\u003e 表示加强\nwry =\u003e wring, 表示\"拧, 扭\", \n\"扭\"曲的, crooked, 歪的, =\u003e 引申为 \"发展方向离开预期路线的, 出错了的\"\n\n\n#### bonhomie\nbon =\u003e 法语, 好的\nhom =\u003e hum , 表示 \"人\", human\n\n好人 =\u003e 和蔼的, 性情温和的\n\n\n#### bootless\nboot =\u003e 效用, 可以这样记忆: boot 有启动计算机, 猛踢一脚的意思, 而 bootless 就是\"没有用的, 无效的, 徒劳的\" 的意思\n\n#### bound\n除了\"边界, 界限\"的意思, 其实 bound 还有以下含义\n\n**be bound to** happen =\u003e very likely to happen\n- You're bound to forget other people's names occasionally\n- These two young musicians **are bound for** international success.\n**I'll be bound** =\u003e 我敢肯定, 我打包票\n\nbound 还是 bind 的过去式, 所以就有了\"捆, 栓, 绑\"的意思, 引申为 \"束缚, 有义务, 受约束\"的意思, 也引申为了\"紧密相关\"的意思\n- We found the girl **bound** and gagged.\n- She feels (duty)-**bound** to tell him everything.\n- The company **is bound by** a special agreement to involve the union in important decisions.\n- We **are** as tightly **bound to** the people we dislike as to the people we love.\n- Economic growth **is** still **bound to** the issues of poverty, social justice and conservation. \n\n-bound =\u003e ... 装帧的书\na leather-bound book\n\n用于交通工具, 还有 destinated for 的意思: going to 去…的；准备前往…的；开往…的\n- She was on a plane **bound for** Moscow when she got sick. 她在前往莫斯科的飞机上突然生病了。\n\n#### vogue☢\na fashion or general liking\n**in vogue**\n- The short hemline(短的裙子下摆 =\u003e 短裙) is very much **in vogue** this spring.\n\n\n#### debunk\n##### bunk\nbunk 有上下铺, 火车卧铺的\"铺\"的意思, 该词可能系由 bunker（箱）逆构而成.\n而 bunk 另一个含义\"nonsense\"则有个故事:\n- 英语单词 bunk 源自美国北卡罗来纳州 Buncombe 县的名称。1820 年 2 月 25 日，在美国第 16 届国会上，南北双方议员就奴隶制问题进行了激烈讨论。在会议中，北卡罗来纳州 Buncombe 县议员费力克斯•沃克发表了冗长的讲话，并且与讨论的问题毫不相关。许多与会者纷纷退场。但沃克不肯缩短自己的讲话。他解释说，他这番长篇演讲不是说给大会听的，而是说给他的家乡 Buncombe 听的（talking to Buncombe），为的是使自己的讲话内容能够发表在 Buncombe 当地报纸上，以证明自己的勤勉工作。\n- 因此，talking to Buncombe 就成了“talking nonsense, 作空洞冗长的演讲”的代名词，后来该短语被缩写为 bunkum，到了 20 世纪又进一步缩减为 bunk，表示“废话”、“空话”。\n- 1916 年美国汽车制造商福特说了一句名言：“History is more or less bunk.”（历史多少有点骗人），使 bunk 一词得以广为流传。约在 1920 年有一位名叫 William E. Woodward 的人针对福特写了《骗人的鬼话》（Bunk）一书。他在书中据 bunk 杜撰了 debunk 一词，用以表示“揭穿”或“暴露”。今天 bunk 在美国几乎成了一个家喻户晓的常用词。\n\n例 \n- Most doctors think his theories are sheer bunk.\n- I've heard enough of your bunk. \n\n\n#### calumniate\n毁谤；恶意中伤；污蔑\n- He has tried to calumniate and destroy everyone whose opinions differ from his. 他会去恶意中伤和摧毁所有与他意见不同的人。\n\n- cal- \n= to deceive, 表示“欺骗，挑刺”。源自拉丁语 calvi \"to deceive, trick,\" cavilla \"a jeering.\"\n\n- cavil (吹毛求疵, 挑剔)的词根也来自这里\n\n##### calumny\n/ˈkæl.əm.ni/\n- 据有关记载，古罗马人对诬陷者施行一种惩戒性做法，即在他的前额打上 K 字印记，K 表示 kalumnia，意思是“诽谤”。以后 kalumnia 演变为 calumnia。英语的 calumny 源自该拉丁词，所以有“诽谤”、“诬陷之词”等义。其派生词 calumniator 则指“诽谤者”。\n- 在中世纪，一个人倘若遭到诽谤，很可能会向诽谤者提出挑战，要求决斗。意为“挑战”的challenge一词，若究其根源，也可一直追溯到这个表示“诽谤”的拉丁词。这两者被联系在一起是因为它们之间存在一种因果关系的缘故。\n\n例 \n- Calumnies are answered best with silence.\n- He was subjected to the most vicious calumny, but he never complained and never sued.\n\n\n#### cadge\n\"to beg\" (1812), \"to get by begging\" (1848), of uncertain origin, perhaps a back-formation from cadger \"*itinerant dealer with a pack-horse*\" (mid-15c.)\n- 像这种小贩都是很缠人的, 所以有了\"乞讨, 索取\"的意思\n\n#### discharge\ncharge 最原始的意思是\"to load\", dis-charge 就是\"unload, 卸货\"的意思\n\n- 卸除刑罚, 脱离疾病 =\u003e 允许从.. 离开, 出院, 释放\n- 卸除职责, 义务 =\u003e 履行职责, 义务\n- \"卸货\"引申为\"排放, 排出(废气, 废物等)\"\n- \"排出\"一义又引申到了武器上面, 意思是\"开火, 开枪\"(发出子弹)\n- charge 有\"收费\"的意思, discharge 也就有了\"还清债务\"的意思\n\n\n\n#### disseminate☢\ndis- =\u003e 分开, 散开\nsemin =\u003e seed, 种子\n-ate =\u003e动词后缀\n使种子分散开来 =\u003e 引申为散布, 传播(信息, 思想)\n- They disseminated anti-French propaganda\n\n##### semin-\n- semen 精液\n- seminal 精液的 =\u003e 引申为 具有深远影响力的, influential, ground-breaking, \n- inseminate =\u003e 使受精, 使受孕\n- seminar =\u003e 也许是\"培养种子人才\"的研讨会的意思?\n\n#### exacerbate\nex- =\u003e 向外, 向上, here probably thoroughly\nacerb =\u003e acid- acri- acu- 尖, 酸, 锐利, sour, sharp\n-ate =\u003e 动词后缀\n\n进一步(完全地)割入, 进一步(用酸)腐蚀 =\u003e 使原本就坏地情况恶化, 加剧\n\n- This attack will **exacerbate** the already tense relations between the two communities.\n\n#### explicate\n可以和 complicate 一起记忆:\nplic- =\u003e 源自拉丁语\"plicare\" to bend, to fold\nex- =\u003e 向外, \n-ate =\u003e 动词后缀, 使....\n- ex-是向外, explicate 就是\"展开\", 也就是 **(展开来)详细解释, 分析**\n- 而 com-是\"一起\", 将不同的东西一起弯曲折叠, 也就是将其变得很复杂\n\n**implicate**\n- 还有个 implicate, im- =\u003e 在内, 也就是将... 包在里面, 不放在表面上 =\u003e 暗示, 牵涉, 牵连\n\n#### extricate☢☢\n这个也可以和\"intricate\"一起记忆\n- ex- =\u003e 向外, 从 ... 离开\n- tric-, trig-  =\u003e hindrance, petty obstacles 障碍\n- -ate\n直接的理解就是\"将..从障碍, 困难里面移出来\" =\u003e **解救, 使摆脱**\n\n**intricate** 就是\"进入障碍, 困境\" =\u003e 引申为 adj. 错综复杂的, 精密复杂的\n这里-ate变成了形容词后缀, 表示 \"...的\", 比如\"accurate, intimate\"\n\n\n#### expurgate☢\nex- 向外, 从... 离开 \npurge =\u003e 肃清, 清除\n-ate =\u003e 动词后缀\nto purge =\u003e 引申到文字上面就是 删节, 删除(不正当的文字)\n\n\n\n#### fad\nfiddle-faddle 的缩写 \nfiddle-faddle 是\"瞎搞, 胡闹\"的意思(过时了)\n后来引申为了\"短暂的风潮, 一时的狂热\"\n- craze, fashion\n\n\n#### fastidious☢☢\n- meticulous 和 fastidious 都是**没有**贬义的, 表示严谨, 一丝不苟的\n- 而 particular 有\"not easily satisfied\"的意思, (挑剔)\n- fussy 则是一个贬义词, 过分挑剔的, 难缠的\n\n\n#### feckless☢☢\n\"effect+less\"的简写, 直译过来就是\"没有效用的\", 多用于形容人, 表示\"无能的, 软弱而不负责任的\"(所以不能产生任何正向的 effect)\n- 类似于 bootless\n\n#### futile☢\nfrugal =\u003e 节俭的\nfutile =\u003e 徒劳的\n\nfus- fund- =\u003e pour 流, 泄\n-ile =\u003e ... 的 \npouring out easily =\u003e 引申为徒劳的, 无用的\n\n##### fus-\n- confuse =\u003e 一起流动, =\u003e 混在了一起, 混乱的 =\u003e 引申到心理上, 迷惑的, 糊涂的\n- fuse =\u003e 熔化, 熔融\n- diffuse =\u003e dif- \"apart, in every direction\" 向四周流 =\u003e 扩散, 弥散, 分散, 散射\n- infuse =\u003e 向内流动, =\u003e 充满, 灌输, 浸润, 泡(茶) =\u003e 使某人充满某种感情, 将某种特性注入某物\n- suffuse =\u003e suf-: \"under\" =\u003e to pour under, overspread, 弥漫于, 充满\n\n\n#### indigenous\n**当地的，土生土长的**\n- endo- 表示“内部”。\n- gen-,gener-,genit-,-genesis = birth, produce, 表示“出生，产生”，医学上引申为“生殖或基因”。\n- -ous,-itious,-eous,-ious,-uous 表形容词，表示“…的”\n\n#### indigent☢\n- endo- 表示“内部”。\n- ig- =\u003e lack\n- -ent =\u003e ...的\n十分贫穷的\n\n#### knotty\n多结的 =\u003e 引申为棘手的, 复杂的\na knotty problem\n\n\n#### jocular\njoke 的形容词形式\n\n\n#### labyrinthine\n我们今天用以泛指“迷宫”的 labyrinth 一词原指神话传说中的希腊建筑师和雕刻家代达罗斯（Daedalus）为克里特国王弥诺斯（Minos）在首都克诺索斯（Knossos）王宫下面所修建的克里特迷宫（the Cretan labyrinth）。\n传说，克里特岛的国王米诺斯（Minos）是宙斯和欧罗巴公主的儿子。在他与兄弟争夺王位时，曾请求海神波塞冬降下神迹来支持他。波塞冬答应了他，从海中升出一头俊美的白色公牛，并要求事后把公牛献祭给他。没想到米诺斯当上国王后，舍不得宰杀这头俊美的白牛，就用另一头牛代替献祭给了波塞冬。波塞冬特别生气，想严惩这个欺骗神灵的国王，就施展法力，使王后爱上了这头白牛。王后后来生下了一个牛头人身的怪物米诺陶洛斯。为了遮羞，米诺斯请当时著名的工匠代达罗斯（Daedalus）建造了一座名叫 labyrinth 的迷宫，把弥诺陶洛斯（Minotaur）关在里面。\n后来弥诺斯的儿子被雅典人所杀，弥诺斯为了报仇，强迫雅典人每年（一说每九年）送童男童女各七名给怪物吞食。到第三次献牲的时候，希腊英雄雅典王子提修斯（Theseus）自告奋勇前往克里特迷宫除妖。迷宫曲径繁多，门户通道复杂难辨，一进去不易找到出路。提修斯杀死怪物后，在弥诺斯的女儿阿里阿德涅（Ariadne）的帮助下，靠她所给的线团引路，才得以逃离迷宫。\n\nlabyrinth一词是14世纪进入英语的。它源自希腊语labýrinthos/labúrinthos，若再进一步追本溯源，则可追溯至希腊词根lábrus/lábrys 'double-bladed axe'（双刃斧）。按字面义，labyrinth便是“the house of the axe”的意思。英国考古学家伊文思（Arthur John Evans, 1851-1941）于1900-1908年间发掘了克里特岛上的克诺索斯古城，出土古代王宫遗址一处，占地5.5英亩，其规模宏大壮观，王宫墙上便可见到这种斧头图形。关于克里特迷宫的传说，反映了公元前2000年克里特国的繁荣。\n\n例 \n- We could not find our way out of the labyrinth of dark and narrow streets. 我们没能从迷宫般又黑又窄的街道中找到出路。\n- Finally through a labyrinth of corridors she found his office. (CID) 穿过迷宫般的走廊，她终于找到了他的办公室。\n\nlabyrinth：['læb(ə)rɪnθ] n.迷宫\nlabyrinthine：[,læbə'rɪnθaɪn]adj. 迷宫的，错综复杂的\n\n##### clue\nclue（线索）：引导忒修斯走出迷宫的线团\n克里特国王米诺斯的儿子被雅典人杀害。米诺斯替儿子报仇，打败了雅典人，强迫他们每年给公牛怪米诺陶洛斯进贡七对童男童女。雅典国王埃勾斯（Aegeus）的儿子、英雄忒修斯（Theseus）为了替民除害，自愿作为贡品前往克里特岛。克里特国王的女儿阿里阿德涅（Ariadne）爱上了忒修斯，送给他一把利剑和一个线团。忒修斯进入迷宫，手持利剑杀死了公牛怪，然后顺着线团原路返回，顺利逃离迷宫。\n英语单词 clue 的来源就与这个故事有关。clue 在古英语中拼作 clew，原意是“线团”。因为忒修斯是凭借线团逃出迷宫的，所以线团成了他破解迷宫的线索。由此，单词 clue 就从“线团”引申出“线索”的含义。\n- clue：[kluː] n. 线索，情节\n- clueless： ['kluləs] adj. 无线索的；愚蠢的\n\n#### lachrymose\nlachrym- =\u003e tear, 泪\n- lachrymal adj, 眼泪的, n. 泪腺\n- lachrymose 爱哭的, 悲伤的, 伤感的\n\n后缀 -ose 表示\"... 多的\". 比如\"morose, grandiose, verbose\"\n\n\n#### laconic☢\nlaconic - 希腊伯罗奔尼撒半岛（Peloponnesus）东南部有个畜牧山区叫做 Laconia，原为古希腊著名奴隶制城邦斯巴达（Sparta）的所在地。古时候该地区的人（即 Laconians）曾以说话简洁、措辞精练著称于世。他们每每以最少的语词表达自己的意思。有一次马其顿国王腓力二世（Philip of Macedon）送信给斯巴达行政官，威胁说，“如果我们进入 Laconia，我们非把你们的城市夷为平地不可！”（If we enter Laconia, we will raze it to the ground.）得到的回复竟然是简短而又含蓄的“如果”二字（If）。\n\n\n#### lout ☢\na young man who behaves in a very rude, offensive, and sometimes violent way.\n不良少年\n\nloutish =\u003e 粗俗的, 无理的, 粗暴的\n\n\n#### madcap ☢\nmad + cap\ncap在这里用来比喻头 =\u003e \"疯头\" =\u003e 愚蠢的, 荒唐的, \n\n\n#### mendacious☢\nnot telling the truth\n- mend- =\u003e fault, physical defect (比如 amend)\n- -acious =\u003e 形容词后缀, 具有 ... 特征的 (比如 capacious, instantaneous, tenacious)\n\n\n#### mishap ☢\nmis + hap\nhap =\u003e luck, chance\n- 不幸, 模仿 mischance 造出来的一个词, mischance 也是\"不幸\"的意思\n\n#### mottle\n![300](notes/2022/2022.7/assets/illamasqua-mottle-swatch-macro.webp)\n\n#### murky\n![300](notes/2022/2022.7/assets/360_F_327352560_Z6yMt0anWYWTVOnulbml9ZqVLlTlah0z.jpg)\n- dark and dirty and difficult to see through\n引申为情况复杂而不明朗的\n有趣的是, get into the murky water of ... 就是中文里面的\"趟浑水\"的意思\n- I don't want to get into the murky water of family arguments.\n\n#### nascent\nLatin nasci \"to be born\" =\u003e `nasc-`, `nat-`, `nai-` \n- nascent =\u003e nasc・ent =\u003e 出生的 =\u003e  新生的, 萌芽的, developing, beginning, dawning, evolving\n- nat・ive\n- nai・ve\n- nat・al\n- in・nat・e\n- re・naiss・ance\n\n#### nexus\n`nect- nex-` =\u003e to bind, tie, connect, 连结, 源自拉丁语\"nectere, 过去分词, **nexus**\"\n\n- connect =\u003e con・nect\n- nexus =\u003e nex・us\n- annex =\u003e an・nex =\u003e 吞并, 兼并, 强占 basically means \"to connect with\" \n\n#### notch\nV 形的槽 =\u003e 引申为刻度, 等级\n- Average earnings in the economy moved up another **notch** in August\n\n- iPhone手机上面的刘海也叫 notch\n![300](notes/2022/2022.7/assets/iphone-x-notch.jpg)\n\n#### onerous\noner- =\u003e load, burden, 来自希腊语onus\n-ous =\u003e ... 的\n**burdensome, troublesome**, difficult to do or needing a lot of effort\n\n- exonerate =\u003e ex・oner・ate =\u003e 免除负担 =\u003e 证明... 无罪, 免除罪责, 免除责任\n\n#### opine☢\n**opt-**  =\u003e to choose, from Latin \"opinari\"\n\n- to choose, to have an opinion =\u003e express an opinion, 发表意见\n\n- opinion\n- option\n- opt (for, to do..)\n- adopt \n\n\n#### opportune☢\nop- =\u003e against, 朝, 对, 向\nport =\u003e harbor, 海港, 港口\n=\u003e 面向港口的 =\u003e (时间)方便的, 合适的, 恰当的, 适宜的\n- Would it be opportune to discuss the contract now?\n\n- inopportune\n- opportunity\n还有一个关于\"海港 port-\"的单词是 importune \n![importune](#importune)\n\n\n#### ossify\nfrom Latin \"ossis\" =\u003e bone\n骨化 =\u003e 引申为 僵化, 使(思维, 行为)固定不变 \n- 变得很硬, 像骨头一样硬, 所以难以改变, 僵化里面的\"僵\"也是\"硬\"的意思\n\n#### paragon\nparagon（模范）：用来检验金子纯度的试金石\n古代西方人为了检验金子的纯度，常常使用一种用纹理细密的深色玉石做成的试金石（**touchstone**）。将要检验的金子在试金石上摩擦，就可以根据划痕颜色来判断金子的纯度。这种试金石在意大利语中被称为paragone，它来自希腊语parakonan，由para（并列、对抗）+ akone（磨石）构成。英语单词paragon就来源于此，本意就是这种试金石。因为试金石是衡量价值的标准，因此衍生出“完美之物、模范”之意。\nparagon： ['pærəg(ə)n] n. 完美之物，模范，试金石 adj. 完美的\n\n![](notes/2022/2022.7/assets/Touchstone.webp)\n\n#### paramount\npara 意为\"超过\", mount 表示\"山顶\", 比山顶还高的 =\u003e 至高无上的\n\n\n#### paunch\n大肚子, 啤酒肚, a fat stomach\npaunchy =\u003e 有 paunch 的\n\n#### perambulate☢\nper- 向前\nambulate 移动, 步行, 走动\n=\u003e to walk around for pleasure\n\n\n#### perfervid\nper- 完全\nfervid, 热情的, 热忱的, \n=\u003e very hot, very ardent, enthusiastic, zealous\n感觉 perfervid 要比 fervid 更强烈一点\n\n##### ferv-\n来自拉丁语, boil, 沸, 热, 常常引申为情感的强烈, 比如汉语里面的热情, 热忱, 热爱.\n- 有趣的是, [汉语里面的“热爱”一词来自英文的 ardently love](https://zh.wikipedia.org/wiki/%E6%8B%89%E4%B8%81%E8%AF%AD)，而该英文词源自拉丁语的 ardenter amare。这种说法在古代汉语中是找不到的。 \n\n- fervent\n- fervid\n- fervor, fervour\n- fervency\n\n#### perspicacious☢\n敏锐的, 有洞察力的\n如果从单词直接来理解就是 \"有能看穿东西的能力的\"\n- per- =\u003e 完全, 贯穿, forth, 向前\n- spic- =\u003e to look, see, 来着拉丁语 specere\n- -acious =\u003e 具有... 特征的\n\n还有一个词很像: **perspicuous** =\u003e 清晰明了的, 易懂的 ☢\n- per- =\u003e 完全, 贯穿, forth, 向前\n- spic- =\u003e to look, see, 来着拉丁语 specere\n- -uous =\u003e ... 的\n从单词直接理解就是 \"能够被直接看穿的\"\n\n- **perspic**acious 的名词是 **perspic**acity\n- **perspic**uous 的名词是 **perspic**uity\n\n#### pithy☢\npith =\u003e 髓, 木髓, 骨髓 引申为 \"意义, 精华\"\npithy 就是\"凝练的, 精辟的\"\n\n有意思的是句子外面那层白色的东西东西也叫 pith\n- a pithy orange\n\n#### philistine\nphilistine（庸俗之辈）：以色列的死敌腓力斯人\n- 腓力斯人（philistine）居住在中东迦南南部海岸的古民族，其领土在后来的文献中被称为“腓力斯地”（Palaestina，巴勒斯坦）。腓力斯人是以色列人的劲敌，在圣经旧约中详细记载了以色列人和腓力斯人的战争和恩怨。在以色列人眼中，腓力斯人是没有信仰的野蛮人，是文明的破坏者。\n- 1689 年，在德国一个叫做耶拿（Jena）的城市中，当地的大学生和市民之间爆发了一场激烈的冲突，有数人在冲突中丧生。事后，耶拿大学的牧师在一次布道中，针对这场大学生和市民之间的冲突发表了尖锐的评论，把与学生作对的市民比作圣经中与以色列人作对的腓力斯人。从此以后，人们用“腓力斯人”（philistine）来形容那些没有接受过大学教育，缺乏文化修养并鄙视文化的人。从这个词还产生了“庸俗主义”（Philistinism）一词，字面意思就是“腓力斯人的方式、习惯、个性和思维方式”，指的是低估、鄙视艺术、审美、精神和智力的态度，也称为“反智主义”。\n- philistine： ['fɪlɪstin] adj. 俗气的；无教养的；腓力斯人的 n. 俗气的人；仇敌；门外汉\n- Philistinism：['fɪləstɪnzəm] n.庸俗主义，实利主义，市侩主义\n\n\n#### affectation\nbehaviour or speech that is not sincere\n矫揉造作，做作，装模作样；假装\n- She has so many little affectations. \n\t她身上有好多矫揉造作的小毛病。 \n\n- 单词 **affect** 也有对应的意思\n\nto pretend to feel or think something, 假装\n- To all his problems she affected indifference. \n\t对他所有的问题，她都装作漠不关心的样子。\n\nto start to wear or do something in order to make people admire or respect you, （为引人注意或自抬身价而）刻意穿戴，故作\n- At university he affected an upper-class accent. \n\t在大学里他拿腔捏调假装上流社会口音。\n- He's recently affected a hat and cane. \n\t近来，他头上戴了顶帽子，手上拿了根拐杖，装腔作势起来。\n\n\n\n#### decry\nde- =\u003e 向下, 从... 离开, 相反, 使相反\ncry =\u003e 呼喊\n反着呼喊 =\u003e 谴责, 抨击, condemn, blame, abuse, blast\n\n#### deracinate\nde- =\u003e 彻底离开, 完全离开\nrad-, radic- =\u003e root\n-ate =\u003e 动词后缀, 使..., 造成..\n=\u003e 连根拔除, 根除, to pluck up by the roots\n\n##### rad-, radic-\n- radish =\u003e rad・ish =\u003e 萝卜\n- radical =\u003e rad・ical =\u003e \"像根一样的\" =\u003e 根本的, 基本的 =\u003e 要根本改变需要很彻底, 激进的改变 =\u003e 激进的, 激进分子  \n- radix =\u003e radix就是Latin的root =\u003e \"数字的根\" =\u003e 基数\n- eradicate =\u003e e・radic・ate =\u003e 连根拔起, 根除\n- irradicable =\u003e ir・radic・able =\u003e 不能根除的\n\n\n#### [根除, 彻底消灭](https://www.writingtips.cc/exterminate-vs-extirpate-vs-eradicate-vs-uproot-vs-deracinate-vs-wipe/)\n**Exterminate** implies utter extinction; it therefore usually implies a killing off.\n\n/ˈek.stɚ.peɪt/\n**Extirpate** implies extinction of a group, kind, or growth, but it may carry less an implication of killing off, as _exterminate_ carries, than one of the destruction or removal of the things essential to survival and reproduction; thus, wolves might be _exterminated_ by hunting in a particular area, but large carnivores in general are _extirpated_ by changed conditions in thickly settled regions; a heresy is often _extirpated,_ rather than _exterminated,_ by the removal of the leaders from a position of influence; a vice cannot easily be _extirpated_ so long as the conditions which promote it remain in existence.\n\n**Eradicate** stresses the driving out or elimination of something that has taken root or has established itself.\n\n**Uproot** differs from _eradicate_ chiefly in being more definitely figurative and in suggesting forcible and violent methods similar to those of a tempest that tears trees out by their roots.\n\n**Deracinate** basically is very close to _uproot_, but in much recent use it denotes specifically to separate (as oneself or one's work) from a natural or traditional racial, social, or intellectual group.\n\n**Wipe** (in this sense used with _out_ ) often implies extermination, but it equally often suggests a canceling or obliterating (as by payment, retaliation, or exhaustion of supply).\n\n\n#### discursive☢\ndiscursive 有两个意思, \"东拉西扯的\"和\"论证的\", 分别可以这样理解:\n- 东拉西扯的\ndis- =\u003e 分开, 散开, apart\ncurs =\u003e run, \n-ive =\u003e ... 的\n到处跑的, 向四周跑的 =\u003e 引申为论证不着边际, 东拉西扯\n\n- 论证的, 推理的\n##### discourse\ndis- =\u003e apart\ncourse =\u003e to run\n=\u003e to run about, to run to and fro =\u003e 类比交谈时候两个人一来一回的说话\n- discursive 就是 discourse 的形容词, 也就是\"交谈的\", 也许是古人论证会采用\"对话式论证\", 就引申为了\"论证的, 推理的\"\n\n##### currere : 流\ncurrency 是 current 的派生词。按字面义，current 的意思是 running，而 currency 的意思则是 running money。它们源自拉丁语动词 currere 'to run'（流）。该词原指液体的流动，在中世纪拉丁语中转指从一地流通到另一地的钱，进入古法语后变为 corir。其变化词形之一的 currant 进入中古英语，最后演变为 current。current 在现代英语用于“现时的”、“现行的”、“水流”、“电流”等义，但早在 15 世纪时 current 也有与金钱流通相关的意思。此义后来转到了 currency。在 17 世纪末，当 currency 开始在英国使用时，它仅指硬币，因为当时还没有纸币。通货之所以称 currency，因为它是 running money（流通货币），也是 the current medium of exchange（流通的交换媒介）。currency 用于此义的最早文字记录见于美国政治家和科学家富兰克林（Benjamin Franklin, 1706-1790）1729 年的著作中。除指“货币”和“通货”之外，currency 也可表示“流通”或“传播”。\n\n英语中源于拉丁语动词 currere 的词还有不少，诸如:\n- concur（原义：“流到一起, 一起流动”，今义：“同时发生”或“同意”），\n- occur（原义：“流向”，今义：“发生”），\n- recur（原义：“再流”，今义：“复发”），\n- course（水流等的走向；进程；课程），\n- discourse（话语；交谈），\n- courier（信使）\n等等。\n\n例 \n- Do you change foreign currency? 你们兑换外币吗？\n- The Euro will eventually replace European national currencies. 欧元将最终取代欧洲各国货币。\n- The word has obtained/attained general currency.\n\t该词已经得到普遍使用了。\n\n#### dissemble☢\n改写自dissimulate \ndis- =\u003e 不, apart\nsimul- =\u003e alike, same\n-ate\n使自己和(真实的样子)不一样, =\u003e 掩饰, 掩盖(真实动机, 情感, 真相等)\n- Henry was not slow to dissemble when it served his purposes\n\n#### effrontery☢\n厚颜无耻；放肆\nef- =\u003e ex-的变形, 向外, 向上\nfront =\u003e forehead, 前额\n-ery =\u003e **名词**后缀, 表示某种情况\n- 不是很好理解, 下面有两个方法帮助记忆:\n\n- For every discussion he comes to the front and argue. How rude!\n- Think: I'm the king, but he stepped ***in front of me***. How dare he be so bold!\n\n- He was silent all through the meal and then had the effrontery to complain that I looked bored!\n\n#### germane☢\ngerm- =\u003e 种子, seed \ngermane =\u003e having the same parents, from the same seeds, =\u003e 同类的东西想必关系密切 =\u003e 关系密切的, 紧密相关的(通常用于 ideas and information)\n- Her remarks could not have been more germane to the discussion.\n\n#### inveigh☢\nin- =\u003e in, 进入\nveigh =\u003e to carry, 运输\n=\u003e 直译就是\"运进来\", \"抨击, 痛斥\"的意思是由 invective 一词带来的, \"把(坏话)运进来\" =\u003e 猛烈抨击, 痛骂\n\ninvective =\u003e noun. criticism that is very forceful, unkind, and often rude\n\n\n#### jettison\nto throw overboard, 原指在船只遇险的时候, 将船上的货物丢弃到海中减轻重量, \"紧急时刻丢弃船, 飞机上面的货物来减轻重量\" =\u003e 引申为\"将... 丢弃, 放弃(某个计划或想法)\", \n\n#### lamentable\nlament是哀悼, 悲叹的意思, lamentable直接理解就是\"让人感到悲伤的, 让人哀悼的\", 现在通常指一个情况太糟糕了, 糟糕到让人有点生气, 应该收到谴责的糟糕, \"可悲的\",  regrettable, distressing, tragic, \n\n\n#### mangy☢\nmange 是家畜的一种皮肤传染病, 症状包括瘙痒, 脱毛, 皮肤粗糙, 水泡\n\"疥癣\", mangy就是\"得了mange的\"\n\n又因为长了这种癣的动物就变得很丑, 很脏 =\u003e 引申为 dirty, old, shabby 的东西\n\n不要和mingy搞混了, mingy是小气的, 吝啬的的意思, 也用于形容数量少得可怜的\n\n\n#### nip\n- 去一会儿(某个地方) e.g. nip to the shop\n- 轻咬, 轻捏\n\n- 一点点(烈性酒) =\u003e 大概是因为那种猛的一下火辣辣的感觉和 nip 的动作很像\n\n##### a nip (in the air)\n用来形容寒冷, 大概是因为冷风在脸上的刺痛感也和nip的动作很像叭\n汉语是\"刺骨\", 英语是\"咬脸\"🤣\n- You can tell the winter's on its way - there a real nip in the air in the morning\n\n##### to nip something in the bud\n把....扼杀在萌芽状态 =\u003e 这个短语和汉语里面的结构几乎一样 \"把...的芽/花苞掐掉\"\n\n\n#### pernicious☢\nper- =\u003e completely 完全的, e.g. perfect, permanent\nnic- =\u003e hurt, poison, 伤害, 比如 innocent, innocuous\n-ious =\u003e ... 的, 形容词后缀\n=\u003e \"完全伤害的\" =\u003e 极其恶劣的, 极其有害的, very harmful, wicked\n\n##### noc-, nox- ☢\n- innocent =\u003e in・noc・ent =\u003e 没有罪的 =\u003e 无罪的, 无辜的, 天真的\n- innocuous =\u003e in・noc・uous =\u003e 无毒的 =\u003e 无害的\n- nocuous =\u003e 有害的\n- obnoxious =\u003e ob表示强调 =\u003e 有害的 =\u003e 令人讨厌的, 可憎的\n- noxious =\u003e 有毒的\n\n#### phenomenal\n非凡的, 杰出的, remarkable\n其实 phenomenon 除了\"现象\"这个含义以外, 还有\"不同寻常的事物\"的含义, 这两个之间的联系可能是 phenomenon 所描述的现象本来就是 unusual 和 interesting 的\n\n\n#### magpie☢\n喜鹊\n![](notes/2022/2022.7/assets/index.jpg)\n##### pied\n因为喜鹊的羽毛颜色是黑白相间的, 所以pied就有了\"杂色的, (黑白)混色的\"的意思\n\n##### piebald\nbald 古时也有\"斑驳的\"含义, 两者相结合, 仍然表示\"有黑白花斑的\"的含义\n\n#### plummet\nplumb- 来着 Latin 里面的 plumbum, 表示 lead, 铅元素\n我们知道铅的密度很大, plummet 就是\"像铅块一样下坠\" =\u003e to fall very quickly and suddenly, 也用于描述价格, 数量等概念的\"暴跌\"\n\n##### aplomb\n- aplomb（沉着）：像铅垂那样处于垂直状态\n古人很早就发现铅的比重较大，特别适合用来确定垂直线，不容易因为风、水流等偏离垂直方向，所以在建筑、测深等领域广泛使用铅垂。英语中表示“铅”的单词plumb来自拉丁语plumbum。它既能表示“铅”这种物质，还可以表示“垂直的”、“使垂直”等含义。英语单词aplomb由a（at，处于……状态）和plomb（plumb，垂直的）组成，字面意思就是“像铅垂那样处于垂直状态”。由于铅垂不易受风或水流影响而左右乱动，所以aplomb引申出“沉着、泰然自若”的含义。\naplomb：[ə'plɒm] n. 垂直，沉着，泰然自若\nplumb：[plʌm] n. 铅垂，垂直 adj. 垂直的 vt. 使垂直，探索，探测\n\n##### plumber\n- plumber（水管工）：古罗马人用铅制作水管\n在英语中，表示“水管工”的单词是plumber，由plumb（铅）+er（人）组成，字面意思就是“铅制品工人”。水管与铅制品有什么关系呢？原来，这是因为在古罗马时期，人们广泛用铅来制作水管。\n古罗马人的冶金技术已经相当发达。在冶炼白银的过程中，他们得到了副产品铅。古罗马人发现金属铅具有很好的特性，用铅制成的日常器皿能保持持久光亮，不会像铜器那样产生绿绣；在葡萄酒中加入铅粉不仅能去除葡萄的酸味，还能使酒味更加醇厚；用铅粉做成的化妆品能够让妇女的皮肤更白。因此，古罗马人的贵族广泛使用铅制品，例如，他们把铅条夹在木棍中用于书写，这就是“铅笔”一词的由来。他们还使用铅做成水管，构建了四通八达的地下管道。这些管道中含有的金属铅逐步溶解之水中，被喝进罗马人的身体内，甚至是孕妇的体内。就这样，一代又一代的罗马人都受到了严重的铅中毒。很多科学家认为，铅中毒是古罗马帝国破落的重要原因之一。\n在拉丁语中，plumbum表示“铅”，由此衍生出英语单词plumb（铅）。由于古罗马的上下水管道都是用铅制成的，因此安装、维修这些管道的工人在英语中就被称为plumber（铅制品工人）了。\nplumb：[plʌm] n. 铅，铅垂，垂直 adj. 垂直的 vt. 使垂直，（用铅垂）探测\nplumber：['plʌmə] n.水管工\nplumbing：['plʌmɪŋ]n. 铅工业，水管工业，水管工作\n\n\n#### ambrosial\nambrosial 是 ambrosia 的形容词\n##### ambrosia\n在希腊神话中，众神食用的食物叫做 ambrosia，神仙们食用后可以永葆美貌并长生不老。该词由 a（不）+mbrotos（=mortos，死亡）+名词后缀-ia，字面意思就是“长生不死”。英语单词 ambrosia 就来源于此。\n- ambrosia：[æm'brəʊzɪə] n. 神的食物，特别美味的食物\n- ambrosial：[æm'brəʊzjəl] adj. 特别美味的，可口的，芬香的\n- ambrosian：[æm'brəuzjəl,-zjən] adj. 特别美味的，可口的，芬香的；神的，敬神的，适用于神的\n\n##### mort- ☢\n= death\n- mor-（死亡）：死神墨尔斯\n在罗马神话中，死神叫做墨尓斯（Mors），对应于希腊神话中的桑纳托斯。在西方文化中，墨尓斯常被描绘为身着黑色长袍，手持长柄镰刀的阴森老人。墨尓斯的名字 Mors 在拉丁语中就是“死亡”的意思，英语中表示“死亡”的词根 mor-/mort-就来源于此。\n- Mors is often connected to Mars, the Roman god of war; 因为火星在肉眼看来是红色的, 所以古人将火星看作\"灾星\"\n\n- mortal：['mɔːt(ə)l] adj. 凡人的，致死的，总有一死的 n. 人类，凡人\n- mortality：[mɔː'tælɪtɪ] n. 死亡数，死亡率，必死性\n- immortal：[ɪ'mɔːt(ə)l] adj. 不朽的，长生不死的，神仙的 n. 神仙，不朽人物\n- immortality： [,ɪmɔr'tæləti] n. 不朽；不朽的声名；不灭\n- moribund：['mɒrɪbʌnd] adj. 垂死的，停滞不前的 n. 垂死的人\n- mortician：[mɔː'tɪʃ(ə)n] n. 殡葬业者，丧事承办人\n- mortuary：['mɔːtjʊərɪ; -tʃʊ-] n. 太平间 adj. 死的，悲哀的\n- mortify： ['mɔrtɪfaɪ] vt. 抑制；苦修；使…感屈辱 vi. 禁欲；苦行；约束\n\n###### mortgage\nmortgage（抵押贷款）：以父亲遗产为担保的贷款\n在古代西方，家庭中的长子在法律上拥有继承父亲遗产的权利。如果长子需要一大笔钱，而又无法从其父亲那里获得，他往往会找其他人借款。而其他人之所以愿意借钱给他，看中的是他的长子继承权，相信他将来继承遗产后可以偿还债务。借钱的时候，借款人会立下誓言，等他父亲去世，他继承遗产后就会偿还债务及其利息。这就是英语单词 mortgage 的来源。mortgage 由 mort 和 gage 组成，mort 表示“死亡”，gage 表示“誓言、保证”，所以 mortgage 一词的字面含义就是“死亡保证”，即以其父亲的死亡（等于遗产）为保证的贷款。\n- mortgage： ['mɔːgɪdʒ] vt.n. 抵押贷款\n\n\n\n#### apprehend☢\nap- =\u003e to, 朝, 向, 去, 或者弱化为强调\nprehend- =\u003e to grasp, catch, seize 抓住\n=\u003e 引申到\"精神上\"就是 understand fully 的意思. \n=\u003e 在破案的时候, 警察需要根据线索\"apprehend\"整个案件, 然后逮捕嫌疑人, 所以 apprehend 也有\"逮捕, 拘捕\"的意思, arrest, catch\n- 而 **apprehensive** 直接看就是 capable of perceiving 的意思, 后来渐渐演变为了\"多虑的, 惴惴不安的, too concerned, anxious\"的意思\n\n##### -prehend-☢\nto grasp, catch, seize 抓住\n在形容词里面最后的 d 一般会变成 s\n- reprehend =\u003e re・prenend =\u003e 不抓 =\u003e 推开, 引申到言语上就是\"谴责, 非难\"的意思\n- reprehensible =\u003e re・prehens・ible =\u003e 应该被 reprehend 的, 应受谴责的\n- comprehend =\u003e com・prehend =\u003e 这里 com 意思为\"一起\", 或者表示强调\"completely\", =\u003e 完全地理解, 充分理解, 领悟\n- comprehension =\u003e 理解, 理解力\n- comprehensive =\u003e \"充分理解的\" =\u003e引申为\"全面的, 详尽的, 包罗万象的\"\n\n##### apprehend? comprehend? What's the difference?\nThe main difference is one of style. *Apprehend* is almost never used in modern spoken English with the meaning ‘understand’. It’s old-fashioned, formal and literary in style. In modern (British) English *apprehend* means ‘arrest’: \n- the police *apprehended* the suspect. \n\n*Comprehend* is more common, but it is used less often than ‘understand’ in conversation. It is often used where the verb is negative: \n- People do not *comprehend* the complexity of the issue; \n- There are some things that the human mind cannot *comprehend*.\n\n#### astound\nastound 是从 astone 来的, a・stone 直接理解就是\"变得像石头一样\", 人在非常震惊的时候会 freezed, 呆住, 就像石头一样. astound 就是\"使其呆住\" =\u003e \"使震惊, 使惊骇\"的意思了 \n\nastounding 就是\"十分令人惊讶的, 令人惊骇的\"的意思, 有 shocked 的含义在里面, 比 surprised 要强烈.\n\n\n#### broach☢\n提起(某个敏感的或者困难的话题)\n原来的意思是\"to pierce\", 类似于汉语里面的\"挑明了说\", 但是含义稍有不同, \n##### broker（经纪人）：打开酒桶卖酒的人\n在古代欧洲的酒吧或其他零售酒水的地方，卖酒的小贩会批发采购一桶一桶的啤酒或其他酒类，然后打开酒桶，装上龙头，然后一杯一杯地卖给喝酒的人。打开酒桶的工具在法语中叫 broche，后来演变为英语单词 broach（钻头、凿子）。而表示“打开酒桶”的法语动词 brochier 产生了名词 brocheor，后来演变成英语中的 broker，字面意思就是“打开酒桶的人”，原本用来表示零售酒水的小贩，后来泛指各种经销商，在金融行业中，则用来表示经纪人、掮客。虽然中文叫法不同，但其实都是经销商、中间人的意思。经纪人其实就是把股票、证券等金融产品贩卖给个体投资者的中间人。\n- broker： ['brəʊkə] n. 经纪人，中间人，掮客 v. 以中间人身份来谈判、安排\n- brokerage：['brokərɪdʒ] n. 佣金；回扣；中间人业务\n- broach： [brəʊtʃ] n. 钻头，凿子，胸针 vt. 提出，给……钻孔、开口，开始讨论\n\n\n#### damper\ndamper 是名词, **put a damper on sth** 的意思是\"给... 浇冷水, 浇灭... 的兴致的意思\", to stop an occasion from being enjoyable\n\n- damp 是\"潮湿的, 阴冷的\"的意思, 也有\"使潮湿的意思\"\n- 一开始 damp 是用来形容煤矿里面的毒气的, 后来变成了形容煤矿里面阴冷潮湿的状态, \n- damper 从上面这个含义引申来了\"to suffocate\"的意思, damper 就是用来控制火焰大小的 挡板\n- 后来用在乐器上面, 就是用来控制声音大小, 使琴弦停止震动的\"阻尼器, 减振器\",\n- **put a damper on** 就类似于踩下钢琴的 damper, 让声音一下减弱, \n\n\n#### ephemeral\n- ep- \n表示“在…上，在…周围，在…后面”。源自希腊语 epi \"on, over, at.\"\n- hemer- \n= day, 表示“一天”。源自希腊语 hemera \"day.\"\n- -al \n表形容词，“…的”，一般缀于名词后。源自拉丁语 -alis, adjective suffix.\n\nephemeron（蜉蝣）：只有一天寿命的小虫\n蜉蝣（ephemeron）是最原始的有翅昆虫，其幼虫生活在淡水湖或溪流中。经过一年左右的成长期后，幼虫或浮升到水面，或爬到水边石块或植物茎上，日落后羽化为亚成虫。亚成虫与成虫相似，出水后停留在水域附近的植物上，一般经24小时左右蜕皮为成虫。蜉蝣成虫不进食，寿命短，只负责繁衍后代的任务，一般只活几小时至数天，所以有“朝生暮死”的说法。蜉蝣的英文名称ephemeron也反映了它的这一特性。ephemeron来自希腊语ephemeros，由epi-（on）+hemera（day）构成，字面意思就是“延续一天的”。\nephemeron：[ɪ'fɛmə,rɑn] n. 蜉蝣；生命极短暂的东西\nephemeral：[ə'fɛmərəl] adj. 短暂的；朝生暮死的 n. 只生存一天的事物\n\n#### exhilarate\nex- =\u003e out, out of, thoroughly\nhilar- =\u003e glad, 开心, 比如 hilarious\n-ate\n将开心带出来 =\u003e 使开心, 使兴奋, 使激动, elate\n\n\n#### exorbitant\n(价格, 要求等)离谱的, 过分的\n\n直接翻译这个单词相当于\"离轨的\", \n\n#### frowzy☢\nnot attractive, new, or fashionable\n邋遢的，不整洁的\n- She was wearing a frowzy dress in a dour shade of purple.\n\n和 frown 长得很像, 可以这样记忆, 一个东西如此的\"frowzy\", 以至于一见到, 一闻到就要皱眉头\"frown\"\n\n#### irascible☢\nire 是 anger, rage, fury 的意思\nirascible 就是 made angry easily 的意思\n\n#### maudlin\nmaudlin（多愁善感的）：圣经中爱掉眼泪的女人抹大拉\n英语单词maudlin来自《圣经》里的一个女性人物：抹大拉的玛利亚。很久以来这个女人一直以一个被耶稣拯救的妓女形象出现在基督教的传说里：她用忏悔的眼泪为耶稣洗脚，用密软的黑发把它们擦干；在耶稣被钉上十字架行刑的日日夜夜里哀哭祈祷、喂他喝水；耶稣死后她进入停尸的墓穴预备亲自用油脂为其净身，却意外发现耶稣死而复活。由于抹大拉常常以流泪的形象出现，所以她的名字成为了多愁善感者的代名词。\nmaudlin: ['mɔːdlɪn] adj. 多愁善感的，易伤感的，易流泪的 n. 伤感\n\n#### medley\n原义是\"徒手混战, hand-to-hand combat\", 徒手大家都是很混乱的\nmedley 的词根和 meddle 是一样的, mesler =\u003e to mix, mingle, meddle, 后来也指一种由许多不同颜色的羊毛混织的花布, 最后引申为了现在的意思: \"混合物, 混杂物, (音乐)串烧, 大杂烩\"\n\n\n#### mirth☢\n来自 merry, 欢乐, 欢笑\n- mirthful, 愉快的, 高兴的, 令人欢乐的\n- mirthless. 沉闷的, 不快的, 忧郁的\n\n##### mir- \nwonder, look, 表示惊奇\n- admire\n- miracle\n- miraculous\n- mirage =\u003e 海市蜃楼, 幻想, 妄想\n\n\n#### pariah\n印度种姓（caste）多而复杂，大致可分四大等级，最高等级叫婆罗门（僧侣），最低等级叫首陀罗（手艺人和劳动者）。印度南部最大的种姓在泰米尔语（Tamil）中叫paraiyar，意为drummers（鼓手），因为此一种姓的人常常充当节庆礼仪的鼓手，故名。英国殖民者家中的仆佣也多半是此一种姓的人。英语pariah一词即由此而来。虽然pariah并非最低贱的种姓，但在最高贵的婆罗门眼中，却被看成种姓之外的“不可接触者”（untouchable），即最受歧视和压迫的“贱民”。因此，pariah在17世纪初进入英语时，即被赋予“贱民”一义，到了19世纪又进而引申为“遭社会遗弃的人”或“被痛恨或排斥的人”。\n\n\n#### patrician\n古罗马建城之初，创建者罗慕路斯召集各氏族中德高望重的首脑人物，由他们组成元老院，担任各种公职，协助他治理罗马。这些人被称为 patres，是 pater（father，家长、家父）的复数形式。英语词根 pater-/patri-（父）就来源于此。\n古罗马建立后很长一段时期内，所有公职都只能由 patres 的后代来担任，因此形成了一个专门的统治阶级，被称为 patricius。该词经由法语进入英语后演变为 patrician（贵族）。罗马公民中，patrician 以外的人被称为 plebeian（平民）。最初，贵族和平民之间不得通婚，平民不能担任任何公职。经过平民阶级的长期抗争后，最后才迫使贵族们废除了这些不平等制度。\n与 nobleman 不同，patrician 是由血统决定的，具有封闭性和世袭性。patrician 的后代永远是 patrician，而 plebeian 的后代永远不能成为 patrician，但如果能取得执政官这样的高位后，就变成了 nobleman，他的后代也能世袭为 nobleman。所以 patrician 算是老牌贵族，而 nobleman 算是新兴贵族。\n- pater-/patri-：父\n- patrician：[pə'trɪʃ(ə)n] n. 贵族，古罗马氏族贵族 adj. 贵族的，显贵的\n- Patrick：['pætrɪk] n. 帕特里克（男子名），意为“高贵的”\n- Patricia：[pə'triʃə] n. 帕特丽夏（女子名），意为“高贵的”\n- paternal：[pə'tɝnl] adj. 父亲的；得自父亲的；父亲般的\n- paternity：[pə'tɝnəti] n. 父权；父系；父系后裔\n- patrimony： ['pætrɪmoni] n. 遗产；祖传的财物；继承物；教会的财产\n- patriarch：['petrɪɑrk] n. 家长；族长；元老；创始人\n\n#### pertinacious\nper- + tenacious(tinacious)\n- per- 作为前缀表示完全, 自始至终, 而 tenacious 本来就有\"坚持的, 顽固的\"的意思, 合在一起表示\"坚决的, 坚定不移的, 顽强的\"\n\n#### precipitate☢\n- precipitate 有三个词性:\n\t- 名词: (化学里面的)沉淀物\n\t- 形容词: 仓促的, 贸然的, 轻率的\n\t- 动词: 促成(...的发生), 加速...的发生\n\n- pre-在前 + cipit-头 + -ate 使 → 使头在前，一头栽下，后用于气象术语，降水，降雪，引申物理术语沉淀，沉积，积灰。\n- 英语中有为数不少的词，其终极词源可以一直追溯到拉丁语 caput（头），capital 即为其中之一。它来自 caput 的派生词 capitālis 'of the head'，因此最初也表示“头的”。英国诗人弥尔顿（John Milton, 1608-1674）在长诗《失乐园》中写了(Serpent's) capital bruise（头部的伤痕）这样的字句，其中 capital 一词即含此义。capital 的几个常用词义均与“头”有联系。旧时一个人犯了 capital crime（死罪）被判以 capital sentence（死刑）或被处以 capital punishment（极刑），不是被砍头就是被绞死。capital letter（大写字母）一般多位于句首和词首。capital 还用以指“首都”、“首府”，该用法出自弥尔顿笔下，始见于《失乐园》。capital 用以指“资本”则始于用牛的头数计算财富的时代，但这一用法直至 18 世纪才通用起来。\n- 除了capital，源自拉丁语caput的英语常用词还有cape（海角），captain（队长，船长），decapitate（斩首），chapter（章，回），precipice（悬崖），precipitate（突然下降；促成；沉淀）等。\n\n\n#### prescience☢\nforeknowledge, knowledge of events before they take place.\n\n##### science\nstate or fact of knowing, from Latin *scientia* \"knowledge, expertness\"\n\n##### scientist\n很多人认为，英语单词 science（科学）和 scientist（科学家）的诞生时间应该相差不远，这是一个很普遍的误解。事实上，science（科学）一词出现于 14 世纪中期，而 scientist（科学家）一词直到 19 世纪 30 年代才诞生，二者相差接近 5 个世纪。那么，在 scientist 一词出现之前，研究 science 的人叫做什么呢？“自然哲学家”，这是 scientist 一词诞生之前科学家们最常使用的称谓，因为他们认为自己研究的是“自然哲学”，如 1687 年，大科学家牛顿出版的科学巨著就叫做《自然哲学的数学原理》，而不是《自然科学的数学原理》。\n然而，到了 19 世纪，越来越多的人对“自然哲学家”这一称谓感到不满了。在 1833 年召开的英国科学促进协会的一次大会上，著名诗人柯勒律治站起来对参会者说：“你们必须停止自称为‘自然哲学家’。”在他看来，真正的哲学家应该像他那样，坐在扶手椅上对着星空进行沉思，而不是像协会的大多数成员那样忙于做各种稀奇古怪的实验。面对柯勒律治的质疑，一位名叫 William Whewell 的“自然哲学家”发言，提出如果认为“哲学家”一词过于宽泛、过于崇高，那么，可以仿照“artist”（艺术家）生造一个词：“Scientist”，用作对科学家的称谓。一年后，在一篇匿名书评中，Whewell 再次提到这个建议。从此以后，scientist 一词逐渐得到普及，成为了科学家的称谓，哲学家和科学家也最终实现了分家。\n- science：['saɪəns] n. 科学，学科\n- scientist：['saɪəntɪst] n. 科学家\n\n#### recalcitrant☢\n\"**refusing to submit, not submissive or compliant**,\" 1823, from French récalcitrant, literally \"**kicking back**\" (17c.-18c.), from Late Latin recalcitrantem (nominative recalcitrans), present participle of recalcitrare \"**to kick back**\" (of horses), also \"be inaccessible,\" in Late Latin \"to be petulant or disobedient;\" from re- \"back\" (see re-) + Latin calcitrare \"to kick,\" from calx (genitive calcis) \"heel\" (see calcaneus). Used from 1797 as a French word in English.\n\n#### renege☢\nre- 这里表示强调\nneg =\u003e to refuse, 比如 negate\n=\u003e 否认自己做过的承诺 =\u003e 违背诺言, 违约, break your word, default, go back\n\n- renegade =\u003e 变节者\n\n#### resplendent☢\nre- 这里表示强调\nsplendent =\u003e to shine, be splendid\n=\u003e 辉煌的, 灿烂的, 华丽的, brilliant, radiant, splendid. glorious\n\n在这里我们可以发现\"光明\"和\"火焰\"在汉语和英语里面都有着十分积极的含义.\n\n#### retaliate☢\n来自拉丁语 retaliare, 偿还，返还，来自 re-, 向后，往回，talis, 同样，同类，词源同 this, that. 即原样返还的，后用于指报仇，以眼还眼，以牙还牙\n\n####  rub-\nrub-：红色，红色的\n古代罗马人发现一种颜色发红的橡木质地特别坚硬，因此广泛使用这种橡木作为建筑材料。这种橡木在拉丁语中称为 rubus，来自 ruber（red，红色）。rubus 的形容词是 robustus，本意是“rubus 做的”。由于 rubus 特别坚硬，因此 robustus 也就衍生出“坚硬、强壮，像橡树一样”的意思。英语单词 robust 就来源于 robustus。英语中表示“红色”的词根 rub-同样来自拉丁语 ruber（红色）。\n- red： [rɛd] n. 红色，红颜料；赤字 adj. 红色的\n- robust： [rə(ʊ)'bʌst] adj. 强健的，强壮的，粗野的，粗鲁的\n- ruby：['rubi] n. 红宝石；红宝石色 adj. 红宝石色的 vt. 使带红宝石色\n- rubicund：['ruːbɪk(ə)nd] adj. 红润的；透红的\n- rubric： ['rubrɪk] n. 红字标题；红色印刷；题目 adj. 印为红字的\n\n#### sectary ≠ secretary\nsecretary 是秘书的意思\nsectary 的词根是 sect-, 是宗派成员的意思.\nsectarian =\u003e adj. 宗派成员\nsectarianism =\u003e n. 宗派主义\n\n#### soliloquy\nsoli- =\u003e alone\nloqu- =\u003e speak (e.g. loquacious)\nto speak alone =\u003e 独白\n\n#### stodgy☢\nStodgy food =\u003e heavy and unhealthy, 胀肚子的, 让人易饱的, 高淀粉的\n=\u003e 引申到人的性格上面 =\u003e boring,  serious, and formal, 古板的, 枯燥乏味的.\n\n#### superimpose\n![300](notes/2022/2022.7/assets/1000x1000bb.jpg)\n\n#### surreptitious☢\n- sur- \n= sub-，用在同辅音词根前表示“在…下面”。源自拉丁语 sub \"under.\"\n- rept- \n= creep, 表示“爬”。源自拉丁语 repere \"to creep.\", 比如 reptile\n- -itious \n表形容词，表示“…的”\n=\u003e to creep under, =\u003e 鬼鬼祟祟的, 偷偷摸摸的, secret, clandestine, furtive, sneaking\n\n#### derelict\nde- =\u003e \"entirely\"\nre- =\u003e 表强调\nlict =\u003e to leave, 留下, 遗弃\n=\u003e 前面两个前缀相当于都是表强调 =\u003e (adj. 建筑)年久失修的, 破败的, 废弃的, (noun. )无家可归的人\n\n- a derelict site\n\n##### dereliction\n除了\"建筑物的废弃\"(noun.)这一个意思, dereliction 引申之后还有\"玩忽职守, 渎职的意思\"\n- What you did was a grave dereliction of duty.\n\n#### temporize☢\n- tempor =\u003e time, age, season, 时间, e.g. temporal\n- -ize =\u003e 动词后缀\nto pass one's time, wait one's time =\u003e (为了取得有利条件而)拖延时间\n\n\n#### indict☢☢\n起诉, 指控, 控告 =\u003e 和定罪不一样, 只是起诉\ncharge, accuse, prosecute, \n并且注意读音:  /ɪnˈdaɪt/ \n- He was indicted on drug charges at Snaresbrook Crown Court. \n\t在斯奈尔斯布鲁克刑事法庭他被起诉犯有与毒品有关的罪行。\n- US Five people were indicted for making and selling counterfeit currency. 5人因制售假钞被起诉。\n\n#### impunity\n- im- 无, 没有\n- pun- 惩罚, 罪责 (例如punish)\n- -ity 名词后缀\n没有罪责, 免除罪责(注意这是一个名词)\n- Criminal gang are terrorizing the city with apparent impunity.\n\n\n#### panacea\n希腊神话中，帕那刻亚（Panakeia）是医药神埃斯库拉庇乌斯的女儿，光明神阿波罗的孙女。埃斯库拉庇乌斯共有5个女儿，分别代表了阿波罗的一种医药能力，其中以帕那刻亚和许癸厄亚最为有名。帕那刻亚代表了治疗，而许癸厄亚代表了清洁卫生与健康。古代医生在开始正式从业前，要宣读著名的希波克拉底誓词，宣誓对象除了阿波罗与埃斯库拉庇乌斯外，还包括许癸厄亚和帕那刻亚姐妹俩。\n帕那刻亚的名字Panakeia在希腊语中是“治疗一切”的意思，由pan（全部）和akos （治愈）构成，相当于英语中的all+cure。她的名字经由拉丁语进入英语后，演变为英语单词panacea，用来表示“包治百病的灵丹妙药”。\npanacea：[,pænə'siə] n.包治百病的灵丹妙药\n\n#### caprice☢☢\n我们在用 caprice 一词时，一般是不会立刻联想到刺猬和山羊的。可是，它们在 caprice 的词义发展中却起了一定作用。caprice 直接借自法语 caprice，但却源于意大利语 capriccio。该意大利词由 capo 'head'（头）和 riccio 'hedgehog'（刺猬）两部分组成，字面义是“刺猬头”，含有 head with hair standing on end（毛骨悚然）或 fright（惊吓）等义，因为人在受惊吓时毛发就会竖起，正像刺猬的硬刺一样。以后 capriccio 因前半部 capr 形似意大利词 capra（山羊）而与山羊的特性相联系。山羊有个奇特的习性，在安详地低头吃草的时候，往往会突然跃起或跳向一旁，然后又若无其事地恢复原状而继续低头吃草，capriccio 因而被赋予了英语 caprice 至今还在使用的“反复无常”、“多变”、“任性”等义。原意大利词 capriccio 也被英语直接吸收，用作音乐术语，指“随想曲”。\n\n例 \n- Her decision was pure caprice. \n\t她的决定纯粹是一时心血来潮。\n- Mary's actions are unpredictable. She is known for her caprice. 玛丽的行动是难以预测的。她的反复无常是出了名的。\n- He burst into a rage, out of caprice. \n\t他莫名其妙地勃然大怒起来。\n\n- whim, impulse, \n\n#### browbeat\n字面意义是\"用眉毛打\", 可以想到指的是对某人吹胡子瞪眼(想要威吓, 吓唬)的意思, 后来就用来指\"威吓, 吓唬\"某人\n\nDon't be browbeaten into working more hours than you want!\n\n#### draconian\n公元前621年，希腊城邦雅典的统治者德拉古（Draco）迫于平民的压力，将以往的习惯法加以编纂，颁布了古希腊的第一部成文法。该法以严酷而闻名。对于刑事犯罪，一律处以死刑。就连盗窃水果这样的轻罪也是死刑。德拉古解释说轻罪本来就该处死，至于重罪，因为找不到比处死更重的刑罚，所以也是处死。德拉古法如此严酷，以至历史学家说它不是用墨水写的，而是用血写的。德拉古法后来被梭伦修改，但由此产生了单词draconian，形容严厉苛刻。\n- draconian：[drə'konɪən] adj. 严厉的，苛刻的。\n\n#### exquisite☢☢\nex- 表示强调\nquis- =\u003e to seek, 探求, 询问\n-ite =\u003e \n深入地探寻的，精益求精的 =\u003e very beautiful and delicate\n- An exquisite piece of china\n- An exquisite painting\n\n- 用于形容情感, 还有\"强烈的\"的意思, \n\t- exquisite joy\n\t- The pain was quite exquisite\n\n- 用于形容品质, 还有\"完美的, 精湛的, 卓越的\"的意思\n\t - A good comedian needs to have an exquisite sense of timing.\n\n\n#### haphazard\n- hap- 是运气的意思, 之前背过的 mishap 也有这个词根.\n- not having an obvious order or plan\n\t无秩序的，无计划的，随意的\n- He tackled the problem in a typically haphazard manner. 像往常一样，他处理这个问题时也很没有章法。\n\n![mishap](#mishap)\n- hazard（冒险）：用骰子来玩的赌博游戏\n英语单词 hazard 来自阿拉伯语 al zahr（the dice），意思是“骰子”。在十字军东征期间，十字军在东方从阿拉伯人那里学会了一些用骰子来玩的游戏，也将这个词带回了欧洲，原本指的是用骰子来玩的赌博游戏。由于这种游戏都需要冒险、赌运气，所以 hazard 一词就衍生出了“冒险、赌运气”等含义。\n- hazard：['hæzəd] vt. 赌运气，冒……的风险，使遭受危险 n. 冒险，危险，冒险的事\n- hazardous：['hæzədəs] adj. 有危险的，冒险的，碰运气的\n- haphazard： ['hæphæzəd] adj. 偶然的；随便的；无计划的 n. 偶然；偶然事件 adv. 偶然地；随意地\n\n#### estrange☢\ne- strange =\u003e 使疏远, 使疏离, 离间\n- he is *estranged* from his wife\n\n\n#### fiasco☢\n- something planned that goes wrong and is a complete failure, usually in an embarrassing way\n\nfiasco - 该词原为意大利语，字面义是bottle（玻璃瓶）或flask（长颈瓶）。从19世纪末期起，英语开始用fiasco来表示“惨败”或“完全失败”。那么，“惨败”与“玻璃瓶”之间究竟有什么联系呢？众说不一，以下两种说法比较可信。\n\n一说与威尼斯玻璃吹制工（glassblower）有关。工人在吹制玻璃饰品时，因技艺不精出了差错或发现有瑕疵，便马上把废玻璃转制成质量稍次的普通玻璃瓶或长颈瓶，意大利语作far fiasco，意即“制成玻璃瓶”，以后fiasco逐渐引申为“失败”。\n\n一说起源于剧院。在某次戏剧表演时，玻璃瓶被意外摔破导致演出完全失败，意大利人也说far fiasco，以后演员忘了台词，人们也用它作比。久而久之，fiasco就被用以喻指“完全失败”。\n\n例 \n- My math test was a complete fiasco. I only answered two questions. 我的数学测验彻底失败。我只回答了两道题。\n- The first lecture I ever gave was a total fiasco. 我讲的第一次课彻底失败了。\n- The party ended in fiasco. 那次聚会以完全失败告终。\n\n\n#### pos- 放置\n##### dispose\ndis- =\u003e 分开, 散开 \n分开摆放, 有序放置 =\u003e 后面多指\"有效地处理, 处置, 清除, 解决, 击败\"\n注意, dispose 在使用时都是固定搭配: **dispose of**\n1. If you **dispose of** something that you no longer want or need, you throw it away.\n\t- Just fold up the nappy and **dispose of** it in the normal manner.  \n\t- ...the safest means of **disposing of** nuclear waste. \n\t- Engine oil cannot be **disposed of** down drains.  \n2. If you **dispose of** a problem, task, or question, you deal with it.\n- You did us a great favour by **disposing of** that problem.  \n- The justices have been arguing about how the case should be **disposed of**. \n3. To **dispose of** a person or an animal means to kill them.\n\t- He alleged that they had hired an assassin to **dispose of** him.  \n\n##### expose\nex- 向外 =\u003e 向外摆放, 向外放置 =\u003e 暴露, 揭发, 曝光\n\n##### impose\nim- \"into, in\" =\u003e 摆放到...里面 =\u003e 引申为 to apply authoritatively, 强制实行, 强加于\n\n##### depose\nde- \"down\" =\u003e lay down, let fall, 将(权贵之人)从位置上拿下来 =\u003e 罢免, 使...免职, 使...下台\n- Mr Ben Bella was deposed in a coup in 1965.\n- ...the deposed dictator. \n\n##### compose\ncom- =\u003e  together, with, =\u003e 一起摆放, =\u003e 创作, 由...组成\n- Air *is composed mainly of* nitrogen and oxygen. 空气主要由氮和氧构成。\n- The committee *is composed of* MPs, doctors, academics and members of the public. 委员会由议员、医生、学者和普通民众组成。\n- The audience *was* *composed* *largely* *of* young people. 观众中大部分都是年轻人。 \n\n后来又有了**使...镇定下来**的意思\n**compose yourself** =\u003e to make yourself calm again after being angry or upset 使自己镇定；使自己平静下来\n- She finally stopped crying and *composed herself*. 她终于停止哭泣，安静了下来。\n**compose your features/thoughts** =\u003e to try to make yourself look or feel calm after being angry or upset 使表情（或思绪等）镇定\n- I tried to *compose my features* into a smile. 我极力使表情平静下来，露出一丝微笑。\n- He took a minute or two to *compose his thoughts* before he replied. 他用一两分钟平静了思绪后才作出回答。\n\n##### propose\npro- =\u003e forth, 向前\n=\u003e 放到前面 =\u003e 建议, 提议, \n=\u003e 求婚\n=\u003e 提名, 推荐\n=\u003e 计划, 打算\n\n\n#### incontrovertible\nin + controvert + ible\n不容置疑的；无可辩驳的\n\n##### controvert\n反驳, 否定\n- This theory was subsequently *controverted* by several researchers in the same field. \n\t这个理论后来被同领域的几位研究者所否定。 \n\n\n#### nocturnal\nnoct- =\u003e night\nnocturnal =\u003e 夜行性的\ndi- =\u003e day\ndiurnal =\u003e 昼行性的\n\n#### scrappy\nscrap =\u003e垃圾, 碎片 =\u003escrappy: 散乱的, 不连贯的, badly organized\nscrap =\u003e fight, quarrel =\u003e scrappy: 好斗的, 爱打架的\n\n#### attenuate☢\nat + tenu + ate\ntenu =\u003e thin, fine, 细\n=\u003e 引申为\"使减少, 使减弱, 使降低\"\n\n- Radiation from the sun is attenuated by the earth's atmosphere. 地球大气减弱了太阳辐射。 \n- You could never eliminate risk, but preparation and training could attenuate it. \n\n\n#### chasm☢\n**（岩石、地面或冰面的）裂隙；峡谷；深渊**\n- cha- \n表示“空白、巨大的空间”。源自希腊语 khasma \"yawning gulf, \"\n\n\n##### chaos（混沌）：混沌之神卡俄斯\n古希腊神话的第一部分是创世阶段，即以神话方式解释世界的来源。在这一阶段出现的神祇通常称为“原始神”或“老神”，他们分别是世界某一部分的拟人化，后来被第二代的泰坦神族推翻。\n卡俄斯（Chaos）是希腊神话中最早的的神灵，代表宇宙形成之前模糊一团的景象。根据古希腊著名历史学家赫西奥德（Hesiod）的《神谱》和早期希腊神话记载：宇宙之初只有卡俄斯，祂是一个无边无际、一无所有的虚空。随后祂依靠无性繁殖从自身内部诞生了大地女神、深渊神、黑暗神、黑夜女神和爱神等五大创世神，世界由此开始。\n卡俄斯（Chaos）在希腊语中拼写为 khaos，本意是“虚空、裂开”，由词干 kha（空洞）和名词词尾-os 构成。该词在拉丁语中拼写变为 chaos，并经由法语进入英语，形成了英语单词 chaos。现在，chaos 常常被用来表示“混乱、混沌”，该含义源自赫西奥德的说法。他将 chaos 描述秩序诞生之前的宇宙，与秩序诞生之后的宇宙 cosmos（希腊语为 Kosmos）相对应。从 chaos 衍生出形容词 chaotic 就是“混乱的”之意。\n与 chaos 同源的单词还有 chasm（裂口、深坑），它源自希腊语 khasma。另外，常见单词 gas（气体）也与 chaos 同源。它来自荷兰语，而在荷兰语中，字母 g 的发音十分接近希腊语中的 kh。\n- chaos： ['keɪɒs] n. 混沌、混乱\n- cosmos：['kɑzmos] n. 宇宙，和谐，秩序\n- chaotic： [keɪ'ɒtɪk] adj. 混乱的、无秩序的\n- chasm：['kæzəm] n. 峡谷；裂口；分歧；深坑\n- gas：[ɡæs] n. 气体；瓦斯；汽油；毒气\n\n#### dappled\n有斑点的，花斑的；斑驳的\n![](notes/2022/2022.7/assets/0002pS-4105.jpg)\n\n#### canvass\ncanvass - 细心的人可能会发现canvass和另一个词canvas在词形上极为相似，仅有一个字母之差。其实何止词形，在词源上两者就有密切的亲缘关系。\n\ncanvas源自拉丁语cannabis 'hemp'（大麻），但却直接借自古法语，在中古英语作canevas，因早先粗帆布系由大麻制成，故canvas即被用以表示“粗帆布”，而后又由此引申出“油画布”、“油画”等义。而canvass则是16世纪时从canvas演变而来的，从一开始就作为动词用。这也说明为什么canvass的s是双写的。canvass的原义是to toss someone in a canvas sheet for pleasure or punishment（为了取乐或体罚而把某人裹在帆布床单里使劲摇），此后词义几经变化，由“痛打”而“痛斥”，由“痛斥”而“（详细）讨论”。旧时粗帆布一度被用来筛东西，由此又引申出“（详细）检查（如选票）”、“征求意见”、“拉选票”等义，尤用于美国英语。\n\n例 \n- We'll have to *canvass* the entire area before the referendum. 我们必须在全民公决前在整个地区进行游说。\n- Candidates *canvassed* the city's neighborhoods for votes.  候选人到城市的邻近地区拉选票。\n- Party members were out *canvassing* as soon as a date for the election was announced. 选举日期一宣布，党员们便四出游说。\n- They decided to *canvass* opinions from the general public. 他们决定广泛征求公众意见。\n\n#### iniquity\nin- 不\niqu- =\u003e =equ, equal, 公平, 公正\n-ity\n=\u003e 相当于 inequity, a very wrong and unfair action or situation\n\n- The writer reflects on human injustice and iniquity.\n\n\n#### headlong\n头向前的, 同 head-first =\u003e 头向前一股脑向前冲 \n=\u003e 速度很快地, adv.  hastily, hurriedly, helter-skelter\n- The car skidded and plunged headlong over the cliff.\n=\u003e 鲁莽地, 轻率地, rashly, wildly, hastily, precipitately\n- Do not leap headlong into decisions\n\n\n#### turgid\n原来是指(器官或者生物组织)肿大的, 肿胀的, swollen, =\u003e 后来可以引申为\"文章, 写作\"严肃的, 枯燥乏味的(充满了废话, 因为废话而肿胀)\n- a couple of pages of turgid prose.\n\n也可以指水流动不畅的, (水)死的\n- The river rolled darkly brown and turgid.\n\n\n#### gainsay\ngain- =\u003e against, 反对, e.g. against\nsay =\u003e 说\n反着说, 对着说 =\u003e  反驳, 反对, 否认\n\n固定搭配: there's no gainsaying =\u003e 无可辩驳, 无可否认\n\n- Certainly **there's no gainsaying** (= it is not possible to doubt) the technical brilliance of his performance.\n- Who could possibly **gainsay** such a judgment? \n- **There is no gainsaying the fact that** they have been responsible for a truly great building.\n\n#### gratuitous☢☢\n来自 grat-, 感谢，词源同 grace,gratitude. 原指表达感谢，报答，免费给予。后词义贬义化，指无缘无故的，莫名其妙的。\n\n - A lot of viewers complained that there was too much **gratuitous** sex and violence in the film. 很多观众抱怨说影片中有太多无谓的色情和暴力镜头。 \n \n#### exasperate☢☢\nex- =\u003e out, out of, thoroughly\nasper =\u003e rough\n-ate\n=\u003e to bring out the rough side of (sb) =\u003e 激怒, provoke, irritate\n- The sheer futility of it all exasperates her.\n\n#### dismantle\ndis- mantle =\u003e 取下 mantle =\u003e 拆开, 拆卸(某个部件)\n=\u003e 逐渐的废除, 取消, 解散(一个系统或者组织)\n\n##### mantle☢\n- (前任留给继任者的)衣钵, 责任\n- 覆盖层\n- 地幔\n- 披风, 斗篷\n\n#### hearken\nto listen, to hear =\u003e hear-ken\n\n#### hubris☢\n来自希腊语 hybris, 对神的放肆无礼。引申词义傲慢，狂妄。\n比如公元前 480 年，波斯王薛西斯带领大军进军希腊，可是在达达尼尔海峡，波斯人刚搭建的两座浮桥都被狂风吹垮，愤怒的薛西斯令人把铁索抛进海里，想要锁住大海，并派人鞭打大海 300 下，以报复大海阻止他前进。神怒了，后果是薛西斯输掉了战争。\n\n\n#### hallow☢\nhallow：['hæləʊ] vt. 使...神圣；把…视为神圣 n. 圣徒\n\nHalloween（万圣夜）：古代英国的萨温节\n一说起“万圣节”，很多中国人都会想到南瓜饼、鬼怪面具、小孩挨家挨户讨糖果等场景。这其实是一个常见的误解，以上活动庆祝的是每年的 10 月 31 日晚上的 Halloween（万圣节前夕、万圣夜），并非每年 11 月 1 日的“万圣节”（Allhallows's Day）。Halloween 是“万圣节”（Allhallows's Day）的前夜，但它的庆祝传统并非源自“万圣节”，而是来自古代凯尔特人的“萨温节”（Samhain，仲夏节、相当于中国的“鬼节”）。\n在古代凯尔特人的历法中，每年十月 31 日是一年中的最后一天，是夏天正式结束、严酷冬季开始的一天。古代凯尔特人信奉“德鲁伊”宗教，相信故人的亡魂会在这一天回到故居，在活人身上找寻生灵，借此再生，而且这是人在死后能获得再生的唯一希望。活着的人惧怕亡魂来夺生，于是人们就在这一天晚上熄掉家里的炉火和烛光，让亡魂无法找到家里来，还把自己打扮成妖魔鬼怪的模样，把亡魂吓走。\n罗马人入侵不列颠群岛后，将萨温节和罗马的两个节日合并。基督教成为罗马国教后，为了压制这种古代异教传统，希望人们淡忘萨温节，就把11月1日定为“万圣节”（AllHallow's Day），用来纪念殉道圣徒，hallow即圣徒之意。然而，萨温节的传统依然没有从人们的记忆中抹掉。人们虽然在万圣节对基督教圣徒表达敬意，却更喜欢在万圣节前一夜按照传统方式进行庆祝，如做南瓜灯、化妆、小孩子挨家挨户讨糖吃等。只是这个夜晚不再叫做“萨温节”，而是被称为**Halloween，是All-hallow eve的缩写**，意思就是“万圣节前夜”，但很多中国人并不明白二者的关系，往往将Halloween和“万圣节”混为一谈。\nHalloween：[ˌhæləʊ'iːn] n.万圣节前夜，鬼节\nAllhallows's Day：万圣节\n\n#### debonair\n(especially of men) attractive, confident, and carefully dressed\n（尤指男人）有魅力的，光彩照人的，风度翩翩的\n- a debonair appearance/manner \n\t光彩照人的形象／富有魅力的举止\n- a debonair young man 风度翩翩的年轻人\n\n英语单词 debonair 来自法语，原本是驯鹰术语，字面意思是“品种上佳的”，用来表示良种的、经过严格训练的鹰。法国人尤爱那种翅膀长并且生性傲慢的雌性猎鹰，认为这种猎鹰“de bonne air”，意思是“of good air”。这里的 air 和空气无关，而是表示样子（血统）。12 世纪进入英国后，表示“驯服的、温顺的、谦恭的”。该词后来不怎么使用，到了现代再次流行，词义演变为“和蔼的、令人愉快的、快活的”。\n\n\n#### impugn\nin- =\u003e in, into, on, upon\npugn =\u003e to fight\n=\u003e 引申到言语上 =\u003e 抨击, 质疑, challenge, question, attack, dispute\n\n##### pugn-: to fight\n- **oppugn** =\u003e op・pugn =\u003e fight against, oppose, resist =\u003e 对...提出质疑, 对...提出怀疑\n- **repugn** =\u003e re・pugn =\u003e rebel, disobey, oppose, resist, fight against =\u003e 反抗, 反对\n- **repugnant** =\u003e re・pugn・ant =\u003e \"反抗的\" =\u003e引申为令人厌恶的, 令人反感的, distasteful, disgust, dislike, hatred\n\t- a repugnant smell\n\t- I find your attitude toward women quite repugnant.\n- **pugnacious** =\u003e pugn・acious =\u003e 具有fight的特征的 =\u003e 好斗的, aggressive, contentious, irritable, belligerent\n\n\n#### merit\nIf something has ***merit***, it has good or worthwhile qualities.\n**不可数名词**\n- The argument seemed to have considerable *merit*.\n- Box-office success mattered more than artistic *merit*.\n- Your feature has the *merit* of simply stating what has been achieved.\n- 同义词： worth, value, quality, credit\n\nThe ***merits*** of something are its advantages or other good points. **复数名词**\n- They have been persuaded of the *merits* of peace.\n- ...the technical *merits* of a film.\n- It was obvious that, whatever its *merits*, their work would never be used.\n- 同义词： advantage, value, quality, worth\n\nIf someone or something ***merits*** a particular action or treatment, they deserve it. **动词**\n- He said he had done nothing wrong to *merit* a criminal investigation. \n- Such ideas *merit* careful consideration.\n- 同义词： deserve, warrant, be entitled to, earn   \n\n\n#### noisome\n- late 14c., noisom, \"harmful, noxious\" (senses now obsolete), from noye, noi \"harm, misfortune\" (c. 1300), shortened form of anoi \"annoyance\" (from Old French anoier, see annoy) + -some. Meaning \"bad-smelling, offensive to the sense of smell\" is by 1570s. Related: Noisomeness.\n\nannoy + -some\n- od-,ol-,oz- = scent, 表示“味”，od-, ol- 源自拉丁语, oz- 源自希腊语，都表示“味道”。\n- -some 表形容词，“充满…的，具有…倾向的”。\n\n#### obloquy☢\nob- =\u003e against\nloqu =\u003e to speak\n=\u003e to speak against =\u003e Very strong public criticism or blame\n\n##### loqu-, locu-: to speak\n- loquacity\t\t=\u003e loqu・acity =\u003e -acity 表示\"有...倾向的\" =\u003e 好辩, 多嘴\n- eloquent\t=\u003e e・loqu・ent =\u003e e- 表示\"向外\" =\u003e 雄辩的, 有说服力的, 有口才的\n- eloquence\t=\u003e e・loqu・ence =\u003e 口才, 雄辩术\n- elocution\t=\u003e e・locu・tion =\u003e 雄辩术, 演说法\n- locution\t\t=\u003e locu・tion =\u003e 语言的表达方式, 语言风格\n- loquacious\t=\u003e loqu・acious =\u003e -acious表示\"有...特征的\" =\u003e 多嘴的, 好辩的\n- colloquy\t\t=\u003e col・loqu・y =\u003e col- 一起, with, together =\u003e 一起说 =\u003e 讨论, 谈话, 会话\n- **colloquial**\t=\u003e col・loqu・ial =\u003e 会话的 =\u003e 口语化的, 口语的\n- collocutor\t=\u003e col・locu・tor =\u003e 对话者, 谈话的对手\n- **colloquium**\t=\u003e col・loqu・ium =\u003e 讨论会\n- colloquia\t\t=\u003e col・loqu・ia =\u003e 学术研讨会, 学术研讨会的报告\n- obloquy\t\t=\u003e ob・loqu・y =\u003e 责骂, 公开的批评\n- **soliloquy**\t=\u003e soli・loqu・y   =\u003e soli- 单独的 =\u003e 自言自语, 独白\n- soliloquist\t=\u003e soli・loqu・ist =\u003e 独语者, 自言自语的人\n- soliloquize\t=\u003e soli・loqu・ize=\u003e 自言自语, 独白\n- somniloquy\t=\u003e somni・loqu・y =\u003e somni- 睡眠 =\u003e n. 说梦话\n- interlocution\t\t=\u003e inter・locu・tion =\u003e inter- 在...之间 =\u003e 对话, 交谈\n- interlocutory\t\t=\u003e inter・locu・tory =\u003e 对话的, 交谈的\n- interlocutor\t\t=\u003e inter・locu・tor =\u003e 对话者, 参与对话的人, 代言人\n- grandiloquence\t=\u003e grandi・loqu・ence =\u003e grandi- 大的 =\u003e 豪言壮语, 夸张的言论\n- grandiloquent\t=\u003e grandi・loqu・ent =\u003e grandi- 大的 =\u003e 卖弄辞藻的, 言辞浮夸的, 过分华丽的\n- circumlocution\t=\u003e circum・locu・tion =\u003e 迂回的说法 \n- magniloquence\t=\u003e magni・loqu・ence =\u003e 夸张的说法\n\n#### phlegmatic\nphlegm（粘液）：导致人性情冷淡的粘液\n根据四体液学说，体内粘液占比较高的人性情冷淡、反应迟钝。因此，英语单词 phlegm（粘液）的形容词 phlegmatic 就产生了“冷淡的、迟钝的”等含义。\n- phlegm：[flem] n. 痰；粘液\n- phlegmatic：[fleg'mætɪk] adj. 冷淡的；迟钝的；冷漠的\n- phlegmy：['flɛmi] adj. 痰的；生痰的；含痰的\n\n\n#### providence☢☢\npro- =\u003e 提前, ahead\nvid- =\u003e 同 vis-,  to see, 看\n-ence \nto see ahead =\u003e 未卜先知, 神灵提前安排好的 =\u003e 天命, 天意, 天道\n- These women regard his death as an act of providence\n\n##### providential☢\npertaining to foresight =\u003e 天意的, 神助的 =\u003e 凑巧的, 时间正好的, lucky, timely\n - a providential opportunity 机缘巧合 \n\n#### provident☢\n相比上面的 providential, 这个特指(在物质准备上)未雨绸缪的, (在经济上)有远见的\n\n\n#### prude☢\na person who is easily shocked by rude things, especially those of a sexual type\n\n- prudish\n\n不要和 pedant 弄混了\n\n\n#### rapacious\nrap- =\u003e to snatch, to seize, 抓捕, 抢夺, 来自拉丁语 rapere\n-acious =\u003e 有...特征的\n=\u003e 强取豪夺的, 贪婪的, 掠夺的, grasping, insatiable, ravenous, greedy\n\n##### rap-\n- rapine noun. 劫掠, 抢夺\n- rape\n- rapacity =\u003e 贪婪, 掠夺\n\n\n#### relinquish\nre- =\u003e back, against\nlinqu- =\u003e to leave\n-ish =\u003e 动词后缀, 造成...\n=\u003e abandon, desert =\u003e give up the pursuit or practice of, desist, cease from, 放弃(a responsibility or claim)\n- He has *relinquished* his claim to the throne\n- She *relinquished*  control of the family investments to her son. \n- 这个放弃的东西和 give up 有所不同, 更为正式\n\n##### delinquent☢\nde- =\u003e completely\nlinqu- =\u003e to leave\n-ent =\u003e 这里是名词后缀和形容词后缀\n- 被遗弃之人 =\u003e (通常指青年)违法者\n\t- juvenile delinquents\n- 不良的, 违法的(背弃了自己的责任的, one who fails to perform a duty or discharge an obligation, offender of the law)\n\t- delinquent teenagers\n- 后来也引申为拖欠欠款的\n\t- She has been delinquent in paying her taxes.\n\n\n#### resonant☢\nresonant 是 resonate 的形容词, \"共振的\" \n=\u003e 从声音角度来理解 -\u003e 声音洪亮的, deep and strong, 像钟振动的时候的声音\n- a deep, **resonant** voice 深沉而洪亮的嗓音\n- a **resonant** concert hall 共鸣效果好的音乐厅\n\n=\u003e 从\"频率相同\"的角度来理解 =\u003e 引起共鸣的, 引起联想的\n- We felt privileged to be the first group of Western visitors to enter the historic palace, **resonant** with past conflicts.\n\n\n#### resourceful\n足智多谋的， 这里的 resource 指的是\"计谋，解决问题的方法，谋略，智慧\" =\u003e good at dealing with problems\n\n#### retard☢\nretard 不单单是用来骂人的, 作为动词, retard 的意思是\"阻碍, 减缓\"某个行动, 某个进程\n- A rise in interest rates would severely **retard** economic growth.\n\n- 所以 retarder 还有\"减速器, 阻滞剂\"的意思\n- retardant =\u003e 起阻滞作用的(adj), 阻滞剂(noun.)\n\n=\u003e (智力)被减缓了的 =\u003e noun. 弱智, 笨蛋\nretarded =\u003e offensive\n\n##### tardy\n缓慢的, 迟缓的\n\n#### sartorial☢\n来自拉丁语里面的\"sartor\"=\u003etailor, 裁缝的, 泛指\"服装的, 制衣的, 衣着的\"\n- sartorial elegance\n\n\n#### sidestep\n向旁边退一步 =\u003e 向旁边躲开 =\u003e 也引申为\"通过讨论别的事情来回避某个话题\"\n- The speaker sidestepped the question by saying that it would take him too long to answer it.\n\n\n\n#### slate\nslate 原本指的是一种岩石 =\u003e 板岩, 页岩, 一种很容易劈成一片一片的岩石 \n=\u003e 因此 slate 也可以用来指代\"石板\"\n- slate roof, 石板瓦屋顶\n\n=\u003e 古时人们还会在石板上写字, 在选举首领的时候, 会先把候选者的名字写在石板上, 因此 slate 也用来指代\"候选人名单\"\n- His novel was chosen from a slate of ten finalists.\n\n=\u003e be slated =\u003e 写在石板上的 =\u003e 计划好的, 预定, 安排\n- The meeting is slated for next Thursday\n- Jeff is slated to be the next captain.\n\n=\u003e slate 还有\"抨击, 批评\"的意思(informal), 也许是在石板上写坏话的意思?\n- Her last book was slated by the critics.\n\n##### wipe the slate clean\n[idiom] to start a new and better way of behaving, forgetting about any bad experiences in the past:\n- A new relationship presents you with the opportunity to wipe the slate clean.\n\n\n#### spleen☢\n来自拉丁语 splen, 来自希腊 splen, 来自 spelgh, 脾。因中世纪医学理论认为脾是人体怒火和怒气郁积之所，因而引申词义\"怒火，怒气\"。\n- spleenful =\u003e 脾气坏的\n- splenetic =\u003e 脾气坏的, 易怒的, irritable, bad-tempered\n- splenic =\u003e 和脾脏相关的\n\n\n#### stonewall\n这是一个复合词，由stone（石）和wall（壁，墙）合成，字面义为“石壁”、“石墙”。美国南北战争（1861-1865）时，南军著名将领Thomas Jonathan Jackson (1824-1863)在布尔溪畔战役（the Battle of Bull Run）中率所部一个旅的兵力，组成一道坚如石壁的防线，顶住了优势北军的进攻，赢得了Stonewall Jackson（石壁杰克逊）的绰号。\n\nstonewall通常多用于喻义。最初只被作为板球（cricket）的一个术语，表示“防守挡击”，19世纪80年代澳大利亚和英国政界人士把它转用到政治方面。有的议员在议会讨论时可以滔滔不绝地一小时接着一小时地演讲或质问，以此组成一道无形的石墙来延阻会议议程的进展。这种用冗长发言等拖延手段阻碍议事的行为，人们便用stonewall一词来作比。\n\n在美国英语中stonewall原先不能算是一个常用词，尽管在任何一本美国词典中都能查到。但在20世纪70年代轰动一时的水门事件调查的高潮中，美国总统尼克松（Richard M. Nixon, 1913-1994）使它的词义获得进一步延伸。他让他的助手们顶住压力，对联邦调查人员设置障碍，尽量延阻他们的调查（to stonewall it）。白宫所作的努力没有成功，最后尼克松总统被迫辞职，他的一些助手被判了刑。stonewall一词却从此被赋予了新的含义，不再囿于板球和议会，开始用于广义上的“妨碍行动”和“设置障碍”。\n\n例 \n- He was **stonewalling** and everybody knew it. (CCE) 他是在用拖延的办法来阻碍议事，这一点大家都看得出来。\n- Queries about civilian casualties during the bombing raid were **stonewalled** by government officials. (CID) 对于人们提出的关于空袭中平民伤亡的质疑政府官员避而不答。\n\n\n#### travesty\n总结一下几个意思相近的词汇\n##### 词典的解释\n**travesty** \n- something that fails to represent the values and qualities that it is intended to represent, in a way that is shocking or offensive\n\n**parody**\n- writing, music, art, speech, etc. that intentionally copies the style of someone famous or copies a particular situation, making the features or qualities of the original more noticeable in a way that is humorous\n\n**burlesque**\n- a type of writing or acting that tries to make something serious seem stupid\n\n**caricature**\n- (the art of making) a drawing or written or spoken description of someone that usually makes them look silly by making part of their appearance or character more noticeable than it really is\n\n##### 进一步比较\n**Caricature**, **burlesque**, **parody**, **travesty** are comparable as nouns meaning a grotesque or bizarre imitation of something and as verbs meaning to make such an imitation.\n\n**Caricature** implies ludicrous exaggeration or distortion (often pictorial) of characteristic or peculiar features (as of a person, a group, or a people) for the sake of satire or ridicule.\n\n**Burlesque** implies mimicry (especially of words or actions in the theater) that arouses laughter. The term usually also suggests distortion (as by treating a trifling subject in mock-heroic vein or by giving to a serious subject a frivolous or laughable turn) for the sake of the comic effect.\n\n**Parody** basically denotes a writing in which the language and style of an author or work are closely imitated for comic effect or in ridicule.\n\n_Parody,_ like _caricature,_ may involve exaggeration or, like _burlesque,_ distortion but ordinarily is more subtle and sustained than the first and quieter and less boisterous than the second.\n\nIn extended use _parody_ may apply, often with more than a hint of bitterness or disgust, to a feeble or inappropriate imitation or to a poor inadequate substitute.\n\n**Travesty** is usually a harsher word than others of this group; it implies a palpably extravagant and often debased or grotesque imitation and more often and more intensely than parody suggests repulsion.\n\n所以caricature和burlesque都是for fun的, 而parody带有一丢丢贬义, travesty的贬义最强烈, 类似于\"闹剧\"\n\n#### whet\nwhet 的本意是 磨(刀), 把刀变锋利\n**whet one's appetite** =\u003e 挑起某人的欲望, 激起某人的欲望\n\n\n#### altruism\n借自法语altruisme，这是法国哲学家孔德（Auguste Comte, 1798-1857）于1830年杜撰的一个词，用作egoism（利己主义）的反义词，由意大利语altrui 'someone else'（别人）或拉丁语alter 'other'（别的）加后缀-isme构成。altruism释义为“利他主义”，也可指“（动物的）自我牺牲行为”。\n\n例 \n- Nobody believes those people are donating money to the president's party purely out of altruism.  谁也不会相信那些人捐款给总统的政党纯粹是出于无私。\n- I doubt whether her motives for donating the money are altruistic — she's probably looking for publicity. \n\t我怀疑她捐钱的动机是否是无私的——也许是为了出风头。\n\n#### diffuse\n![fus-](#fus-)\n\ndiffuse\"散射\"的含义很好理解, 除了散射某个物质或者光线, diffuse 也可以指散布某种思想, 某个信息\n- Television is a powerful means of diffusing knowledge.\n\n而如果某一篇文章是 diffuse 的就不是一件好事了, 说明其思想不明确, 主题很含糊, =\u003e 含糊的, 费解的, vague, not clear and difficult to understand\n- a diffuse literary style\n\n\n#### domineer\n- dom-,domin-,tam- = house, 表示“屋，家，控制”\n- -er 是施动者名词后缀，表示“人或物”，一般缀于动词后，来自古英语。\n\n- The hoover roared into life, ***domineering*** the room\" -dominating the room, becoming the center of attention in the room-\" like a proud king\" .  \n\t“吸尘器咆哮着，在房间里横行霸道的”—充斥着房间，把它变成了整个房间里注意力的中心—“就像一位傲慢的国王\n\n#### prevaricate☢☢\n- 1580s, \"to go aside from the right course or mode of action\" (originally figurative, now obsolete), a back formation from prevarication or else from Latin praevaricatus, past participle of praevaricari \"***to make a sham accusation, deviate\" (from the path of duty)***, literally \"walk crookedly;\"\n- The meaning \"***to act or speak evasively, swerve from the truth***\" is from 1630s. Related: Prevaricated; prevaricating.\n\n- 在别人指责他的时候, 反而去无端的指责别人, pre- =\u003e 在...之前, vari =\u003e 变化, vary, change\n- 搪塞, 含糊其辞, evade, \n\n#### glaze\n\n![300](notes/2022/2022.7/assets/Pour-Mirror-Glaze-over-Cake.webp)\n\n![300](notes/2022/2022.7/assets/how-to-glaze-pottery-at-home-1.webp)\n\n- glaze 作为动词, 还有\"眼神变呆滞\"的意思, \n\t- Among the audience, eyes **glazed** over and a few heads started to nod.\n\n\n#### mercenary\n除了作为名词\"雇佣兵\", mercenary 还可以作为形容词: 唯利是图的, 或许是因为唯利是图的人就像雇佣兵一样, 只要给钱干什么都可以.\n- He had some mercenary scheme to marry a wealthy widow.\n\n\n#### emulation☢\n- imitative rivalry\n- effort to equal or excel in qualities or actions that one admires in another or others.\n\n- 其实在计算机软件里面我已经见过这个单词了, emulation=\u003e仿真\n\n**emulate**\n- 效仿, 竞争并试图跟上\n\n#### shopworn\n在商店里面放久了的(东西), 也引申为\"陈词滥调\"\n\n\n#### spate☢\n本意是\"河流猛涨\", 后面引申为\"大量(不愉快的时间)的同时发生\" (noun)\n![300](notes/2022/2022.7/assets/f1579b5e-760b-4118-b64b-a32967ae1e30-large16x9_thumb_41633.jpg)\n- Police are investigating **a spate of** burglaries in the Kingsland Road area.\n\n#### rash\n疹子 =\u003e a rash of 也可以用于形容令人不快的时间的突然大量出现\n- There has been **a rash of** robberies/ accidents / complaints in the last two years.\n\n得了 rash 让人很烦躁 =\u003e 还能用于形容人\"轻率的, 鲁莽的, 毛躁的\"\n- That was a rash decision.\n\n#### bulge☢\n(身体的某个部分)鼓起来, 凸出来\n- Jiro waddled closer, his belly bulging and distended\n- His eyes seemed to bulge like those of a toad\n也可以作名词, \"凸起\"\n- Why won't those bulges on your hips and thighs go?\n\n**be bulging with** =\u003e be full of ... (塞满了以至于鼓起来了)\n- a bulging briefcase\n\na sudden large increase of... =\u003e **a bulge in sth**\n- a bulge in aircraft sales\n\n#### mirage\n![mir-](#mir-)\n从\"海市蜃楼\"引申为\"妄想, 幻想\"\n- Immortality is just a mirage.\n\n\n#### denigrate☢\nde- =\u003e completely\nnigr- =\u003e black\n-ate\n=\u003e 使...完全变黑, 词的意思和词的字面组成都和汉语里面的\"抹黑\"一样, 贬损, 诋毁, disparage, run down\n\n\n#### adamant ☢\n早在乔叟时代，adamant 一词就已是英语的一分子，它源于希腊语 adamas（最坚硬的金属），但却借自古法语 adamaunt（最坚硬的石头）。因此，有好几个世纪 adamant 一直被作为 diamond（金刚石）的同义词使用（其实两者系同源词）。它被广泛用作形容词，表示“坚硬的”、“坚决的”或“固执的”等义，则是 20 世纪 30 至 40 年代以后的事。\n\n例 \n- The government remains adamant that it will not yield to pressure. 政府的态度仍然坚决，不愿屈服于压力。\n- We tried to negotiate, but they were adamant.  我们想谈判，但他们很固执。\n- He's absolutely adamant in/about not allowing smoking in his house. 他坚决不允许在他的房子里抽烟。\n\n#### belligerent\n好战的, aggressive, hostile, animosity, combative\n而用于战争中可以指代\"交战的\"\n- the belligerent countries =\u003e 交战国\n\n\n#### coeval\nco- =\u003e with together\nev- =\u003e age\n-al =\u003e ...的\n同时代的\n\n##### ev- \n- medieval\n- primeval\n- longeval =\u003e 长命的\n- longevity =\u003e long・ev・ity =\u003e 长寿\n\n#### imperious☢☢\nimper- =\u003e 同 empire, command, 命令, 统治\n-ious =\u003e 形容词后缀\n- 命令的, 统治的 =\u003e 专横跋扈的（拼音：zhuān hèng bá hù）\ndomineering, dictatorial, bossy, haughty\n\n\n#### nonchalant\nnon- =\u003e 不\nchal =\u003e heat, to be warm, 比如calorie\n-ant =\u003e ...的\n不热心的 =\u003e 冷淡的, 漠不关心的, 毫不在乎的\n\n#### dexterous\n在古英语时期 right 的词形为 riht，那时它就已有了“公理”、“正直”、“合适的”、“正当的”、“直的”等抽象意义了。到了 13 世纪，人们开始有较明确的方向感，right 被用来表示“右”。由于大多数人用右手做事比较灵活，所以认为用右手是正确的，于是 right 又从“右的”引申出“正确的”一义。\n\n以 right 指政治态度的保守或右倾可以追溯至 18 世纪的法国大革命。在 1789 年的法国国民会议（French National Assembly）上，政治上倾向于保守的贵族都坐在会议厅的右侧（right wing），即主席右边的席位，而激进的民主派则在左侧（left wing），即主席左边的席位。以后的欧美会议或议会仍旧沿用这种座位安排。right wing 也因此被用来表示“（政治上的）右翼（的）”或“右派（的）”，而 left wing 则指“左翼（的）”或“左派（的）”。\n\n##### adroit / dexterous\n英语有两个外来语和 right 一词有较密切的关系。一个是 **dexterous**/dextrous，源于拉丁语 dexter（右）；另一个是 **adroit**，源于法语 droit（右）。但二词都被赋予“灵巧的”或“敏捷的”一义，可以说是受了大多数人惯用右手这一点的影响。（参见 left, sinister）\n\n#### sinister☢\nsinister 本是个拉丁词，意思是“左的”、“在左边的”，15 世纪被引入英语，作为纹章学的术语，表示“在（纹章）持有人左边的”，如在盾徽（coat of arms）上，a lion sinister on a field of blue 是指“盾徽蓝色纹地左边的狮子”。早在古罗马时代，sinister 具有正反两义：“吉祥的”和“不吉祥的”。古罗马占卜师面朝南，东在其左，故视左为吉祥；古希腊占卜师面朝北，西在其左，故视左为不吉祥。嗣后，后一个词义占了上风。从 16 世纪起 sinister 在英语中就被赋予了“不祥的”、“兆头坏的”这一意义。在占卜术中左边看到的征兆，如飞鸟在左边出现，为什么被认为是不吉利的呢？古希腊历史学家、传记作家普卢塔克（Plutarch, 46?-120?）解释说，因为西边（占卜师的左边）是日落的方向。\n\n有人认为，sinister 的词义从“左的”引申为“不祥的”是对左撇子的一种无理的偏见。法语借用词 **gauche** 词义的演变也有类似情况。gauche 原义为“左的”，可是在英语中却被用以表示“不善交际的”、“不雅致的”、“笨拙的”等义。与此相反，dexterous/dextrous（源于拉丁文 dexter，意为“右”）和 adroit（源于法语 droit，也指“右”）这两个词在英语均为褒义词，均表示“灵巧的”或“敏捷的”。甚而，连表示“左右两手都善于使用的”ambidextrous 一词也包含有 dextrous 这一成分。所以这也难怪左撇子对这种语言现象有看法了。(有没有可能只是因为大部分人的左手都很笨拙呢?)\n\n例 \n- The man was dressed in a black suit and wore dark glasses. There was something **sinister** about him. (LLA) 那个男人身穿黑西装，还戴着墨镜，浑身透着一股邪气。\n- To the little girl, the old woman looked **sinister** and strange. 在小女孩看来，老太太的样子凶恶而古怪。\n- His **sinister** laugh made me think of a villain. 他的奸笑使我联想到了恶棍。\n- \"The personnel director wants to see you.\" \"Oh dear, that sounds **sinister** (= as if there is going to be trouble).\" (CID) “人事处长要见你。”“噢，听起来不吉利。”\n\n#### inchoate☢\n来自拉丁语 incohare, 挂上，开始，来自 in-, 进入，使，cohum, 皮带，牛轭，词源同 haw, hedge. 其原义为给牛挂上轭带，使牛开始耕田，后引申词义开端。\n\n\n#### superfluous\nsuper- =\u003e over\nflu =\u003e to flow\n-ous\n=\u003e to overflow =\u003e 溢出来的, 过多的, 多余的, 过剩的\n- I rid myself of many **superfluous** belongings and habits that bothered me.\n\n\n#### crucify\n把...钉在十字架上 =\u003e 折磨, 严惩, 强烈地批评\n- She'll crucify me if she finds out you still here.\n\n#### excruciate\nex- =\u003e out, out from thoroughly\ncruciate =\u003e crucify\nthoroughly crucify =\u003e (精神上或者肉体上)残酷地折磨\n\n#### coronation\ncorona- =\u003e 冠, 冕\n-tion =\u003e 名词后缀, 表示过程\n=\u003e 加冕典礼\n\n- coronavirus\n- corona 日冕\n- coronary 冠状的, 花冠的, 冠心病, 和心脏相关的\n- coronal relating to the crown\n\n\n#### countenance\ncountenance 居然可以是动词 =\u003e 接收, 认可, 赞同, approve of, give support to\n- The school will not **countenance** bad behaviour.\n\n当然 countenance 可以是名词, 表示\"赞同\"\nWe will not **give/lend countenance to** any kind of terrorism.\n\n还有一个意思是 the appearance or expression of someone's face, 面容；脸色；面部表情, 也许赞同的意思是从这个意思引申过去的.\n- He was of noble countenance.\n\n#### dissolute☢\n- 不要和\"dissolve(溶解)弄混了\"\ndissolute 字面上理解是\"松散的, loose\", 引申到道德品行上, =\u003e 品行不端的, 放纵的, 放荡的\n\n- He led a dissolute life, drinking, and womanizing till his death.\n\n#### goldbrick\n- to avoid performing work or duties; shirk\n- The soldiers **goldbricked** through drills.\n\ngoldbrick 的字面意义为“金砖”，但实际含义却是“假金砖”。该词源出 19 世纪中期的美国淘金热。1848 年 1 月 24 日在加利福尼亚州发现了黄金。次年，想发财的人纷纷涌向西海岸，被人们称为 forty-niners（49 年冒险家）的淘金者据估计约有 8 万名。矿工采出金子以后，为了便于搬运，往往把金子铸成金砖，goldbrick 一词就应运而生了。开初，goldbrick 确一度指“金砖”。后来有的骗子利用一些人的贪婪心理，用铅或铁铸成块，然后在表面镀上一层金。这些外表像是纯金的金砖使不少人上了当，受了骗。有记载说，圣路易有一位名叫 Patrick Burke 的人在 1887 年花了 3700 美元买了一块假金砖。不久，goldbrick 一词就开始用来喻指“假金砖”、“冒牌货”、“虚有其表的东西”。在第一次世界大战期间，士兵们接过这个词，用来指那些直接征自民间而未受过军事训练的军官，这些人往往受到部属的轻视。以后，goldbrick 的词义又进而引申为“逃避工作的人”、“偷懒的人”，在第二次世界大战期间非常通用，并且一直沿用至今。这一类人想显示自己的能力以博得同事的好评而又借故什么也不干，因此人们就以早期开发西部那些骗人的 goldbrick 喻之。goldbrick 在作动词用时则表示“逃避工作”或“诈骗”等义。\n\n#### lampoon☢\n\"**A personal satire; abuse; censure written not to reform but to vex**\", from French lampon, a word of unknown origin, said by French etymologists to be from lampons \"let us drink,\" which is said to have been a popular refrain(副歌) for scurrilous songs, in which case it would be originally a drinking song.\n\n#### leaven☢\n本来是\"使发酵\"的意思, 又因为发酵的时候面团会\"涨起来, 变大\" =\u003e 所以引申为\"使变得有趣, 使幽默\"的意思\n\n- Even a speech on a serious subject should **be leavened with** a little humour.\n\n\n#### lofty\n\"高傲的, 高尚的\"这两个中文词汇里面都有\"高\"这个字, 用来形容傲慢和崇高, 有意思的是, 英语单词 lofty 和这两个意思都对上了:\n- lofty 可以是\"高, high\"的意思: a lofty ceiling/mountain/wall\n- 还可以引申到\"高尚的\"的意思: a lofty ideals\n- 也可以指\"高傲的, 自大的\": a lofty attitude/distain/contempt\n\n\n#### luminary\nluminary 可不是\"发光的东西的意思\", 但是这个字面意思能很形象地引出 luminary 地真实含义:\"专家, 知名人士, 明星\", star(明星) 是闪闪发光的, 也就是 luminous 的, 所以也是一个 luminary\n\nLuminaries of stage and screen = famous actors\n\n\n#### malign\n有害的, 邪恶的, evil\n- She describes pornography as \"a malign industry\".\n诽谤，污蔑，中伤\n- She has recently been maligned in the gossip columns of several newspapers. 她最近受到几家报纸漫谈专栏的污蔑。 \n\n##### mal-\n- malignity =\u003e the condition of being malign\n- malignant =\u003e 恶意的, 邪恶的, (肿瘤, 癌症)恶性的\n- malignance =\u003e the condition of being maliganant\n\n#### mawkish\nshowing emotion or love in an awkward or silly way\n无病呻吟的；多愁善感的\n- The film lapses into mawkish sentimentality near the end.\n\n1660s, \"sickly, nauseated\" (a sense now obsolete), from Middle English mawke \"maggot\" (early 15c.; see maggot), but the literal sense of \"maggoty\" is not found. Figurative meaning \"sickeningly sentimental, insipid\" is recorded by 1702\n\n#### mettle☢\nmetal 的变体.  起初两个单词是完全一样的, 可以换着用, q 它的比喻义就是构成\"一个的精神组成, 道德组成\" =\u003e 精神, 勇气, 才能, \"natural temperament,\" 特指 \"ardent masculine temperament, spirit, courage\". \nThe spellings diverged early 18c. and this form took the figurative sense. Related: Mettled.\n\n- The team showed/proved its **mettle** in the final round. 运动队在最后一轮中展现了他们的拼搏精神。\n- The real test of her political **mettle** came in the May elections. 五月份的选举是对她政治才能的真正考验。 \n\n#### minutia, minutiae\n细枝末节, triviality, nuance\n\n\n#### nonentity\nentity =\u003e 实体, 独立的事物, 独立的存在\nnon + entity =\u003e 都不是一个独立的事物 =\u003e 无足轻重的事物, 小人物, nobody\n\n#### omniscient\nomni- \"all\"\nscient- \"knowledge\"\n=\u003e 全知的, 无所不知的\n\n##### omni-: all\n- omnipotent =\u003e omni・potent =\u003e 无所不能的\n- omnidirectional =\u003e omni・directional =\u003e 全向的, 所有方向的 \n- omnificent =\u003e omni・ficent =\u003e 有无限创造力的\n- omnipresent =\u003e omni・present =\u003e 无所不在的\n- omnivore =\u003e omni・vore =\u003e  杂食动物 (vor- \"eat\")\n\n##### vor-: eat\nvoracious =\u003e vor・acious =\u003e 贪婪的\ndevour =\u003e de・vour =\u003e 吞食, 挥霍, 吞没\napivorous =\u003e api・vor・ous =\u003e 食蜜的 api=bee \ncarnivore =\u003e carni・vor・e =\u003e 食肉动物\nherbivore =\u003e  =\u003e herbi・vor・e =\u003e 食草动物\ninsectivore =\u003e  =\u003e insecti・vor・e =\u003e 食虫动物\n\n\n#### precipice☢\n- 悬崖, 峭壁, \n- 险境, (可以理解为在悬崖边上摇摇欲坠)\n\nprecipice 在词的组成上和 precipitate 差不多:\n![precipitate☢](#precipitate☢)\n\n- We stand on the precipice of doom.\n\n#### presume☢☢\npre- =\u003e before\nsume =\u003e to take, obtain, buy\n\n1. **=\u003e to take for granted, overconfidently =\u003e 擅自做某事, 越权做某事, 冒昧地做某事**\n(没有被允许就 take 了 sth, 没有给钱就 obtain 了 sth)\n\n- I wouldn't **presume** to tell you how to do your job, but shouldn't this piece go there? \n- I don't wish to **presume** (= make a suggestion although I have no right to), but don't you think you should apologize to her? 恕我冒昧，但你难道不觉得应该向她道歉吗？\n- He **presumes** on her good nature (= takes unfair advantage of it). 他利用了她的善良本性。 \n\n2. **to believe something to be true because it is very likely, although you are not certain\n\t假定，推定，认定**\n- I presume (that) they're not coming, since they haven't replied to the invitation. 鉴于他们没有回复邀请，我认为他们不会来了。\n- You are Dr. Smith, I presume? 我想你是史密斯博士吧？ \n\n##### sum-, sympt-: to take, obtain, buy\n###### assume\n- as- =\u003e to, toward, up to\n\n1. **=\u003e to arrogate, take upon oneself** \n- assume 和 presume 的意思很像, 都是\"没有被允许就擅自 take sth\"的意思\n\n2. **=\u003e 冒充, 假装**\n- He assumed a look of indifference but I knew how he felt.\n\n3. **=\u003e 假定, 臆断, 想当然的认为**\n- We mustn't assume the suspects' guilt\n\n4. **=\u003e to take or begin to have responsibility or control, sometimes without the right to do so, or to begin to have a characteristic**\n- The new president assumes office at midnight tonight.\n- The terrorists assumed control of the plane and forced it to land in the desert. \n\n###### consume\ncom- =\u003e with, together, 也可以是强调\n一起拿, 或者(强调)拿 =\u003e 消耗, 吃, 喝, (大火, 情绪)吞噬\n\n###### resume\nre- 重新\n重新获得, =\u003e 恢复, 重返(某地或者某个职位), 中断后继续\n\n###### 派生词\n- **presumption**\n\tA presumption is something that is accepted as true but is not certain to be true.\n\t- ...the **presumption** that a defendant is innocent until proved guilty.\n- **presumptive**\n\tbelieved to be something, or likely to be true, based on the information that you have\n\t可据以推定的；推测为真的\n\t- **presumptive** signs of pregnancy\n- **presumptuous**\n\tIf you describe someone or their behaviour as presumptuous, you disapprove of them because they are doing something that they have no right or authority to do.\n\t[disapproval] 强调presume里面的\"*without right*\"\n\t- It would be **presumptuous** to judge what the outcome will be\n- **assumption**\n\tsomething that you accept as true without question or proof\n\t假定；假设；臆断\n\t- People tend to make **assumptions** about you when you have a disability.\n\n\n###### [presume, assume, what's the difference?](https://www.merriam-webster.com/words-at-play/assume-vs-presume)\nAssume and presume both mean \"to take something for granted\" or \"to take something as true.\" The difference between the words lies in the *degree of confidence* held by the speaker or writer. If he or she is making an informed guess **based on reasonable evidence**, **presume** is the word to use; if a guess is made **based on little or no evidence**, **assume** is usually used.\n\n\n#### proffer\n= pro + offer \n伸出去offer=\u003e\n=\u003e to offer sth by holding it out, or to offer advice or an opinion\n\n- He shook the warmly proffered hand\n- I didn't think it wise to proffer an opinion.\n\n#### promulgate☢\n- pro- =\u003e forth\n- mulgate =\u003e to milk (a cow, sheep, etc)\n=\u003e 去把乳汁从动物的乳房里面挤出来 =\u003e 用来比喻\"make known by open declaration, publish, announce\"\n\n- A new constitution was promulgated last month.\n\n\n#### propensity\npro- =\u003e forward\npens =\u003e to hang, cause to hang, weigh\n-ity =\u003e 名词后缀, 指某种性质\n=\u003e 联系汉语词汇\"倾向\", 向一边倾斜, 悬挂 =\u003e (尤指不良的)倾向, 嗜好\n\n- Mr Bint has a **propensity** to put off decisions to the last minute.\n\n#### proscribe☢\n\"write in front of\" =\u003e (官方)禁止, prohibit, ban, forbid\n\nIn Latin one of the 5 different meaning of the verb proscribere is:\n\u003e   To outlaw one by hanging up a tablet with his name and sentence of outlawry, confiscation of goods, etc.\n\nThat is where the meaning of the English verb proscribe is derived from.\n\nAnother meaning is:\n\u003e     To publish a person as having forfeited his property, to punish with confiscation, to confiscate one's property.\n\n[source](https://ell.stackexchange.com/a/30511/155475)\n\n#### protuberant\npro- =\u003e forth\ntuber =\u003e　swelling, 膨胀\n-ant =\u003e ...的\n=\u003e 向前膨胀的, 向前凸出的 =\u003e (特指五官)突出的\n\n- large protuberant eyes\n![bulge☢](#bulge☢)\n\n\n#### provisory☢\nproviso 指的是法律条款里面的附加条件\n所以 provisory 就是\"有条件的, 有前提的, containing a proviso\"\n\n- provisional 是指临时的, 暂时的, provisory 也有这个意思.\n\n\n#### rapprochement☢\n- an agreement reached by opposing groups or people\n（与敌对群体或敌对之人达成的）和睦，和解，恢复友好关系\n\n这个是一个直接从法语里面借过来的词: \"establishment of cordial relations,\" from French rapprochement \"reunion, reconciliation,\" 字面意义是 \"a bringing near,\" 由 rapprocher \"bring near,\" 演变而来. 其中 re- \"back, again\" + aprocher (approach)\n\n记忆: r(e) appro(a)ch e ment\n\n#### recoil☢\n1. verb /rɪˈkɔɪl/ **注意重音**\n- to move back because of fear or disgust (= dislike or disapproval)退缩，畏惧；（因为厌恶而）退避\n\t- He leaned forward to kiss her and she recoiled in horror. 他俯身去吻她，她惊恐地后退避开。\n\t- I recoiled from the smell and the filth. 那儿又脏又臭，我躲开了。\n- to refuse to accept an idea or principle, feeling strong dislike or disapproval（对想法、原则等）厌弃，深恶痛绝，强烈反对\n\t- She wondered how it would be to touch him and recoiled at the thought. 她想知道摸一摸他会是什么感觉，但这个念头一进入脑海，她马上感到一阵厌恶。\n\n2. noun [ U ] /ˈriː.kɔɪl/ **注意重音**\n\tthe sudden backward movement that a gun makes when it is fired（枪、炮的）后坐力，反冲\n\n\n#### recondite☢\nre- 一再 + cond-藏 + -ite, 表形容词 → 一再被藏起来的 → 深奥的。\n\n#### resurgence\nreappearance\nrevival\nreturn\nrenaissance\nresurrection\n\n#### septic\n化脓的, 感染了的\nsepsis =\u003e 脓毒症, 脓毒症\n\n\n#### soph∙ism☢\nn.诡辩\nsoph- \n= wise, 表示“智慧，聪明”。\n-ism \n抽象名词后缀，表示“…主义”；“宗教”；“制度、行为”；“…学”、“…术”、“…论”、“…法”；“疾病名称”；“情况、状态”等。\n\nsophism - 公元前 5 世纪中叶至前 4 世纪之间，古希腊有一批哲学家以讲授辩论术、修辞、伦理学等知识为职业，希腊语称之为 sophistēs，英语作 sophist，汉语译为“智者”或“智者派”。由于他们在思想倾向上有共通之处，遂被称为一派，但他们并不构成一个固定的学派，也没有统一的学说。后来因为柏拉图、亚里士多德批评他们不追求真理而是求在辩论中用不诚实的手段达到取胜的目的，所以人们把他们贬称为“诡辩家”。他们之中也确有人擅长诡辩，因而在当时和后来一些人的心目中，“智者”一词不过是“诡辩家”的同义语，sophist 也因此被赋予“诡辩者”一义。\n\n14世纪时从sophist派生出动词sophisticate，表示“诡辩”、“掺假”等义，而形容词sophisticated乃是sophisticate的过去分词形式，原义是“掺假的”。到了19世纪末sophisticated的词义开始扬升，被赋予“老于世故的”、“老练的”、“高雅的”等义。至于“复杂的”、“精密的”、“尖端的”等义则是第二次世界大战之后才见诸使用的。如果我们再往上溯源，我们会发现sophist和sophisticated的终极词源乃是希腊语sophós 'wise'（有智慧的；明智的）。出自此源的英语单词还有sophism（诡辩），sophistication（复杂精密），sophistry（诡辩术）\n\n例 \n- His witty conversation showed him to be very sophisticated. 他妙趣横生的谈话显示出他是个老于世故的人。\n- I can't work this sophisticated new equipment. (FWF) 我不会操作这种新的精密设备。\n- He won the argument by sophistry. 他靠诡辩赢得那场争论。\n\n#### sophomore\n大学一年级学生在英国叫first-year student，在美国则称freshman；大学二年级学生在英国叫second-year student，在美国则称sophomore。\n\n美国英语之所以用 freshman，是因为一年级学生是 fresh（新来的，刚到的）。而 sophomore 始见于 17 世纪，1688 年之前在剑桥大学曾被用以指“大学二年级学生”。它源自希腊语 sophós 'wise'（聪明的）和 mōros 'foolish'（傻的）。由于大学二年级学生粗知皮毛，有时不免要卖弄聪明，sophomore 的字面含义就是“卖弄聪明的傻瓜”（wise fool）。这一说法已经被广为接受。基于这一说法，由 sophomore 衍生的 sophomoric 用以表示“一知半解而又过于自信的”、“知识浅薄却自命不凡的”、“幼稚的”等义，也就不难理解了。\n\n关于sophomore的由来还有一说认为，该词系由sophism（诡辩）一词已被废弃的变体sophom加-or构成。\n\nfreshman与sophomore除了分别指大学一年级学生和大学二年级学生，在美国英语中还可分别指四年制**高中**的一年级学生和二年级学生。\n\n例 \n- They met in their freshman year at college and married soon after they graduated. 他们在大学一年级时相遇，毕业后不久就结了婚。\n- Mary was elected president of the sophomore class. (NED) 玛丽当选二年级的班长。\n\n#### philosopher\nsophism - 表示“哲学”和“哲学家”的 philosophy 和 philosopher 源自希腊语 philósophos，由 phílos 'loving'和 sophós 'wise'两部分构成。因此，按字面原义讲，philosophy 是 the love of wisdom（对智慧的热爱）的意思，而 philosopher 则是 a lover of wisdom（热爱智慧的人）之义。据认为 philosopher 一词系古希腊哲学家、数学家毕达格拉斯（Pythagoras, 580?-500? BC）所杜撰。在古希腊人们通常都称哲学家为 sophoi，相当于英语 sophist 或 wiseman（哲人），但毕达格拉斯则认为该词过于狂妄。他说：“除了上帝，没有一人是 sophós（相当于英语 wise）的，就叫我 philósophos（相当于英语 lover of wisdom）吧。”\n\n源于希腊语 phílos 'loving'的英语组合语素 phil(o)-表示“爱好”或“亲”，它出现在不少英语单词中，如 philately（集邮），philobiblic（有爱书癖的），philanthropy（慈善），philander（玩弄女性），philology（语文学）等。\n\n#### steadfast☢\nsteadfast 是一个褒义词, 并且这个词表示某个状态已经 staying the same for a long time 并且 not changing quickly or unexpectedly\n类似于汉语里面的\"坚定不移的, 毫不动摇地, 坚定的\"\n- a steadfast ally\n\n\n#### sybarite☢\n在古希腊南部城邦中，有一座城市叫做锡巴里斯（Sybaris）。这座城市因富饶与奢糜而闻名于世。他们吃的是珍贵的海鞘，家里养着鹌鹑，喜欢把玫瑰花瓣撒在床上睡觉。甚至因为懒惰，他们还发明了夜壶，并且把夜壶装饰得十分华美，宴会和旅行时都随身携带。这种穷奢极欲最终导致了锡巴里斯的灭亡。据说有次锡巴里斯和邻近一个弱国交战。当双方军队接近时，对方的军鼓手忽然奏起锡巴里斯节日游行时的曲目。锡巴里斯的战马一听到这些乐曲，便整齐地迈开舞步翩翩起舞，根本不听骑兵的使唤。为了在节日游行中显得更优雅，锡巴里斯骑兵队特意把战马训练得可以跟着固定的音乐节拍起舞，没想到这却成了他们的死亡旋律。锡巴里斯灭亡了，却给我们留下了一个英语单词 sybarite，它本指锡巴里斯人，现在用来形容奢侈淫乐的人。\nsybarite：['sɪbə,raɪt] n.奢侈淫乐的人，纵情享乐的人，锡巴里斯人\nsybaritic：[,sɪbə'rɪtɪk] adj.奢侈淫乐的，柔弱的，放纵的\n\n\n#### terse\nters- =\u003e clean, wipe off, neat\n=\u003e 所以terse就是干净的, 无累赘的意思 =\u003e引申到语言上就是\"简短的, 简要的\", 后来又逐渐有了\"生硬的, 不友好的\"的含义(brusque)\n\n#### understate\nunder + state =\u003e 轻描淡写\n\n\n#### bogus☢\n- false, not real, or not legal\n\t假的；假冒的，伪造的；非法的\nbogus原是地道美语，产生于19世纪初叶。关于其由来说法很多，以下两种较为可信：\n\n其一，1827年5月俄亥俄州佩恩斯维尔市（Painesville）警察破获了一起伪币案。在捉拿罪犯的现场，一大群人围观一台制造伪币的机器，这台造币机样子奇特，人群中有个人说它就像个bogus。翌日，当地《电讯报》（Telegraph）报道此事竟然用了bogus一词，称制造伪币的机器为bogus，随后有人把伪币叫做bogus money或简称为bogus。天长日久，大凡假的或伪造的东西人们均以bogus来表示。美国作家马克·吐温曾用过该词，使之得以推广。\n\n其二，1857年《波士顿信使报》（Boston Courier）称，该词源出于一个臭名昭著的骗子，他有个意大利名字叫Borghese。该报说，Borghese行骗有术，手段高明。他签了许多空头支票，作案之后迅即离开。到了1837年，Borghese由于开了大量一钱不值的支票、汇票及各种票据而臭名远扬于美国南部和西部各地。他的大名Borghese也就逐渐成了“伪造的”或“假的”的同义词，以后很可能由于法语bagasse（废物）一词的影响而被缩略为bogus。\n\n例 \n- He was arrested and charged with carrying a bogus passport. (CAE) 他被捕并被指控持假护照。\n- The man was arrested when he handed the cashier a bogus ten-dollar bill. (WBD) 那人递给出纳员一张十元的假钞票时被逮捕了。\n- She produced some bogus documents to support her claim. (CID) 为了证明她的所有权，她出示了一些伪造的证件。\n- The museum quickly discovered that the painting was bogus. (NED) 博物馆很快就发现这幅画是赝品\n\n\n#### intelligible\nintellig・ible\n- \"**able to understand, intelligent**,\" from Latin intelligibilis, \"that can understand; that can be understood\". In Middle English also \"to be grasped by the intellect\" (rather than the senses).\n\n\n#### limpid☢\n(液体,气体等)清澈的；清澄的\n- lymph- \n原意表示“无色液体”。现多用于医学词汇，表示“淋巴”，因其能分泌无色的淋巴液得名。\n- 来自拉丁语 limpa, 泉水，水仙子, water goddess\n\n**Lympha**\nThe Lympha (plural Lymphae) is an ancient Roman **deity of fresh water**\n\n- 不要和limp弄混了啊, limp是\"瘸着走\"和\"柔软的\"的意思, 但是这个是limp**i**d 不是limp**e**d, 只是读起来一样\n\n\n#### toady\n旧时庸医行骗时，常常指使其下人当众吞食蟾蜍（toad）或装作吃蟾蜍。蟾蜍素被认为有毒，不能食用。蟾蜍被吞食之后，庸医再装模作样地予以解“毒”，让下人服用万应灵药，以此显示他的医道高明和妙药灵验。吃蟾蜍者就叫toad-eater。1744年英国小说家菲尔丁（Henry Fielding, 1707-1754）曾对该词的词义作了如下解释：“toad-eater是个隐喻，取自关于江湖医生的下人吞食蟾蜍以示其主人解毒有方的故事。该比喻是基于这样一种联想：处于隶属地位的人，为了顺应庇护人的心意，取得其欢心，被迫做出令人恶心之至的事。”据此，toad-eater一词由“吃蟾蜍者”转义为“谄媚者”或“马屁精”，to eat someone's toad一语被用以比喻“拍某人马屁”，也就不难理解了。toad-eater始见于17世纪，到了18世纪缩略为toady，原义也逐渐丧失，如今仅用于喻义。\n\n例 \n- He was a dictatorial prime minister with a cabinet of weaklings and toadies. (CID) 他是一个独断专行的首相，他的内阁里都是一帮怯弱无能、逢迎拍马的人。\n- She gets good grades only because she toadies to the teacher. 她是靠拍老师的马屁才得高分的。\n- She was always toadying to the boss, but she didn't get a promotion out of it! (CID) 她总是拍老板的马屁，但她并没有因此而得到提升。\n\n#### spurn☢\n- 轻蔑地拒绝；摒弃\n乍一看喝spur有点像? =\u003e 其实的确有点关系\n\nOld English spurnan \"**to kick (away), strike against; reject, scorn, despise**,\" from Proto-Germanic spurnon (source also of Old Saxon and Old High German spurnan, Old Frisian spurna, Old Norse sporna \"**to kick, drive away with the feet**\")\n\n抛弃, 轻蔑地拒绝, 摒弃, 断然拒绝(就像用脚踢开一样)\nShe spurned my offer to help.\n\n\n#### counterproductive\n起反作用的, 如果你想用一个词来表达\"起反作用的\", counterproductive 就是你想要的词.\n- Improved safety measures in cars can be counterproductive as they encourage people to drive faster.\n\n\n#### censorious\noften criticizing other people =\u003e 说三道四的, 喜欢批评其他人的\n\n- Despite string principles he was never censorious.\n\n\n#### exacting☢\nexacting 不是贬义词(高标准的, 要求严格的), 而 demanding 带一些贬义(要求多的)\n\n\n#### fleeting\nfleet 除了\"舰队, 车队, 机群\"的意思外, 还可以做形容词\"快速的\"\n所以 fleeting 就是\"转瞬即逝的, short and quick\"\n- The girls caught only a fleeting glimpse of the driver\n\n#### quail\n鹌鹑 =\u003e verb. 害怕, 退缩\n- 鹌鹑长下面这个样子\n![500](notes/2022/2022.7/assets/1800.jpg)\n- 鹌鹑喜欢藏在草丛里面, 不经常被人见到, 并且鹌鹑受惊的时候常常跑开或者呆住, 而不是飞走. \n- 尽管鹌鹑看起来胖胖的, 它在迁徙的时候是可以飞起来的.\n![500](notes/2022/2022.7/assets/index%202.jpg)\n\n\n#### fusty\nfust- 木棍儿\n来自古法语 fuste, 瓶塞，酒塞，来自拉丁语 fustis, 短棍，木棍，其原义为瓶塞味的。后来引申词义霉臭的，守旧的。(比较 corked.)\n- 类似于汉语里面\"迂腐的\"\n- The **fusty** old establishment refused to recognize the demand for popular music. \n\n##### corked \n/ˈkoɚkt/ adjective\n(of wine): having an unpleasant taste because of a damaged or decayed cork\n- corked wine\n\n\n#### enamored☢\nenamo(u)r + ed\n迷恋的, 倾心的(like and admire a lot)\n\n##### enamour\n\"**to inflame with love, charm, captivate**,\" from Old French enamorer \"**to fall in love with; to inspire love**\", from en- \"**in**, **into**\"  + amor \"**love**,\" from amare \"**to love**\".\n\nSince earliest appearance in English, **it has been used chiefly in the past participle (enamored) and with *of* or *with***.\n\n#### prologue\n序言\n**interlude**\na short period when a situation or activity is different from what comes before and after it 插曲，间歇\n**epilogue**\n后记\n\n**顺序:**\nprologue =\u003e 🙎‍♂️🏃‍♀️🤾‍♀️🖐 =\u003e interlude =\u003e 👻🚚🚁🎨 =\u003e epilogue\n\n#### burlesque\n滑稽歌舞杂剧\n\n##### Burlesque vs Stripping\n- Strip routines typically are just dancing and taking off clothes. \n- Burlesque is a performance - the routines can take months to create. Costumes are involved, they typically tell a story or make a political statement. It's VERY rare that you'll see full nudity, or even nipples in a burlesque routine. The literal definition is 'the art of the tease'. It's about teasing the audience. On the contrary in many strip clubs there is no tease at all, the dances come out naked and stay that way.\n- Read more in this link:　[ELI5: Burlesque vs Stripping : explainlikeimfive](https://www.reddit.com/r/explainlikeimfive/comments/23yicg/eli5_burlesque_vs_stripping/)\n\n\n#### doctrinaire \nadherent of doctrine, based on and following fixed beliefs rather than considering practical problems. =\u003e 教条主义的， 脱离实际的\n\n- He is firm but not **doctrinaire**.\n\n#### dingy\ndark, depressing and possibly dirty, shabby, gloomy, dull\n\n#### gash☢\na long and deep cut, 类似于汉语里面的\"口子\", 唯一不同的是, 英语里面的“口子”可以当动词用\n\n- He gashed his leg while felling trees\n\t- 注意， 这里 felling trees 并不是指\"他从树上掉下来了\"(这里没有 off)， fell 作为动词（不是 fall 的过去式）有”砍倒， 砍伐“的意思， 也有击倒(某人)的意思\n\n#### lacerate\nlacerate类似于gash的动词版本, 划伤, to cut your body badly and deeply.\n\n#### antedate\nantedate = ante+date =\u003e 就是把...的日期往前写(比如一个check本来是1月10号写的, 但是你写1月1号, 就是antedate a check, Ante-dated cheques can be used when the contract is delayed **to avoid unnecessary trouble for the party**.)\n\n\n#### abstemious☢\nabs- =\u003e off, away from\ntem- =\u003e strong drink, 烈酒\n-ious =\u003e ...的\n=\u003e 离开酒的, =\u003e 对美食和酒有节制的, temperate, sparing, moderate, sober\n\n\n#### vicissitude\na passing from one state to another, a variation in circumstance, fortune, character, etc.  变迁, \n- He experienced several great social vicissitudes in his life.\n- The vicissitudes of life... whether I get tenured or not tenured, whether I win the lottery or lose money\n- Many graduates prefer a safe civil-service career to the vicissitudes of starting a business\n\n#### vicarious☢\nvicarious 的词根和上面的 vicissitude 一样:\nvic- =\u003e vicis- \"a change, exchange, substitution\"\n- experienced as a result of watching, listening to, or reading about the activities of other people, rather than by doing the activities yourself. 间接感受到的；间接获得的\n\t- She took a vicarious pleasure in her friend's achievements.\n\t- She invents fantasy lives for her own vicarious pleasure.\n\t- Lots of people use television as their vicarious form of social life.\n\n#### jibe\n1. jibe = gibe =\u003e sneer, taunt, jeer, 嘲讽, 嘲弄\n- a cheap jibe about his loss of hair.\n\n2. jibe with =\u003e be consistent with\n- The numbers don't jibe\n\n\n#### pleat☢\npleat 是 plait 的另一个形式, 但是逐渐它们的意思有了区别: \n- 搜索pleat会得到下面这张图\n![](notes/2022/2022.7/assets/c8884f32aa3b07a5af95268d60107dfd.png)\n- 搜索plait则会得到下面这张图\n![300](notes/2022/2022.7/assets/Basic-Plait-7.jpg)\n\n#### denude\nde- =\u003e away, 也可以表示加强\nnude- =\u003e to strip\n=\u003e to strip or divest of all covering, lay bare\n=\u003e to remove the covering of something, especially land\n- Mining would denude the forest\n=\u003e 引申为 take away sth from\n- In such areas we see villages denuded of young people.\n\n#### mellifluous☢\nmelli- =\u003e honey, 蜂蜜\nflu- =\u003e flow, 流动\n-ous\n=\u003e 像蜂蜜一样(顺滑地)流动的, 像蜂蜜一样甜美的 =\u003e (声音)悦耳的, (音乐)优美的, (气味)甜美的, (文笔)流畅的, 反正就是\"具有蜂蜜流动的性质的\"\n\n- a deep mellifluous voice =\u003e 浑厚悦耳的嗓音(这里说明了翻译成\"甜美的\"并不是很恰当, 在汉语里面甜美的有一种娇滴滴的感觉)\n- the mellifluous sound of the cello\n\n##### melli- \n- melliferous =\u003e 产蜜的\n- mellifluence =\u003e the property of being mellifluous\n- mellifluent =\u003e mellifluous\n\n\n#### cronyism\ncrony =\u003e very close friend, 密友\ncronyism =\u003e \"密友主义\"? =\u003e 指领导人给自己的朋友安排工作, 任人为亲, 任用亲信\n\n#### downpour\n侧重sudden, unexpected, heavy rain\n瓢泼大雨(pour), 倾盆大雨(down, pour), 滂沱大雨\n\n#### effervesce\n![](notes/2022/2022.7/assets/coca-cola-with-ice-114028212-570941275f9b5814080dea61.jpg)\nv. 冒泡泡(像汽水一样)\n- ex- =\u003e out\n- ferv- =\u003e boil, 煮沸后会冒泡泡 =\u003e to bubble\n- -esce =\u003e 表示动作的起始\n![-esce](#-esce)\n\n\n#### unimpeachable\n- un- \n- impeach =\u003e 弹劾\n- -able\n=\u003e 无法弹劾的 =\u003e 弹劾意味着官员犯了罪 =\u003e 无法弹劾的就是\"品格高尚的, 无可挑剔的, 无法指摘的\", completely honest and reliable.\n\n- He is a man of **unimpeachable** integrity and character\n- **unimpeachable** proof =\u003e 确凿的证据\n\n![incontrovertible](#incontrovertible)\n\n#### prolix\npro- =\u003e forth\nlix =\u003e liquere, to flow\n=\u003e pour out  =\u003e 说话就像倒水一样, 一发不可收拾 =\u003e verbose, lengthy, 啰嗦的, 长篇大论的, 冗长的\n\n- The author's **prolix** style has done nothing to encourage  sales of the book.\n\n#### nondescript☢\nnon + descript\n都没有什么能够描述其特点的东西 =\u003e 平平无奇的, 平庸的, 平常的, 不起眼的, undistinguished, ordinary, dull, commonplace\n\n- Her clothes told me nothing: they were as nondescript as it was possible to be.\n\n![nonentity](#nonentity)\n\n#### categorical\n不是\"类别的\"的意思! 应该是 \"像类别一样的, 界限分明的, 没有任何模糊空间的, 确定无疑的, 明确的\"\n- without any doubt or possibility of being changed\n\t确定无疑的，明确的\n\n- a categorical statement/reply/assurance\n\n#### duplicity\ndishonest talk or behaviour, especially by saying different things to two people\n\ndu + plic =\u003e uplic- =\u003e two fold, duplicate\nity- \n=\u003e the state of being double, double-minded, treacherous\n\n#### epithet☢\nepi- =\u003e in addition, 例如 \"epilogue\"\nthet =\u003e to put, set\n=\u003e to put additionally, to add on, =\u003e 除了一个事物本身的名字以外的描述 =\u003e descriptive name for a person or thing =\u003e an adjective added to a person's name or a phrase used instead of it, usually to criticize or praise them (褒贬皆可)\n\n- The singer's 104-kilo frame earned him **the epithet of \"Man Mountain\"** in the press.\n\n#### gripe☢\n抱怨, 发牢骚, a strong complaint, \n=\u003e gripe 之前还有肠绞痛的意思(produce a gripping pain in the bowels) =\u003e (肚子疼得)发牢骚 =\u003e 引申到一般的\"发牢骚, 抱怨\"\n\n#### intercessor\na person who intercedes =\u003e 调解者, 说情者, 求情者\n![intercede☢](#intercede☢)\n\n\n#### savvy\n这个单词很有意思, 两个 v 连在一起.\n注意读音/ˈsæv.i/\n1. If you describe someone as having savvy, you think that they **have a good understanding and practical knowledge of something**.[informal]\n\t- He is known for his political **savvy** and strong management skills.\n\t- Synonyms: understanding, perception, grasp, ken  \n2. If you describe someone as savvy, you think that they **show a lot of practical knowledge**. [informal]\n\t- She was a pretty **savvy** woman.\n\t- Synonyms: shrewd, sharp, astute, knowing\n\n之所以这个词看起来很不\"英语\", 是因为这个词是法语混杂西班牙语(两个语言混在一起叫\"pidgin\"), French savez(-vous)? \"**do you know**?\" or Spanish sabe (usted) \"**you know**,\"\n- savvy是加勒比海盗里面Jack的口头禅, 相当于\"懂不?\", 你可以在油管上看Captain Jack说3分钟\"savvy\" =\u003e  [Capt. Jack Sparrow all SAVVY moments. - YouTube](https://www.youtube.com/watch?v=xG6RHY_WJpM)\n\n\n#### effluvium\ne- =\u003e ex- \"out\"\nfluvium =\u003e fluere =\u003e flow\n=\u003e to flow out =\u003e 后来引申为(泄露, 排放的)令人不快的物质\n\n#### ruminate\nrumin- =\u003e 反刍\n-ate\n=\u003e (动物)反刍 =\u003e 引申到人的思想过程 =\u003e 反复思考, 反复考虑, 长时间思考\n- She **ruminated** for weeks about whether to tell him the truth or not.\n\n英语里面还有一个词的意思和ruminate类似:\n##### regurgitate\n1. to bring back swallowed food into the mouth\n（使）（咽下的食物）返回到口中; 不是为了继续咀嚼, 而是为了吐出来, 比如喂幼崽.\n- Owls **regurgitate** partly digested food to feed their young. 猫头鹰将半消化的食物吐出来喂幼鸟。\n\n2. [disapproving] If you regurgitate facts, you just repeat what you have heard without thinking about it.\n\t（不加思考地）重复；照搬；照本宣科\n- Many students simply **regurgitate** what they hear in lectures. 很多学生只会鹦鹉学舌般地重复从课堂上听来的东西。\n\n原封不动地使用学到的知识, 这个引申义和 ruminate 的含义几乎是相反的, ruminate knowledge 是为了更好地理解, 而 regurgitate knowledge 是完全没有任何理解的.\n\n#### bluff☢\nbluff（吓唬）：玩扑克牌时的虚张声势\n玩过扑克牌的人应该都知道，在玩牌时，人们往往会虚张声势，明明抓了一手烂牌，却摆出一副胸有成竹、志在必得的模样，吓得对方不敢出手，从而在游戏中获利。这种虚张声势在英语中就叫做 bluff。这个单词最早出现于美式英语中，可能来自荷兰语 bluffen（吹嘘）。它原本仅仅是一个玩扑克牌的专业术语，但现在已经广泛应用于各种场景。\nbluff：[blʌf] n.v.吓唬，虚张声势\n\n#### brazen\nbrazen - brazen 源自古英语 braes 'brass'（黄铜），所以有时用作 brass 的形容词，表示“黄铜制的”或“黄铜般的”，但在现代英语中更常用以表示“脸皮厚的”或“厚颜无耻的”。以 brass（黄铜）喻指厚颜这一用法可以追溯到伊丽莎白时代（1558-1603）。《牛津英语词典》1642 年的一条引语对此作了解释：“His face is of brasse, which may be said either ever or never to blush.”厚颜者也许从未脸红过，故其脸犹如黄铜制的。有一短语 as bold as brass 常用作 brazen 的同义语，也表示“厚颜无耻的”。\n- 黄铜做的脸皮 =\u003e 不会脸红的 =\u003e 厚颜无耻的\n\n例 \n- How can you believe such a brazen lie? (FWF) 你怎么能相信如此无耻的谎言呢？\n- I could not do anything so brazen as that. (CCE) 我不会做出那样不要脸的事。\n- There were instances of brazen cheating in the exams. (CID) 考试中有明目张胆的作弊行为。\n\n#### cavort\nto jump or move around in a playful way, sometimes noisily, and often in a sexual way\n雀跃；嬉戏玩闹；（常指）调情玩乐\n- They were spotted cavorting beside the swimming pool.\n\u003e Cavorting requires a good mood, lots of energy, and some running room. Children love to cavort, and so do parents when they win the lottery. The origins of the word are unclear, perhaps coming from the word curvet, meaning “leap gracefully or energetically,” and leaping is a great addition to any cavorting. There are lots of synonyms, so if you ever get tired of cavorting, you could always prance, frolic, lark, rollick, romp, or carouse. The choice is yours.\n\n有可能是从 curvet 这个单词来的, 意思是\"a leap by the horse\", 所以在英语里面更像是\"马跃\"\n\n\n#### croon☢\n低声吟唱, 低语, to sing or hum in a quiet and gentle voice.\n- To croon is to sing a soft or emotional song. A father might croon a lullaby to his baby as she falls asleep. Elvis Presley was known to croon to the ladies.\n\n最开始的意思是 to bellow like a bull, =\u003e 像牛一样哞哞叫\n\n\n#### default\ndefault 还可以作为动词, 表示\"fail to act (a legal responsibility)\", 比如违约, 拖欠债务等等\n- People who default on their mortgage repayments may have their home repossessed.\n\n\n#### diatribe\ndia- =\u003e away \ntribe =\u003e to wear, rub\n=\u003e a wear away (of time),  a waste of time =\u003e 写文章, 说话只是为了抨击某个人或事 =\u003e 怒斥, 檄文(noun)\n\n\n#### ensconce☢\nen- =\u003e make, put in\nsconce =\u003e \"small fortification, shelter\"\n=\u003e to cover with a fort =\u003e 安顿, 安置 =\u003e (引申到人)\"安坐\", \"**ensconce yourself**\" =\u003e \"舒舒服服地坐着\"\n\n- After dinner, I **ensconced myself** in an armchair with a book.\n\n#### evict☢\ne- =\u003e ex- =\u003e out\nvict =\u003e conquer\n=\u003e expel (by legal process), recover property by judicial means =\u003e to force sb to leave somewhere, usually because he/she has broken a law or contract.\n\n\n#### fecund\n\"多产的, 肥沃的\"\nThe adjective fecund describes things that are highly fertile and that easily produce offspring or fruit. Rabbits are often considered to be fecund animals, and you may hear jokes in poor taste about people reproducing like rabbits if they have a lot of children.\n\n\u003e The word fecund comes from the Latin word fecundus, meaning fruitful. But the English word does not just describe something or someone fertile, the adjective fecund can also be used to describe someone who is innovative or highly intellectually productive. Your fecund imagination will be an asset if you have to tell ghost stories around the fire at camp while eating s'mores but that same fecund imagination could be less helpful if you're at home alone on a stormy night and you think you hear a knock at the door!\n\n#### giddy☢\nIf you've ever spun in circles until you fell to the ground laughing, you know how it feels to be giddy. This adjective can mean dizzy, elated, or — as in the spinning around example — a lightheaded, lighthearted combination of the two.\n- 除了\"晕\", giddy还用来形容因为激动高兴而不能正常思考的状态\"飘飘然的\"\n\n\u003e The hackneyed phrase \"giddy as a schoolgirl\" calls forth the image of a kid giggling with her friends over some adolescent foolishness. Giddy has been used to describe someone incapable of serious thought or easily excited as far back as the 16th century. Given that, in modern usage, giddy describes someone silly and frivolous, it's interesting to know that the Old English source for this word has a slightly darker tinge: gidig means \"insane\" or \"god-possessed.\"\n\n#### incursion☢\nin- \ncurs- =\u003e run, 跑 比如\"precursor\"\n-ion\n=\u003e run in (hostilely) =\u003e 侵犯, 入侵, 介入(noun)\n\nWhen an army crosses a border into another country for battle, they are making an **incursion** into enemy territory. An **incursion** is an invasion as well as an attack.\n\n**Incursion** can also be used to describe other things that rush in like an army such as an invasive species into a new region or floodwaters entering your home. When an airplane heads onto a runway it is not supposed to land on, risking airport safety, it is known as a runway **incursion**. And an **incursion** of cold air could make September feel like December.\n\n\n#### malaise☢\nIf you are experiencing **malaise**, chances are you are feeling blue or looking green. Malaise is a slump; you're not feeling your best — either mentally or physically.\n\n\u003e Mal is French for \"**bad**,\" and aise means \"**ease**.\" When experiencing malaise, ease yourself down on the couch to recover. Malaise is frequently used figuratively to describe slumps that other things go through as well. The 20-year economic malaise in Japan is one example, but you'll also hear of educational malaise, political malaise, and even \"a general malaise.\" Wherever you turn, there's malaise.\n\n- a general feeling of **being ill** or **having no energy**, or an uncomfortable feeling that **something is wrong**, especially with society, and that you **cannot change** the situation\n\t身体不适；萎靡；心神不宁；（尤指对社会的）不满，无奈\n\t- They claim it is a symptom of a deeper and more **general malaise** in society. \n\t- We were discussing the roots of the current **economic malaise**.\n\n#### myriad\n来自希腊语 myrias, 大量的，无数的，**一万**，可能来自 meu, 流动，流出，水流，词源同 emanate, marine. 即由流动的水引申词义丰饶的，许多的，无数的。需注意的是，该词在古希腊语为**单个词所表示的最大数**。词义演变比较 abundant.\n- 比最大的计量单位还大 =\u003e 不可估量的, 许多, 无数\n\n- Myriad comes from the Greek myrioi, the word for ten thousand, or less specifically, a countless amount. Myriad can be a noun, like *a myriad of choices*, or an adjective, like when you study *myriad subjects* in college. If you lift a rock you might find *a myriad of bugs*. Sticklers often look down their noses at using myriad as a noun, but that usage came first.\n\n\n#### parry\nparry 和 parachute 的词根是一样的, para- 表示保护, defend, 在这里 parry 保护的方式是\"躲避, fence, dodge, avoid\", 无论是 physically 还是 verbally.\n\n- Sword fighters **thrust** and **parry**. To thrust is to try to stab, and to parry is to avoid getting stabbed by blocking a thrust. Though it comes from fencing, parry is also handy in [dodgeball](https://www.youtube.com/watch?v=bJ1vEQKX-hE) and awkward conversations\n\n- The word parry means to block or evade a movement, like in fencing, but it can also refer to an evasion that is **verbal** rather than physical. If someone asks you who you have a crush on, but you don’t want to answer, parry the question — change the subject or ask a question in return. When used in this way parry retains its sense of defending yourself through evasion.\n\n\n#### petulant☢\n耍小孩子脾气的, 偏贬义的一个词, 可能是受单词\"pet\"的影响? pet =\u003e 除了\"宠物\"还有\"宠儿, 掌上明珠\"的意思(偏贬义) =\u003e 掌上明珠是想要被宠爱的 =\u003e 像小孩一样peevish, 觉得自己不是最重要的就生气的\n- Choose the adjective petulant to describe a person or behavior that is **irritable in a childish way**.\n\nThe adjective, petulant, is a disapproving term used to describe a bad-tempered child, an adult behaving like an angry child or behavior of this type. Angry or annoyed mean the same thing, but if you choose the word, petulant, you are indicating that it is unreasonable or unjustified. Petulant came to English in the late 16th century from the Latin petulantem \"forward, insolent\" but was not recorded to mean childishly irritable until the late 1700s.\n\n\n#### preposterous\n字面上理解就是 pre =\u003e before, poster =\u003e behind, ous =\u003e ...的, before-behind =\u003e 颠三倒四的, 不符合常理的, =\u003e 十分愚蠢的, 荒谬的\n\nTo a vegetarian, the idea of eating a 52-ounce T-bone steak would seem preposterous — absolutely absurd.\n\nWhen the word preposterous was first used, it meant reversing the normal order of things — putting what was last first, and vice versa. **Imagine putting on your underwear over your pants and you'll see that there's a kind of absurdity in something that's backwards**, which is why preposterous came to mean \"ridiculous.\" The word is often used as part of an exclamation: a chef who is asked to cook with nothing but jelly beans might exclaim, \"**That's preposterous**!\"\n\n#### refractory☢\n\"stubborn, obstinate, perverse, resisting, unyielding,\" \nfrom past participle stem of refringere \"**to break up**\". The notion is said **to be \"breaking back\" all attempts to enforce obedience**.\n\"宁为玉碎\", 即使要 break 很多东西也要...的 =\u003e difficult to deal with, unwilling to obey\n\n\n#### retouch\n\"重新碰\" =\u003e 润色, 修整, 修饰\n- We had the wedding photos retouched to make it seem like a sunny day.\n\n#### retrench☢\nre- =\u003e back\ntrench =\u003e to cut, 比如 truncate\n=\u003e to cut backwards =\u003e 削减, 减少(开支)\n\n\n#### rickety\nrickets 是佝偻病, 佝偻病是指儿童的骨骼变得柔软和脆弱，通常是由于维生素 D 的极端和长期缺乏所致。罕见的遗传问题也会引起佝偻病。\n\nrickety 就是\"换了佝偻病一样的\" =\u003e 快要散架了的, 摇摇晃晃的, 不结实的\n- a rickety chair.\n\n- 一个长得很像的单词:　cricket =\u003e 板球运动, 蟋蟀\n\n#### sepulchral\nSomething that reminds you of death is sepulchral. A dreary, misty graveyard at night usually feels sepulchral. 阴森恐怖的\n\n- The curtain rose to reveal a gloomy, sepulchral set for the play.\n\nA sepulchre is a tomb or a crypt — a kind of stone room meant for burying a dead body. Something that's sepulchral reminds you of a sepulchre, either because it looks or feels like an actual tomb, or simply because it makes you think of death or dying. An empty building might be sepulchral, or a gloomy gathering. The Latin root word is sepelire, \"to bury or embalm.\"\n\n#### shipshape\n- ship, 船，shape, 形状，布置。比喻用法，因船行海上多风浪，所以船上的物品摆放必须牢固整齐，因而引申该词义。\n- The builders have gone, but it'll take a while to get things **shipshape** again.\n\n#### snub☢\nTo **snub** is to ignore or refuse to acknowledge someone. If you want to snub **your** former best friend, you can refuse to even look at her when you pass in the hallway.\n\n- I think she felt **snubbed** because Anthony hadn't bothered to introduce himself. 冷落, 怠慢, 忽视\n\nWhen you **snub** someone, you deliver an insult by pretending to not even notice someone that you know. There’s an element of disdain and rejection to a **snub**, as if you’re too good to even acknowledge the person. As a noun, a **snub** is that act of cold rejection. Your former friend probably noticed the **snub**, and she’ll probably **snub** you from now on. **Snub** also means \"very short,\" like the nose on a bulldog.\n\n**嗤之以鼻**有点不一样, 也有点相似(冷落, 奚落, 傲慢):\n嗤之以鼻是一个汉语成语，读音为chī zhī yǐ bí，意思是：用鼻子轻蔑地吭气，表示**瞧不起**；用鼻子吭气，表示**看不起**；用鼻子吭声冷笑，表示**轻蔑**。\n\n#### thick-skinned\n这个单词不能翻译成\"厚脸皮的\", 因为在汉语里面\"厚脸皮的\"通常是贬义的.\n或者说, thick-skinned可以理解成偏褒义的\"脸皮厚\" =\u003e 不在乎外界的批评, 不会轻易被困难打击\n\n##### [Is \"thick-skinned' a positive comment in American English?](https://www.quora.com/Is-thick-skinned-a-positive-comment-in-American-English)\n“Thick-skinned” means someone is not upset or offended over trivial things. They may even shrug off insults. It's **generally positive** but it's a quality that if taken too far becomes insensitivity. **It doesn't mean “shameless”**.\n\nThe opposite — “thin-skinned” — is always negative. Someone who is thin-skinned takes offense too easily and interprets even positive criticism as an attack. 类似于\"玻璃心的\"?\n\nBritish English has the term “**brass necked**” meaning “shameless”. For example: “Your brother has got a real brass neck. I can't believe he's handing out business cards at your dad’s funeral!”\n\n#### trifle\ntrifle是一种甜点:\n![300](notes/2022/2022.7/assets/retro-trifle-0566615.jpg)\n也许是因为 trifle 是\"小\"甜点? (不过看起来并不小啊), 所以 trifle 又用来防泛指琐碎的东西, \"小零碎, 小玩意儿\" unimportant things\n- I brought a few trifles back from India - pieces of jewellery and fabric mainly.\n\ntrivial =\u003e having little value or importance\n\n#### unanimous\nun- =\u003e one, 来自拉丁语 unus\nanim- =\u003e life, spirit, 生命, 精神等\n-ous\n=\u003e \"合一的, 所有人就像是同一个人一样, 没有任何冲突\" =\u003e 全部同意的, 意见一致的, 一致通过的\n\n##### anim-（生命）：代表生命之源的灵魂\n古代人认为生命的本质是因为灵魂的存在。在拉丁文中，灵魂分为阴性和阳性两个单词，既阴性的**anima**（阿尼玛）和阳性的**animus**（阿尼姆斯）。表示生命的词根**anim**就来自这两个拉丁文。由于生命和灵魂密不可分，所有词根 anim 既有“生命”、“能动”的含义，也有“精神”、“心”的含义。\n- anim-：生命，能动，精神，心\n- anima： ['ænɪmə] n. 灵魂，生命；神圣之灵\n- animus：['ænɪməs] n. 敌意；意图, 基本态度；女性的男性意向(卡尔荣格提出的一个心理学概念)\n- **animosity**：['ænə'mɑsəti] n. 憎恶，仇恨，敌意\n- animal：['ænɪm(ə)l] n. 动物，**有生命能动的**\n- animate：['ænɪmet] vt. 使有生气；使活泼；鼓舞；推动 adj. 有生命的\n- animation：[,ænɪ'meʃən] n. 动画，原意是使其具有生命，使其动起来\n- unanimous：[juː'nænɪməs] adj. 全体一致的\n- equanimity：[,ɛkwə'nɪməti] n.（心情）平静坦然\n\n#### amorphous\na- =\u003e 没有\nmorph- =\u003e form, shape, 源自希腊语\"morphe\" \n-ous\n=\u003e 没有形状的, 无定形的, having no fixed form or shape\n\n##### morph-（形态）：梦神摩尔甫斯\n摩尔甫斯（Morpheus）是希腊神话中的梦神，是睡神 Hypnos 的儿子，掌管人们的梦境。摩尔甫斯能够形成、塑造人们的梦境，还能以各种形态出现在人们的梦境中。摩尔甫斯通过梦，向人们传递神的旨意。摩尔甫斯的形象通常是一个背长双翼的俊美男子。由于太忙，摩尔甫斯没有结婚，不过也有人说彩虹女神伊里丝跟他是两口子。\n梦神的名字 Morpheus 在希腊语中是“形态制造者”之意，来自希腊语 morphe（形态）。英语单词 morphine（吗啡）就源自梦神的名字 Morpheus，因为吗啡具有梦境一样的止痛作用。英语词根 morpho-/-morph（形态）同样源自希腊语 morphe（形态），在科学领域应用极其广泛。\n- morpho-/-morph：形态，态\n- morphine： ['mɔːfiːn] n. 吗啡\n- morphic：['mɔ:fik] adj. 形态学的，语形学的\n- amorphous：[ə'mɔrfəs] adj. 无定形的\n- morphology： [mɔr'fɑlədʒi] n. 形态学，形态论；词法，词态学\n\n#### canon☢\ncanon ≠ cannon\ncannon是加农炮的意思, 也就是下面这个玩意儿\n![300](notes/2022/2022.7/assets/Cannon-Antietam-National-Battlefield-Maryland.webp)\n~~而Canon则是佳能相机的意思~~\nCanon, 注意少了一个n, (one “n”) refers to **a collection of rules or texts that are considered to be authoritative**. Shakespeare and Chaucer are part of the canon of Western literature, so you might read their work in an English class.\n\nA canon can also be a body of work, like the Shakespeare canon, which includes all of the Bard's plays and poems. These days, many schools and colleges include more diverse and underrepresented authors in literature classes and encourage students to read works not included in the standard literary canon. The literary canon can change with time, and so can the cultural canon. Don't confuse this word with cannon with two n's, the big gun that shoots bowling-size balls at the enemy.\n\nCanon是一个有宗教背景的词汇:\n- a Christian priest with special duties in a cathedral \n\t大教堂教士\n- a rule, principle, or law, especially in the Christian Church\n\t原则；准则；法规；（尤指）基督教教规\n\n应该是后来才有了\"作品全集\"这个意思\n\n\n#### despot☢\n暴君\n英语单词 despot 来自希腊语的 despotes，本意是“一家之主、领主”。该词的前一半来自 domestic（家庭的、国内的），后一半来自 potent（有力的）。古希腊的一家之主对于家中成员和奴隶拥有绝对权威，因此 despot 一词含有“独裁、暴虐”的含义。在拜占庭帝国，despot 曾被用于宫廷贵族的称号、诸侯国亲王的称号，甚至被用作拜占庭帝国皇帝的称号。现在，despot 常用来表示那些独裁、暴虐的一国之主。\n- despot： ['despɒt] n. 暴君，专制君主，独裁者\n- despotic：[dɪ'spɑtɪk] adj. 暴虐的，暴君的；专横的\n- despotism： ['despətɪz(ə)m] n. 独裁，专制，独裁政治\n\n#### impeccable\nim- =\u003e not, opposite\npecc- =\u003e to stumble, sin\n-able\n=\u003e faultless, 无懈可击的, perfect\n\n##### pecc-\npeccadillo 来自西班牙语, 过失, 小错误, \npeccable 容易犯错的\n\n- [incontrovertible](#incontrovertible)\n- [unimpeachable](#unimpeachable)\n\n#### detain\nde- =\u003e from, away\ntain =\u003e to hold\n=\u003e to hold off, keep back, withhold =\u003e 关押, 拘留, (不一定是执法机关, 比如医院强制让一个病人留在那里也叫detain, 可以翻译为\"留院观察\", discharge也是这样, 既可以用于医院也可以用于监狱)\n- A suspect has been detained by the police for questioning.\n\n=\u003e (短时间的)耽搁, 延误\n- Thank you. We won't detain you any further.\n\n\n#### taciturn\ntac-,tic-,retic- \n= silent, 表示“安静”。源自拉丁语 tacere \"to be silent.\"\n\nSomeone who is taciturn is reserved, not loud and talkative. The word itself refers to the trait of reticence, of seeming aloof and uncommunicative. A taciturn person might be snobby, naturally quiet, or just shy.\n\nHaving its origin in the Latin tacitus, \"silent,\" taciturn came to be used in mid-18th-century English in the sense \"habitually silent.\" Taciturnity is **often considered a negative trait**, as it suggests someone uncommunicative and too quiet. Jane Austen wrote, \"We are each of an unsocial, taciturn disposition, unwilling to speak, unless we expect to say something that will amaze the whole room, and be handed down to posterity with all the éclat of a proverb.\"\n\n- reticent =\u003e unwilling to speak about your thoughts or feelings\n\t沉默寡言的；不愿交谈的\n- reticence\n- tacit\n\t- 不说话 =\u003e 默认的, 暗中的\n\t- Something tacit is **implied or understood without question**. Holding hands might be a tacit acknowledgment that a boy and girl are dating.\n\t- The adjective tacit refers to information that is **understood without needing to acknowledge it**. For example, since we know that the sky is blue, that kind of assumption is tacit. Lawyers talk about \"tacit agreements,\" where parties give their silent consent and raise no objections.\n\n#### stoke\nTo stoke is to **poke a fire and fuel it so that it burns higher**. Stoke can also mean \"**incite**\"(figuratively) — a principal's impassive silence in the face of requests for more tater tots might stoke the flames of student anger.\n![](notes/2022/2022.7/assets/Pasted%20image%2020220723211533.png)\nWhen a surfer says, \"I am so stoked,\" it means she is excited — the fire of enthusiasm is burning hotter. It's interesting to reflect on how many words in our language have to do with the tending of fires, an activity that has become much less common in recent human history.\n\n煽动 =\u003e 有趣的是汉语里面的这个词也和火有关, 只不过汉语是扇风, 英语是\"拨旺（炉火)\"\nincite, stir up, instigate; incite; arouse; foment; provoke\n\n#### foment\nStand outside the school cafeteria passing out flyers with nutritional details on school food, and you may foment a revolution — foment means **stirring up something undesirable**, such as trouble.\n\nYou would never say, \"Hooray, we fomented a revolution.\" Instead you'd say, \"Those good for nothing scalawags fomented the rebellion.\" **Don't confuse foment and ferment.** Ferment can mean \"to stir up\" in a good way — a football game can ferment excitement in a town, or foment trouble through traffic tie-ups and litter.\n\n#### supersede\nsuper- =\u003e above\nsede- =\u003e to sit\n=\u003e 坐在...的头上, =\u003e 取代老的事物\n\n- Hand tools are relics of the past that have now been **superseded** by the machine.\n\n\n#### aversion\n厌恶, \"反\"感\na- =\u003e off, away, \nvers =\u003e to turn\n-ion\n=\u003e a turning away from =\u003e 看到不喜欢的东西想要躲开 =\u003e 厌恶, 反感, 痛恨, strong dislike of sth\n\n#### defer☢\ndefer 除了\"延期\"的含义以外, \"**defer to** someone\"  还可以表示\"(出于尊敬或者权威而)服从\"的意思, \n- Doctor are encouraged to **defer to** experts.\n\n所以 **deference** 就是名词版本的\"顺从, 尊敬, 服从\"\n\n- She covered her head out of deference to Muslim custom.\n\n\n#### opprobrious☢\nop- =\u003e in front of, before\nprobr =\u003e probum, reproach, infamy\n-ious\n=\u003e reproach in front of..., expressing scorn, disgrace, contempt\n=\u003e 谴责的, 恶毒的, 令人不齿的\n- Opprobrious is a **heavy-duty** word to describe something taunting or shameful. Opprobrious words criticize in a mean, hurtful way.\n\n- Opprobrious comes from the Latin opprobare which means \"to reproach or taunt.\" If someone is being opprobrious, she's being abusive and mean. **Insults are opprobrious, while constructive criticism is not**. No one wants to be treated in an opprobrious way. We can also use this word for bad behavior that causes shame — someone cheating on a test is opprobrious. **Opprobrious actions are disgraceful, ignominious, and inglorious**.\n\n#### dais☢\ndais是13世纪借自古法语deis的，所以原词形也是deis，原指设在平台上专供贵宾使用的桌子，取的是拉丁语discus的另一词义“桌子”。到了16世纪deis从英语里消失了，只保留在苏格兰语里。到了19世纪，苏格兰小说家及诗人司各特（Sir Walter Scott, 1771-1832）才又起用了这个词，但用的是近代法语的拼法dais，仍指“讲台”或“主席台”。\n![](notes/2022/2022.7/assets/dais_noun_004_1024.jpg)\n![400](notes/2022/2022.7/assets/lectern-podium-and-dais.webp)\n\n\n#### abstain☢\nabs- =\u003e off, away from\n-tain =\u003e to hold, 例如\"contain\"\n=\u003e withhold, to hold back, refrain, keep off\n=\u003e 戒, 戒绝 (某个不良嗜好)\n- to abstain from alcohol/smoking/sex\n=\u003e (选举时)弃权\n\n- abstain 单词的构造和 withhold 非常像, withhold 里面的 with-表示\"向后, 相反\"\n\n##### -tain: to hold\n- attain =\u003e at・tain =\u003e at- 表示加强, 达到, 获得\n- obtain =\u003e ob・tain =\u003e ob- 也表示加强, 达到, 得到\n- contain =\u003e con・tain =\u003e con- 这里也应该表示强调, 包含, 容纳\n- maintain =\u003e main・tain =\u003e main- -\u003e hand =\u003e 保持, 维护\n- entertain =\u003e enter・tain =\u003e hold together?\n- detain =\u003e de・tain =\u003e de- =\u003e off, 拘留, 耽搁\n- pertain =\u003e per・tain =\u003e per- through -\u003e hold through, =\u003e belong to, te attached legally\n- retain =\u003e re・tain =\u003e re- back =\u003e to hold back, 保留\n- sustain =\u003e sus・tain =\u003e sus- -\u003eunder, beneath =\u003e 在下面 hold, 支持, 支撑\n\n#### forbear☢\nfor- =\u003e 完全, very much\nbear =\u003e 忍受\n=\u003e 完全忍受, 完全克制 =\u003e 自制, 克制\nto prevent yourself from saying or doing something, especially in a way that shows control, good judgment, or kindness to others\n\n- forbearance =\u003e noun.\n\n\n#### illuminati\n- 光照会, 光明会（拉丁语：Illuminati）是 1776 年 5 月 1 日启蒙运动时成立于巴伐利亚的一个秘密组织。该组织经常被各种阴谋论指控参与控制全世界的事务，透过掌握货币发行权、策划历史事件，并安插政府和企业中的代理人，以获得政治权力和影响力，最终建立一个“新世界秩序”\n- 或者, 指的是漫威里面的秘密英雄组织: \n\t当当当~ 你能认出几个?\n\t![](notes/2022/2022.7/assets/Illuminati_(comics).jpg)\n\n#### husband☢\nhusband 还可以是个动词, 表示\"节约地使用\"\n- husband precious resources\n\n\n#### disabuse\ndis- =\u003e apart\nabuse =\u003e 侮辱, 毁谤\n=\u003e \"消除毁谤, 让毁谤和受害者分开\" =\u003e free from mistake, fallacy, or deception\n\n- He thought that all women liked children, but she soon disabused him of that (idea/notion).\n\n#### puckish\n- liking to make jokes about other people and play silly tricks on them 顽皮的，淘气的\n\npuck【淘气小妖】 + -ish 形容词后缀。\nPuck 是凯尔特神话和英国民间故事里恶作剧的小精灵，又叫 Robin Goodfellow. 莎士比亚的《仲夏夜之梦》中，正是这个家伙将年轻的恋人们的生活弄得一团糟。\n\n#### labile☢\nprone to lapse =\u003e changing often or easily\n\n- emotionally labile characters\n\n#### minuscule\nextremely small\n- The film was shot in 17 days, a miniscule amount of time.\n\n- 原来指的是小写字母 \"minuscula\"，在早期的拉丁字母体系中并没有小写字母，公元4世纪--7世纪的安塞尔字体和小安塞尔字体是小写字母形成的过渡字体。公元8世纪，法国卡罗琳王朝时期，为了适应流畅快速的书写需要,产生了卡罗琳小写字体，传说它是查理一世委托英国学者凡·约克在法国进行文字改革整理出来的。它比过去的文字写得快，又便于阅读，在当时的欧洲广为流传使用。它作为当时最美观实用的字体，对欧洲的文字发展起了决定性的影响，形成了自己的黄金时代。[Source](http://www.0755tt.com/courseware/ad/b/b_1/b_1_1/b_1_1_01/b_1_1_01_03.htm)\n\n\n#### anomaly☢\n异常， 这个单词乍一看好像是 a+normal 组成的，但是实际上是 an+homalos（homos）\n- an- =\u003e not\n- homalos =\u003e even, from \"homos\" =\u003e same\nnot even, not the same, unevenness, derivation from the common rule.\n\n- 形容词: anomalous\n\n\n#### defuse☢\nde- =\u003e remove\nfuse =\u003e 炸弹的引信\n=\u003e 移除炸弹的引信 =\u003e 拆除炸弹\n=\u003e 拆除炸弹也就解除了一个紧张的情况 =\u003e 引申为\"平息某个危险或者紧张的情况\"\n- defuse the crisis/tension\n\n- 这个词是近代才出现的(要先有炸弹才能造这个词)\n不要和 diffuse 弄混了\n![diffuse](#diffuse)\n\n#### impenetrable\nim + penetrat(e) + able\n=\u003e 无法穿透的\n\n=\u003e 引申到思想, 文字, =\u003e 一个费解的情况, 文字, 思想就像一团无法被理智\"刺穿\"的迷雾 =\u003e 费解的, 看不破的, impossible to understand\n\n\n\n\n#### bumble\nTo bumble is to **move or speak in an awkward, fumbling way**. You might bumble your way through your first dance performance, tripping over your own two feet.\n\nWhen you bumble, you walk unsteadily or speak with a stutter. You can also bumble something, or completely mess it up. An inexperienced teacher might bumble her attempts at managing a huge class of middle school students, and you might worry that you'll bumble your first interview as a radio reporter. Bumble was first used in the 1500's, and it's probably an imitative word, or one that sounds like what it means.\n\n##### bumblebee\n大黄蜂, \n为什么大黄蜂叫 bumblebee 呢? 其实大黄蜂本来不叫这个名字, Darwin 给大黄蜂起的名字原来叫 humblebee =\u003e 因为蜜蜂一直都在嗡嗡叫(hum)\n- [How the humblebee became the bumblebee](https://www.theguardian.com/environment/2010/aug/01/humblebee-bumblebee-darwin)\n\n\n\n#### hangdog\n在中世纪的欧洲，给人类造成死亡或伤害的动物，像人一样，也要受到起诉和审讯。因这些罪行而受到起诉并被判死刑的动物有老鼠、猪、狗及毁灭庄稼的害虫。有记载说，1595年在荷兰西部城市莱顿（Leyden）有个小孩被狗咬伤手指致死，这条狗因此被人们吊死。狗被吊死之事莎士比亚在他的剧作中曾提到过五次。hangdog一词很可能即由此而来，常用于a hangdog look这一搭配，17世纪末此语原指“卑怯的样子”，如今多表示“羞愧的神态”或“自觉有罪的样子”，hangdog现在常用作形容词，表示“羞惭的”或“感到有罪的”。\n\n例 a hangdog look/expression 羞愧的神情／表情\n\nA hangdog look is one that betrays a feeling of shame, embarrassment, or fear. Your hangdog expression after sneaking a whoopee cushion onto your teacher's chair is a dead giveaway that you're guilty.\n\nUse the adjective hangdog to describe someone's cowering appearance or the sheepish look on her face. You might have a hangdog look if you're afraid of getting in trouble, or if you regret your actions. The now-obsolete root noun hang-dog was used in the 17th century to mean \"a despicable, low person,\" or someone who's \"only fit to hang a dog,\" or sometimes \"only fit to be hung (like a dog).\"\n\n- 还真有几分神似\n![300](notes/2022/2022.7/assets/35F42BC200000578-0-image-m-18_1467669584591.jpg)![](notes/2022/2022.7/assets/d50f38c61b29bbecdaf63075ca4d39b8.jpg)\n\n#### kindred☢\nYour **kindred** are your people. If you say are going to visit your kindred during the holidays, that means you are going to visit your relatives.\n\nThe word kindred can be used as either an **adjective** or a **noun**. **The noun version is somewhat archaic** — you are more likely to encounter this word in classic literature than in casual conversation. You may be more familiar with the adjective version of the word, which has gained popular usage in the term “**kindred spirit**” or “**kindred soul**,” which is used to describe those who share similar attitudes, characteristics, or beliefs.\n\n\"of a similar **kind**\" =\u003e 这样要好记一点\n\n- They sell dried fruit and nuts and other **kindred** products.\n\n\n#### mash \u0026 smash\nmash = make sth into a paste during **cooking**\nsmash = fighting, destroying something\n\nmash means to make something into a paste so you can eat it (usually potatoes or vegetables)\nsmash means to crush something into small pieces by hitting/punching/destroying it\n- [Source](https://hinative.com/en-US/questions/15936585)\n\n#### obeisance☢\nobeisance 不是 obey 的名词, obedience 才是 obey 的名词形式.\n\n- An obeisance is an act, usually physical, showing dutiful obedience. A supplicant might perform obeisance, touching his face to the ground, before humbly asking for help. obeisance是出于礼仪和尊敬的打招呼\n\n- Obeisance is often used in **historical or religious contexts** and often refers to **bowing or kneeling**. Figuratively, it means an act of respect though sometimes with the **negative connotation of slavishly doing as expected**. Your boyfriend might bring you and your mother flowers **in obeisance to** the idea that the parents should be courted as much as the child. Consumers who want this software must show obeisance to the Internet — it can't be bought in a store or anywhere else.\n\n#### prudent \u0026 prudish\nprudish是从prude来的, 但是prudent不是从prude来的, 即使它们长得更像.\n\n- **prudent**\nprudent是一个褒义词\n\nDescribe an action as prudent if it is the wise thing to do under the existing circumstances. If you're getting in trouble, it is probably prudent to keep your mouth closed and just listen.\n\nIf you show good and careful judgment when handling practical matters, you can be described as prudent. Similarly, a wise and well-thought-through decision or action can be called prudent. The word comes from a contracted form of the Latin prōvidēns, from the verb \"to foresee.\" The English word provident, \"wise in planning for the future,\" is the non-contracted descendent of the same Latin root.\n\n- **prudish**\nprudish 则和 prude 一样, 都是偏贬义的.\n\nTo be prudish is to be extremely proper, almost a little too proper. To be called prudish isn't a compliment.\n\nTo be proper is to be polite and have good manners. To be prudish is to take being proper to an exaggerated or ridiculous degree. For example, it's definitely a bad idea to use a naughty word in class, but a friend who scolds you when you use it privately could be considered prudish. They're going a little too far. Prudish behavior is also called priggish, prim, prissy, puritanical, and straight-laced. Others usually think prudish people should lighten up.\n\n\n\n#### repeal☢\nappeal 有\"呼吁\"的意思, repeal 的意思和 appeal 相反, 就是\"不宣传了\", recall 一个 appeal =\u003e 废除, 撤销(一个法案, 一条法律)\n\n\n\n#### suffuse\n![fus-](#fus-)\nThe verb suffuse means to **spread and fill a space**, like the way the smell of wildflowers might suffuse a meadow.\n\nSuffuse is a synonym for steep. Like tea whose flavor grows stronger the more it steeps, when you suffuse something it spreads throughout until an area is full, or even overly full. Another synonym, infuse, looks a lot like suffuse. Both words come from the Latin word fundere, which means \"to pour.\"\n\n#### wrongheaded\n执迷不悟的\n\nSomething that's wrongheaded is **foolish, misguided, and stubborn**. A wrongheaded politician might run for president despite polls showing there's no way he can win.\n\nA wrongheaded entrepreneur may go ahead with her plans to open an ice cream shop for dogs despite being advised not to by everyone she knows. You might feel angry about a judge's wrongheaded decision in an important court case. Anyone who uses bad judgement is wrongheaded, especially when the mistake seems obvious. The adjective wrongheaded has been around since the 1730's.\n\n\n#### babble\n类似于婴儿说话的方式 =\u003e 嘟囔, 啊吧啊吧, =\u003e 引申到兴奋含糊难懂的说话方式, 乱哄哄的声音\n=\u003e 还可以用来形容溪流的\"潺潺作响\", \n\n- To babble is **to talk on and on without a particular goal**. It might drive you crazy when your little sister babbles endlessly about her favorite video game.\n- The children babbled excitedly among themselves. \n- She was babbling something about her ring being stolen.\n\n#### jabber\nto speak quickly and excitedly, which is very difficult to understand.\n这个也是有点拟声的感觉(echoic), 加吧加吧, 啊吧啊吧, 叽里咕噜\n\"急促而含混不清地说\"\n\nWhen someone starts to jabber, they start talking on and on about this or that, or that or this, in an excited, sometimes incoherent way. Jabber is a close cousin to **blabber**.\n\nWhen someone jabbers, sometimes their words seem to fly out of their mouths like quick punches (jabs!) from a boxer. Some examples of jabber? You know, like when someone has a hobby that you don’t really care about or understand but they won’t quit talking about it? Yeah, they jabber. Or your best friend is relating, without end, his excitement about the coming tiddlywinks championship and you comprehend neither the game nor its importance? That friend jabbers, too.\n\nHe's always blabbering on about computers.\n\n\n#### besiege☢\nsiege =\u003e 围攻, 围困, (名词)\nbesiege =\u003e 围攻, 围困 (动词)\n\n#### beleaguer☢\n- besiege, surround, blockade\n- be- =\u003e around\n- leaguer =\u003e to camp \n=\u003e 在...周围安营扎寨, =\u003e 围攻\n=\u003e 引申为 trouble persistently, harass, 困扰, 使烦恼\n\nWe have issues in our community that continue to **beleaguer** and plague us.\n\n#### hedonism☢\nhedon- =\u003e 来自希腊语里面的 hedone, pleasure, \n-ism\n=\u003e 快乐主义 =\u003e 享乐主义\n\n\n#### euphemism☢\neuphemism 始见于 17 世纪，源自希腊语 euphēmismós 'speaking fair, speaking with good words'，其中 eu-相当于 good，phēmē相当于 speech，其成分的含义多少说明了该词的用法。euphemism 就是汉语中的“婉言”、“委婉语”或“委婉说法”。委婉语的作用在于替代直截了当、令人不悦或粗俗无礼的说法。euphemism 的反义词 dysphemism（粗直语）系由前缀 dys-（坏的）和 euphemism 合成，它产生于 19 世纪末期。\n\n例\n- 'Maiden lady' is a euphemism for 'old maid'. “老小姐”（或“未婚妇女”）是“老处女”的委婉语。\n- 'Large' is a word often used as a euphemism for the word 'fat'. \n- The article made so much use of euphemism that often its meaning was unclear. \n- \"Beautician\" is the euphemism for a hairdresser. \n\n\n#### cacophony\ncaco- =\u003e harsh, bad, evil\nphony =\u003e sound \n=\u003e bad sound =\u003e a unpleasant mixture of loud sounds\n=\u003e 嘈杂的声音, 喧嚣, \n\n如果说 symphony 是乐器\"一起(sym-)\"演奏的优美乐曲, 那么 cacophony 就是嘈杂的喧嚣, 两者的意思基本上是相反的.\n\n#### menstruation☢\nmenstru- =\u003e from Latin \"mensruus\", which means \"monthly\", from mensis \"month\"\n-ation\n=\u003e 每月的事 =\u003e 月事, 月经\n\n\n#### clasp☢\n只要用一个\"环\"紧紧箍住某个东西就叫\"clasp\", 所以可以是\"抱住, 抓住, 握住...\"\nA bracelet is held together by a clasp. A girl who gets a nice one from her boyfriend might clasp her arms around him. A clasp is a fastener. To clasp is to hold tightly.\n\nIn all uses of the word, clasp means to hold together tightly. You want your bracelet or belt clasp to be strong so it doesn't come apart. And when you take a child on a walk across a busy intersection, you clasp their hand tightly. The word is not related to the word **clap**, but if you clap your hands together, then keep them there, you turn a clap to a clasp.\n\n#### commiserate☢\ncom + miser + ate =\u003e 一起悲伤 =\u003e 表示同情, 惋惜, to express sympathy to someone about some bad luck.\n\n- I began commiserating with her over the defeat.\n\n#### conjure☢\ncon- =\u003e with, together\njure =\u003e to swear (an oath) \n=\u003e 一起念咒语, =\u003e 用魔法变出, 召唤出, 变戏法\n\n- The magician conjured up a dove from his hat.\n\nconjurer, conjuror =\u003e 魔术师, 变戏法的人\n\n#### conspicuous☢\ncon- =\u003e 这里表示强调\nspicu =\u003e to look at, observe\nous\n=\u003e 很容易就看到的 =\u003e 显眼的, 醒目的\n\n- In China, her blonde hair was conspicuous.\n\n\n#### crass\n来自拉丁语 crassus =\u003e solid, thick, fat, dense =\u003e 引申为stupid and without considering how other people might feel.\n\nA crass comment is very stupid and shows that the speaker doesn't care about other people's feelings. In today's day and age, you don't have to wear black to a funeral, but to show up in clown pants is simply crass.\n\nThe source of this adjective is Latin crassus, \"thick, dense, fat.\" A similar development of meaning can be seen in English dense in the sense of \"stupid, slow to understand,\" from Latin densus, \"thick, dense,\" and in English thick, which can also be used to mean \"stupid.\"\n\n\n#### curtail\ncur- =\u003e to cut, from Latin curtus\n-tail \n=\u003e cut short, cut off the end of, 把尾巴剪短 =\u003e 减少, reduce or limit.\n\n=\u003e 这个单词听起来比较官方, 所以平时不怎么见到, 但是又没那么官方, 所以平时还是可以用\n\nTo curtail something is to **slow it down**, **put restrictions on** it, or **stop it entirely**. If I give up cake, I am curtailing my cake-eating.\n\nCurtail is an **official-sounding word** for stopping or slowing things down. The police try to curtail crime — they want there to be less crime in the world. A company may want to curtail their employees' computer time, so they spend more time working and less time goofing around. Teachers try to curtail whispering and note-passing in class. When something is curtailed, it's either stopped entirely or stopped quite a bit — it's cut short.\n\n#### beseech☢\nbe- + seek =\u003e to entreat, beg urgently\n\nIf you're begging for something but you want to sound **formal and a little old-fashioned**, say \"I beseech you!\" It really captures how urgent and desperate you are, yet perhaps saves a shred of your dignity.\n\n所以beseech是有点点礼貌的\"恳请\", 没有那种卑躬屈膝的感觉.\n\n##### entreat\n这两个词的组成方式其实挺像的:\nen- + treat =\u003e to treat sb in a certain way,  to treat =\u003e 招待对方, 给对方好处, 为了让对方同意自己的请求 =\u003e 恳请, 恳求, 请求, to try **very hard** to persuade someone to do something.\n\n- We would spend every meal time **entreating** the child to eat her vegetables.\n\n\n#### deride☢\nde+ride? 不骑? 其实在这里 rid 是 laugh 的意思, 和 ridicule 一样.\nde- =\u003e down\nride =\u003e redere, to laugh\n=\u003e 居高临下地嘲笑 =\u003e laugh at in contempt, mock, ridicule, scorn by laughter.\n\n- He derided my singing as pathetic.\n\n#### desiccate☢\nThe verb desiccate means to dry out, dry up and dehydrate. It's helpful to desiccate weeds but certainly not crops.\n\nAs anyone who's been stuck in the desert will tell you, being desiccated by the burning sun isn't much fun. Stemming from the Latin word desiccare, which means to \"dry up,\" desiccate also means to preserve something by drying it out. Without desiccation, raisins or beef jerky would not be possible!\n\n\ndesiccative, desiccant =\u003e 干燥剂, 就是食品包装袋里面装着硅胶的那个小袋袋\n\n##### desiccate? dehydrate?\nDesiccate is narrower in its range of reference and implies a complete deprivation of moisture, especially of vital juices, and often therefore, in its common extended use, a withering or shriveling. It is applicable to animal and vegetable products preserved by thorough drying or it may be applied to persons or to their attitudes, activities, or expression which have lost all their spiritual or emotional freshness or vitality. Desiccate 通常指代富有活力的，富有生命力的物质被脱水，也可以被用作比喻义，表示某种物质的枯竭：\n- The global economy is desiccating by the day.\n- He was politically and emotionally desiccated by the scandal.\n\nDehydrate implies extraction or elimination of water; it is often preferred to desiccate, of which it is a close synonym, when the reference is to foods. 而 Dehydrate 就是\"脱水\"，用来指代并且只用来指代“脱水，干燥”这个过程。\n\n#### desultory☢\nwithout a clear plan or purpose and showing little effort or interest. 散漫的，漫无目的的，心不在焉的\n- skipping about, jumping, flitting, 来自 desultor, 马戏团里面在几匹飞奔的马上面跳来跳去的人, \n- de- =\u003e down, sul =\u003e to jump, leap.\n\n\n#### diffident\nlacking confident, distrustful (of oneself), wanting confidence in another's power.\ndif- =\u003e away, \nfid =\u003e to trust, 比如 fidelity\n-ent \n=\u003e 对自己不信任的 =\u003e 不自信的, 羞怯的, \n\nYou shouldn't be so diffident about your achievements - you've done really well!\n\n#### effulgent\nef- =\u003e ex-, out\nfulg =\u003e to shine, flash, 散发光辉\n-ent\n=\u003e 光芒四射的, 璀璨的, 绚烂的, 灿烂的, 炫耀的, to shine out, to gleam forth.\n\n- an effulgent canopy of stars\n- her effulgent beauty\n- an effulgent smile\n\n![resplendent☢](#resplendent☢)\n\n\n#### engross\nen- =\u003e 使...\ngross =\u003e 大 = greate\n=\u003e 使...大? =\u003e 其实是用大写字母书写的意思 \n=\u003e 那为什么还有\"使...着迷, 使...全神贯注\"的意思呢?\n=\u003e engross 还有一个意思是\"to write or type out formally (a deed, agreement, or other document) preparatory to execution\", 也许是因为 engross 的都是一些很重要的文件, 需要全神贯注?\n![300](notes/2022/2022.7/assets/young-girl-coloring-engross-drawing-52772758.jpg)\n\nEngross is a verb that means to consume all of your attention or time. Once you engross yourself in the culture of high salaries and unlimited spending accounts, it's hard to go back to cooking at a sandwich shop.\n\n\n#### fawn\nfawn是小鹿的意思\n![300](notes/2022/2022.7/assets/Pasted%20image%2020220725154544.png) ![300](notes/2022/2022.7/assets/fawn-dingel.webp)\n而 fawn, fawn over \"巴结\"这个含义来自另一个词源, 用来形容\"小狗摇尾巴开心的样子\", 引申为\"court flavor, grovel, act slavishly\", 汉语里面也有类似的词: \"摇尾乞怜\" (汉语成语，拼音为 yáo wěi qǐ lián，指像狗那样摇着尾巴乞求主人爱怜。指卑躬屈膝地献媚、讨好，以求得到一点好处。出自唐·韩愈《应科目时与人书》)\n\n#### germinate\n开始形成；萌发\ngerminate 可以看作 germ 的动词, germ 是种子的意思(现在主要指细菌了)\n\n![germane☢](#germane☢)\n\n\n#### ill-bred\n和汉语里面\"没教养\"是一模一样的\n\n\n#### inimical☢\nhostile, enemy-like =\u003e 可以用 enemy 来辅助记忆, 词根是一样的, 就是两个 e 都变成了 i\n\n\n#### insouciant☢\n漫不经心的, 无忧无虑的, 散漫的, nonchalant, casual, carefree\nin- =\u003e not\nsouc =\u003e care, 例如 solicit\n-ant\n=\u003e 不关心的, =\u003e 漫不经心的\n\nOnly people with no real troubles can afford to be insouciant during times like these. Runway models are great at looking insouciant, strolling the catwalk apparently without a care in the world.\n\nSome prefer their musical idols to be insouciant, seeming not to care what their fans think or want. Others like them more eager to please, happy to take requests and engage. The two obvious examples are Louis Armstrong and Miles Davis. Armstrong would smile and encourage the audience to participate, while Davis was the insouciant master who showed no concern for or interest in what his listeners might prefer: some people found his insouciant manner irresistible.\n\n#### moratorium☢\nmora- =\u003e pause, delay\ntor(y)\n-ium\n=\u003e 暂停, 间歇\n\nA moratorium is **the suspension of a particular activity**––you could have a moratorium on fishing, baking, the use of candles, the wearing of matching socks.\nGenerally, moratoriums go into effect when something becomes seen as being not okay for now, but might go back to being okay later. After the water fountain started to burble up green sludge, the principal put a moratorium on drinking any water at school until the fountains were fixed and the water tested.\n这是一个比较formal的词, 通常是一个offcial agreement\n- a five-year worldwide **moratorium** on nuclear weapons testing\n\n\n#### mordant☢\nmord- =\u003e mordere, to bite, nip, sting\n=\u003e 尖刻的, 尖锐的, 辛辣的 \n=\u003e 可以看到汉语和英语都喜欢用\"尖\"来形容语言的冷酷与刻薄, 或者鞭辟入里, 一针见血, 透彻, 切中要害, 充满恶意的话就像针一样让人刺痛, 鞭辟入里的语言也直击问题的症结.\n\n- A wicked, mordant sense of humour has come to the fore in Blur's world\n\n\n#### ostracize\n公元前5世纪，古希腊雅典人民推翻僭主的独裁统治后，建立了民主制度。为了保卫民主制度，避免有野心的政治家恢复僭主政治，雅典人民实施一项称为“陶片放逐法”的政治制度。所有的雅典公民在一片陶片上刻上他认为可能危害雅典民主政治的人的名字，将陶片投入票箱。如果有人获得超过6000张选票且得票数第一，则被放逐，期限为10年。\n\n陶片放逐法有助于雅典公民表达自己的政治意见，约束官员行为，但这种方式简单粗暴，在很大程度上取决于公民的情绪。因此陶片放逐法很容易成为政客党派斗争的工具。例如，雅典政治家阿里斯提德曾在参加陶片放逐投票的路上，被一个目不识丁的路人拦下，请他帮忙在陶片上刻下阿里斯提德的名字。阿里斯提德问他为什么想放逐阿里斯提德。那人回答“不为什么，我甚至不认识这个人。只是到处都在谈论他，说他是什么‘正义之士’，我实在是听烦了。”阿里斯提德平静地帮他在陶片上刻下自己的名字。投票结果揭晓，被放逐的正是阿里斯提德。公元前 415 年，雅典平民领袖海柏波拉斯被放逐，后来外国外被支持寡头政治的激进分子暗杀。此事在雅典人民中造成极大震动。从此后，陶片放逐法自然终止。\n\n英语单词 ostracize 正是来自陶片放逐法。它源自希腊语 ostrakizein，而该词源自希腊语 ostrakon （陶片，瓦片，贝壳）。英语中 oyster、ostracean 等单词均与此同源。\n\nostracize： ['ɑstrəsaɪz] vt. 放逐，排斥，排挤\nostracism： ['ɒstrəsɪz(ə)m] n. 排斥，放逐，陶片放逐法\noyster： ['ɒɪstə] n.牡蛎，生蚝，沉默寡言之人\n\n\n#### slouch☢\n现代社会最常见的姿势之一: slouch\n![](notes/2022/2022.7/assets/38755492_l.jpg)\n![](notes/2022/2022.7/assets/index%203.jpg)![](notes/2022/2022.7/assets/index-1.jpg)\n\n#### snob\n旧时在牛津、剑桥等英国大学的学生名册中，出身高贵的学生姓名后面标有 NOB 的字样，NOB 是 nobility（高贵的出身，贵族身份）一词的简略形式；出身平民的普通学生姓名则被标以 s. nob.，这是拉丁语 sine nobilitate 'without nobility'（无高贵出身）的缩略。据认为，英语 snob 一词可能即由此而来，因此最初含有低贱者或平民之意。\n长期以来剑桥师生曾一直用该词来指“市民”（townsman），以别于“穿长袍的大学师生”（gownsman）。snob 的今义据说是英国小说家萨克雷（William Makepeace Thackeray, 1811-1863）最先使用的。1847 年他出版了著名散文集《势利人脸谱》（Book of Snobs），这是由 45 个特写组成的英国社会各阶层势利人的肖像。他把英王乔治四世（George IV, 1762-1830）也称作 snob，因为他以“欧洲第一绅士”自诩，但身上却丝毫没有绅士的特征。经萨克雷这么一用，snob 成了一个常用词，现通常多指“势利的人”、“自以为懂行的人”或“自命不凡的人”。intellectual snob 是“自以为很有学识的人”，academic snob 是“自封为学者的人”，music snob 指“自以为懂音乐的人”，wine snob 指“自命不凡、非上等酒不喝的人”，而 snob appeal/value 则是“对势利顾客的吸引力（价值）”。\n\n例 \n- He's too much of a snob to mix with that earthy crowd. (FWF) 他这个人势利透顶，不屑与那些粗人为伍。\n- A Rolls-Royce has snob appeal. (LDC) 劳斯莱斯汽车对势利顾客是有吸引力的。\n- John is a snob who acts as though he is better than we are. (NED) 约翰是个自命不凡的人，好像比我们都高明似的。\n\n形容词: snobbish\n\n#### tortuous\ntort- =\u003e From Latin torquere \"to twist,\" 扭曲, 比如 contort, distort\n-uous\n=\u003e 弯弯曲曲的, 曲折的, 扭曲的 =\u003e 引申到言语, 拐弯抹角的, \n- He took a tortuous route through back streets.\n- ...these long and tortuous negotiations aimed at ending the conflict. \n\nIt is important not to confuse it with **torturous**, which means characterized by great pain. \"The contemporary string quartet was **tortuous** in its tonal shifts, but only **torturous** at the point where the violinist ran her nails up and down a chalkboard.\"\n\n#### twig\n![400](notes/2022/2022.7/assets/labeledTwig-621x1024.jpg)\n\n#### stint☢\n- He has just finished **a stint of compulsory military service**.\n- Perhaps her most productive period was **her five-year stint as a foreign correspondent** in New York.\n\nThe noun stint means **a set amount of time in which you do something** — often work of some sort. \"She served **a stint in the army**, followed by **a stint in an office setting**, before settling on a career as a lounge singer.\" Unlike a project or vocation, a stint can refer to the stretch of time spent doing a particular job. You apply for a job, but you refer to your past stint in the Peace Corps.\n\n- 来自古英语styntan, 使钝化, 使缩短. 不是很理解这两个意思之间的联系, 难道是\"工作了一段时间了被'磨钝了, 磨短了'?\"\n\n - The bride's parents did not **stint on the champagne** - there was plenty for everyone.\n- Don't **stint yourself** - take another slice of cake.\n\nAs a verb, stint means **to be sparing or frugal**, or restrict in a stingy manner (\"to skimp\"). \"The school board chose to make cuts at the administrative level, rather than **stint on** the children's education.\"\n\n\n#### waylay\n躺在路边(准备埋伏某人) =\u003e ambush\n\n- A man on his way to deposit $12,000 in a bank was **waylaid** by two men who snatched his bag. \n- I meant to leave earlier but I was **waylaid** on the way out of a meeting by my manager.\n\nTo waylay, or to be waylaid, is usually not a good thing: Mom would not be proud. Robbers waylay their victims. Outlaws waylaid stagecoaches in the Old West. The verb's origin, from wegelage, means \"lying in wait, with evil or hostile intent.\" You might also use waylay to show someone being interrupted from finishing the task at hand: \"I should’ve been studying, but was waylaid by my friend's invitation to go bungee jumping.\"\n\n\n#### vociferous☢\nvoci =\u003e to speak\nfer =\u003e to carry\n-ous\n=\u003e to shout, yell, cry out\n=\u003e 抱着某个观点大声疾呼的 =\u003e\nIf you describe someone as vociferous, you mean that they **speak with great energy and determination, because they want their views to be heard**. \n- He was a vociferous opponent of Conservatism.\n- His resentment of her behaviour was becoming more vociferous.\n\n\n#### supplant☢\nsup- =\u003e under\nplant =\u003e sole of the foot(脚底)\n=\u003e to trip up, overthrow, defeat, dispossess =\u003e \"从根基掀起, 掀翻\" =\u003e 取代, replace, oust, displace, supersede\n\nPrinted books will soon be supplanted by e-books.\n\n\n#### discern\ndis- =\u003e apart, 分开 \n-cern =\u003e to separate\n=\u003e to separate apart =\u003e 将物体从背景(其他东西)分开 =\u003e 辨别出, 看出\n- I could just discern a figure in the darkness.\n- It is difficult to discern any pattern in these figures.\n\n#### instill☢\nin-  =\u003e in\nstill =\u003e a drop 比如 distill\n=\u003e put in by drops, to drip, trickle =\u003e to introduce liquid/feelings... little by little.\n=\u003e 逐渐灌输\n- It is part of a teacher's job to **instill** confidence in/into his or her students.\n\n- Parents work hard to develop, or instill, **positive beliefs and values** in their children. Interestingly, there's no corresponding word for when parents pass down their bad habits.\n\n- Instill comes from the Latin verb stillare, meaning \"to drip.\" For some people, this word provides an apt metaphor for the way that parents and teachers cultivate understanding in young learners, patiently introducing wisdom \"drop by drop.\" (Of course, for others, instill conjures up the image of a persistently dripping faucet that just won't be quiet.)\n\n\n#### shred\n![](notes/2022/2022.7/assets/Should-I-Shred-My-Mail.jpg)\n\nshred 既可以指这个动作, 也可以指 shredded 后的细长条\n\n#### dogged\n这里的 dog 就是指 dog, 狗狗有时很固执, 坚持不懈(比如忠犬八公每天都去等待主人), dogged 就是有这个特质的人.\n\n- Her ambition and **dogged determination** ensured that she rose to the top of her profession.\n\n#### execrate☢\nexecrate = ex + sacrare + ate \n- ex- =\u003e out\n- ecr =\u003e sacr =\u003e holy\n- -ate\n=\u003e to devote off or away, to curse, 使其不再神圣, 咒骂, 憎恶, 谴责, loathe, detest, abhor, curse, denounce, damn\n\n##### consecrate\ncon- =\u003e with, together\nsecr- =\u003e sacr-, holy\nate\n=\u003e 把...神圣化, make or declare sacred by certain ceremonies and rites. 比如把一个新修建的教堂神圣化\n\n#### indemnity☢\nin- =\u003e not, opposite of, without\ndemn =\u003e damnum, damage\n-ity\n=\u003e security or exemption against damage =\u003e 后来引申到损坏后的补偿金, 赔款, 保险的赔款.\n\nIndemnity is protection against loss or harm — it is **most often used in insurance**.\n\nIf you suffer an injury or there's damage to your house, an indemnity makes up for the loss — if it's part of your insurance. An indemnity may also keep something or someone from being held responsible for harm. Protection indemnity is mainly offered for unlikely events. If you regularly crash hot-air balloons, you won’t get indemnity for the next one you rent. In fact, the balloon rental company will probably demand their own indemnity in case you crash again.\n\n#### coltish\ncolt 指的是小马驹\n![](notes/2022/2022.7/assets/istockphoto-1007782950-612x612.jpg)\ncoltish就是\"像小马驹一样的\",  full of energy but clumsy or awkward, because they lack physical skill or control. \n\nAn energetic, playful person can be described as coltish. A coltish preschooler might skip happily across the room and then slide to a stop in her socks.\n\nThere's something a little young and awkward implied in the word coltish, which arose in the 14th century from the sense of a colt, or young horse, as a lively, frolicking, long-legged creature. Skinny-legged teenagers dashing around a mall are coltish, and a soccer team of five year-olds is made up of happy, coltish players.\n\n\n#### éclat\nWhen you do something with éclat, you do it with great style or an amazing effect. A skilled magician performs all of her tricks with éclat.\n\nEclat is the gusto or flair with which you make an entrance at a party or sing a song at your school talent show. It's also a certain measure of success, especially the type that other people consider you to have — your skill on the soccer field might lend you a certain amount of éclat among your classmates, for example. The word éclat comes from the French éclat, which means \"splinter or fragment,\" but also \"flash of brilliance.\"\n\n\n#### splice☢\n这个单词看起来好像是表示\"分开\"的含义, 但是它的意思是 to join two piece of rope, film, etc. together at their ends to form one long piece. =\u003e 引申为结婚\n\n- Scientists have discovered how to splice pieces of DNA.\n- He taught me to edit and splice film.\n- An old friend of mine, newly spliced, recently invited me to dinner in his new marital home.\n\n\n#### coda\nCoda comes from the Latin word cauda, which means “tail”.\n\nA coda is a passage at the end of a piece of music that brings the music to a close. \n\nIn the song “Hey Jude” by The Beatles, the final “nana na na” part is considered a coda, and it is almost four minutes long.\n[The Beatles - Hey Jude - YouTube](https://youtu.be/A_MjCqQoLLA?t=239)\n\n#### discrete\n不要和 discreet 搞混了\n- discrete: 独立的, 各自的, 独立的\n- discreet: 审慎的, 谨慎的, 小心的\n\ndiscrete =\u003e discretionary\n\n#### badger\n獾\n獾之所以被称为badger，是因为它的前额上有醒目的白色条纹，就像是一个徽章（badge）。\n![](notes/2022/2022.7/assets/_123042157_f086f397-3f9d-4827-a4d6-f41e34de0cae.jpg)\n\nbadger：['bædʒə] n.獾vt. 纠缠不休；吵着要；烦扰\n\n中世纪时流行一种残忍的狗咬獾游戏。人们将抓来的獾放到一个盒子里，然后将一只狗放入盒子里，让狗反复去咬獾，每次咬住獾后，狗主人就将它们分开，如此反复，以在规定时间内咬住獾次数最多为胜。由于在这种游戏中，狗会反复去咬獾，因此badger一词衍生出“烦扰、纠缠不休”之意。\n\n\n\n#### contravene\ncontra- =\u003e against \nvene =\u003e to come\n=\u003e to come against, oppose, come or be in conflict with\n\n- The company knew its actions **contravened** international law.\n- The Board has banned the film on the grounds that it **contravenes** criminal libel laws.\n- He said the article did not **contravene** the industry's code of conduct.\n\n可以看到 contravene 的都是某个规则和法律\n\nTo contravene means to go against or defy. You might contravene your parents' ban on sweets when your friend offers to share her candy because chocolate tastes too good to resist!\n\nIf you contravene something in practice, you act in direct violation of a particular law or rule. Think about the times when someone has told you not to cross a line and you do anyway. You can also contravene in words though, which means you contradict or argue against a statement. Let's say you're debating gun control. If your opponent says that for the safety of all, it should be legal to carry a concealed weapon wherever you go, you might answer that the more concealed weapons there are, the more violence. You are contravening your opponent's argument.\n\n#### diaphanous\n- dia- \n表示“穿过，二者之间”。\n- phan- \n= show, 表示“显示”。源自希腊语 phainein \"to bring to light, cause to appear, show\"\n- -ous \n表形容词，表示“…的”\n\n![](notes/2022/2022.7/assets/erwin-blumenfeld--diaphanous-coronet-magazine.jpg)\n\nIf a dress is so see-through that light shines through it, it's diaphanous. You could also call it \"sheer\" or \"transparent,\" but diaphanous sounds much fancier.\n\nIf you want a classic example of diaphanous clothing, check out all those nineteenth century Romantic paintings of goddesses clad in lightweight gowns flouncing around in the middle of forests at night. Those gowns are diaphanous, and so are the fluttery translucent muslin curtains in your kitchen window and the gauzy tutu your little sister loves to wear. The Greek root, diaphanes, \"see-through,\" combines dia-, \"through,\" and phainesthai, \"to show.\"\n\n![](notes/2022/2022.7/assets/2000.webp)\n你知道吗, 星云有发光的(Emission Nebula)和不发光的(Reflection nebula)两种, 那些不发光的星云折射了其他星体的光线, 所以才能被我们看到. [Source](https://en.wikipedia.org/wiki/Reflection_nebula)\n\n#### gossamer\nGossamer is **something super fine and delicate** — like a spider web or the material of a wedding veil.\n\nThe original gossamer, from which these meanings come from, is the fine, filmy substance spiders excrete to weave their webs. A dress can be gossamer-like, if its fabric is so sheer as to be see-through, or almost. Your chances of going to a good college are \"gossamer thin\" if you've never cracked a book in high school.\n\nadj: very delicate and light. 精巧轻盈的；轻薄精致的\n- gossamer wings\n- a gossamer veil\n- ...the daring gossamer dresses of sheer black lace.\n\n\n#### dullard\na boring, unintelligent, and unimaginative person\n-ard 是一个贬义后缀, 表示\"不好的人\", 相当于汉语里面的\"...鬼\"\n- coward =\u003e 胆小鬼\n- drunkard =\u003e 酒鬼\n- sluggard =\u003e 懒鬼\n\n#### earnest☢\nIf you are earnest, you pursue your purpose in a **steady, sincere, and eager way**. The phrase \"**in earnest**\" uses earnest as a noun, as in, \"Once you stop fooling around and start studying in earnest, you'll find you learn the material quickly.\" Oscar Wilde's classic play The Importance of Being Earnest plays on the fact that Ernest is also a man's name.\n\n(be) in earnest相当于\"郑重其事地(的), 认真地(的), 当真地(的)\"\n\nHe was a very **earnest** young man.\nThese fanatics are **in deadly earnest** when they say they want to destroy all forms of government.\nThe election campaign has begun **in earnest**.\nI thought he was joking - I didn't realize he was **in earnest**.\n\n\n#### elicit\nelicit是一个主动的过程, 需要你先进行某个动作, 然后对象给出某种response, 如果你什么都没有做, 只是看着某个事件发生, 那么这就不是elicit.\nWhen you elicit, you're bringing out a **response** of some sort. A good comedian elicits a lot of laughs.\n\nElicit has to do with creating or provoking a response. A great speech will elicit cheers — a bad speech will elicit boos. Teachers try to elicit responses from students. If a friend smiles at you, it will probably elicit a smile of your own. In court, a lawyer might try to elicit mistakes and inconsistencies in the testimony of a witness. In all cases, whatever is elicited is some kind of response.\n\n#### estimable☢\nestimate + able? 可估计的?\n其实 estimate 在以前的意思还有\"称赞...的价值\"的意思, 是从 esteem 来的, 所以 estimable 就是 deserving praise, admiration 的意思\nThis is a word for **people** who deserve respect. A hardworking scholar who has written several books might be ***estimable***. **Things** can be ***estimable***, too. You might describe an impressive book, restaurant, or film as ***estimable***. Estimable is related to esteem, which can be used to mean “regard highly.” Being ***estimable*** is the opposite of being disgraceful.\n\n#### facile☢\nfrom French *facile* \"easy,\" (言语或者理论)轻率的, 未经深思熟虑的, 信口开河的, 随意的\n\n- a facile explanation\n- We must avoid facile recriminations about who was to blame.\n\n\n#### fracas☢\n不要混淆了 fracas 和 farce\nfarce 是 \"闹剧\"\nfracas 是 \"a noisy argument or fight\"\n\n- He was injured in a Saturday-night fracas outside a disco. \n- The prime minister has joined the fracas over the proposed changes.\n- No one had prepared anything so the meeting was a bit of a farce.\n\n\n#### hale\n用来形容老年人身体棒, 你这老身板子真好, 读音上和 healthy 前半截挺像的.\n还有一个通常形容老年人的是 sprightly =\u003e lively and active, 精神矍铄的, 注意它虽然有 ly 但是是一个形容词:\n- He's a sprightly old man of 75.\n\n\n#### harrow☢\nTo harrow is to **cause worry and upset**, the way a truly scary movie might harrow you, making it hard to sleep without turning on the light.\n\nHarrow is an uncommon verb that was originally used in a religious context. You're much more likely to hear the adjective harrowing used for things that are extremely distressing. But if your cat torments you nightly with her incessant meowing, you might try yelling, \"Why do you harrow me?\" In agriculture, harrow has a completely different meaning: it's a device that helps up break up the soil. And if you harrow your land, you use such a device.\n\n\n#### incendiary☢\nin- =\u003e 使...\ncend =\u003e =cand, to shine, glow\niary\n=\u003e person who sets malicious fires, 也就是使...燃起来(的人) \n=\u003e 后来引申为能引起燃烧的, e.g. an incendiary bomb, 燃烧弹 \n=\u003e 引申到语言上, 有煽动性的, incendiary remarks, 煽动性的言论\n- 有趣的是汉语\"煽动性的\"也和火有关.\n\n#### irradicable\n= ineradicable =\u003e in + eradicable\n\n\n#### macabre\n在欧洲历史上最猖獗的瘟疫莫过于 14 世纪蔓延于整个欧洲的黑死病（Black Death），死亡人数之多超过历史上任何流行病或战祸。法国编年史家傅华萨（Jean Froissart）说，约有三分之一的欧洲人死于这次流行病。另有资料说，这次大瘟疫夺走了三分之二的欧洲人的性命。除此而外，从 1337 年至 1453 年英法两国之间时断时续的百年战争也在很大程度上增大了人口的死亡率。焚化尸体的火堆几乎随处可见。因此，中世纪末期人们时常想到死亡，死亡也就成了当时西欧戏剧、诗歌、音乐和美术的一个普通主题。最风行的一种题材是在戏剧里和画面上表现生者和死者共舞，象征死亡的骷髅带领众人一步步走向坟墓。这种舞蹈古法语作 danse macabre，英语作 dance macabre，意思是 dance of death（死亡之舞），若照字面义讲则是 dance of the Maccabees（马卡比家族之舞）。它可能起源于 14 世纪德国一出据《圣经旧约·次经》（Old Testament Apocrypha）里有关 Maccabee 家族的故事编写而成的道德剧或奇迹剧。这个家族为了拯救叙利亚犹太人领导了一场长达近 30 年的解放战争，公元前 142 年建立了一个由 Maccabee 家族后裔统治的犹太国家，公元前 63 年为罗马人所灭。Maccabee 家族的故事充满了阴惨的气氛，读起来令人毛骨悚然。多数学者认为，danse macabre 得名于故事中七位殉难的 Maccabee 兄弟的舞蹈，而 macabre 则显然为 Maccabee 之讹误。macabre 直到 19 世纪末期才作为形容词出现于英语之中，并被赋予“以死亡为主题的”、“阴惨的”、“令人毛骨悚然的”等义。\n\n例 \n- Retrieving the mangled bodies from the wreckage was a macabre task. (CAE) 把残缺不全的尸体从残骸中挖出是一项令人毛骨悚然的工作。\n- The macabre story of headless ghosts frightened the children. (FWF) 恐怖的无头鬼故事把孩子们给吓着了。\n- The film contains a macabre account of life in Auschwitz and other concentration camps. (CID) 影片记载了奥斯维辛集中营和其他集中营的恐怖生活。\n\n#### nonplus☢\nTo *nonplus* is to baffle or confuse someone to the point that they have nothing to say. Something weird and mysterious can *nonplus* you, like a play that is performed entirely by chickens.\n\nIf you know a little French or Latin, you'll recognize that \"non plus\" means \"**no more**.\" When something bewildering nonpluses you, **there's no more you can say or do about it**. A goal of getting poor grades, running with a bad crowd, and refusing to eat would leave your parents nonplussed. Sometimes people misuse nonplus to mean \"unimpressed,\" but that's not correct: to nonplus is to puzzle, confuse, and dumbfound.\n\n- he was **nonplussed** by the sudden announcement\n- 注意过去式是两个s: nonplu==ss==ed\n\n#### peripatetic\n- **peri-** \n表示“周围，靠近”。\n- **pat-** \n= walk, 表示“走”。源自希腊语 patein \"to tread, walk.\"\n- **-etic** \n  表形容词，通常放在一个名词前，“与…相关的，…的”。\n  \n- 来自希腊语 peripatein, 走动，讲学，授课，来自 peri-, 在周围，-pat, 走动，词源同 path, foot. 该词原用于形容古希腊哲学家亚里士多德给弟子讲课时，喜欢边走边讲，后引申词义巡回工作的，流动的等。\n\n\ntravelling around to different places, usually because you work in more than one place（通常指因工作而）巡游的，流动的\n- a peripatetic music teacher 流动的音乐教师\n\n\n#### precarious☢\nGrab for the adjective precarious when something is **unstable, dangerous or difficult and likely to get worse**. Are you totally broke and the people you owe money to keep calling? You're in a precarious financial situation!\n\nThe Latin root of precarious means \"**obtained by asking or praying**.\" This fits well as precarious always signals that help is needed desperately. If your life is precarious or you are in a precarious situation, things could become difficult, maybe even dangerous, for you. If your footing or hold on something is precarious, it is unstable or not firmly placed, so that you are likely to slip or lose your grip.\n\n\n#### rift☢\nrift 本来是用来形容坚硬的物体(比如地面或者岩石)上的裂痕的, 但是和中文里面一样, 这个词也能够用来描述人际关系里面的\"裂痕\"=\u003e \n- The marriage caused a **rift** between the brothers and they didn't speak to each other for ten years.\n- The interview reflected a growing **rift** between the President and the government.\n- He has warned that the serious **rifts** within the country could lead to civil war.\n- They hope to heal the **rift** with their father.\n\n\n#### be hoist by/with one's own petard\n这是一个英语俗语, 来自莎士比亚的著名戏剧\u003c哈姆雷特\u003e, hoist虽然现在的意思是\"用绳子吊起来\", 在这里的意思是\"被炸飞\", petard是炸弹的意思, 整个句子字面的含义就是\"被自己的炸弹炸飞\" =\u003e 被自己的阴谋害死, 也就是汉语里面的\"搬起石头砸自己的脚\"\n\n#### rile☢\n来自 roil, 后来读音在美国之间变成了rile, 意思有了细微的区别, rile就是roil对方的情绪, 让对方恼怒, 愤怒.\n\n##### roil☢\nroil 是\"to (cause to) move quickly in a twisting circular movement\"的意思, 比如:\n- Fierce winds roiled the sea. 强风搅动着大海。\n- A massive tower of smoke roiled skyward. 一个巨大的烟柱翻腾上升着。 \n引申为\"to cause something to stop working in the usual or expected way\" 使…停止正常运作\n- Fears about Japan **roiled** world financial markets last week. 上个星期对日本的担忧让世界金融市场停止了正常运作。\n- The immigration debate has **roiled** the country for more than a year. 有关移民的辩论让国家一年多都没能正常运转。\n\n#### squelch☢\nsquelch 形容的是走在下雨天的泥地里面, 脚下发出 biajibiaji 一样的声音的情形, to make a sucking sound like the one produced when you are walking on soft, wet ground（如走在湿软土地上似的）发吧唧声，发扑哧声\n- He got out of the car and **squelched** through the mud to open the gate.\n- As the hikers walked down the path by the house, she could hear the **squelch** of their boots in the mud.\n- He **squelched** across the turf.\n- His sodden trousers were clinging to his shins and his shoes **squelched**.\n也许是因为这个声音和挤压软的东西的时候的声音很相似, 所以 squelch 又有 put a stop to 的意思\nYou can squelch an idea or a rebellion. This word has several meanings, but it's usually a verb for crushing things. A mean remark could squelch your self-confidence, and a powerful military could squelch an invading country.\n\n\n#### disenchant☢\ndis + enchant =\u003e 不再抱有幻想的 =\u003e  幻想破灭了的, 失望的, 感到幻灭的.\n\n- she is disenchanted with the marriage\n\n#### inception\n我记住了盗梦空间, 却还不知道盗梦空间电影英文单词的意思: Inception =\u003e 开端, 成立, 建立, 创立\nThe inception is the beginning. Since its inception, Wikipedia has been created by its users.\n![](notes/2022/2022.7/assets/MV5BMjAxMzY3NjcxNF5BMl5BanBnXkFtZTcwNTI5OTM0Mw@@._V1_FMjpg_UX1000_.jpg)\nInception sounds like conception, but their meanings are distinct. Conception usually refers to the moment of becoming pregnant. Inception refers more to the beginning, to entering upon an undertaking. Inception implies the start of a specific thing like a campaign or a company. Subsequent events take place after the inception. At the moment of conception, most women are at the inception of motherhood.\n\n\n#### wanting\n这是一个形容词, 和 want 的意思有点像, 但是用法有点有趣:\n- He analyzed his game and found it **wanting**\n- Eleanor was scrutinized, too, and often found **wanting**.\n类似于 inadequate, disappointing, 这里的 wanting 的意思是\"不达标的\"\n要是想要指代 wanting 的东西, 可以用 be wanting in ...\n- He is **wanting** in moral constraints.\n- She is a little **wanting** in charms.\n\n#### colossal\n在爱琴海和地中海之间有一个岛名叫罗得岛（Rhodes）。据史料记载，公元前4世纪时，罗得岛曾是地中海东部一个交通枢纽，那里不仅商业繁荣，而且雕刻艺术业也很兴旺发达。公元前292年，罗得岛人击败了马其顿人的入侵，为了庆祝胜利，他们把在战争中缴获的兵器都熔化了，请希腊雕刻家卡瑞斯（Chares）制作一个青铜太阳神巨像。这座巨型塑像历时12年（292-280 BC）才告完工。据古罗马史学家普林尼（Pliny, 23-79）所述，塑像身高约100英尺（一说120英尺），矗立在海港入口处。后人将这一壮观建筑连同埃及金字塔、巴比伦空中花园、摩索拉斯陵墓等誉为世界七大奇观，但在公元前224年这座太阳神像在一场地震中倒塌了。古人按岛名Rhodes称此塑像为Colossus of Rhodes，或简称之为Colossus。Colossus是拉丁语，源于希腊语kolossós，有“巨型塑像”之意。以后Colossus在英语中成了“巨型塑像”的代称，还可喻指“极重要的人或物”，拼写也由大写变为小写。colossal一词便是18世纪时从colossus派生出来的形容词，意为“巨像的”，随后多作“巨大的”、“庞大的”解，只取colossus“大”的涵义。\n![](notes/2022/2022.7/assets/colossus-of-rhodes-5.jpg)\n例 \n- There was a colossal statue of the king in the middle of the square. (LLA) 在广场中央有一座国王的巨像。\n- The Colossus of Rhodes was a very large bronze statue of Apollo at the entrance to Rhodes port in ancient times. (CID) 罗得岛巨像是古时矗立在罗得岛海港入口处的一座阿波罗青铜巨型塑像。\n- It was a colossal waste of time. (CAE) 这在时间上是极大的浪费。\n- Rembrandt was an artistic colossus. (LDC) 伦勃朗是位艺术大师。\n\n#### aboveboard\n光明正大的\n\"in open sight, without trickery or disguise,\" 1610s, from above and board. \"A figurative expression borrowed from gamesters, who, when they put their hands under the table, are changing their cards.\"\n\n#### grandstand☢\n![](notes/2022/2022.7/assets/Hoosier_Lottery_Grandstand_rec.jpg)\n\n#### equivocate☢\nequi- =\u003e equal\nvoc- =\u003e 声音, 叫喊\n-ate\n=\u003e 说模棱两可的话, 含糊其辞 \n- She accused the minister of **equivocating**, claiming that he had deliberately avoided telling the public how bad the problem really was.\n- He is **equivocating** a lot about what is going to happen if and when there are elections. \n- He had asked her once again about her finances. And again she had **equivocated**. \n\nWhen you are unwilling to make a decision and almost intentionally go back and forth between two choices, you are equivocating. When politicians equivocate, they are often afraid of upsetting, and thus alienating, voters with their decisions.\nA key part of equivocate is the root vocate, which comes from the Latin vocare or \"voice.\" When you give your voice to two opposing views in order to mislead or keep your options open, you're equivocating. Think of the expression, to talk out of both sides of your mouth. If you want to go to a party and your parents keep saying \"maybe, it depends,\" tell them to stop equivocating and give you a straight answer.\n\n#### disarray\narray 想必是一个很整齐的情况, 那么 disarray 就是 array 的反义词 =\u003e 混乱, 凌乱, 杂乱\n- Her clothes were in disarray.\n- Ever since the oil crisis, the industry has been in (the state of) disarray.\n\n#### anterior\n前端的, 前面的, 先前的\nante- =\u003e before 比如之前背过的antediluvian\n**后面的**：posterior\n**里面的**：interior\n**外面的**：exterior\n\n更高等的：superior\n更低等的：inferior\n\n\n#### pandemonium \npan・demon・ium\n在英国诗人弥尔顿（John Milton, 1608-1674）最享盛名的史诗《失乐园》（Paradise Lost）中，Pandemonium 是 capital of Hell（地狱之都），即撒旦的宫殿，魔鬼聚集的地方。显然，这是诗人仿 pantheon（万神殿）创造的一个拉丁词，由 pan（一切）和 demonium（魔鬼）组合而成，其中-ium 乃拉丁语后缀，在英语的拉丁语借词中常含有地点之义，诸如 auditorium（礼堂；听众席），sanatorium（疗养院），gymnasium（体育馆）等。但若进而追本溯源，我们会发现 pandemonium 的终极词源乃是希腊语 pân 'all'和 daímōn 'demon'的组合，字面义也是 all demons（一切魔鬼）。18 世纪以后，凡是坏人集中的地方，充满邪恶或混乱嘈杂的地方，都可以用 pandemonium 来表示。到了 19 世纪中期之后，该词又进而引申为“大混乱”或“嘈杂骚乱”，不再指地点，跟魔鬼及地狱也已没有什么联系了。\n\n例 \n- Pandemonium broke out when the results were announced. \n- Pandemonium reigned in the classroom until the teacher arrived. 教师到来之前教室里乱哄哄的。\n- When the verdict was read, pandemonium erupted in the courtroom. 裁决宣读之后，法庭一片混乱。\n- There was sheer pandemonium in the dance hall when someone shouted 'Fire!'.\n\n#### debark\ndebark 和 embark 是一对反义词，显然这里的 bark 是交通工具的意思， 来自法语 barque， “small ship”\ndebark 就是从船上下来，后来泛指从其他交通工具上面下来 \nembark 就是登上船，后来也泛指其他交通工具，也可指代开始一段旅途\n- 由于bark还有树皮的意思， 所以debark还有“去除树皮”的意思\n\n#### convoke\ncon + voke =\u003e to call together, 把大家召集到一起, =\u003e 召集....(开会), 召开会议\n\nHe has convoked a summit conference in Brussels.\n\n\n#### hodgepodge\n大杂烩\n\nA hodgepodge is **a random assortment of things**. A dorm room might be furnished with a hodgepodge of milk crates, antique mirrors, and a poster of a kitten hanging on a branch with one paw.\n\nHodgepodge is a funny-sounding word for a somewhat funny occurrence — a grouping of things or people that don't fit together. If you made a stew with bacon, oatmeal, and chocolate cake, you’ve made a hodgepodge (and a bellyache waiting to happen). The piles of stuff stacked in attics tend to be a hodgepodge. British people call it a hotchpotch. A hodgepodge can also be called a mishmash.\n\nNew Age thinking seems to be a hodgepodge of old and new ideas.\n\n\n#### hike\n除了\"远足\", hike 还可以用来形容费用或者价格的猛增\n- The recent **hike** in train fares came as a shock to commuters.\n- Retailers have **hiked (up)** prices again.\n- ...a sudden 1.75 per cent **hike** in interest rates.\n- His economic plan, with its tax **hikes** and spending cuts, will slow the economy.\n- It has now been forced to **hike** its rates by 5.25 per cent. \n- The federal government **hiked** the tax on hard liquor. \n\n\n#### cherubic\n长相像 cherub 一样的, cherub 是什么呢?\n![](notes/2022/2022.7/assets/index%204.jpg)\ncherub 是小天使, 所以 cherubic 就是娃娃脸的, 长者可爱胖乎乎的圆脸蛋的.\n\n#### commencement\nA commencement is the act of starting out, or blazing a new trail.\n- Would passengers please turn off their mobile phones before the **commencement** of the flight.\n- All should be at least 16 years of age at the **commencement** of this course. \n\nThe suffix -ment makes the word commencement a noun — a thing, an activity, a start. The word can be used for the beginning of anything, from a business meeting to a vacation trip to a marriage. Anything that begins has a moment of commencement. That's why a graduation ceremony is called a commencement — **a graduate is embarking on a new life, and the commencement ritual marks the official beginning of that life**.\n\n#### adjourn☢\nad- =\u003e to\n-journ =\u003e day\n=\u003e to another day. The notion is of setting a date for re-meeting.\n休会, 休庭\n- The meeting was adjourned until Tuesday.\n\n#### dandy\n在形容衣着外表的时候, dandy是一个名词.\nA man who is very concerned with how he looks can be called a dandy. The term is rather old-fashioned — it was commonly used to refer to such men in the 1800s, like the famous dandy Beau Brummell.\n\ndandy还可以做一个形容词, 这时候就不单单用来形容外表了:\nAs an adjective, dandy means **excellent**. If you think your new car is dandy, you're excited to own such a great car. In modern use, dandy is often used sarcastically, with just a small change in wording or emphasis: \"My new car is just dandy. It's broken down twice today already!\" The word dandy is also frequently used in the phrase \"fine and dandy\".\n\n\n#### dearth\ndear + 名词后缀-th =\u003e dear 这这里是\"贵\"的意思, 物以稀为贵, 当一个东西很贵的时候说明它十分稀少, 所以这个单词的意思是 \"不足, 缺乏, 缺少, a lack of sth\"\n- a dearth of new homes in the reigion.\n\n#### debut☢\n来自法语, 所以读音比较奇怪 /ˈdeɪ.bju/\n\nA debut is a **first appearance**, a launch, or public introduction. So before you make your big debut at the office, check and make sure you don't have spinach in your teeth.\n\nPerhaps you’ve heard of debutantes making their official debut into society, or actresses and actors making their debut on stage. A fun fact: **debut** and **premiere** are often thought to be interchangeable, but they’re not. A debut, as you now know, is a first public appearance. But a premiere, while also a “first,” isn't necessarily live. When a movie opens or an interview is broadcast for the first time, they're called premieres.\n\n##### debutante\nA debutante is usually a wealthy girl whose parents wish to introduce her to society in a BIG way — in \"a debutante ball\" that looks like something out of a scene from Gone with the Wind. 翻译: 才开始社交的富家千金? \n\nIn the United States, debutante balls usually, but not always, take place in the South and are a way to introduce wealthy young women to especially eligible young bachelors. The word debutante is derived from the French word debut, meaning “a first performance or showing.” The original French word debutante referred to a new actress making her first appearance on the stage. So, think of a debutante as a young woman making her debut in society.\n\n#### epitomize☢\nepitomize 来自 epitome, epitome 是\"**典范, the best possible example**\"的意思, epitomize 就是\"**to be the perfect example of a quality or type of thing**\"\n\n- With little equipment and unsuitable footwear, she epitomizes the inexperienced and unprepared mountain walker.\n\n\n#### fetter\nfetter 是脚镣的意思, 现在多用于比喻义, 形容困扰某人的枷锁, 也可以作为一个动词, 表示\"束缚, \"\n- ...the fetters of social convention. \n- ...a private trust which would not be fettered by bureaucracy.\n- The black mud fettered her movements. \n\n- fetter 和 feet 长得挺像的.\n\n#### hackneyed\n14 世纪伦敦东北郊有一个名叫 Hackney 的小村庄（今为一自治区）因盛产好马而遐迩闻名。该地所育之马膘肥体壮，抬腿高，善于奔跑，适用于驾车或日常骑用。久而久之，这种马就被取名为 hackney，出租的马也叫 hackney，出租马车则称 hackney carriage/coach 或简称 hackney。到了 1637 年查理一世统治时期，出租马车在伦敦街头几乎随处可见，可说是泛滥成灾。于是 hackney 一词又从“筋疲力尽的出租马匹”（worn-out horse）转指“平庸无奇的事物”。\n到了 18 世纪，从 hackney 派生出了形容词 hackneyed，表示“陈腐的”、“平庸的”、“老生常谈的”等义。与此同时，hackney 常被缩略为 hack，仍指“出租的马”、“出租马车”或引申为“雇佣文人”，如今在美国英语还用以指“出租车”。\n\n例 \n- Politicians all use the same **hackneyed** expressions. (LLA) 政客们说的都是那套陈词滥调。\n- All those slogans we used to chant sound so **hackneyed** now. 我们过去喊的那些口号如今听起来陈腐得很。\n- One needs a special license to drive a **hack**. (FWF) 开出租汽车需持有特殊执照。\n- We rode around the park in a **hack**. 我们坐着出租马车游览了公园。\n\n#### hamstring\nYour hamstrings are **groups of muscles and tendons between your hips and knees**. If you pull a hamstring while running or jumping, you'll feel pain at the back of your thigh. Ouch!\n\n![](notes/2022/2022.7/assets/iStock-900945864_480x480.webp)\n\nThe hamstring muscle group is **one of the most powerful in your body** — attached to the knee and hip, these are the muscles and tendons that make it possible for you to run, walk, and jump. It's fairly common for athletes to injure their hamstrings and be (temporarily) immobilized. Fittingly, hamstring is also **a verb** meaning \"**render powerless**.\" So an astronaut might complain that a lack of funding will hamstring NASA, making planned Mars voyages impossible.\n\n\n- The company **was hamstrung by** traditional but inefficient ways of conducting business. 这家公司被其传统但低效的经营方式所束缚。 \n- If he becomes the major opposition leader, he could **hamstring** a conservative-led coalition.\n\n\n#### hoodwink\nhood 就是一个头套, 或者连帽衫的帽子\nwink 就是眨眼, 和眼睛相关\nhoodwink 就是套上头罩, 被遮蔽了双眼, =\u003e 欺骗\n\n##### blindfold\n一个类似的单词是 blindfold, 就是给...带上眼罩, 但是这个单词没有欺骗的意思.\n\n#### inimitable☢\nin + imitable\n=\u003e 无法模仿的, =\u003e very unusual or of very high quality and therefore impossible to copy. 可以翻译为\"无与伦比的\"\n\n- He was describing, in his own **inimitable** style, how to write a best-selling novel.\n\n- unique, unparalleled, unrivalled, incomparable\n\n#### inexorable☢\n 不可阻拦的\n- the inexorable progress of science 不可阻挡的科学进步\nin- not\nexorable =\u003e able to be entreated, able to be moved by entreaty, 这里的entreat指的是\n\n#### persecute☢\n这个单词和 prosecute 很像, 但是两者的含义有所不同.\n\u003e One you do in court, the other you do if you're a jerk\n\nprosecute 是在法庭上\"起诉\", 不管被 prosecute 的人有没有罪, 一切都是公开透明, 公正的. \n但是 persecute 是迫害, 骚扰, 纠缠, 是不正当的, 是单单因为种族, 宗教或者政治信仰而进行的不正当行为.\n\n**Prosecute** is most often used to refer to bringing legal action against someone else, and is related to the Latin word for \"pursue.\" It is often confused with **persecute** which means \"to harass, torment, or punish, especially for one's beliefs.\" If you find yourself frequently one the wrong side of prosecution, you might end up feeling persecuted. \n\n#### reciprocate\nreci- =\u003e 向后, 往回\nproc- =\u003e 向前, 去\n-ate \n=\u003e to move forwards and backwards, 往返运动(这个意思现在还在使用, 用在机器的零部件上), =\u003e 这和人际关系很像: 我帮你个忙, 你帮我个忙, 我请你吃顿饭, 你再请我吃顿饭, 都是这种往返进行的动作 =\u003e 所以引申为\"回报, 酬答, 回礼, to behave in the same way as someone else\"\n- Their attraction to each other as friends is reciprocated.\n- He reciprocated the party leader's good wishes.\n- Some electric razors have reciprocating heads.\n\n\n#### strut☢\nto walk in a proud way trying to look important.\n- He struts around town like he owns the place.\n\n也可以指房屋/车辆的\"支柱, 撑杆, a strong rod\"\n\n##### strut your stuff\n类似于\"露一手\", 就是自信的展示自己最擅长的/最拿手, 自豪的东西或者技能.\n\nHe was the type of guy who liked to show off and strut his stuff.\n\n\n#### unexceptionable☢\nun + exception + able\n- 无懈可击的, 在这里exception可以理解为缺陷, 可以指责的地方\n\n- 反义词: exceptionable\n\n[unimpeachable](#unimpeachable)\n[incontrovertible](#incontrovertible)\n[impeccable](#impeccable)\n\n#### exponent☢\n除了数学里面的指数, 这个单词还能代表\"拥护者\"\nAn exponent is a person who is a big promoter of something. Are you an exponent of the four-day school and work week?\n\nYou may already know the mathematical meaning of exponent: a numeric notation showing how many times a number is multiplied by itself. How did exponent come to mean a strong advocate or promoter of something? Well, its Latin ancestor was a verb meaning \"to put forth\" and it's easy to see how this could be generalized to refer to people. After all, aren't you an exponent of freedom of expression?\n\n#### blatant\nblatant 一词由 16 世纪英国诗人埃德蒙•斯宾塞 （Edmund Spenser）杜撰。斯宾塞在其长篇史诗《仙后》中创造了这个词，用来形容一头长有千条舌头、代表“诽谤”的怪兽。其词源很有可能是拉丁语 blatire（喋喋不休）。\n- very obvious and intentional, when this is a bad thing 明目张胆的，公然的\n- **a blatant lie** 弥天大谎\n- The whole episode was **a blatant attempt** to gain publicity. 整个事件完全是旨在宣传的露骨炒作。\n\n#### screen\nscreen 除了屏幕的意思还有以下含义:\n- 放映, 你可以 screen a film\n- 屏风, \n- 由屏风这个含义引申为\"遮挡\" She raised her hands to screen her eyes from the bright light.\n- 想象医生通过 X 光片来检查你有没有疾病 =\u003e 检查, 测试\n\tWomen over 50 should be screen for breast cancer.\n- 除了医学检查, screen 也可以指其他检查, 你甚至可以 screen 你的来电决定要不要接电话\n\tCompletely unsuitable candidates were screened out (=tested and refused) in the first interview.\n\n\n#### allay\nal- =\u003e =a-: down, aside\nlay =\u003e to lay\nlay 是放置的意思, 当然也是 lie 的过去式, 可以这样记忆: 一个病人挣扎着要坐起来, 另一个人安抚他重新让他躺下, 或者把一个快要掉下去的东西重新安放好. =\u003e To allay someone's fears or doubts means to make them to feel it less or feel calm again.\n\n- The government is trying to allay public fears/concern about the spread of covid.\n\n\n#### argot\n首先注意这个单词里面的 t 不发音\nargot 类似于汉语里面的\"黑话\", 就是圈内人才懂的话, 一些不正式的, 比 slang 更小众的, 比 jargon 更不专业的业内名词.\n\nStackExchange 上面有 [关于这个的讨论](https://linguistics.stackexchange.com/questions/2812/argot-vs-jargon)\n\n\u003e Based on just the definitions you quote, computer professionals do not speak argot, they speak jargon. **The jargon of computer professionals was *not* constructed for the purpose of hiding the meaning of what they are saying from outsiders - it may have that effect, but that was not the purpose**. The purpose is to have short hand words that have specific defined meanings that allow for more efficient communication. For example the word \"file\" can replace the phrase \"a block of information stored as a unit on an information storage device\". So jargon is a matter of efficiency.\n\u003e \n\u003e From your definitions \"argot\" has the purpose of **secrecy** that would prevent eavesdroppers from understanding the meaning of the conversation.\n\u003e \n\u003e **Slang** is ad-hoc but is inherently formed out of the intent to **broadly communicate**. 也就是说 slang 虽然有 argot 里面\"不正式\"的含义在, 但是不局限于一个小群体, 只要是 native speaker 都听得懂.\n\u003e \n\u003e Jargon is driven from the attempt not so much to broadly communicate, as to **DEEPLY** communicate - **to provide more content in the same amount of verbal space**.\n\n总结一下: \n|  | 正不正式 | 使用人群 |   容不容易懂   |\n| --------- | ---- | ---- | ---- |\n| **argot:**| informal(ad-hoc)|small group of people | hard to understand|\n| **slang**|informal(ad-hoc)|large group of people | easy to understand|\n| **jargon** |formal(有明确的定义)| small group of people |hard to understand|\n| **Plain English** |formal(有明确的定义)| large group of people |easy to understand|\n其实使用人群和容不容易懂是等价的, 所以如果我们加上最后的 English, 也就是一般的话, 就涵盖了所有的 2x2=4 种组合, 真是奇妙.\n\n\n#### barbarous\nbarbarian 指的是野蛮人, 未开化的人, 粗人, 所以 barbarious 就是像 barbarian 一样的人或事, =\u003e 粗野的, 野蛮的, 残忍的, 暴虐的\n\n#### bungle☢\nTwo prisoners **bungled** an escape bid after running either side of a lamp-post while handcuffed.\n\n- mess up, blow, ruin, spoil\n\n\n#### circumscribe\ncircum + scribe =\u003e to encompass, confine, restrain mark out bounds or limits for.\n这个单词在数学里面是\"做外接圆\"的意思, 从词的组成上看是很形象的. 现在一般使用的词义是引申义 =\u003e 也就是限制, 约束, 抑制, 可以理解为划定了活动范围, 活动受限制的意思.\n\nThere are laws circumscribing the right of individual citizens to cause bodily harm to others.\n\n#### congenial \u0026 genial☢\n两者词都表示 kind, friendly and pleasant, 但是它们指代的对象有所不同。\n- genial 通常指代人的性格, 比如\"He is a genial host.\" \n- congenial 通常指一个人的性格给其他人带来的感觉, 或者指一个气氛很和谐友好.\n- genial applies to individuals, while congenial is generally reserved for persons collectively or for environments.\n- 从词根上我们可以这样理解: con- -\u003e with, together, 一群人在一起的时候, 需要大家都很 genial 才能形成一个 congenial 的氛围.\n\n这个回答把两者的区别说明的很全面:　[meaning - Is there any difference between \"congenial\" and \"genial\"? - English Language \u0026 Usage Stack Exchange](https://english.stackexchange.com/a/589162/452046)\n\n#### defraud \u0026 fraud☢\n**Fraud** is a noun for the practice of lying to someone in order to gain something, either money or some other beneficial intangible. It is also the word for a person who commits fraud. Another term for this kind of person is a **fraudster**. This term is mainly used in British English, but can be found in some US publications as well.\n\n**Defraud** is a verb that describes a practice of lying to someone or an institution to steal money specifically. This includes acts like identity theft and electronic hacking.\n\nA person who defrauds someone else is a **defrauder**.\n\n#### deplorable\nde- =\u003e entirely\nplor =\u003e weep, cry out \nable\n=\u003e 可悲的, (糟糕到极点以至于感到悲伤), very bad, 极其恶劣的\n\n- They are forced to live in deplorable conditions.\n\n\n#### embed\n之前这个词都是在编程时候才遇到的. 当然它还有其他含义:\nThe verb embed means to **implant something or someone** — like to embed a stone into a garden pathway or to embed a journalist in a military unit.\nWhen you stick something firmly within a particular environment, you are embedding it. If you are an archeologist, you might spend a lot of your time looking for pottery shards embedded in the earth. If you are a web site designer, you might embed video clips on web pages. And if your newspaper is covering a war overseas, you might consider embedding a journalist in a military troop in order to have a source reporting back from the front lines.\n\n#### be of French/German/Chinese/etc. extraction\n祖籍是....\n\n\n#### voracious \u0026 ferocious  ☢\n下面是一个英语母语者的理解:\n这两个单词更侧重于\"贪婪\"\n- **Voracious** - closest synonym is insatiable. This word is almost always used as an adjective in conjunction with the word appetite.\n- **Rapacious** - closest synonym is greedy; a word not used in common parlance. I would expect to see this word used only in literature and journalism.\n下面这两个单词则更侧重于\"凶猛, 残暴\"\n- **Ferocious** - closest synonym is fierce. It is often (though not always) used as an adjective to describe nature (i.e. a ferocious animal or a ferocious storm).\n- **Vicious** - closest synonym is cruel. Usually refers to the actions of a person.\n\n#### imbroglio☢\nim- =\u003e in\n-broglio =\u003e to confuse, 来自boil\nthe state of being in a confusing situation =\u003e 混乱局面, 错综复杂的局面 \n- 想象水沸腾时候的混乱场景\n\nDeng said it might be better to let \"future generations, which may be wiser\" to tackle the sovereignty imbroglio.\n\n#### paroxysm\n这个单词长得很不一样.\nA paroxysm is a convulsion or sudden fit, brought on because you're freaking out or coming down with something.\n\nParoxysm is from the Greek word paroxysmos, which basically means \"to irritate.\" If you're irritated to the point of having a wild fit, like if you see someone trying to steal your car, you might go into a paroxysm of rage. When the clowns performed their act, the audience went into a paroxysm of giggles. A paroxysm can be medical, too, like when an illness suddenly attacks, and you get symptoms like chills and a fever right away.\n\n形容词: paroxysmal\n\n#### municipal\n来自拉丁语 municeps, 市民，自治镇公民，\n- 来自 munus, 共同职责，共同责任，相互给予，礼物, 词源同 common, mutual.\n- -cep, 承担，词源同 capable, accept. \n该词原指古罗马时期享有罗马公民的特权但是按照自己的法律进行管理的特别自治镇。后词义通用化，引申词义市政的，市政当局的。\n\n##### munificent\nmuni- =\u003e 礼物, 这里可以理解为\"好处\"\nfic- =\u003e to make\n-ent\n=\u003e present-making, 能make很多好处的,  =\u003e 慷慨的, 大方的, \n\nIf you give your best friend a bracelet for her birthday, then you’re a good friend. If you give her a diamond bracelet, a racehorse, and an oil well, then you’re a munificent friend, meaning you are very lavish when it comes to giving gifts. (And it’s possible you may also be broke.)\n\nIf you’re the generous type, you may already know that the word munificent traces back to the Latin word munificus, meaning “generous or bountiful,\" which in turn originated from the word munus, meaning “gift or service.” Put those two concepts together and you have big-time gift giving on a lavish scale. Use the word munificent to describe instances of over-the-top generosity — think Oprah on a gift-giving binge at Christmastime.\n\n##### commune（公社）\n在欧洲封建时期，国家的土地被分封给各个诸侯。土地成了封建君主和领主的私人财产。后来，随着资本主义工商业的兴起，新兴资产阶级逐渐发展壮大。他们定居在城市里，通过赎买和战争手段从封建领主手中夺取了城市的主权，建立了由全体市民共同治理的自治城市。在法语中，自治城市的这种治理机构被称为 commune，来自拉丁语 communis，由 com（共同）+munia（职责、公职、功能）构成，表明自治城市的主权属于全体市民，治理职责由全体市民承担，而不是某个人独揽大权。\n1871 年，法国巴黎人民起义，推翻了资产阶级临时政府，建立了世界上第一个无产阶级政权的雏形——Paris Commune，中文译为“巴黎公社”。从此以后，commune（公社）成为社会主义国家的一种独特组织形式，新中国成立后也曾经历过建设“人民公社”的阶段。\n源自拉丁语 communis 的英语单词还有 community、common、communism 等。来自拉丁语词根 munia 的英语单词还有 municipal（市政的）、municipality（市政当局）。\n- commune：['kɒmjuːn] n. 公社，自治城市\n- communal：['kɒmjʊn(ə)l] adj. 公共的，公有的，公社的\n- community：[kə'mjuːnɪtɪ] n. 社区，社会，共同体，团体\n- common：['kɒmən] n. 普通，平民 adj. 共同的，普通的\n- communism：['kɒmjʊnɪz(ə)m] n. 共产主义\n- communist：['kɒmjʊnɪst] n. 共产主义者 adj. 共产主义的\n\n- municipal：[mjʊ'nɪsɪp(ə)l] adj. 市政的，市的，内政的\n- municipality：[mjʊ,nɪsɪ'pælɪtɪ] n.市政当局，市民，自治市或区\n\n\n#### pillory☢\npillory 是古代的一种刑具, 长下面这个样子, 下面有个柱子, 所以词形上看起来和 pillar 很像:\n![300](notes/2022/2022.7/assets/TitusOates-pilloried_300dpi.jpg)\n- 如今pillory这个刑罚已经废除了, pillory也引申出了(公开)抨击, 严厉批评, 辱骂的意思. (就是像被戴上了pillory了一样, 所以一般指公开的羞辱, 批评)\n\n- A man has been forced to resign as a result of being pilloried by some of the press.\n\n#### probity☢\n- prob-  =\u003e test, 表示“测试，证明”，复合词根，由 pro- 向前 + be- 存在组成。\n- -ity  =\u003e 表名词，指具备某种性质。\n\n来自 probe, 试探，检验，-ity, 名词后缀。即经得起检验的，引申词义正直，诚实。\n\nHer **probity** and integrity are beyond question.\n\n\n#### et cetera\nAlso etcetera, from Latin **et cetera**, literally \"**and the others**,\" from et \"**and**\" + neuter plural of ceterus \"**the other, other part, that which remains**,\" \netc. 就是et cetera的缩写, 所以etc. 的读音是 /et ˈsedərə/\n\n\n#### reprobate\nprobate现在的意思是\"验证遗嘱的有效性\"(verb \u0026 noun), 也就是prove to be worthy, 那么加上前缀re- =\u003e opposite of, reversal of previous conditions, 就是没\"rejected as worthless, rejected by god\"的意思, 再引申到人上面就是\"堕落者, 没有道德底线的人\"\n\n- “He was just an old reprobate who lived poor and died broke,” Grandma said.\n- “Those old reprobates, they live in my building, you know...”\n\n#### bad egg / good egg\n有趣的是英语里面的 bad egg 就是汉语里面的\"坏蛋\", 在英语里面这个说法是 [informal + somewhat old-fashioned](https://www.merriam-webster.com/dictionary/bad%20egg) \n- 那么或许汉语里面的坏蛋就是从英语直译过来的?\n- 另外, 英语里面还有一个说法是 Good egg, 就是 bad egg 的反义词.\n\n [**Origin**](https://www.idioms.online/bad-egg/)  \n- Egg has been used to describe people since the 1600s but this idiom arose during the mid-1800s. It alludes to an egg which seems perfectly fine on the outside but when broken open turns out to be rotten. The antonym, good egg, arose later.\n- The allusion is clearly to the disappointment felt when cracking or shelling an egg, only to find that it is bad.\n**例句**\n- I’m telling you, Robert is just a bad egg. I wouldn’t trust him if I were you.”\n- “It’s disappointing when someone you think is a good friend turns out to be a bad egg.”\n- “I’m not surprised at all that he lied. I always knew he was a bad egg.”\n\n\n#### slack\n松的, 或者作为名词\n- These tent ropes are too slack - they need tightening.\n- The men pulled on the ropes to take up the slack (= to tighten them).\n萧条的，不景气的；冷清的；懈怠的, 或者作为动词\n- Business is always slack at this time of year. \n- Discipline in Mr Brown's class has become very slack recently.\n- Everyone **slacks off/up** at the end of the week.\n\n**slacks**\na pair of trousers, that are not part of a suit（不属于套装一部分的）长裤, 难道是因为长裤都松松垮垮?\n- wool slacks 羊毛长裤\n\n\n##### cut sb some slack\nto not judge someone as severely as you usually would because they are having problems at the present time\n给…方便；对…网开一面, \n- \"Andrew's late again.\" \"**Cut him some slack** - his wife just had a baby.\" “安德鲁又迟到了。”“对他就网开一面吧——他妻子刚生小孩。”\n\n或许可以理解为\"量绳子的长度的时候不要刚刚好, 留一些余量, 这样就 slack 一点\", 绳子比喻标准, 意思是在标准上对某人松一点, 因为它们有特殊情况.\n\n\n#### surly☢\nsurly 是\"sirly\"的变形, sir+ly, like a sir, 词义贬义化, 变成了坏脾气的, 不友好的, 粗鲁无礼的, 有趣的是, 现在这个词的意思和\"sir\"完全不沾边了, 反而更像是粗人, boorish.\n- Surly describes behavior nobody wants to be around. Think of the irritable old guy who lives on your street and always seems to be simmering with some sullen nasty anger, whose every utterance he spits out with a rude snarl. He's the poster boy for surly.\n\n- Surly behavior is always frowned upon, but the word's origins are in the behavior of English nobility. Surly's roots are in sirly, as in sir, meaning **arrogant, haughty, and superior**. Its current meaning implies all that and more, none of it appealing — rude, snotty, sullen, mean and cranky can be added to the list. Generally speaking, if you find yourself in a surly mood, avoid your friends and loved ones.\n\n##### lordly\nused to decribe someone who behaves as if they are better than other people\n傲慢的，高傲的\n- a lordly air 高傲的神情\n\n#### air\nnoun (MANNER)\n- manner or appearance 神态，样子；风度\n- She has an air of confidence about her. 她一副自信的样子。\n\n#### votary☢\nvow + -ary =\u003e one consecrated by a vow, 宣誓了的人, 最初指的是 monk or nun (宣誓忠诚于某个 religion 的人), 后来引申到追随某一个 cause 或 person 的人, 追随者, 仰慕者, 拥护者\n\"How could you, a votary of non-violence, exhort others to take up arms and join this war?\" he wrote to him.\n\n#### anathema\n英语单词 anathema 来自希腊语 anathema，由 ana（up）+ thema （to put, to place）构成，字面意思就是“上交给上帝（由其处置）”。这个希腊语实际上是古代希腊人在翻译希伯来语的《旧约》时对希伯来语 herem（或 cherem）的翻译。herem 在希伯来语中最早指不符合犹太教信仰、上帝憎恶的“当灭之物”，如异教徒用来崇拜其他神灵的建筑、器具，以及其他与犹太教义不符的财物。犹太人打败异教徒后捕获的这些物品不可拿回家私自享用，而应该献给上帝，由上帝来处置。“献给上帝”的方式一般是烧掉或砸碎。与 herem 对应的希腊语 anathema 也就含有“上帝憎恶之物、异教徒所用之物”的含义。\n在宗教领域，anathema 表示一种非常严格的惩罚“革出教门”，仅仅适用于严重违反教规的信徒，相当于宣布该人是“上帝憎恶之物”。在日常生活中，anathema 表示“非常讨厌的人或物”。\n- anathema：[ə'næθəmə] n. 咒逐，革出教门；被诅咒者，非常讨厌的人或物\n\n#### apoplectic☢\napoplexy 是\"中风, 脑卒中\"的意思, 也有\"狂怒(extreme anger)\"的意思, 这两个单词的联系可能是\"老人情绪波动大了(比如非常生气), 就可能中风\"?\n总之 apoplectic 就是\"**extremely and obviously angry**\"的意思.\n\n- He was apoplectic with rage/fury\n\n#### galvanize☢\n这个单词来自意大利科学家**Luigi Aloisio Galvani**, 他是生物电研究领域的先驱, 他在解剖青蛙的时候发现解剖刀接触青蛙腿上的神经肌肉会使青蛙的肌肉抽搐. \ngalvanize 就是\"刺激某人\"的意思(例如使某人突然兴奋起来/打起精神), 很容易想到这是在模拟人触电的样子(simulated by galvanic electricity)\n\n - Western charities were galvanized by TV pictures of starving people. \n- The prospect of his mother coming to stay galvanized him into action and he started cleaning the house.\n\n这个单词也可以指\"电镀\" 例如镀锌, 镀锡\n\n\n#### immure☢\nim- =\u003e in\nmure =\u003e wall, 比如 mural, 壁画\n=\u003e enclose with walls, shut up, confine, \n=\u003e 囚禁, 安葬, =\u003e (比喻义)限制...的发展, 桎梏\n- The aristocracy chose to **immure** its dead in church vaults or specially constructed mausoleums. 贵族选择将逝者安葬在教堂的地窖或专门建造的陵墓中。\n- The false uncle sealed the mouth of the underground chamber and **immured** Aladdin in the darkness.\n- Constantly imitating past masters does not take cooking any further forward - it **immures** it in history.\n\n\nWhen you immure someone or something, you put it behind a wall, as in a jail or some other kind of confining space.\n\nYou may recognize the -**mur**- in immure as the root for \"wall,\" as in mural, which is a painting on a wall, or **intramural**, literally \"inside the walls,\" as, for instance, the walls of a school — intramural sports are played among teams from the same school. You don't need a jail to immure someone. Rapunzel was immured in her tower. At the end of Shakespeare's Romeo and Juliet, the lovers are immured in the tomb.\n\n#### aphorism\naphorism - 世称“医学之父”的古希腊医师希波克拉底（Hippocrates, 460?-377?BC）曾写了一本名为 Aphorismoi（《格言集》）的书。书中收录的头条格言是最有名的一条，译成英语作 Art is long, life is short（艺术长存，生命短暂），这一格言一直流传至今。英语中意为“格言”或“警句”的 aphorism 一词即源出该书名。\n\n例\n- \"A man is known by the company he keeps\" is an aphorism. “观其交友，知其为人”是一句格言。\n- Oscar Wilde was famous for such aphorisms as 'Experience is the name everyone gives to their mistakes'. (CID) 奥斯卡·王尔德因诸如“经验是人们用来文饰自己过失的名词”这样的警句而著称。\n\n#### gaffe☢\n出丑；失礼；失言\nA gaffe is a mistake that embarrasses you in front of others. If you run into a friend out with her grey-haired father, and you blurt out, \"Oh, hi, you must be Tara's grandfather!\" then you've made a gaffe.\n\nGaffe rhymes with laugh, and you'll be lucky if that's how people respond to your social blunder. A gaffe seems to occur most often when you literally don't know your audience — you make a joke about the mayor; you didn't know you were talking to his sister. That's definitely a gaffe. And who knew your hosts come from a culture that takes offense if you refuse to try every dish?\n\n#### faux pas☢\n/ˌfoʊ ˈpɑː/\n失言；失礼, \"出洋相?\"\nIf you misread a party invitation and arrive in a penguin costume, only to realize that the other guests are wearing elegant gowns and tuxedos, you'll understand what it means to commit a faux pas, or an awkward social mistake.\n\nFaux pas literally means \"false step\" in French, and that's a great description of what you do when you make a faux pas. It's a matter of stepping in the wrong direction, or saying exactly the wrong thing. A faux pas can offend people sometimes, but more often it's just embarrassing for everyone involved.\n\n\n#### blackmail\n英语单词 blackmail 由 black（黑色的）+mail（邮件）构成，为什么是“敲诈勒索”而不是“黑色邮件”的意思呢？难道敲诈勒索时要给对方发黑色邮件吗？其实，blackmail 中的 mail 与现代英语中表示“邮件”的 mail 毫无关系，它源自古时苏格兰方言，指的是“地租”或“税金”。16 世纪时苏格兰农民向地主交租时，以白银形式缴纳的地租叫做 white mail，而那些没有白银的穷困农民只能以牲畜或农产品形式缴纳地租，这种地租就叫做 black mail。农民缴纳 black mail 时，地主往往会以压低牲畜或农产品价格为手段，对农民进行百般刁难和敲诈勒索，因此 black mail 逐渐产生了“敲诈勒索”的含义。并且当时苏格兰和英格兰交界处盗匪流行，人们被迫向盗匪缴纳保护费才能求得一时平安。这种保护费也被称作 black mail。由于这两个原因，合写在一起的英语单词 blackmail 就完全用来表示“敲诈勒索”了。\n- blackmail：['blækmeɪl] n.敲诈勒索，敲诈勒索所得vt.敲诈，勒索\n\n- Don't think you can blackmail me (into doing that). I'll report you to the police!\n- Threatening a scandal, he blackmailed the firm into paying him for keeping quiet.\n- She's always using emotional blackmail and playing on other people's feelings. \n- He accused his mother of using emotional blackmail to stop him leaving home.\n\n#### barefaced\nbare + face + ed=\u003e 不遮住自己的脸的 =\u003e not trying to hide your bad behaviour =\u003e 不要脸的, 厚颜无耻的, 厚脸皮的\n\n- That's a barefaced lie!\n\n![effrontery☢](#effrontery☢)\n\n\n#### apocryphal☢\n- apocryphal：[ə'pɒkrɪf(ə)l] adj. 伪的，可疑的, 杜撰的\n《圣经•旧约》是犹太教和基督教的经典著作，是用犹太人所使用的希伯来文字编写的。基督教传到欧洲后，这部经典著作也被翻译成欧洲语言版本，先后出现了希腊文和拉丁文的版本。但是，在这两个版本中，却增加了一些不是从希伯来语翻译过来的内容，而是用希腊文和拉丁文直接编写的内容，如《禧年书》、《所罗门诗篇》、《以诺书》等。后世的人们对这部分内容的作者和权威性存有疑问，因此在后续版本中没有将这些内容收录至《圣经》正册。这些因为作者和权威性存疑而未被收录至《圣经》正册的基督教著作就被称为 apocrypha，由 apo（away）+ kryptein（to hide）构成，字面意思就是“暗藏的、隐晦的、不公开的”，表示这部分内容没有得到确认，不适合公布。\n- apocrypha：[ə'pɒkrɪfə] n. 伪经、旁经，作者（或真实性、权威性）可疑的著作\n\n#### fail-safe\nvery unlikely to fail 万无一失的，万全的\n- a fail-safe plan \n\nIf something is fail-safe, it has been designed so that if one part of it does not work, the whole thing does not become dangerous.\n- a fail-safe device 故障保护装置\n\n\n#### enmity\n- 这个单词和 enemy 很像, 表示\"敌意\"\n- 这个单词也和 amity 很像. 其实 amity 就是 enmity 去除 negative prefix, 表示\"友好, 和睦\"\n\n\n#### methodical\nmethod是方法, + -ical =\u003e 人做事有条理的, 井然有序的\n\n- a methodical approach\n- Tom is a very methodical person and writes lists for everything.\n\n#### solvent☢\nsolvent是溶解的介质 =\u003e 溶剂\n用于商务 =\u003e 有偿还能力的\n\n其中的联系在哪里? \n- 能够偿还债务的 \u003c=\u003e 能够\"溶解\"债务的?(使债务消失)\n- 能够偿还债务的 \u003c=\u003e 能够\"solve\"债务的?(使债务被解决)\n\ndissolve 是溶解的动词形式\n\n#### stentorian\n斯屯托耳（Stentor）是希腊神话传说中一名参加特洛伊战争的希腊军人，天生一副大嗓门，声如洪钟，一个人的声音比得上 50 人，因此在军中担任传令官。据说天后赫拉曾经化身为他的形象，用他的大嗓门来鼓励希腊将士。斯屯托耳后来与神使赫尔墨斯比赛谁的嗓门更大。结果斯屯托耳比输了，力竭而亡。但他凭借自己的大嗓门名垂青史，英语单词 stentorian 就源自他的名字 Stentor。\n- stentorian： [sten'tɔːrɪən] adj.（声音）洪亮的，响亮的\n\n#### zenith\n最高点, 鼎盛时期, 顶峰\n\n##### nadir\n最低谷, 最糟糕的时期\n\n**词源:** \n阿拉伯人在历史上对天文学和数学的发展作出过巨大的贡献。许多借自阿拉伯语的天文学和数学术语至今仍在英语中使用，zenith 即为其中之一，它源自阿拉伯语 samt arrās 'path/way over the head'（头顶上的道路）中的 samt 'path/way'（道路）。那么 samt 是怎么演变成 zenith 的呢？中世纪的西班牙曾是伊斯兰教文化与基督教文化密切接触的地方。当 samt 传入西班牙语时，很可能是文牍误将 samt 中的 m 抄成 ni，致使 samt 变形为 zenit。在早期的手书中是很容易出此类错误的。以后进入法语作 cenit/cenith，14 世纪末进入英语，在乔叟的作品中写成 cenyth，到了莎士比亚时代词形才演变为 zenith，词义也随之发生了变化，由“头顶”到“天顶”，并进而引申为“顶峰”或“顶点”。zenith 的反义词 nadir（天底，最低点，最糟的时刻）也出自阿拉伯语，是从阿拉伯语 nazīr as-samt 'opposite the zenith'中的 nazīr 'opposite'衍变而来的。\n\n例 \n- Her dancing career is now past its zenith. \n- He was then at the zenith of his career. \n- At its zenith the Roman Empire covered almost the whole of Europe. \n- Their relationship has reached its nadir. \n\n\n#### aberrant\nab- =\u003e off, away from\nerr- =\u003e wander, mistake\n-ant\n=\u003e wandering from the usual course, to wander away, go astray.\n\n#### abnegate\nab- =\u003e off, away from\nneg =\u003e to deny\n-ate\n=\u003e deny (something) to oneself \n=\u003e 当 deny 的对象是\"诱惑, 享受\"的时候, 对应中文里面的\"克制\"\n=\u003e 当 deny 的对象是\"responsibility, obligation\"的时候, 大概可以翻译成\"不愿承担（责任, 义务...）\"\n- “It is not the intention of the Myanmar government to apportion blame or to **abnegate** responsibility. We condemn all human rights violations and unlawful violence.”\n=\u003e 当 deny 的对象是\"权力\"的时候, 对应中文里面的\"放弃(权力)\"\n- It’s not a matter of **abnegating** rights, O.K.?\n\n=\u003e 当然 abnegate 的对象还可以是 god, religion, position 等等...\n\n##### abnegate? abdicate?\nWhat's the difference between abnegate and abdicate? \nBoth mean to renounce power or authority, but **abdicate** is usually reserved for **higher offices of power**. The king abdicates the throne. The CEO, who gives up day-to-day responsibility? He abnegates responsibility.\n\n\n\n#### gastronomy\nGastronomy is all about food — the study of food, the history of food, making good food — how we have come to eat what we eat.\n美食学\ngastro- =\u003e 胃\nnomy =\u003e 科学, 某一领域的知识\n=\u003e 关于胃的知识, 关于胃的科学 =\u003e 美食学\n\n\n#### blight\nblight指的是植物的一种疫病\"枯萎病\", 是由微生物引起的, 顾名思义得了这种病的植物会很快地枯萎死去.\n![](notes/2022/2022.7/assets/Pasted%20image%2020220729195253.png)\n1845 到 1849 年发生在爱尔兰的\"大饥荒\"(The Great Famine)就是由土豆枯萎病(potato blight)直接导致的. 这次饥荒导致超过 100 万人死亡, 爱尔兰人口锐减 20~25%, 对爱尔兰的社会和历史造成了深远的影响.\n\n因为这个病杀伤力很强, 所以 blight 又引申为\"something that spoils or has a very bad effect on something, of for a long time\"\n- His arrival **cast a blight on** the wedding day.\n\n或者也可以作为动词, 表示 to spoil something\n- A broken leg blighted her chance if winning the championship.\n\n\n#### flush, blush\n它们都有\"脸红\"的意思, 有什么区别吗?\nIf a face is flushed, it would be from **exertion or anger**. If a face is blushing, it would be from **embarrassment**. Both imply a roseate complexion, so in that sense they are the same, but they imply **different causes** for the redness.\n- 所以 flush 一般指情绪激动或者身体活动带来的\"脸红\"\n- blush 指的是因为尴尬, 羞愧而脸红\n\n#### inadvertent☢\n无意的, 不是故意的\nin + advertent\n是从inadvertence back-formation来的\n那么后面的这个 advertent 是什么意思呢?\n=\u003e 来自 Latin *advertere* \"to direct one's attention to\", 字面上的意思就是\"to turn toward\", 这也是 advertise 这个单词的词源\nad- =\u003e to, toward\nvertere =\u003e  to turn\n- 转过身来面对 =\u003e 注意到某事, 比如说我看到了左边有一棵漂亮的樱花树, 自然想要转过身走近点仔细欣赏.\n\nWhen your actions are inadvertent you're not paying attention to their consequences. Remember that inadvertent ends with -ent by remembering this sentence: “We inadvertently ripped the tent.”\n\n- In this he failed–though he did achieve a certain belated, inadvertent triumph.\n- Even we humans have achieved the dubious technical distinction of being able to make our own disasters, both intentional and inadvertent.\n\n\n#### abstruse☢\nab- =\u003e off, away from\ntruse =\u003e to thrust, push\n=\u003e to thrust away, =\u003e 引申到精神上 \"难懂的, 晦涩的\"\n\n- The passage can be cryptic, **abstruse**, arcane; these are symptoms of the curse of knowledge.\n- Blacks and whites alike scratched their heads at Grabarek’s **abstruse** testimony, but it was clear the mayor was no friend of Fuller.\n\n It is useful when describing something that is overly confusing, or if someone is deliberately making a story or a situation more complicated than necessary. It sounds and looks like obtuse, but abstruse is almost its opposite. Obtuse is dull or lacking a sharpness of intellect. While Abstruse is president of the chess club, Obtuse is hanging out by the parking lot smoking cigarettes.\n \n **synonyms**: deep, recondite esoteric\n\n#### obtrusive☢\n来自obtrude\nob- =\u003e in front of\ntrude =\u003e to thrust, push\n=\u003e to push ... in front of (everyone's attention), thrust forward forcibly or unduly =\u003e 使...过分地显眼, 扎眼, 扰乱, \n- He didn't want to **obtrude on/upon** her privacy.\n- A 40 watt bulb would be quite sufficient and would not obtrude.\nobstrusive 就是 obstrude 的形容词, =\u003e 扎眼的, 过分显眼的.\n\n#### ad-lib☢\n这个单词既可以是动词, 又是名词, 又是形容词\n\n- 作为**动词**, 它的同义词有:\nextemporize, improvise \n\n- 作为**形容词**, 它的同义词有:\nextemporaneous, extemporary, extempore, impromptu, off-the-cuff, offhand, offhanded, unrehearsed, unprepared\n\n#### amicable\n这个单词不能简单地理解为\"friendly\", 它特指在冲突时保持的友善氛围, 可以理解为汉语里面的\"不伤和气的\"\nWhen people have an amicable relationship, they are **pleasant to each other and solve their problems without quarrelling**.\n\nThe adjective amicable means \"friendly\" — but **in particular, use it when describing relations one might otherwise expect to be unfriendly.** The end of a romantic relationship that's less than amicable might involve broken dishes or broken bones.\n\nAmicable, not surprisingly, comes from the Latin word for \"friend,\" amicus. Perhaps the things most commonly described as amicable are **divorces**. The parties in a divorce often tend to be so childish and the proceedings so messy that it's nice to have a word that reflects the absence of those qualities. Other nouns that commonly pair with amicable include **relationship, split, parting, solution,** and **breakup**.\n\n- The meeting ended on reasonably **amicable** terms.\n- Our discussions were **amicable** and productive\n\n##### amicable? amiable?\nAmicable refers to a friendliness or goodwill **between people or groups**. Amiable refers to **one person's friendly disposition.** A group might have an amicable meeting, because the people there are amiable. \n\n\namicable 的反义词可以是 acrimonious, =\u003e full of anger, arguments and bad feelings. 来自单词 acrid.\n\n\n#### atrophy\na- =\u003e not, without\ntrophy =\u003e nourishment\n=\u003e a wasting away through lack of nourishment\n=\u003e 萎缩, 衰退 (verb \u0026 noun)\n\n\n#### awash\n被水淹没的\n=\u003e be awash with 引申为\"充斥, 泛滥\", 汉语里面的泛滥也是用水来比喻某个事物很多, 但是 awash 没有\"泛滥\"的贬义色彩在.\n\n- a company awash with cash\n- The city is awash with drugs.\n\n\n#### barb\n倒刺, 联系 barbed wire\n\n还可以用来代指\"带刺的话\"\n- The **barb** stung her exactly the way he hoped it would.\n\n\n#### belabor☢\nbe + labor =\u003e to exert one's strength upon\n=\u003e 引申为 to explain something more than necessary\n- There is no need to belabor the point.\n\n\n#### be on/riding the crest of a wave\nto be very successful for a limited of time.\n\n- The band are riding the crest of a wave with the success of their new single.\n- Mrs Singh is still riding the crest of a wave of popularity.\n\n在汉语里面可以翻译成\"如日中天\", 两者都是比喻的用法.\n\n\n#### gargantuan\ngargantuan（巨大的）：法国名著《巨人传》的主人公高康大\n英语单词 gargantuan 来自法国讽刺作家拉伯雷（Rabelais）的作品《巨人传》（Gargantua and Pantagruel）中主人公的名字 gargantua（高康大），gargantua 的本意是“大喉咙”。高康大是一个食欲巨大的巨人国王，他出生时要喝 17913 头母牛的奶，衣服要用几万尺布，胖得有十八层下巴，他把巴黎圣母院的大钟摘下来当马铃铛，他的一泡尿淹死了 260416 人。在小说中，拉伯雷痛快淋淳地批判教会的虚伪和残酷，特别痛斥了天主教毒害儿童的经院教育。高康大原本聪慧过人，但几十年的经院教育却要把他变成呆头呆脑，糊里糊涂，只有在接受人文主义教育之后才变成名副其实的“巨人”。\n- gargantuan： [gɑː'gæntjʊən] adj.巨大的，庞大的\n\n\n#### acrimonious☢\nacrimonious 和 acrid 的意思差不多, 只不过 acrid 是形容感官的, acrimonious 是比喻的用法, 用来形容言语的.\n**acrid**\n- Sharp, strong and bitter to the taste, usually unpleasant.\n**acrimonious**\n- bitter and angry\n\n- I inhale deeply, and yes, smell faintly the sweet **acrid** smell of alcohol.\n- A hundred yards away a dump of Wellington boots, gas masks and capes was fired, and **acrid** smoke enveloped the line of men pushing forward to the bridge.\n- “Let me handle him,” urged a hatchet-faced man with sunken **acrimonious** eyes and a thin, malevolent mouth.\n- Relying on bottled oxygen as an aid to ascent is a practice that’s sparked **acrimonious** debate ever since the British first took experimental oxygen rigs to Everest in 1921.\n\n#### abate☢\na- =\u003e to, 这里应该是表示强调\nbate =\u003e to beat\n=\u003e beat down, cast down, strike down =\u003e put an end to\n\n- The storm had abated; I was facedown, almost totally buried in sand.\n- Adrenaline had numbed her body for hours, abating all the usual signs of fatigue, but suddenly everything returned in a rush.\n\n#### aggregate \u0026 aggravate\nTo **aggregate** is to collect many units into one. If you're writing a novel, you might create a character who is an aggregate of five or six real people.\n\nPeople who chew with their mouths open often **aggravate** the people near them, meaning that they exasperate their neighbors.\n\n#### turnip\nTurnips are **a root vegetable commonly associated with potatoes or beets**, but their closest relatives are radishes and arugula\n![](notes/2022/2022.7/assets/index%205.jpg)\n芜菁, 又称为蔓菁、葑、诸葛菜、大头菜、圆菜头、圆根\nWú jīng\n在古代中国三国时期蜀国诸葛亮将其作为军粮，而后第一次世界大战时期的德国，也将芜菁作为主要的应急粮食解决军粮问题。将芜菁的内部挖空，里面塞入棉布卷，也可当成灯来使用。芜菁可食用的部分是球茎，建议以生食为佳，不仅可维持它清脆、鲜甜的口感，还可避免维生素 C 等营养素在烹调过程中流失。 芜菁与萝卜同属十字花科，并且萝卜部分品种跟芜菁的形状很相似，都是圆球状，所以有些人就会将其混淆。但是两种植物还是有区别的。\n\n\n#### abrogate☢\nab- =\u003e off, away from\nrogate =\u003e rog + ate, to propose (a law), ask, request, 其中 rog 的意思是\"move in a straight line\" =\u003e 手伸出去就是 \"to stretch out (the hand)\" =\u003e 引申为 to propose, ask.\n=\u003e 把伸出去的手缩回来 =\u003e 引申为\"annul, repeal\", 多用于 formal 的场合, 比如国家 abrogate 法律, 国际组织 abrogate 条约, 总统 abrogate 法规\n\n\n#### allude / allure\n**allude** =\u003e make a more or less disguised reference to\n- “He alluded to the problem but did not mention it”\n\n**allure** =\u003e dispose or incline or entice to, the power to entice or attract through personal charm\n- She’s actually smiling, coquettishly even; there’s a hint of her former small-screen mannequin’s allure, flickering over her face like momentary static. [The Handmaid's Tale]\n- The young man in the picture exuded virile allure.\n\n#### aloft / aloof\nSomeone who's **aloof** isn't warm and friendly, instead being distant and reserved. That emotionally cold and detached fellow who keeps to himself, drinking espresso and reading French philosophy, would best be described as aloof.\n- As he lost his **aloof**, thorny manner, he was welcomed by the fashionable crowd.\n\nSomething up in the air or really high is **_aloft**._ _Aloft_ has a soft, floaty sound to it, and it's a great word for talking about flying birds, airborne ballet dancers, and soaring spitballs.\n- The nose of the plane rose up and we were **aloft**.\n\n\n#### appropriate\nappropriate 用在钱款上面不一定就是\"挪用, 侵吞\"的意思. 如果政府 appropriate 了某个 fund 用于某个项目, 则 appropriate 的意思是\"拨款, reserve\"的意思\n\n#### observe☢\nobserve 有两个意思我之前没有注意到; \n\n**to make a remark about something**\n说，讲；评论，评述 formal\n- \"I've always found German cars very reliable,\" he observed. “我一直认为德国汽车很可靠，”他说道。\n-  She observed that it would soon be time to stop for lunch. 她说很快就到停下来吃午饭的时间了。\n\n**to obey a law, rule, or custom**\n遵守，奉行（法律、规则或习俗）formal\n- People must observe the law. Nobody should be an exception. 人们必须遵纪守法，谁都不应该例外。\n- The old people in the village still observe the local traditions. 村里的老人仍然遵守当地的传统。\n- Do you observe Passover? 你们过逾越节吗？\n\n#### byzantine\n![](notes/2022/2022.7/assets/web3-christy-pantocrator-dianelos-georgoudis-cc-by-sa-3-0.webp)\n拜占庭的, 因为拜占庭时代的艺术错综复杂, 十分精美, 所以byzantine 又作为一般形容词, 表示very complicated, difficult to understand, even secretive\n\n拜占庭（Byzantine Empire）是一个古希腊城市，也为现今土耳其伊斯坦布尔（君士坦丁堡）的旧名，相传是从墨伽拉来的殖民于公元前 667 年建立的。直至 4 世纪中期，该城发展成东罗马帝国（即拜占庭帝国）的中心，更名为君士坦丁堡，直至 1453 年又更名为科斯坦丁利耶（君士坦丁堡突厥语发音）。 \n\n#### baroque\n巴洛克艺术简称巴洛克（意大利语：Barocco 法, 英语：Baroque）是对欧洲 17 世纪时流行艺术的总称，原本是意大利戏剧中的一种表演手法，戏剧家需要观察剧情是否处于冗长枯燥的氛围之中，如果处于就会故意发出巨大的爆炸声，利用这种突然的刺激来唤醒观众的注意。 \n![罗马市梵蒂冈的圣彼得大教堂，用巴洛克表现庄严奢华感](notes/2022/2022.7/assets/Lazio_Roma_SIgnazio_tango7174.jpg)\n巴洛克艺术以西班牙和法国为中心, 在雕塑、绘画、建筑等视觉化的领域里大为发展，于 17 世纪晚期在全欧洲引发风潮，收获了巨大的反响。尤其是建筑领域的巴洛克艺术，最终反超了原始的戏剧领域，成为现今巴洛克的代表。\n\n巴洛克的作品大量使用金色的特征，在边框上和内容中择一留白，在能放入装饰的地方尽量放入方形、圆形、三角形等几何图形的规则装饰，给人一种充满严肃感和对称感，以凡尔赛宫为其艺术巅峰。巴洛克传承自文艺复兴风格，在 18 世纪开始进入洛可可和新古典主义的领域。 \n\n和 byzantine 一样, baroque 这个词也被用于形容精美繁复的风格. Baroque things are complicated and elaborate. \n- He was a **baroque** figure dressed in theatrical, but elegant, clothes.\n- He is among the most **baroque** of contemporary poets.\n\n#### choleric☢\n`chol-,chole-,cholo-` \n= bile, 表示“胆，胆汁”。古希腊人认为霍乱与黑胆汁过多有关，所以也表示“霍乱”。源自希腊语 khole \"bile.\"\n\n英语单词 choler 来自拉丁语 cholera，本意是胆汁。根据体液学理论，**胆汁多的人脾气暴躁易怒**，因此原本表示胆汁的单词 choler 衍生出“愤怒”之意。另一个表示“胆汁”的单词 bile 在口语中也常常被用来表示“愤怒”。\n\n- choler：['kɒlə] n. 胆汁，愤怒\n- choleric：['kɑlərɪk] adj. 易怒的；暴躁的；胆汁质的\n- cholera 霍乱\n- bile：[baɪl] n. 胆汁，愤怒\n\n#### cleave\ncleave有两个看上去很矛盾的意思:\nCleave, a verb, has two very different meanings. It can describe **cutting or splitting something apart with a sharp instrument**, or — oddly enough — it can describe **sticking to something like glue**.\n\nTo cleave or not to cleave, that is the question. Cleave can refer to being in close contact, to staying really, really close to someone or something: \"If you are walking in the pitch-black woods without a flashlight, you want to cleave to the person in front of you.\" On the other hand, it can mean to split apart with a sharp tool — which is not the action you want to happen while walking in the woods. We've seen that movie.\n\ncleave to ... =\u003e be loyal to ...\n\n#### cosset☢\n来自古英语 cot-sit, 其中 cot 是婴儿床的意思, 坐在婴儿床里面的小孩是被宠爱的, =\u003e 引申为\"to give a lot of attention to making someone comfortable and to protecting them from anything unpleasant\", 可以翻译成\"宠爱\"也可以翻译成\"溺爱\", 褒义贬义皆可\n\n\n#### coward / cowardice / cower\n前两个是 wArd, 而后面一个是 wEr, 虽然三个单词的意思很相近, 但是 cower 的词根是不一样的.\n\n还有一个意思差不多的单词是 craven =\u003e extremely cowardly\n\n\n#### hermetic\n很多炼金术士、神秘学学者和新柏拉图学派学者认为，埃及神话中的智慧之神托特（Thoth，又译透特或图特）等同于希腊神话中的赫尔墨斯（Hermes）。托特是古埃及神话的智慧之神，相传是古埃及文字的发明者。人们将托特尊称为“三倍伟大的赫尔墨斯”(Hermes Trismegistos, 即 Thrice-Great Hermes)，因为他是最伟大的哲学家、最伟大的祭司、最伟大的国王。三倍伟大还有另一种解释：那就是托特（或赫尔墨斯）的学问被分为三大分支：炼金术、占星术和巫术，它们被认为是宇宙智慧的三个方面。托特创立了赫尔墨斯神智学，据说完全密封玻璃瓶的方法就是他发明的。从赫尔墨斯的名字 Hermes 衍生出英语单词 hermetic。\n- hermetic：[hɜː'metɪk] adj. 密封的，不透气的，炼金术的 n. 炼金术士\n- hermetical：[hɝ'mɛtɪkəl] adj. 不透气的，与外界隔绝的\n- hermeticism：[hə:'metisizəm] n. 赫尔墨斯神智学\n\n\n#### embroil☢\nto get deeply involved in a difficult situation\n - She had no desire to **embroil** herself in lengthy lawsuits with the tabloid newspapers. 她不想卷入和那些小报的漫长的官司纠纷中。\n- The United Nations was reluctant to get its forces **embroiled** in civil war.\n\n联系 embroider, 刺绣, 刺绣都是复杂精巧的, embroiled 一个困境就像刺绣错综复杂的线一样, 难以解开\n\nimbroglio 错综复杂的局面, a very confusing and complicated situation, 就相当于 embroil 的名词,\n\n- It brings Wayne's story to a suitably epic conclusion while at the same time offering the dramatic **imbroglios**, action set pieces, twists and surprises the form demands.\n- After a year of false starts and legal **imbroglios**, the contentious nightclub Brooklyn Mirage opened last Saturday as a huge, architecturally ambitious destination for deep house and techno parties.\n\n#### inclement☢\nclement 是\"温和的\"意思, in + clement 就是\"不温和的\", 这两个词都特指天气, inclement 通常用于指 cold, stormy 的天气, \n\n#### industrious / industrial\nindustrious =\u003e 人勤劳的, 勤奋的\nindustrial =\u003e 工业的\n\n#### indulge\n- indulge\n- indulgent\n- indulgence\n\n\n#### institute\n作为动词, institute 并不一定是要建立某个 institution, 只要 start 就可以了, 当然这是一个比较 formal'的 word\n- To institute something means to establish or advance it. You might institute the hiring of Spanish-speakers at your company, or, if workers complain about being overworked, you might institute a new policy on taking breaks.\n- The woman had had a child, and her boyfriend did not want to marry her; she was seeking to **institute** an action against him.\n\ninstitute 还可以作为名词, 大概相当于汉语里面的\"机构, 研究所, 学院\", institution 和 institute 的区别并不大, 网上一些论坛的讨论也没有很明确的结论. 不过似乎 institute 更偏向于教育, 科学机构, 比如 Massachusetts Institute of Technology.\n\n#### mettle☢\n**ability and determination** when competing or doing sth **difficult**, **ability to do sth well** under **difficult** circumstances.\n困境下的坚持, 才干与勇气, 这个词感觉用中文里面的一个词说不全面.\n\n- The team showed/proved its **mettle** in the final round. \n- The real test of her political **mettle** came in the elections.\n\n**on your mettle**\n大概就是展示你的 mettle 的意思, 就是在一个困难的局面下全力以赴的意思.\n- Both players were **on their mettle** in the final round. \n- Cooking for such important people really **puts you on your mettle**.\n\n#### posit / postulate☢\nposit = postulate\n- Starvation was **posited** as the most probable cause of death.\n- He **posited** three basic laws of nature\n- I started to think a lot about infinity, and what it was like there, and how all the **postulates** and theorems and principles were true across all the universe.\n- Others **postulate** that a mini black hole passed through the Earth in Siberia and out the other side.\n\n#### agape / agog\n这两个词只有一丢丢像. \n- agog 是十分激动, 期盼, 兴奋\n- 而 agape 是把嘴巴张开, 虽然 agog 的时候也可能把嘴巴张开, 但是嘴巴张开也可以表示惊讶等等. \n\n##### agape\nagape 在词典里面有两个读音, 这是因为两个读音的意思不一样: \n\n\u003e agape is pronounced either “uh-gaype” or “uh-gah-pay”, depending on which meaning you’re referring to.\n\u003e \n\u003e The first pronounciation (a-gape) refers to a person’s mouth being wide open in surprise, wonder or astonishment. It’s an adjective.\n\u003e \n\u003e The second pronounciation (a-ga-pe) has Greek origins and means the highest form of love - the love of God for man, under unconditional circumstances.\n\n\n#### bait☢\n注意 bait 作为动词并不是\"诱惑\"的意思, 而是\"激怒\"的意思. \n- to intentionally make a person angry by saying or doing things to annoy them.\n\n- He delighted in baiting his mother.\n\n而\"诱惑\"可以对应单词\"entrap\"\n\n\n#### demoralize\n这里的 moral 是 morale 的意思, 所以 demoralize 是 dishearten, discourage 的意思.\n- Losing several games in a row had completely demoralized the team. 连续输掉数场比赛使全队上下彻底泄了气。 \n\n#### masquerade\n可以记忆为 mask(q)uerade  =\u003e 用mask伪装.\n\n-   He listed all the different things his mother would put in it, like cauliflower, peas, onions, and his least favorite, turnips that **masqueraded** as the much more palatable potato until he bit into them.\n-   A friend who was in the class said the teacher was just a wild-haired disorganized person **masquerading** as a wild-haired creative person.\n-   It was human history, **masquerading** as God’s Purpose, revealing herself to an under-age audience.\n-   It **masquerades** as a two-dollar word, but it’s really worth only about twenty cents.\n\n\n#### applicable\n就是 apply 的 adj 形式\n\n\n#### gorge / disgorge\ngorge 本意是\"throat, 喉咙\", 后来引申为\"狼吞虎咽\"的意思\n\n- 峡谷两边很陡峭, 中间有一条河流, 和喉咙有相似之处, 所以 gorge 也引申为\"峡谷\"的意思\n\ndisgorge 就是 gorge 的反义词, \n- 大量排放（液体、气体等物质）\n\t- The pipe was found to be **disgorging** dangerous chemicals into the sea. \n- （同时从某地点或车辆中）涌出（人）\n\t- The delayed commuter train **disgorged** hundreds of angry passengers. \n- （从胃里）吐出\n\t- Flies **disgorge** digestive fluid onto their food to soften it up. \n- 不情愿地提供（信息）；被迫交出（金钱）\n\t- The judge has forced EXIP to **disgorge** $400,000 in illegal profits.\n\n\n#### bombast / bombard☢\nbombast =\u003e pomposity, ranting, bragging, 浮夸的空话\nbombard =\u003e 连续轰炸\n\n- bombast =\u003e bombastic(adj.)\n- bombard =\u003e bombardment(n.)\n\n#### Bravado / Bravura\n\n**Bravado**: A swaggering show of courage\n- His tales of adventure are always told with **bravado**\n\n**Bravura**: Brilliant and showy technical skill\n- In a final **bravura** the ballerina(female ballet dancer) appeared to be floating in water\n\n\n#### ethereal / ephemeral\n##### ethereal\nSomething ethereal is **airy and insubstantial**, like a ghostly figure at the top of the stairs. This word can also describe something delicate and light, like a singer’s ethereal voice.\n\nEthereal comes from the Greek word for ether, which means “air” or more specifically “the upper regions of space.” An ethereal substance or sound is one that carries the feeling of light and air — something you might see in a vision that strikes you as heavenly or supernatural.\n\n\n##### ephemeral\nSomething that is **fleeting or short-lived** is ephemeral, like a fly that lives for one day or text messages flitting from cellphone to cellphone.\n\nEphemeral (ə-FEM-ər-əl) was originally a medical term with the specific meaning \"lasting only one day,\" as a fever or sickness (Hemera means \"day\" in Greek.) The word became more general, coming to mean \"lasting a short time,\" covering the life spans of plants or insects and then eventually anything that is fleeting or transitory. A related word is the plural noun ephemera, meaning \"things that are meant to last for only a short time.\" Posters for a rock concert are often ephemera, unless the band is so famous that they get saved and sold on eBay.\n\nA thing can be ethereal but not ephemeral, like a nebula.\nA thing can also be ephemeral but not ethereal, like the Qin Empire.\n\n#### impetus☢\nAn **impetus** is **the force behind something**, whether it's a boulder rolling down a hill or a person making a decision.\n\nVery little would get done if there were no such thing as an impetus: an impetus is **some kind of force that gets something or somebody moving**. If you push a car that's out of gas, you're the impetus that's getting it moving. An impetus doesn't have to be physical. Advertisers hope their commercials will be an impetus to buy the product.\n\n- The recent publicity surrounding homelessness has given (a) fresh **impetus** to the cause.\n\n##### impetuous\nimpetus 的形容词形式就是 impetuous, \"充满 impetus 的\", 词义贬义化, 就变成了冲动的, 鲁莽的, 草率的, 轻率的 的意思了.\n\nHe was young and **impetuous**.\nHe tended to react in a heated and **impetuous** way.\n\n#### immemorial☢\nancient\n- a modern version of immemorial myth.\n\n**since/from time immemorial**\nfor a very long time =\u003e 相当于\"自古以来\", 是一种固定搭配\n- Her family had farmed that land since time immemorial.\n\n\n#### rave☢\nrave 强调的是说话时十分激动的状态, 说的话可以是好的, 也可以是坏的\n- Grandpa is silent as I rant and **rave**.\n- “Oh, Frightful,” I said, “you are a **raving** beauty.”\n- “This child is going to wind up stark **raving** mad!”\n- But tomorrow, I might be **raving** that my parents are trying to kill me, and I’ll believe it as completely as I believe the earth is round.\n- She **raved** about/over the clothes she had seen at the Paris fashion shows. 她对巴黎时装展上展出的服装赞不绝口。\n- The show has received **rave** reviews/notices in all the papers. 这场演出受到所有报纸的高度评价／吸引了所有报纸的注意并得到热情洋溢的报道。\n\n\n#### vitiate\nAs some sneaky five-year-olds know, crossing one’s fingers while making a promise is an effective way to **vitiate**, or destroy the validity of, an agreement.\n\nVitiate is often used when a legal agreement is made invalid, but it can also refer to the debasement or corruption of something or someone. If a malicious five-year-old on the playground teaches the other children to lie with their fingers crossed, she would be responsible for vitiating the playground community. The first syllable of this word is pronounced \"vish,\" like the first syllable in vicious.\n\n##### Why do we cross our fingers for luck and also cross them when we tell a lie?\n![](notes/2022/2022.7/assets/210px-Hands-Fingers-Crossed.jpg)\n有趣的是, Finger-crossing 既可以用于祈求好运, 也可以在撒谎的时候用来免除自己的\"罪责\". \n\n\u003e It is speculated that Christians started making the cross symbol with their fingers when lying to protect themselves against God’s wrath for breaking one of the commandments. As to when this started, it has been speculated that it dates all the way back to the beginnings of Christianity. . . when Christians had to lie about being Christians since the religion was outlawed, often under penalty of death. However, as with the “luck” finger crossing, direct evidence is hard to come by, so we’re left with educated theories.\n\u003e \n\u003e These days we often use the evocative power of words to accomplish the same end, with phrases like ‘I have my fingers crossed,’ when wishing for good luck, and the kids’ response: ‘You can’t get me: I had my fingers crossed!’ when they’re trying to catch one another in a lie.\n\n#### compress\ncompress既可以指物理上的\"按压, 压缩\", 也可以指抽象意义上的\"压缩 =\u003e 缩短, 精炼\", 计算机里面的文件压缩也叫\"compress\"\n\n- Firmly **compress** the soil in the pot so that the plant is secure. 把盆里的土压实使植株固定。\n- **compressed** air 压缩空气\n- The course **compresses** two year's training into six intensive months. 这项课程把两年的培训内容压缩成 6 个月的强化训练。\n- I managed to **compress** ten pages of notes into four paragraphs. 我设法把 10 页的笔记压缩成了 4 个段落。\n- to **compress** data/files 压缩数据／文件\n\n\n#### courteous☢\n有礼貌的, 不是一个贬义词!\n\n#### clarion\nclarion 几乎总是出现在词组\"clarion call\"里面, 表示\"...的号角, 号召\"的意思. \n- ... to issue/sound a clarion call for change\n\n关于 clarion 的历史网上说法不一, 有的人认为这是一种音调很高的 trumpet, 有的人认为这只是小号的高音域(high register), (没有找到关于 clariond 的文物图片), 但总之 clarion 的声音一定是很响亮的, 很高昂的.\n\n#### loath / loathe,  Is It 'Loath' or 'Loathe'?\n'**Loath**' is an **adjective**; '**loathe**' is a **verb**. \n![](notes/2022/2022.7/assets/loathe-2551-7f04a097ca502926e8fea6d11e80f09b@1x.jpg)\nFor example: \"No wonder my child **loathes** his food; I'm **loath** to try it myself.\"\n\n**Loathe** is a verb (“to dislike greatly”). You loathe that guy at work who steals your food from the refrigerator (you probably loathe many more people than that, but the guy who steals your food is just the most convenient example).\n\n**Loath** is an adjective (“not willing”). You are loath to confront the guy at work who keeps stealing your food from the refrigerator, because he often talks to himself and has a peculiar smell.\n\n#### stouthearted\nstout =\u003e 结实的, 顽强的\nheart\ned\n=\u003e 心脏强大的 =\u003e brave and determined\n\n- Even the most stouthearted of hikers would have had to turn back in this weather.\n\n\n#### adumbrate☢\nad- =\u003e to\numbr =\u003e shadow, to cast in a shadow\nate\n=\u003e to cast a shadow over =\u003e to outline, sketch\n用影子来表示某个更加复杂的东西, 也就是大概描述的意思.\n![](notes/2022/2022.7/assets/ART_Light_and_shadows_ObjectShadowsInSand.jpg)\n\n#### umbrage\n这里的词根和 adumbrade 一样, umbrage 可以理解为被阴影覆盖的状态, 也就是被冷落了, 被忽视了, 没有被尊重, 通常用在固定词组\"take umbrage\"里面.\n\n![400](notes/2022/2022.7/assets/big-businessman-overshadow-to-small-businessman-yellow-background-as-metaphor-73114032.jpg)\n\n\n#### ostensible / ostentatious☢\n**ostensible**\nappearing or claiming to be one thing when it is really something else\n表面上的；假称的，假托的\n\n**ostentatious**\ntoo obviously showing your money, possessions, or power, in an attempt to make other people notice and admire you\n铺张的，摆阔的；炫耀的，卖弄的；招摇的\n\n\n#### puissance / impuissance☢\npuissance 是权力的意思, 而 impuissance 才是软弱无能的意思, im前缀表示缺少. 容易被~~pussy~~这个单词影响而搞混\n\n\n\n#### slipshod\n这是一个很有趣的单词, slip =\u003e slippers 也就是拖鞋, shod =\u003e\"wearing shoes\" , 合起来就是\"wearing slippers or loose shoes\", \"穿拖鞋的\"\n在大街上看到穿拖鞋的, 走路大大咧咧的人我们会觉得他们很散漫, 而古人也这么觉得, 所以 slipshod 也就有了\"工作马虎的, 不严谨的\"意思\n\n- She complained that the carpenter's work had been slipshod.\n\n\n#### wholly or wholely ?☢\nwholly 才是对的!!!!!!!!!\n\n#### loll / lull\nlull =\u003e 安静的间隙, 安抚, 使安静\nloll =\u003e 懒散地躺着或者坐着\n\n#### waddle or wobble\n- **waddle** is a **swaying gait**, or **to walk with short steps, tilting the body from side to side** \n- **wobble** is an **unsteady motion**, or to move with an uneven or **rocking motion**, or unsteadily to and fro. \n\n**waddle**是**摇晃着向前走**, **wobble**就只是**摇晃**而已\n\n\n#### bump\n为了在论坛里面让自己的帖子重新回到开头，就可以发一个bumping，类似于汉语里面的“顶”，“不要沉”\n\n\n#### give you a leg up on...\n助你一臂之力.\n有趣的是汉语里面是手臂, 而英语里面是腿, 不过英语里面的意思是将双手合拢作为其他人踩的地方的意思.\n![](notes/2022/2022.7/assets/A-Leg-Up-black-300x300.png)\n- to hold one's hands together so that someone can step into them while climbing up onto something \n\u003e I don't think I can get on this horse without help. Can someone **_give me a leg up_**?\n\n\n#### to top it all off\nIf you have been describing bad things that happened, and then say that **to top it all off** something else happened, you mean that the final thing was even worse:\n- The washing machine started leaking, my car broke down, then **to top it all off** I locked myself out of the house.\n\n\n\n\n\n\n","lastmodified":"2022-10-15T14:06:29.818505986Z","tags":null},"/notes/2022/2022.9/That-being-so-With-that-said-vs-Having-said-this":{"title":"'That being so' \u0026 'With that said' vs 'Having said this'","content":"# 因而, 尽管如此, 话虽如此\n\n\u003cdiv align=\"right\"\u003e 2022-09-22\u003c/div\u003e\n\nTags: #English \n\n- 'That being so' 类似于汉语里面的'因而', 前后两个句子是衔接关系, 或者因果关系\n- 'With that said' 类似于汉语里面的'话虽如此', 表示句子 A 和句子 B 之间出现了一个 gap, 但是否定意思没有'Having said that'那么的强烈\n- 'Having said that' 类似于'尽管如此, 话虽如此' 表示前后句子出现了一个冲突.\n\nSource: 这个回答说的很好, 有丰富的例句. [phrase usage - \"That being so\" \u0026 \"With that said\" vs \"Having said this\" - English Language Learners Stack Exchange](https://ell.stackexchange.com/a/182094)\n\n\n","lastmodified":"2022-10-15T14:06:29.818505986Z","tags":null},"/notes/2022/2022.9/Vision-Transformer-ViT":{"title":"Vision Transformer (ViT)","content":"# Vision Transformer\n\n\u003cdiv align=\"right\"\u003e 2022-09-22\u003c/div\u003e\n\nTags: #ViT\n\n## How the Vision Transformer works in a nutshell[^1]\n\nThe total architecture is called Vision Transformer (ViT in short). Let’s examine it step by step.\n\n1.  **Split** an image into **patches**\n2.  **Flatten** the patches\n3.  Produce lower-dimensional **linear embeddings** from the flattened patches\n4.  Add **positional embeddings**\n5.  Feed the sequence as an input to a **standard transformer encoder**\n6.  **Pretrain** the model with image labels (fully supervised on a huge dataset)\n7.  **Finetune** on the downstream dataset for image classification\n\n![](notes/2022/2022.9/assets/img_2022-10-15.gif)\n\n\n\n\n\n\n[^1]: [How the Vision Transformer (ViT) works in 10 minutes: an image is worth 16x16 words | AI Summer](https://theaisummer.com/vision-transformer/)","lastmodified":"2022-10-15T14:06:29.818505986Z","tags":null},"/notes/2022/2022.9/indention-in-YAML":{"title":"indention in YAML","content":"# YAML里面的缩进\n\n\u003cdiv align=\"right\"\u003e 2022-09-05\u003c/div\u003e\n\nTags: #YAML #Frontmatter\n\nYAML(YAML Ain't Markup Language)可以在 Markdown 里面用于存储原始数据. 但是 YAML 里面的缩进却难住了我.\n\n一个 YAML 的例子如下:\n```YAML\n doe: \"a deer, a female deer\"\n ray: \"a drop of golden sun\"\n pi: 3.14159\n xmas: true\n french-hens: 3\n calling-birds:\n   - huey\n   - dewey\n   - louie\n   - fred\n xmas-fifth-day:\n   calling-birds: four\n   french-hens: 3\n   golden-rings: 5\n   partridges:\n     count: 1\n     location: \"a pear tree\"\n   turtle-doves: two\n```\n可以看到 YAML 是允许缩进的, 但是容易忽视的一点是: **YAML 里面的缩进只能是空格(Space), 不可以是制表符(Tab)**.\n这是因为 Tab 在不同的文字处理软件里面的处理方式各不相同, 而 YAML 的设计初衷就是为了让它简单通用.\n\nSource: [YAML Tutorial: Everything You Need to Get Started in Minutes | Cloudbees Blog](https://www.cloudbees.com/blog/yaml-tutorial-everything-you-need-get-started)\n","lastmodified":"2022-10-15T14:06:29.842506245Z","tags":null}}