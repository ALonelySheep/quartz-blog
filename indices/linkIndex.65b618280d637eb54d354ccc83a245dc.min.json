{"index":{"links":{"/":[{"source":"/","target":"/notes/2021/2021.12/Johnson-Lindenstrauss-Lemma-Publish-Version","text":"Johnson Lindenstrauss Lemma"},{"source":"/","target":"/notes/2022/2022.5/%E4%BB%8E%E4%BA%8C%E9%A1%B9%E5%88%86%E5%B8%83%E5%88%B0%E6%B3%8A%E6%9D%BE%E5%88%86%E5%B8%83%E5%86%8D%E5%88%B0%E6%8C%87%E6%95%B0%E5%88%86%E5%B8%83-From-Binomial-Distribution-to-Poisson-Distribution-to-Exponential-Distribution","text":"从二项分布到泊松分布再到指数分布-From Binomial Distribution to Poisson Distribution to Exponential Distribution"},{"source":"/","target":"/notes/2021/2021.10/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%96%B9%E5%B7%AE%E7%9A%84%E5%88%86%E6%AF%8D%E5%B8%B8%E5%B8%B8%E6%98%AFn-1","text":"为什么方差的分母常常是n-1"},{"source":"/","target":"/notes/2021/2021.11/%E9%85%89%E7%9F%A9%E9%98%B5%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%AB%E9%85%89%E7%9F%A9%E9%98%B5","text":"酉矩阵为什么叫酉矩阵"},{"source":"/","target":"/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E7%9A%84%E6%B1%82%E5%AF%BC","text":"矩阵的求导"},{"source":"/","target":"/notes/2022/2022.1/D2L-4-%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC","text":"D2L-4-矩阵求导"},{"source":"/","target":"/notes/2021/2021.6/Diffie-Hellman%E9%97%AE%E9%A2%98","text":"Diffie-Hellman问题"},{"source":"/","target":"/notes/2022/2022.1/Dummy_Variables","text":"Dummy_Variables"},{"source":"/","target":"/notes/2022/2022.2/KL_Divergence-KL%E6%95%A3%E5%BA%A6","text":"KL_Divergence-KL散度"},{"source":"/","target":"/notes/2022/2022.10/Gilbert-Strang-%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%9F%A9%E9%98%B5%E7%9F%A5%E8%AF%86","text":"Gilbert Strang 深入浅出机器学习的矩阵知识"},{"source":"/","target":"/notes/2022/2022.2/Logit","text":"What on earth is Logit"},{"source":"/","target":"/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE","text":"为什么Softmax回归不用MSE"},{"source":"/","target":"/notes/2022/2022.4/D2L-64-Kernel-Regression","text":"Kernel Regression"},{"source":"/","target":"/notes/2022/2022.2/Norm-in-Regularization-Intuition","text":"Norm in Regularization - Intuition"},{"source":"/","target":"/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5","text":"Cross_Entropy-交叉熵"},{"source":"/","target":"/notes/2022/2022.2/D2L-32-Convolution-%E5%8D%B7%E7%A7%AF","text":"Convolution-卷积，数学与ML含义"},{"source":"/","target":"/notes/2022/2022.2/D2L-36-1x1%E5%8D%B7%E7%A7%AF%E5%B1%82","text":"1x1卷积层有什么用"},{"source":"/","target":"/notes/2022/2022.4/D2L-66-Kernel-Regression-and-Attention","text":"Kernel Regression and Attention"},{"source":"/","target":"/notes/2022/2022.4/D2L-67-Attention-Scoring-Function","text":"Attention Scoring Function"},{"source":"/","target":"/notes/2022/2022.4/D2L-68-Additive-Attention","text":"图解Additive Attention"},{"source":"/","target":"/notes/2022/2022.4/D2L-69-Scaled-Dot-Product-Attention","text":"Scaled Dot-Product Attention"},{"source":"/","target":"/notes/2022/2022.4/D2L-70-Seq2Seq-with-Attention-Bahdanau-Attention","text":"Seq2Seq with Attention - Bahdanau Attention"},{"source":"/","target":"/notes/2022/2022.4/D2L-71-Multi-Head_Attention","text":"图解Multi-Head_Attention"},{"source":"/","target":"/notes/2022/2022.4/D2L-72-Self-Attention","text":"图解Self-Attention"},{"source":"/","target":"/notes/2022/2022.4/D2L-74-Transformer","text":"智能版加权平均 is All You Need: 图解Transformer"},{"source":"/","target":"/notes/2022/2022.5/%E7%AB%AF%E5%88%B0%E7%AB%AF%E5%AD%A6%E4%B9%A0-End_to_End_Learning-E2E","text":"端到端学习-End_to_End_Learning-E2E"},{"source":"/","target":"/notes/2022/2022.2/%E5%A5%BD%E7%9A%84%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%B9%E8%B4%A8","text":"好的预测模型的特质"},{"source":"/","target":"/notes/2022/2022.1/D2L-1-What_is_a_tensor","text":"What_is_a_tensor"},{"source":"/","target":"/notes/2022/2022.1/D2L-7-%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC","text":"自动求导"},{"source":"/","target":"/notes/2022/2022.1/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%AF%94%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%9B%B4%E9%AB%98%E6%95%88","text":"为什么反向传播比前向传播更高效"},{"source":"/","target":"/notes/2022/2022.2/D2L-17-MLP-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA","text":"MLP-多层感知机"},{"source":"/","target":"/notes/2022/2022.2/D2L-22-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F","text":"权重衰减"},{"source":"/","target":"/notes/2022/2022.2/D2L-24-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7","text":"数值稳定性"},{"source":"/","target":"/notes/2022/2022.2/D2L-25-%E8%AE%A9%E8%AE%AD%E7%BB%83%E6%9B%B4%E5%8A%A0%E7%A8%B3%E5%AE%9A-Xavier%E5%88%9D%E5%A7%8B%E5%8C%96","text":"D2L-25-让训练更加稳定-Xavier初始化"},{"source":"/","target":"/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","text":"经典模型"},{"source":"/","target":"/notes/2022/2022.7/%E6%9C%89%E6%95%88%E5%9C%B0%E8%83%8C%E8%AF%B5GRE%E5%8D%95%E8%AF%8D","text":"有效地背诵GRE单词"},{"source":"/","target":"/notes/2021/2021.8/%E5%A6%82%E4%BD%95%E5%8A%AA%E5%8A%9B%E5%B7%A5%E4%BD%9C_Paul_Graham","text":"如何努力工作_Paul_Graham"}],"/notes/2021/2021.10/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5_Covariance_Matrix":[{"source":"/notes/2021/2021.10/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5_Covariance_Matrix","target":"/notes/2021/2021.10/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%96%B9%E5%B7%AE%E7%9A%84%E5%88%86%E6%AF%8D%E5%B8%B8%E5%B8%B8%E6%98%AFn-1","text":"为什么方差的分母常常是n-1"},{"source":"/notes/2021/2021.10/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5_Covariance_Matrix","target":"/notes/2021/2021.12/Covariance-%E5%8D%8F%E6%96%B9%E5%B7%AE","text":"covariance"}],"/notes/2021/2021.10/Compiler-2_Bottom-Up_Parsing-%E8%87%AA%E5%BA%95%E5%90%91%E4%B8%8A%E5%88%86%E6%9E%90":[{"source":"/notes/2021/2021.10/Compiler-2_Bottom-Up_Parsing-%E8%87%AA%E5%BA%95%E5%90%91%E4%B8%8A%E5%88%86%E6%9E%90","target":"/notes/2021/2021.10/Compiler-3_%E7%AE%97%E7%AC%A6%E4%BC%98%E5%85%88%E5%88%86%E6%9E%90","text":"\"算符优先分析\""}],"/notes/2021/2021.10/Compiler-3_%E7%AE%97%E7%AC%A6%E4%BC%98%E5%85%88%E5%88%86%E6%9E%90":[{"source":"/notes/2021/2021.10/Compiler-3_%E7%AE%97%E7%AC%A6%E4%BC%98%E5%85%88%E5%88%86%E6%9E%90","target":"/","text":"为什么?"}],"/notes/2021/2021.10/Compiler-4_LR%E5%88%86%E6%9E%90":[{"source":"/notes/2021/2021.10/Compiler-4_LR%E5%88%86%E6%9E%90","target":"/notes/2021/2021.10/Compiler-4-1_%E4%BB%80%E4%B9%88%E6%98%AF-LR-%E5%88%86%E6%9E%90","text":"Compiler-4-1_什么是 LR 分析"},{"source":"/notes/2021/2021.10/Compiler-4_LR%E5%88%86%E6%9E%90","target":"/notes/2021/2021.10/Compiler-4-2_LR0_Parse","text":"Compiler-4-2_LR(0)_Parse"},{"source":"/notes/2021/2021.10/Compiler-4_LR%E5%88%86%E6%9E%90","target":"/notes/2021/2021.10/Compiler-4-3_SLR_parse","text":"Compiler-4-3_SLR_parse"},{"source":"/notes/2021/2021.10/Compiler-4_LR%E5%88%86%E6%9E%90","target":"/notes/2021/2021.10/Compiler-4-4_LR1_%E5%88%86%E6%9E%90","text":"Compiler-4-4_LR(1)_分析"},{"source":"/notes/2021/2021.10/Compiler-4_LR%E5%88%86%E6%9E%90","target":"/notes/2021/2021.10/Compiler-4-5_LALR1","text":"Compiler-4-5_LALR(1)"}],"/notes/2021/2021.10/Part.29_Fisher_Linear_DiscriminantPattern_Classification-Chapter_4":[{"source":"/notes/2021/2021.10/Part.29_Fisher_Linear_DiscriminantPattern_Classification-Chapter_4","target":"/notes/2021/2021.10/Dot_Product_and_Linear_Transformation-%E5%90%91%E9%87%8F%E5%86%85%E7%A7%AF%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2","text":"Dot_Product_and_Linear_Transformation-向量内积与线性变换"},{"source":"/notes/2021/2021.10/Part.29_Fisher_Linear_DiscriminantPattern_Classification-Chapter_4","target":"/notes/2021/2021.10/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5_Covariance_Matrix","text":"协方差矩阵_Covariance_Matrix"}],"/notes/2021/2021.11/MIT_18.065-Part_2-Matrix_Factorization":[{"source":"/notes/2021/2021.11/MIT_18.065-Part_2-Matrix_Factorization","target":"/notes/2021/2021.11/LU%E5%88%86%E8%A7%A3%E7%9A%84%E4%BE%8B%E5%AD%90","text":"LU分解的例子"}],"/notes/2021/2021.11/MIT_18.065-Part_4-LU_Factorization":[{"source":"/notes/2021/2021.11/MIT_18.065-Part_4-LU_Factorization","target":"/notes/2021/2021.11/LU%E5%88%86%E8%A7%A3%E7%9A%84%E4%BE%8B%E5%AD%90","text":"LU分解的例子"}],"/notes/2021/2021.11/MIT_18.065-Part_7-Eigenvalues-and-Eigenvectors":[{"source":"/notes/2021/2021.11/MIT_18.065-Part_7-Eigenvalues-and-Eigenvectors","target":"/notes/2021/2021.11/%E7%90%86%E8%A7%A3%E7%9B%B8%E4%BC%BC%E7%9F%A9%E9%98%B5","text":"notes/2021/2021.11/理解相似矩阵"}],"/notes/2021/2021.11/MIT_18.065-Part_9-Singular-Value-Decomposition-SVD":[{"source":"/notes/2021/2021.11/MIT_18.065-Part_9-Singular-Value-Decomposition-SVD","target":"/notes/2021/2021.11/%E5%85%B3%E4%BA%8E%E7%89%B9%E5%BE%81%E5%80%BC%E7%9A%84%E4%B8%80%E4%B8%AA%E7%BB%93%E8%AE%BA","text":"关于特征值的一个结论"},{"source":"/notes/2021/2021.11/MIT_18.065-Part_9-Singular-Value-Decomposition-SVD","target":"/notes/2021/2021.11/%E5%85%B3%E4%BA%8E%E7%A7%A9%E7%9A%84%E4%B8%80%E4%B8%AA%E7%BB%93%E8%AE%BA","text":"关于秩的一个结论"},{"source":"/notes/2021/2021.11/MIT_18.065-Part_9-Singular-Value-Decomposition-SVD","target":"/notes/2021/2021.11/SVD-Intuition","text":"SVD Intuition"},{"source":"/notes/2021/2021.11/MIT_18.065-Part_9-Singular-Value-Decomposition-SVD","target":"/notes/2021/2021.11/A-Fancy-Example-of-SVD","text":"notes/2021/2021.11/A Fancy Example of SVD"},{"source":"/notes/2021/2021.11/MIT_18.065-Part_9-Singular-Value-Decomposition-SVD","target":"/notes/2021/2021.11/SVD-Intuition","text":"SVD Intuition"}],"/notes/2021/2021.11/OS-11_%E4%B8%AD%E6%96%AD":[{"source":"/notes/2021/2021.11/OS-11_%E4%B8%AD%E6%96%AD","target":"/../../../1_Project/2021.6_CPU/CP0_CoProcessor0_in_CPU","text":"CP0_CoProcessor0_in_CPU"}],"/notes/2021/2021.12/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1-Parameter_Estimation":[{"source":"/notes/2021/2021.12/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1-Parameter_Estimation","target":"/notes/2021/2021.12/Maximum_Likelihood_Estimation-%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1","text":"Maximum_Likelihood_Estimation-极大似然估计"}],"/notes/2021/2021.12/%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83-Mutivariate_Gaussian":[{"source":"/notes/2021/2021.12/%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83-Mutivariate_Gaussian","target":"/notes/2021/2021.9/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83_%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_Normal_Distribution-Gaussian_Distribution","text":"正态分布_高斯分布_Normal_Distribution-Gaussian_Distribution"}],"/notes/2021/2021.12/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E7%9A%84%E5%88%A4%E5%88%AB%E5%87%BD%E6%95%B0":[{"source":"/notes/2021/2021.12/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E7%9A%84%E5%88%A4%E5%88%AB%E5%87%BD%E6%95%B0","target":"/notes/2021/2021.12/%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83-Mutivariate_Gaussian","text":"多元高斯分布-Mutivariate_Gaussian"}],"/notes/2021/2021.12/Bayesian-Decision-Theory-Part1":[{"source":"/notes/2021/2021.12/Bayesian-Decision-Theory-Part1","target":"/notes/2021/2021.12/Understanding-Bayes-Theorem","text":"根据Bayes定理"}],"/notes/2021/2021.12/Chi-Squared_Distribution-%E5%8D%A1%E6%96%B9%E5%88%86%E5%B8%83":[{"source":"/notes/2021/2021.12/Chi-Squared_Distribution-%E5%8D%A1%E6%96%B9%E5%88%86%E5%B8%83","target":"/notes/2021/2021.9/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83_%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_Normal_Distribution-Gaussian_Distribution","text":"正态分布_高斯分布_Normal_Distribution-Gaussian_Distribution"}],"/notes/2021/2021.12/Johnson-Lindenstrauss-Lemma-Publish-Version":[{"source":"/notes/2021/2021.12/Johnson-Lindenstrauss-Lemma-Publish-Version","target":"/notes/2021/2021.12/Union_Bound-%E5%B8%83%E5%B0%94%E4%B8%8D%E7%AD%89%E5%BC%8F-Booles_inequality","text":"notes/2021/2021.12/Union_Bound-布尔不等式-Boole's_inequality"},{"source":"/notes/2021/2021.12/Johnson-Lindenstrauss-Lemma-Publish-Version","target":"/notes/2021/2021.9/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83_%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_Normal_Distribution-Gaussian_Distribution","text":"正态分布_高斯分布_Normal_Distribution-Gaussian_Distribution"},{"source":"/notes/2021/2021.12/Johnson-Lindenstrauss-Lemma-Publish-Version","target":"/notes/2021/2021.12/Chi-Squared_Distribution-%E5%8D%A1%E6%96%B9%E5%88%86%E5%B8%83","text":"notes/2021/2021.12/Chi-Squared_Distribution-卡方分布"},{"source":"/notes/2021/2021.12/Johnson-Lindenstrauss-Lemma-Publish-Version","target":"/notes/2021/2021.12/Chernoff-Bounds","text":"Chernoff Bounds"},{"source":"/notes/2021/2021.12/Johnson-Lindenstrauss-Lemma-Publish-Version","target":"/notes/2021/2021.12/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E5%87%BD%E6%95%B0%E7%9A%84%E6%9C%9F%E6%9C%9B","text":"关于随机变量函数的期望"},{"source":"/notes/2021/2021.12/Johnson-Lindenstrauss-Lemma-Publish-Version","target":"/notes/2021/2021.12/Chernoff-Bounds","text":"Chernoff Bounds"}],"/notes/2021/2021.12/Maximum_Likelihood_Estimation-%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1":[{"source":"/notes/2021/2021.12/Maximum_Likelihood_Estimation-%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1","target":"/notes/2022/2022.2/Likelihood_Function-%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0","text":"Likelihood_Function-似然函数"}],"/notes/2021/2021.6/Diffie-Hellman%E9%97%AE%E9%A2%98":[{"source":"/notes/2021/2021.6/Diffie-Hellman%E9%97%AE%E9%A2%98","target":"/notes/2021/2021.6/%E5%9B%BE%E7%81%B5%E5%BD%92%E7%BA%A6-Turing-Reduction","text":"图灵归约 Turing Reduction"}],"/notes/2021/2021.8/%E5%A6%82%E4%BD%95%E5%8A%AA%E5%8A%9B%E5%B7%A5%E4%BD%9C_Paul_Graham":[{"source":"/notes/2021/2021.8/%E5%A6%82%E4%BD%95%E5%8A%AA%E5%8A%9B%E5%B7%A5%E4%BD%9C_Paul_Graham","target":"/notes/2021/2021.8/%E5%A6%82%E4%BD%95%E5%8A%AA%E5%8A%9B%E5%B7%A5%E4%BD%9C_Paul_Graham","text":"如何努力工作_Paul_Graham"}],"/notes/2021/2021.8/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%88%86%E5%B8%83%E4%B8%8E%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E7%9A%84%E8%81%94%E7%B3%BB_Relation_of_Laplace_distribution-_and_Gaussian_distribution":[{"source":"/notes/2021/2021.8/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%88%86%E5%B8%83%E4%B8%8E%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E7%9A%84%E8%81%94%E7%B3%BB_Relation_of_Laplace_distribution-_and_Gaussian_distribution","target":"/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error","text":"Why_do_cost_functions_use_the_square_error"}],"/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E7%9A%84%E6%B1%82%E5%AF%BC":[{"source":"/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E7%9A%84%E6%B1%82%E5%AF%BC","target":"/notes/2021/2021.8/Normal_Equation_Proof_2_Matrix_Method","text":"Normal_Equation_Proof_2_Matrix_Method"}],"/notes/2021/2021.8/How_to_Work_Hard-Paul_Graham":[{"source":"/notes/2021/2021.8/How_to_Work_Hard-Paul_Graham","target":"/notes/2021/2021.8/%E5%A6%82%E4%BD%95%E5%8A%AA%E5%8A%9B%E5%B7%A5%E4%BD%9C_Paul_Graham","text":"notes/2021/2021.8/如何努力工作_Paul_Graham"}],"/notes/2021/2021.8/Linear_RegressionGradient_Descent":[{"source":"/notes/2021/2021.8/Linear_RegressionGradient_Descent","target":"/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error","text":"Regarding the $\\frac1 2$ term"},{"source":"/notes/2021/2021.8/Linear_RegressionGradient_Descent","target":"/notes/2021/2021.8/Different_Gradient_Descent_Methods","text":"Batch Gradient Descent 批梯度下降 BGD"}],"/notes/2021/2021.8/Mean_Squared_Error_%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE":[{"source":"/notes/2021/2021.8/Mean_Squared_Error_%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE","target":"/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error","text":"Why_do_cost_functions_use_the_square_error/为什么损失函数要使用均方误差"},{"source":"/notes/2021/2021.8/Mean_Squared_Error_%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE","target":"/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error","text":"Regarding the frac1 2 term"}],"/notes/2021/2021.8/Normal_Equation_Proof_2_Matrix_Method":[{"source":"/notes/2021/2021.8/Normal_Equation_Proof_2_Matrix_Method","target":"/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E7%9A%84%E6%B1%82%E5%AF%BC","text":"矩阵的求导"},{"source":"/notes/2021/2021.8/Normal_Equation_Proof_2_Matrix_Method","target":"/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E8%BF%B9%E7%9A%84%E6%80%A7%E8%B4%A8","text":"矩阵迹的性质"},{"source":"/notes/2021/2021.8/Normal_Equation_Proof_2_Matrix_Method","target":"/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E8%BF%B9%E7%9A%84%E6%80%A7%E8%B4%A8","text":"这里"}],"/notes/2021/2021.8/Part.13_Cost_Function-Logistic_RegressionML_Andrew.Ng.":[{"source":"/notes/2021/2021.8/Part.13_Cost_Function-Logistic_RegressionML_Andrew.Ng.","target":"/notes/2021/2021.8/Mean_Squared_Error_%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE","text":"均方误差"},{"source":"/notes/2021/2021.8/Part.13_Cost_Function-Logistic_RegressionML_Andrew.Ng.","target":"/notes/2021/2021.8/%E5%87%B8%E4%BC%98%E5%8C%96%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98","text":"凸函数"},{"source":"/notes/2021/2021.8/Part.13_Cost_Function-Logistic_RegressionML_Andrew.Ng.","target":"/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5","text":"Cross_Entropy-交叉熵"},{"source":"/notes/2021/2021.8/Part.13_Cost_Function-Logistic_RegressionML_Andrew.Ng.","target":"/notes/2021/2021.9/%E8%AF%81%E6%98%8ELogistic%E5%9B%9E%E5%BD%92%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%98%AF%E5%87%B8%E5%87%BD%E6%95%B0","text":"证明Logistic回归的损失函数是凸函数"}],"/notes/2021/2021.8/Part.14_Logistic_RegressionGradient_DescentML_Andrew.Ng.":[{"source":"/notes/2021/2021.8/Part.14_Logistic_RegressionGradient_DescentML_Andrew.Ng.","target":"/notes/2021/2021.8/Sigmoid_Function","text":"Logistic的导函数"},{"source":"/notes/2021/2021.8/Part.14_Logistic_RegressionGradient_DescentML_Andrew.Ng.","target":"/notes/2021/2021.8/Relation_Between_Linear_RegressionGradient_Descent_%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%92%8C%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E5%85%B3%E7%B3%BB","text":"Relation_Between_Linear_Regression\u0026Gradient_Descent_梯度下降和线性回归的关系"}],"/notes/2021/2021.8/Part.15_Advanced_OptimizationML_Andrew.Ng.":[{"source":"/notes/2021/2021.8/Part.15_Advanced_OptimizationML_Andrew.Ng.","target":"/notes/2021/2021.8/Different_Gradient_Descent_Methods","text":"Link:其他Gradient_Descent Different_Gradient_Descent_Methods"}],"/notes/2021/2021.8/Part.3_Linear_RegressionML_Andrew.Ng.":[{"source":"/notes/2021/2021.8/Part.3_Linear_RegressionML_Andrew.Ng.","target":"/notes/2021/2021.8/Mean_Squared_Error_%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE","text":"平方误差函数/Squared Error Function/Mean Squared Error"},{"source":"/notes/2021/2021.8/Part.3_Linear_RegressionML_Andrew.Ng.","target":"/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error","text":"Why_do_cost_functions_use_the_square_error"}],"/notes/2021/2021.8/Part.4_Cost_Function_IntuitionML_Andrew.Ng.":[{"source":"/notes/2021/2021.8/Part.4_Cost_Function_IntuitionML_Andrew.Ng.","target":"/notes/2022/2022.2/%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%9B%B0%E9%9A%BE","text":"可视化损失函数的困难"}],"/notes/2021/2021.8/Part.5_Gradient_DescentML_Andrew.Ng.":[{"source":"/notes/2021/2021.8/Part.5_Gradient_DescentML_Andrew.Ng.","target":"/notes/2021/2021.8/Different_Gradient_Descent_Methods","text":"Different_Gradient_Descent_Methods"},{"source":"/notes/2021/2021.8/Part.5_Gradient_DescentML_Andrew.Ng.","target":"/notes/2021/2021.8/Linear_RegressionGradient_Descent","text":"Linear Regression \u0026 Gradient Descent"},{"source":"/notes/2021/2021.8/Part.5_Gradient_DescentML_Andrew.Ng.","target":"/notes/2021/2021.8/Linear_RegressionGradient_Descent","text":"Linear_Regression\u0026Gradient_Descent"},{"source":"/notes/2021/2021.8/Part.5_Gradient_DescentML_Andrew.Ng.","target":"/notes/2021/2021.8/Relation_Between_Linear_RegressionGradient_Descent_%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%92%8C%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E5%85%B3%E7%B3%BB","text":"Relation_Between_Linear_Regression\u0026Gradient_Descent_梯度下降和线性回归的关系"},{"source":"/notes/2021/2021.8/Part.5_Gradient_DescentML_Andrew.Ng.","target":"/notes/2021/2021.8/%E5%87%B8%E4%BC%98%E5%8C%96%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98","text":"notes/2021/2021.8/凸优化与线性回归问题"},{"source":"/notes/2021/2021.8/Part.5_Gradient_DescentML_Andrew.Ng.","target":"/notes/2021/2021.8/%E5%87%B8%E4%BC%98%E5%8C%96%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98","text":"凸优化与线性回归问题"}],"/notes/2021/2021.8/Part.9_Normal_EquationML_Andrew.Ng.":[{"source":"/notes/2021/2021.8/Part.9_Normal_EquationML_Andrew.Ng.","target":"/notes/2021/2021.8/Normal_Equation_Proof_2_Matrix_Method","text":"Normal_Equation_Proof_2_Matrix_Method"},{"source":"/notes/2021/2021.8/Part.9_Normal_EquationML_Andrew.Ng.","target":"/notes/2021/2021.8/Mean_Squared_Error_%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE","text":"最小二乘法"},{"source":"/notes/2021/2021.8/Part.9_Normal_EquationML_Andrew.Ng.","target":"/notes/2021/2021.8/Mean_Squared_Error_%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE","text":"均方误差"}],"/notes/2021/2021.8/Sigmoid_Function":[{"source":"/notes/2021/2021.8/Sigmoid_Function","target":"/notes/2021/2021.8/Sigmoid-Definition","text":"Sigmoid的含义是"}],"/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error":[{"source":"/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error","target":"/notes/2022/2022.2/KL_Divergence-KL%E6%95%A3%E5%BA%A6","text":"KL-散度（相对熵）"},{"source":"/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error","target":"/notes/2021/2021.8/Linear_RegressionGradient_Descent","text":"因为平方项的2被约掉了"},{"source":"/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error","target":"/notes/2021/2021.8/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%88%86%E5%B8%83%E4%B8%8E%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E7%9A%84%E8%81%94%E7%B3%BB_Relation_of_Laplace_distribution-_and_Gaussian_distribution","text":"Related_Post"}],"/notes/2021/2021.9/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83_%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_Normal_Distribution-Gaussian_Distribution":[{"source":"/notes/2021/2021.9/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83_%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_Normal_Distribution-Gaussian_Distribution","target":"/notes/2021/2021.8/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%88%86%E5%B8%83%E4%B8%8E%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E7%9A%84%E8%81%94%E7%B3%BB_Relation_of_Laplace_distribution-_and_Gaussian_distribution","text":"拉普拉斯分布与高斯分布的联系_Relation_of_Laplace_distribution _and_Gaussian_distribution"}],"/notes/2021/2021.9/%E8%AF%81%E6%98%8ELogistic%E5%9B%9E%E5%BD%92%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%98%AF%E5%87%B8%E5%87%BD%E6%95%B0":[{"source":"/notes/2021/2021.9/%E8%AF%81%E6%98%8ELogistic%E5%9B%9E%E5%BD%92%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%98%AF%E5%87%B8%E5%87%BD%E6%95%B0","target":"/notes/2021/2021.8/Sigmoid_Function","text":"Logistic Function"}],"/notes/2021/2021.9/Part.19_Regularized_Linear_RegressionML_Andrew.Ng.":[{"source":"/notes/2021/2021.9/Part.19_Regularized_Linear_RegressionML_Andrew.Ng.","target":"/notes/2021/2021.9/%E6%AD%A3%E5%88%99%E9%A1%B9%E4%B8%8D%E5%BD%B1%E5%93%8D%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%87%B8%E6%80%A7","text":"正则项不影响线性回归损失函数的凸性"},{"source":"/notes/2021/2021.9/Part.19_Regularized_Linear_RegressionML_Andrew.Ng.","target":"/notes/2021/2021.9/%E6%AD%A3%E5%88%99%E9%A1%B9%E4%BC%9A%E6%B6%88%E9%99%A4%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E6%B3%95%E5%8F%AF%E8%83%BD%E7%9A%84%E4%B8%8D%E5%8F%AF%E9%80%86%E6%80%A7","text":"notes/2021/2021.9/正则项会消除正规方程法可能的不可逆性"}],"/notes/2021/2021.9/Part.20_Regularized_Logistic_RegressionML_Andrew.Ng.":[{"source":"/notes/2021/2021.9/Part.20_Regularized_Logistic_RegressionML_Andrew.Ng.","target":"/notes/2021/2021.9/%E6%AD%A3%E5%88%99%E9%A1%B9%E4%B8%8D%E5%BD%B1%E5%93%8DLogistic%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%87%B8%E6%80%A7","text":"正则项不影响Logistic回归损失函数凸性"}],"/notes/2021/2021.9/Part.26_Probabilistic_Interpretation_of_MSEML_Andrew.Ng.":[{"source":"/notes/2021/2021.9/Part.26_Probabilistic_Interpretation_of_MSEML_Andrew.Ng.","target":"/notes/2021/2021.8/Mean_Squared_Error_%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE","text":"Mean_Squared_Error-均方误差"},{"source":"/notes/2021/2021.9/Part.26_Probabilistic_Interpretation_of_MSEML_Andrew.Ng.","target":"/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error","text":"Why_do_cost_functions_use_the_square_error"},{"source":"/notes/2021/2021.9/Part.26_Probabilistic_Interpretation_of_MSEML_Andrew.Ng.","target":"/notes/2021/2021.9/%E7%8B%AC%E7%AB%8B%E5%90%8C%E5%88%86%E5%B8%83-IID","text":"独立同分布-IID"},{"source":"/notes/2021/2021.9/Part.26_Probabilistic_Interpretation_of_MSEML_Andrew.Ng.","target":"/notes/2021/2021.9/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83_%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_Normal_Distribution-Gaussian_Distribution","text":"notes/2021/2021.9/正态分布_高斯分布_Normal_Distribution-Gaussian_Distribution"},{"source":"/notes/2021/2021.9/Part.26_Probabilistic_Interpretation_of_MSEML_Andrew.Ng.","target":"/notes/2021/2021.9/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83_%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_Normal_Distribution-Gaussian_Distribution","text":"正态分布的表达式"}],"/notes/2021/2021.9/i.e._Meaning":[{"source":"/notes/2021/2021.9/i.e._Meaning","target":"/notes/2021/2021.9/OK_should_be_in_capital-lettersor_okay","text":"notes/2021/2021.9/OK_should_be_in_capital letters(or_okay)"}],"/notes/2022/2022.1/%E7%9F%A9%E9%98%B5%E7%9A%84%E4%B8%8D%E5%90%8C%E4%B9%98%E6%B3%95-Hadamard-Kronecker":[{"source":"/notes/2022/2022.1/%E7%9F%A9%E9%98%B5%E7%9A%84%E4%B8%8D%E5%90%8C%E4%B9%98%E6%B3%95-Hadamard-Kronecker","target":"/D2L-1-What_is_a_tensor","text":"Tensor Product"}],"/notes/2022/2022.1/D2L-10-%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D":[{"source":"/notes/2022/2022.1/D2L-10-%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D","target":"/notes/2021/2021.8/Different_Gradient_Descent_Methods","text":"Different_Gradient_Descent_Methods"}],"/notes/2022/2022.1/D2L-11-%E6%B3%9B%E5%8C%96Generalization":[{"source":"/notes/2022/2022.1/D2L-11-%E6%B3%9B%E5%8C%96Generalization","target":"/notes/2021/2021.9/%E6%AD%A3%E5%88%99%E9%A1%B9%E4%B8%8D%E5%BD%B1%E5%93%8D%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%87%B8%E6%80%A7","text":"正则项不影响线性回归损失函数的凸性"}],"/notes/2022/2022.1/D2L-4-%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC":[{"source":"/notes/2022/2022.1/D2L-4-%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC","target":"/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E7%9A%84%E6%B1%82%E5%AF%BC","text":"矩阵的求导"},{"source":"/notes/2022/2022.1/D2L-4-%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC","target":"/notes/2022/2022.1/Einstein-Notation","text":"Einstein Notation"}],"/notes/2022/2022.1/D2L-6-%E8%AE%A1%E7%AE%97%E5%9B%BE":[{"source":"/notes/2022/2022.1/D2L-6-%E8%AE%A1%E7%AE%97%E5%9B%BE","target":"/notes/2022/2022.1/D2L-5-%E6%8B%93%E5%B1%95%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99","text":"例子 线性回归"}],"/notes/2022/2022.1/D2L-7-%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC":[{"source":"/notes/2022/2022.1/D2L-7-%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC","target":"/notes/2022/2022.1/D2L-6-%E8%AE%A1%E7%AE%97%E5%9B%BE","text":"计算图"},{"source":"/notes/2022/2022.1/D2L-7-%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC","target":"/notes/2022/2022.1/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%AF%94%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%9B%B4%E9%AB%98%E6%95%88","text":"为什么反向传播比前向传播更高效"}],"/notes/2022/2022.1/Dummy_Variables":[{"source":"/notes/2022/2022.1/Dummy_Variables","target":"/notes/2022/2022.1/Kronecker-delta-%E5%85%8B%E7%BD%97%E5%86%85%E5%85%8B%CE%B4%E5%87%BD%E6%95%B0","text":"Kronecker delta"}],"/notes/2022/2022.1/One-hot_Encoding-%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81":[{"source":"/notes/2022/2022.1/One-hot_Encoding-%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81","target":"/notes/2022/2022.1/Dummy_Variables","text":"notes/2022/2022.1/Dummy_Variables"}],"/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B":[{"source":"/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","target":"/notes/2022/2022.2/D2L-39-LeNet","text":"LeNet"},{"source":"/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","target":"/notes/2022/2022.2/D2L-40-AlexNet","text":"AlexNet"},{"source":"/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","target":"/notes/2022/2022.2/D2L-41-VGG","text":"VGG"},{"source":"/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","target":"/notes/2022/2022.3/D2L-42-NiN","text":"NiN"},{"source":"/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","target":"/notes/2022/2022.3/D2L-45-ResNet","text":"ResNet"},{"source":"/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","target":"/notes/2022/2022.3/D2L-46-DenseNet","text":"DenseNet"},{"source":"/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","target":"/notes/2022/2022.4/D2L-53-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN","text":"RNN"},{"source":"/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","target":"/notes/2022/2022.4/D2L-57-LSTM-%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C","text":"LSTM"},{"source":"/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","target":"/notes/2022/2022.4/D2L-56-%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83GRU","text":"GRU"},{"source":"/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","target":"/notes/2022/2022.4/D2L-60-Encoder-Decoder","text":"Encoder-Decoder"},{"source":"/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","target":"/notes/2022/2022.4/D2L-75-BERT","text":"BERT"}],"/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%82%E6%95%B0%E4%B8%8D%E8%83%BD%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%BA%E5%90%8C%E4%B8%80%E4%B8%AA%E5%B8%B8%E6%95%B0":[{"source":"/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%82%E6%95%B0%E4%B8%8D%E8%83%BD%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%BA%E5%90%8C%E4%B8%80%E4%B8%AA%E5%B8%B8%E6%95%B0","target":"/notes/2022/2022.2/D2L-23-Dropout-%E4%B8%A2%E5%BC%83%E6%B3%95","text":"(Dropout-丢弃法)"}],"/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE":[{"source":"/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE","target":"/notes/2021/2021.9/%E8%AF%81%E6%98%8ELogistic%E5%9B%9E%E5%BD%92%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%98%AF%E5%87%B8%E5%87%BD%E6%95%B0","text":"证明Logistic回归的损失函数是凸函数"},{"source":"/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE","target":"/notes/2022/2022.2/%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%9B%B0%E9%9A%BE","text":"对于多个样本需要加上求和符号"},{"source":"/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE","target":"/notes/2021/2021.9/%E8%AF%81%E6%98%8ELogistic%E5%9B%9E%E5%BD%92%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%98%AF%E5%87%B8%E5%87%BD%E6%95%B0","text":"详细证明见: 证明Logistic回归的损失函数是凸函数"}],"/notes/2022/2022.2/%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%9B%B0%E9%9A%BE":[{"source":"/notes/2022/2022.2/%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%9B%B0%E9%9A%BE","target":"/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE","text":"为什么Softmax回归不用MSE"}],"/notes/2022/2022.2/%E5%A5%BD%E7%9A%84%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%B9%E8%B4%A8":[{"source":"/notes/2022/2022.2/%E5%A5%BD%E7%9A%84%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%B9%E8%B4%A8","target":"/notes/2022/2022.2/D2L-22-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F","text":"1995年，克里斯托弗·毕晓普证明了 具有输入噪声的训练等价于Tikhonov正则化。"},{"source":"/notes/2022/2022.2/%E5%A5%BD%E7%9A%84%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%B9%E8%B4%A8","target":"/notes/2022/2022.2/D2L-23-Dropout-%E4%B8%A2%E5%BC%83%E6%B3%95","text":"Dropout"}],"/notes/2022/2022.2/%E5%AF%B9%E4%BA%8E%E7%AD%89%E4%BB%B7%E7%9A%84%E7%BD%91%E7%BB%9C-%E5%B0%8F%E7%9A%84%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%8F%82%E6%95%B0%E6%9B%B4%E5%B0%91":[{"source":"/notes/2022/2022.2/%E5%AF%B9%E4%BA%8E%E7%AD%89%E4%BB%B7%E7%9A%84%E7%BD%91%E7%BB%9C-%E5%B0%8F%E7%9A%84%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%8F%82%E6%95%B0%E6%9B%B4%E5%B0%91","target":"/notes/2022/2022.2/2%E4%B8%AA3x3%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%A0%86%E5%8F%A0%E5%90%8E%E7%AD%89%E4%BB%B7%E4%BA%8E%E4%B8%80%E4%B8%AA5x5%E5%8D%B7%E7%A7%AF%E6%A0%B8","text":"2个3x3卷积核堆叠后等价于一个5x5卷积核"},{"source":"/notes/2022/2022.2/%E5%AF%B9%E4%BA%8E%E7%AD%89%E4%BB%B7%E7%9A%84%E7%BD%91%E7%BB%9C-%E5%B0%8F%E7%9A%84%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%8F%82%E6%95%B0%E6%9B%B4%E5%B0%91","target":"/notes/2022/2022.2/%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%8F%82%E6%95%B0%E5%A4%A7%E5%B0%8F%E7%9A%84%E8%AE%A1%E7%AE%97","text":"卷积层参数大小的计算"}],"/notes/2022/2022.2/2%E4%B8%AA3x3%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%A0%86%E5%8F%A0%E5%90%8E%E7%AD%89%E4%BB%B7%E4%BA%8E%E4%B8%80%E4%B8%AA5x5%E5%8D%B7%E7%A7%AF%E6%A0%B8":[{"source":"/notes/2022/2022.2/2%E4%B8%AA3x3%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%A0%86%E5%8F%A0%E5%90%8E%E7%AD%89%E4%BB%B7%E4%BA%8E%E4%B8%80%E4%B8%AA5x5%E5%8D%B7%E7%A7%AF%E6%A0%B8","target":"/notes/2022/2022.2/%E5%AF%B9%E4%BA%8E%E7%AD%89%E4%BB%B7%E7%9A%84%E7%BD%91%E7%BB%9C-%E5%B0%8F%E7%9A%84%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%8F%82%E6%95%B0%E6%9B%B4%E5%B0%91","text":"notes/2022/2022.2/对于等价的网络, 小的卷积核参数更少"}],"/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5":[{"source":"/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5","target":"/notes/2022/2022.2/KL_Divergence-KL%E6%95%A3%E5%BA%A6","text":"KL散度"},{"source":"/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5","target":"/notes/2022/2022.2/Entropy-%E7%86%B5","text":"熵(Entropy)"},{"source":"/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5","target":"/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss","text":"交叉熵是怎样衡量输出和真实值的差别的呢?"},{"source":"/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5","target":"/notes/2022/2022.2/D2L-13-Softmax_Regression","text":"Softmax回归"},{"source":"/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5","target":"/notes/2022/2022.1/One-hot_Encoding-%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81","text":"One-hot_Encoding-独热编码"}],"/notes/2022/2022.2/D2L-12-Predication_or_Inference-Difference":[{"source":"/notes/2022/2022.2/D2L-12-Predication_or_Inference-Difference","target":"/notes/2021/2021.12/Bayesian_EstimationInference%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1","text":"Bayesian_Estimation(Inference)贝叶斯估计"}],"/notes/2022/2022.2/D2L-13-Softmax_Regression":[{"source":"/notes/2022/2022.2/D2L-13-Softmax_Regression","target":"/notes/2022/2022.2/Softmax%E5%87%BD%E6%95%B0","text":"Softmax函数"},{"source":"/notes/2022/2022.2/D2L-13-Softmax_Regression","target":"/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss","text":"D2L-14-Cross Entropy as Loss"},{"source":"/notes/2022/2022.2/D2L-13-Softmax_Regression","target":"/notes/2022/2022.2/Relation_between_Softmax_and_Logistic_Regression","text":"Relation_between_Softmax_and_Logistic_Regression"}],"/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss":[{"source":"/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss","target":"/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5","text":"交叉熵"},{"source":"/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss","target":"/notes/2022/2022.2/KL_Divergence-KL%E6%95%A3%E5%BA%A6","text":"KL_Divergence"},{"source":"/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss","target":"/notes/2022/2022.2/Logit","text":"Logit"},{"source":"/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss","target":"/notes/2022/2022.1/One-hot_Encoding-%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81","text":"One-hot编码"},{"source":"/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss","target":"/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5","text":"Cross Entropy - 交叉熵"},{"source":"/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss","target":"/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE","text":"为什么Softmax回归不用MSE"},{"source":"/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss","target":"/notes/2022/2022.5/Cross_Entropy_Loss_Input_Format-%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%BE%93%E5%85%A5%E6%A0%BC%E5%BC%8F","text":"Cross_Entropy_Loss_Input_Format-交叉熵损失函数输入格式"}],"/notes/2022/2022.2/D2L-17-MLP-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA":[{"source":"/notes/2022/2022.2/D2L-17-MLP-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA","target":"/notes/2022/2022.2/%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2-Affine_Transformation","text":"仿射变换"},{"source":"/notes/2022/2022.2/D2L-17-MLP-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA","target":"/notes/2022/2022.2/D2L-13-Softmax_Regression","text":"Softmax"}],"/notes/2022/2022.2/D2L-18-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-Activation_Functions":[{"source":"/notes/2022/2022.2/D2L-18-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-Activation_Functions","target":"/notes/2021/2021.8/Sigmoid_Function","text":"Sigmoid_Function"}],"/notes/2022/2022.2/D2L-20-%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE%E4%B8%8E%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE":[{"source":"/notes/2022/2022.2/D2L-20-%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE%E4%B8%8E%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE","target":"/notes/2021/2021.12/Cross-Validation","text":"Cross Validation"}],"/notes/2022/2022.2/D2L-21-%E6%A8%A1%E5%9E%8B%E5%AE%B9%E9%87%8F":[{"source":"/notes/2022/2022.2/D2L-21-%E6%A8%A1%E5%9E%8B%E5%AE%B9%E9%87%8F","target":"/notes/2022/2022.2/VC%E7%BB%B4-VC_Dimension","text":"VC维"}],"/notes/2022/2022.2/D2L-22-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F":[{"source":"/notes/2022/2022.2/D2L-22-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F","target":"/notes/2022/2022.2/Regularization-%E6%AD%A3%E5%88%99%E5%8C%96","text":"正则化"},{"source":"/notes/2022/2022.2/D2L-22-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F","target":"/notes/2022/2022.2/Norm-in-Regularization-Intuition","text":"Norm in Regularization - Intuition"}],"/notes/2022/2022.2/D2L-24-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7":[{"source":"/notes/2022/2022.2/D2L-24-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7","target":"/notes/2021/2021.8/Sigmoid_Function","text":"$Sigmoid$ Function"}],"/notes/2022/2022.2/D2L-25-%E8%AE%A9%E8%AE%AD%E7%BB%83%E6%9B%B4%E5%8A%A0%E7%A8%B3%E5%AE%9A-Xavier%E5%88%9D%E5%A7%8B%E5%8C%96":[{"source":"/notes/2022/2022.2/D2L-25-%E8%AE%A9%E8%AE%AD%E7%BB%83%E6%9B%B4%E5%8A%A0%E7%A8%B3%E5%AE%9A-Xavier%E5%88%9D%E5%A7%8B%E5%8C%96","target":"/notes/2022/2022.2/D2L-24-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7","text":"梯度问题的根本原因"},{"source":"/notes/2022/2022.2/D2L-25-%E8%AE%A9%E8%AE%AD%E7%BB%83%E6%9B%B4%E5%8A%A0%E7%A8%B3%E5%AE%9A-Xavier%E5%88%9D%E5%A7%8B%E5%8C%96","target":"/notes/2022/2022.4/D2L-54-Gradient-Clipping-%E6%A2%AF%E5%BA%A6%E5%89%AA%E8%A3%81","text":"Gradient Clipping-梯度剪裁"},{"source":"/notes/2022/2022.2/D2L-25-%E8%AE%A9%E8%AE%AD%E7%BB%83%E6%9B%B4%E5%8A%A0%E7%A8%B3%E5%AE%9A-Xavier%E5%88%9D%E5%A7%8B%E5%8C%96","target":"/notes/2022/2022.2/Xavier%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E8%AF%A6%E7%BB%86%E4%BE%8B%E5%AD%90","text":"Xavier初始化的详细例子"},{"source":"/notes/2022/2022.2/D2L-25-%E8%AE%A9%E8%AE%AD%E7%BB%83%E6%9B%B4%E5%8A%A0%E7%A8%B3%E5%AE%9A-Xavier%E5%88%9D%E5%A7%8B%E5%8C%96","target":"/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%82%E6%95%B0%E4%B8%8D%E8%83%BD%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%BA%E5%90%8C%E4%B8%80%E4%B8%AA%E5%B8%B8%E6%95%B0","text":"为什么参数不能初始化为同一个常数?"}],"/notes/2022/2022.2/D2L-27-Computation-%E5%B1%82%E5%92%8C%E5%9D%97":[{"source":"/notes/2022/2022.2/D2L-27-Computation-%E5%B1%82%E5%92%8C%E5%9D%97","target":"/notes/2022/2022.2/D2L-29-Computation-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B1%82","text":"我们可以自定义一个层"}],"/notes/2022/2022.2/D2L-33-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN":[{"source":"/notes/2022/2022.2/D2L-33-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN","target":"/notes/2022/2022.2/%E5%BD%92%E7%BA%B3%E5%81%8F%E7%BD%AE-Inductive-bias-learning-bias","text":"归纳偏置 Inductive bias"}],"/notes/2022/2022.2/D2L-37-CNN%E7%9A%84%E8%AE%A1%E7%AE%97%E5%A4%8D%E6%9D%82%E5%BA%A6":[{"source":"/notes/2022/2022.2/D2L-37-CNN%E7%9A%84%E8%AE%A1%E7%AE%97%E5%A4%8D%E6%9D%82%E5%BA%A6","target":"/notes/2022/2022.2/D2L-32-Convolution-%E5%8D%B7%E7%A7%AF","text":"数值计算 卷积与傅里叶变换"}],"/notes/2022/2022.2/D2L-38-%E6%B1%A0%E5%8C%96%E5%B1%82-Pooling_Layer":[{"source":"/notes/2022/2022.2/D2L-38-%E6%B1%A0%E5%8C%96%E5%B1%82-Pooling_Layer","target":"/notes/2022/2022.2/D2L-34-%E5%8D%B7%E7%A7%AF%E5%B1%82-%E5%A1%AB%E5%85%85-Padding","text":"填充"},{"source":"/notes/2022/2022.2/D2L-38-%E6%B1%A0%E5%8C%96%E5%B1%82-Pooling_Layer","target":"/notes/2022/2022.2/D2L-35-%E5%8D%B7%E7%A7%AF%E5%B1%82-%E6%AD%A5%E5%B9%85-Stride","text":"步幅"}],"/notes/2022/2022.2/D2L-40-AlexNet":[{"source":"/notes/2022/2022.2/D2L-40-AlexNet","target":"/notes/2022/2022.2/D2L-22-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F","text":"权重衰减"},{"source":"/notes/2022/2022.2/D2L-40-AlexNet","target":"/notes/2022/2022.2/D2L-39-LeNet","text":"LeNet"},{"source":"/notes/2022/2022.2/D2L-40-AlexNet","target":"/notes/2022/2022.2/D2L-41-VGG","text":"VGG"},{"source":"/notes/2022/2022.2/D2L-40-AlexNet","target":"/notes/2022/2022.3/D2L-42-NiN","text":"NiN"},{"source":"/notes/2022/2022.2/D2L-40-AlexNet","target":"/notes/2022/2022.3/D2L-43-GoogLeNetInception","text":"GoogLeNet(Inception)"},{"source":"/notes/2022/2022.2/D2L-40-AlexNet","target":"/notes/2022/2022.3/D2L-45-ResNet","text":"ResNet"}],"/notes/2022/2022.2/D2L-41-VGG":[{"source":"/notes/2022/2022.2/D2L-41-VGG","target":"/notes/2022/2022.2/D2L-40-AlexNet","text":"AlexNet"},{"source":"/notes/2022/2022.2/D2L-41-VGG","target":"/notes/2022/2022.2/2%E4%B8%AA3x3%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%A0%86%E5%8F%A0%E5%90%8E%E7%AD%89%E4%BB%B7%E4%BA%8E%E4%B8%80%E4%B8%AA5x5%E5%8D%B7%E7%A7%AF%E6%A0%B8","text":"notes/2022/2022.2/2个3x3卷积核堆叠后等价于一个5x5卷积核"}],"/notes/2022/2022.2/Likelihood_Function-%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0":[{"source":"/notes/2022/2022.2/Likelihood_Function-%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0","target":"/notes/2021/2021.12/Maximum_Likelihood_Estimation-%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1","text":"Maximum_Likelihood_Estimation-极大似然估计"}],"/notes/2022/2022.2/Norm-in-Regularization-Intuition":[{"source":"/notes/2022/2022.2/Norm-in-Regularization-Intuition","target":"/notes/2022/2022.1/D2L-10-%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D","text":"小批量随机梯度下降"},{"source":"/notes/2022/2022.2/Norm-in-Regularization-Intuition","target":"/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error","text":"L1范数约束后的解很\"稀疏\", 这在特征选择时是很有用的"}],"/notes/2022/2022.2/Regularization-%E6%AD%A3%E5%88%99%E5%8C%96":[{"source":"/notes/2022/2022.2/Regularization-%E6%AD%A3%E5%88%99%E5%8C%96","target":"/notes/2022/2022.2/D2L-22-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F","text":"D2L-22-权重衰减"}],"/notes/2022/2022.2/Relation_between_Softmax_and_Logistic_Regression":[{"source":"/notes/2022/2022.2/Relation_between_Softmax_and_Logistic_Regression","target":"/notes/2022/2022.2/D2L-13-Softmax_Regression","text":"Softmax回归"},{"source":"/notes/2022/2022.2/Relation_between_Softmax_and_Logistic_Regression","target":"/notes/2022/2022.2/Softmax_Regression_is_Over-parameterized","text":"Softmax_Regression_is_Over-parameterized"}],"/notes/2022/2022.2/Softmax%E5%87%BD%E6%95%B0":[{"source":"/notes/2022/2022.2/Softmax%E5%87%BD%E6%95%B0","target":"/notes/2022/2022.2/Softmax_Regression_is_Over-parameterized","text":"notes/2022/2022.2/Softmax_Regression_is_Over-parameterized"},{"source":"/notes/2022/2022.2/Softmax%E5%87%BD%E6%95%B0","target":"/notes/2021/2021.8/Sigmoid_Function","text":"Sigmoid_Function"}],"/notes/2022/2022.3/D2L-42-NiN":[{"source":"/notes/2022/2022.3/D2L-42-NiN","target":"/notes/2022/2022.2/%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%8F%82%E6%95%B0%E5%A4%A7%E5%B0%8F%E7%9A%84%E8%AE%A1%E7%AE%97","text":"卷积层参数大小的计算"},{"source":"/notes/2022/2022.3/D2L-42-NiN","target":"/notes/2022/2022.2/D2L-36-1x1%E5%8D%B7%E7%A7%AF%E5%B1%82","text":"D2L-36-1x1卷积层"},{"source":"/notes/2022/2022.3/D2L-42-NiN","target":"/notes/2022/2022.2/Logit","text":"Logit"}],"/notes/2022/2022.3/D2L-43-GoogLeNetInception":[{"source":"/notes/2022/2022.3/D2L-43-GoogLeNetInception","target":"/notes/2022/2022.3/D2L-42-NiN","text":"NiN"},{"source":"/notes/2022/2022.3/D2L-43-GoogLeNetInception","target":"/notes/2022/2022.2/D2L-36-1x1%E5%8D%B7%E7%A7%AF%E5%B1%82","text":"1x1卷积层"}],"/notes/2022/2022.3/D2L-45-ResNet":[{"source":"/notes/2022/2022.3/D2L-45-ResNet","target":"/notes/2022/2022.2/D2L-21-%E6%A8%A1%E5%9E%8B%E5%AE%B9%E9%87%8F","text":"模型容量"},{"source":"/notes/2022/2022.3/D2L-45-ResNet","target":"/notes/2022/2022.2/D2L-24-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7","text":"后一种问题"},{"source":"/notes/2022/2022.3/D2L-45-ResNet","target":"/notes/2022/2022.2/D2L-24-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7","text":"D2L-24-数值稳定性"},{"source":"/notes/2022/2022.3/D2L-45-ResNet","target":"/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE","text":"为什么Softmax回归不用MSE"},{"source":"/notes/2022/2022.3/D2L-45-ResNet","target":"/notes/2022/2022.3/D2L-43-GoogLeNetInception","text":"D2L-43-GoogLeNet(Inception)"},{"source":"/notes/2022/2022.3/D2L-45-ResNet","target":"/notes/2022/2022.3/D2L-42-NiN","text":"D2L-42-NiN"}],"/notes/2022/2022.3/D2L-51-%E8%AF%AD%E8%A8%80%E7%9A%84%E7%BB%9F%E8%AE%A1%E7%89%B9%E5%BE%81":[{"source":"/notes/2022/2022.3/D2L-51-%E8%AF%AD%E8%A8%80%E7%9A%84%E7%BB%9F%E8%AE%A1%E7%89%B9%E5%BE%81","target":"/notes/2022/2022.3/D2L-50-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-%E4%BC%A0%E7%BB%9F%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8D%E8%B6%B3","text":"拉普拉斯平滑"}],"/notes/2022/2022.4/D2L-53-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN":[{"source":"/notes/2022/2022.4/D2L-53-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN","target":"/notes/2022/2022.3/D2L-48-%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B-Sequence_Models","text":"隐变量自回归模型"},{"source":"/notes/2022/2022.4/D2L-53-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN","target":"/notes/2022/2022.2/D2L-17-MLP-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA","text":"MLP"},{"source":"/notes/2022/2022.4/D2L-53-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN","target":"/notes/2022/2022.4/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CFeedforward-neural-network","text":"notes/2022/2022.4/前馈神经网络(Feedforward neural network)"},{"source":"/notes/2022/2022.4/D2L-53-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN","target":"/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5","text":"Cross_Entropy-交叉熵"},{"source":"/notes/2022/2022.4/D2L-53-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN","target":"/notes/2022/2022.5/Harmonic_Mean-%E8%B0%83%E5%92%8C%E5%B9%B3%E5%9D%87%E6%95%B0","text":"调和平均数(Harmonic mean)"},{"source":"/notes/2022/2022.4/D2L-53-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN","target":"/notes/2022/2022.4/RNN%E4%B8%ADoutput%E5%92%8Chidden_state%E7%9A%84%E5%8C%BA%E5%88%AB","text":"RNN中output和hidden_state的区别"}],"/notes/2022/2022.4/D2L-55-%E5%9C%A8%E6%97%B6%E9%97%B4%E4%B8%8A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD":[{"source":"/notes/2022/2022.4/D2L-55-%E5%9C%A8%E6%97%B6%E9%97%B4%E4%B8%8A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD","target":"/notes/2022/2022.2/D2L-24-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7","text":"D2L-24-数值稳定性"},{"source":"/notes/2022/2022.4/D2L-55-%E5%9C%A8%E6%97%B6%E9%97%B4%E4%B8%8A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD","target":"/notes/2022/2022.2/D2L-25-%E8%AE%A9%E8%AE%AD%E7%BB%83%E6%9B%B4%E5%8A%A0%E7%A8%B3%E5%AE%9A-Xavier%E5%88%9D%E5%A7%8B%E5%8C%96","text":"D2L-25-让训练更加稳定"},{"source":"/notes/2022/2022.4/D2L-55-%E5%9C%A8%E6%97%B6%E9%97%B4%E4%B8%8A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD","target":"/notes/2022/2022.2/Regularization-%E6%AD%A3%E5%88%99%E5%8C%96","text":"正则化"},{"source":"/notes/2022/2022.4/D2L-55-%E5%9C%A8%E6%97%B6%E9%97%B4%E4%B8%8A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD","target":"/notes/2022/2022.4/%E9%80%92%E6%8E%A8%E5%85%AC%E5%BC%8F-a_tb_t+c_ta_t-1-%E8%BD%AC%E9%80%9A%E9%A1%B9%E5%85%AC%E5%BC%8F","text":"notes/2022/2022.4/递推公式 $a_{t}=b_{t}+c_{t}a_{t-1}$ 转通项公式"},{"source":"/notes/2022/2022.4/D2L-55-%E5%9C%A8%E6%97%B6%E9%97%B4%E4%B8%8A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD","target":"/notes/2022/2022.2/D2L-25-%E8%AE%A9%E8%AE%AD%E7%BB%83%E6%9B%B4%E5%8A%A0%E7%A8%B3%E5%AE%9A-Xavier%E5%88%9D%E5%A7%8B%E5%8C%96","text":"梯度归一化"},{"source":"/notes/2022/2022.4/D2L-55-%E5%9C%A8%E6%97%B6%E9%97%B4%E4%B8%8A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD","target":"/notes/2022/2022.1/D2L-5-%E6%8B%93%E5%B1%95%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99","text":"D2L-5-拓展链式法则"}],"/notes/2022/2022.4/D2L-56-%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83GRU":[{"source":"/notes/2022/2022.4/D2L-56-%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83GRU","target":"/notes/2022/2022.4/%E5%87%B8%E7%BB%84%E5%90%88-Convex-Combination","text":"凸组合 - Convex Combination"}],"/notes/2022/2022.4/D2L-57-LSTM-%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C":[{"source":"/notes/2022/2022.4/D2L-57-LSTM-%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C","target":"/notes/2022/2022.4/D2L-56-%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83GRU","text":"D2L-56-门控循环单元GRU"},{"source":"/notes/2022/2022.4/D2L-57-LSTM-%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C","target":"/notes/2022/2022.4/%E5%87%B8%E7%BB%84%E5%90%88-Convex-Combination","text":"凸组合 - Convex Combination"}],"/notes/2022/2022.4/D2L-59-%E5%8F%8C%E5%90%91%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C":[{"source":"/notes/2022/2022.4/D2L-59-%E5%8F%8C%E5%90%91%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C","target":"/notes/2022/2022.3/D2L-48-%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B-Sequence_Models","text":"之前的笔记 - 序列模型-Sequence_Models"}],"/notes/2022/2022.4/D2L-61-Sequence-to-Sequence-Learning-Seq2Seq":[{"source":"/notes/2022/2022.4/D2L-61-Sequence-to-Sequence-Learning-Seq2Seq","target":"/notes/2022/2022.4/D2L-60-Encoder-Decoder","text":"Encoder-Decoder"},{"source":"/notes/2022/2022.4/D2L-61-Sequence-to-Sequence-Learning-Seq2Seq","target":"/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss","text":"交叉熵损失函数"},{"source":"/notes/2022/2022.4/D2L-61-Sequence-to-Sequence-Learning-Seq2Seq","target":"/notes/2022/2022.4/D2L-53-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN","text":"困惑度 Perplexity"},{"source":"/notes/2022/2022.4/D2L-61-Sequence-to-Sequence-Learning-Seq2Seq","target":"/notes/2022/2022.4/D2L-62-BLEU-Bilingual-Evaluation-Understudy","text":"D2L-62-BLEU (Bilingual Evaluation Understudy)"}],"/notes/2022/2022.4/D2L-63-Beam-Search":[{"source":"/notes/2022/2022.4/D2L-63-Beam-Search","target":"/notes/2022/2022.4/D2L-61-Sequence-to-Sequence-Learning-Seq2Seq","text":"Seq2Seq"},{"source":"/notes/2022/2022.4/D2L-63-Beam-Search","target":"/notes/2022/2022.4/Viterbi-Algorithm","text":"Viterbi Algorithm"},{"source":"/notes/2022/2022.4/D2L-63-Beam-Search","target":"/notes/2022/2022.4/D2L-62-BLEU-Bilingual-Evaluation-Understudy","text":"BLEU"}],"/notes/2022/2022.4/D2L-64-Kernel-Regression":[{"source":"/notes/2022/2022.4/D2L-64-Kernel-Regression","target":"/notes/2022/2022.4/D2L-66-Kernel-Regression-and-Attention","text":"notes/2022/2022.4/D2L-66-Kernel Regression and Attention"}],"/notes/2022/2022.4/D2L-66-Kernel-Regression-and-Attention":[{"source":"/notes/2022/2022.4/D2L-66-Kernel-Regression-and-Attention","target":"/notes/2022/2022.4/D2L-64-Kernel-Regression","text":"Kernel Regression 里面的采用高斯核的时候"}],"/notes/2022/2022.4/D2L-67-Attention-Scoring-Function":[{"source":"/notes/2022/2022.4/D2L-67-Attention-Scoring-Function","target":"/notes/2022/2022.4/D2L-68-Additive-Attention","text":"D2L-68-Additive Attention"},{"source":"/notes/2022/2022.4/D2L-67-Attention-Scoring-Function","target":"/notes/2022/2022.4/D2L-69-Scaled-Dot-Product-Attention","text":"D2L-69-Scaled Dot-Product Attention"}],"/notes/2022/2022.4/D2L-69-Scaled-Dot-Product-Attention":[{"source":"/notes/2022/2022.4/D2L-69-Scaled-Dot-Product-Attention","target":"/notes/2021/2021.11/%E5%86%85%E7%A7%AF%E5%92%8C%E7%9B%B8%E5%85%B3%E6%80%A7%E7%9A%84%E8%81%94%E7%B3%BB-DotInner_Product__Correlation","text":"内积可以衡量两个向量之间的相似程度"},{"source":"/notes/2022/2022.4/D2L-69-Scaled-Dot-Product-Attention","target":"/notes/2022/2022.2/D2L-23-Dropout-%E4%B8%A2%E5%BC%83%E6%B3%95","text":"Dropout"}],"/notes/2022/2022.4/D2L-70-Seq2Seq-with-Attention-Bahdanau-Attention":[{"source":"/notes/2022/2022.4/D2L-70-Seq2Seq-with-Attention-Bahdanau-Attention","target":"/notes/2022/2022.4/D2L-61-Sequence-to-Sequence-Learning-Seq2Seq","text":"Seq2Seq"},{"source":"/notes/2022/2022.4/D2L-70-Seq2Seq-with-Attention-Bahdanau-Attention","target":"/notes/2022/2022.4/D2L-68-Additive-Attention","text":"Additive Attention"}],"/notes/2022/2022.4/D2L-71-Multi-Head_Attention":[{"source":"/notes/2022/2022.4/D2L-71-Multi-Head_Attention","target":"/notes/2022/2022.4/D2L-67-Attention-Scoring-Function","text":"Attention Scoring Function"}],"/notes/2022/2022.4/D2L-72-Self-Attention":[{"source":"/notes/2022/2022.4/D2L-72-Self-Attention","target":"/notes/2022/2022.4/D2L-73-Positional_Encoding","text":"D2L-73-Positional_Encoding"}],"/notes/2022/2022.4/D2L-74-Transformer":[{"source":"/notes/2022/2022.4/D2L-74-Transformer","target":"/notes/2022/2022.4/D2L-60-Encoder-Decoder","text":"Encoder Decoder"},{"source":"/notes/2022/2022.4/D2L-74-Transformer","target":"/notes/2022/2022.4/D2L-72-Self-Attention","text":"Self-Attention"},{"source":"/notes/2022/2022.4/D2L-74-Transformer","target":"/notes/2022/2022.4/D2L-71-Multi-Head_Attention","text":"多头注意力"},{"source":"/notes/2022/2022.4/D2L-74-Transformer","target":"/notes/2022/2022.3/D2L-45-ResNet","text":"ResNet"},{"source":"/notes/2022/2022.4/D2L-74-Transformer","target":"/notes/2022/2022.3/D2L-44-Batch_Normalization-%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96","text":"Batch_Normalization-批量归一化"},{"source":"/notes/2022/2022.4/D2L-74-Transformer","target":"/notes/2022/2022.2/D2L-23-Dropout-%E4%B8%A2%E5%BC%83%E6%B3%95","text":"Dropout"},{"source":"/notes/2022/2022.4/D2L-74-Transformer","target":"/notes/2022/2022.4/D2L-72-Self-Attention","text":"自注意力忽略了原始序列的位置信息"}],"/notes/2022/2022.4/D2L-75-BERT":[{"source":"/notes/2022/2022.4/D2L-75-BERT","target":"/notes/2022/2022.4/D2L-77-BERT-Fine-tune","text":"D2L-77-BERT - Fine-tune"}],"/notes/2022/2022.4/D2L-76-BERT-Pretrain":[{"source":"/notes/2022/2022.4/D2L-76-BERT-Pretrain","target":"/notes/2022/2022.3/D2L-50-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-%E4%BC%A0%E7%BB%9F%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8D%E8%B6%B3","text":"语言模型(Language Model)"}],"/notes/2022/2022.4/D2L-77-BERT-Fine-tune":[{"source":"/notes/2022/2022.4/D2L-77-BERT-Fine-tune","target":"/notes/2021/2021.8/Mean_Squared_Error_%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE","text":"Mean_Squared_Error_均方误差"}],"/notes/2022/2022.5/%E7%AB%AF%E5%88%B0%E7%AB%AF%E5%AD%A6%E4%B9%A0-End_to_End_Learning-E2E":[{"source":"/notes/2022/2022.5/%E7%AB%AF%E5%88%B0%E7%AB%AF%E5%AD%A6%E4%B9%A0-End_to_End_Learning-E2E","target":"/notes/2022/2022.2/%E5%BD%92%E7%BA%B3%E5%81%8F%E7%BD%AE-Inductive-bias-learning-bias","text":"归纳偏置"}],"/notes/2022/2022.5/F1_Score":[{"source":"/notes/2022/2022.5/F1_Score","target":"/notes/2022/2022.5/Harmonic_Mean-%E8%B0%83%E5%92%8C%E5%B9%B3%E5%9D%87%E6%95%B0","text":"Harmonic_Mean-调和平均数"},{"source":"/notes/2022/2022.5/F1_Score","target":"/notes/2022/2022.5/Precision-Recall-and-Accuracy","text":"Precision Recall"}],"/notes/2022/2022.6/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-%E5%91%A8%E6%9C%9F%E6%80%A7%E6%95%B0%E6%8D%AE%E6%97%B6%E9%97%B4%E7%AD%89":[{"source":"/notes/2022/2022.6/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-%E5%91%A8%E6%9C%9F%E6%80%A7%E6%95%B0%E6%8D%AE%E6%97%B6%E9%97%B4%E7%AD%89","target":"/notes/2022/2022.1/One-hot_Encoding-%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81","text":"One-hot_Encoding-独热编码"}],"/notes/2022/2022.6/JS%E6%95%A3%E5%BA%A6":[{"source":"/notes/2022/2022.6/JS%E6%95%A3%E5%BA%A6","target":"/notes/2022/2022.2/KL_Divergence-KL%E6%95%A3%E5%BA%A6","text":"Kullback–Leibler divergence"}],"/notes/2022/2022.7/%E6%9C%89%E6%95%88%E5%9C%B0%E8%83%8C%E8%AF%B5GRE%E5%8D%95%E8%AF%8D":[{"source":"/notes/2022/2022.7/%E6%9C%89%E6%95%88%E5%9C%B0%E8%83%8C%E8%AF%B5GRE%E5%8D%95%E8%AF%8D","target":"/","text":"incontrovertible"},{"source":"/notes/2022/2022.7/%E6%9C%89%E6%95%88%E5%9C%B0%E8%83%8C%E8%AF%B5GRE%E5%8D%95%E8%AF%8D","target":"/","text":"unimpeachable"},{"source":"/notes/2022/2022.7/%E6%9C%89%E6%95%88%E5%9C%B0%E8%83%8C%E8%AF%B5GRE%E5%8D%95%E8%AF%8D","target":"/","text":"unimpeachable"},{"source":"/notes/2022/2022.7/%E6%9C%89%E6%95%88%E5%9C%B0%E8%83%8C%E8%AF%B5GRE%E5%8D%95%E8%AF%8D","target":"/","text":"incontrovertible"},{"source":"/notes/2022/2022.7/%E6%9C%89%E6%95%88%E5%9C%B0%E8%83%8C%E8%AF%B5GRE%E5%8D%95%E8%AF%8D","target":"/","text":"impeccable"}]},"backlinks":{"/":[{"source":"/notes/2021/2021.10/Compiler-3_%E7%AE%97%E7%AC%A6%E4%BC%98%E5%85%88%E5%88%86%E6%9E%90","target":"/","text":"为什么?"},{"source":"/notes/2022/2022.7/%E6%9C%89%E6%95%88%E5%9C%B0%E8%83%8C%E8%AF%B5GRE%E5%8D%95%E8%AF%8D","target":"/","text":"incontrovertible"},{"source":"/notes/2022/2022.7/%E6%9C%89%E6%95%88%E5%9C%B0%E8%83%8C%E8%AF%B5GRE%E5%8D%95%E8%AF%8D","target":"/","text":"unimpeachable"},{"source":"/notes/2022/2022.7/%E6%9C%89%E6%95%88%E5%9C%B0%E8%83%8C%E8%AF%B5GRE%E5%8D%95%E8%AF%8D","target":"/","text":"unimpeachable"},{"source":"/notes/2022/2022.7/%E6%9C%89%E6%95%88%E5%9C%B0%E8%83%8C%E8%AF%B5GRE%E5%8D%95%E8%AF%8D","target":"/","text":"incontrovertible"},{"source":"/notes/2022/2022.7/%E6%9C%89%E6%95%88%E5%9C%B0%E8%83%8C%E8%AF%B5GRE%E5%8D%95%E8%AF%8D","target":"/","text":"impeccable"}],"/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B":[{"source":"/","target":"/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","text":"经典模型"}],"/../../../1_Project/2021.6_CPU/CP0_CoProcessor0_in_CPU":[{"source":"/notes/2021/2021.11/OS-11_%E4%B8%AD%E6%96%AD","target":"/../../../1_Project/2021.6_CPU/CP0_CoProcessor0_in_CPU","text":"CP0_CoProcessor0_in_CPU"}],"/D2L-1-What_is_a_tensor":[{"source":"/notes/2022/2022.1/%E7%9F%A9%E9%98%B5%E7%9A%84%E4%B8%8D%E5%90%8C%E4%B9%98%E6%B3%95-Hadamard-Kronecker","target":"/D2L-1-What_is_a_tensor","text":"Tensor Product"}],"/notes/2021/2021.10/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%96%B9%E5%B7%AE%E7%9A%84%E5%88%86%E6%AF%8D%E5%B8%B8%E5%B8%B8%E6%98%AFn-1":[{"source":"/","target":"/notes/2021/2021.10/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%96%B9%E5%B7%AE%E7%9A%84%E5%88%86%E6%AF%8D%E5%B8%B8%E5%B8%B8%E6%98%AFn-1","text":"为什么方差的分母常常是n-1"},{"source":"/notes/2021/2021.10/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5_Covariance_Matrix","target":"/notes/2021/2021.10/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%96%B9%E5%B7%AE%E7%9A%84%E5%88%86%E6%AF%8D%E5%B8%B8%E5%B8%B8%E6%98%AFn-1","text":"为什么方差的分母常常是n-1"}],"/notes/2021/2021.10/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5_Covariance_Matrix":[{"source":"/notes/2021/2021.10/Part.29_Fisher_Linear_DiscriminantPattern_Classification-Chapter_4","target":"/notes/2021/2021.10/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5_Covariance_Matrix","text":"协方差矩阵_Covariance_Matrix"}],"/notes/2021/2021.10/Compiler-3_%E7%AE%97%E7%AC%A6%E4%BC%98%E5%85%88%E5%88%86%E6%9E%90":[{"source":"/notes/2021/2021.10/Compiler-2_Bottom-Up_Parsing-%E8%87%AA%E5%BA%95%E5%90%91%E4%B8%8A%E5%88%86%E6%9E%90","target":"/notes/2021/2021.10/Compiler-3_%E7%AE%97%E7%AC%A6%E4%BC%98%E5%85%88%E5%88%86%E6%9E%90","text":"\"算符优先分析\""}],"/notes/2021/2021.10/Compiler-4-1_%E4%BB%80%E4%B9%88%E6%98%AF-LR-%E5%88%86%E6%9E%90":[{"source":"/notes/2021/2021.10/Compiler-4_LR%E5%88%86%E6%9E%90","target":"/notes/2021/2021.10/Compiler-4-1_%E4%BB%80%E4%B9%88%E6%98%AF-LR-%E5%88%86%E6%9E%90","text":"Compiler-4-1_什么是 LR 分析"}],"/notes/2021/2021.10/Compiler-4-2_LR0_Parse":[{"source":"/notes/2021/2021.10/Compiler-4_LR%E5%88%86%E6%9E%90","target":"/notes/2021/2021.10/Compiler-4-2_LR0_Parse","text":"Compiler-4-2_LR(0)_Parse"}],"/notes/2021/2021.10/Compiler-4-3_SLR_parse":[{"source":"/notes/2021/2021.10/Compiler-4_LR%E5%88%86%E6%9E%90","target":"/notes/2021/2021.10/Compiler-4-3_SLR_parse","text":"Compiler-4-3_SLR_parse"}],"/notes/2021/2021.10/Compiler-4-4_LR1_%E5%88%86%E6%9E%90":[{"source":"/notes/2021/2021.10/Compiler-4_LR%E5%88%86%E6%9E%90","target":"/notes/2021/2021.10/Compiler-4-4_LR1_%E5%88%86%E6%9E%90","text":"Compiler-4-4_LR(1)_分析"}],"/notes/2021/2021.10/Compiler-4-5_LALR1":[{"source":"/notes/2021/2021.10/Compiler-4_LR%E5%88%86%E6%9E%90","target":"/notes/2021/2021.10/Compiler-4-5_LALR1","text":"Compiler-4-5_LALR(1)"}],"/notes/2021/2021.10/Dot_Product_and_Linear_Transformation-%E5%90%91%E9%87%8F%E5%86%85%E7%A7%AF%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2":[{"source":"/notes/2021/2021.10/Part.29_Fisher_Linear_DiscriminantPattern_Classification-Chapter_4","target":"/notes/2021/2021.10/Dot_Product_and_Linear_Transformation-%E5%90%91%E9%87%8F%E5%86%85%E7%A7%AF%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2","text":"Dot_Product_and_Linear_Transformation-向量内积与线性变换"}],"/notes/2021/2021.11/%E5%85%B3%E4%BA%8E%E7%89%B9%E5%BE%81%E5%80%BC%E7%9A%84%E4%B8%80%E4%B8%AA%E7%BB%93%E8%AE%BA":[{"source":"/notes/2021/2021.11/MIT_18.065-Part_9-Singular-Value-Decomposition-SVD","target":"/notes/2021/2021.11/%E5%85%B3%E4%BA%8E%E7%89%B9%E5%BE%81%E5%80%BC%E7%9A%84%E4%B8%80%E4%B8%AA%E7%BB%93%E8%AE%BA","text":"关于特征值的一个结论"}],"/notes/2021/2021.11/%E5%85%B3%E4%BA%8E%E7%A7%A9%E7%9A%84%E4%B8%80%E4%B8%AA%E7%BB%93%E8%AE%BA":[{"source":"/notes/2021/2021.11/MIT_18.065-Part_9-Singular-Value-Decomposition-SVD","target":"/notes/2021/2021.11/%E5%85%B3%E4%BA%8E%E7%A7%A9%E7%9A%84%E4%B8%80%E4%B8%AA%E7%BB%93%E8%AE%BA","text":"关于秩的一个结论"}],"/notes/2021/2021.11/%E5%86%85%E7%A7%AF%E5%92%8C%E7%9B%B8%E5%85%B3%E6%80%A7%E7%9A%84%E8%81%94%E7%B3%BB-DotInner_Product__Correlation":[{"source":"/notes/2022/2022.4/D2L-69-Scaled-Dot-Product-Attention","target":"/notes/2021/2021.11/%E5%86%85%E7%A7%AF%E5%92%8C%E7%9B%B8%E5%85%B3%E6%80%A7%E7%9A%84%E8%81%94%E7%B3%BB-DotInner_Product__Correlation","text":"内积可以衡量两个向量之间的相似程度"}],"/notes/2021/2021.11/%E7%90%86%E8%A7%A3%E7%9B%B8%E4%BC%BC%E7%9F%A9%E9%98%B5":[{"source":"/notes/2021/2021.11/MIT_18.065-Part_7-Eigenvalues-and-Eigenvectors","target":"/notes/2021/2021.11/%E7%90%86%E8%A7%A3%E7%9B%B8%E4%BC%BC%E7%9F%A9%E9%98%B5","text":"notes/2021/2021.11/理解相似矩阵"}],"/notes/2021/2021.11/%E9%85%89%E7%9F%A9%E9%98%B5%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%AB%E9%85%89%E7%9F%A9%E9%98%B5":[{"source":"/","target":"/notes/2021/2021.11/%E9%85%89%E7%9F%A9%E9%98%B5%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%AB%E9%85%89%E7%9F%A9%E9%98%B5","text":"酉矩阵为什么叫酉矩阵"}],"/notes/2021/2021.11/A-Fancy-Example-of-SVD":[{"source":"/notes/2021/2021.11/MIT_18.065-Part_9-Singular-Value-Decomposition-SVD","target":"/notes/2021/2021.11/A-Fancy-Example-of-SVD","text":"notes/2021/2021.11/A Fancy Example of SVD"}],"/notes/2021/2021.11/LU%E5%88%86%E8%A7%A3%E7%9A%84%E4%BE%8B%E5%AD%90":[{"source":"/notes/2021/2021.11/MIT_18.065-Part_2-Matrix_Factorization","target":"/notes/2021/2021.11/LU%E5%88%86%E8%A7%A3%E7%9A%84%E4%BE%8B%E5%AD%90","text":"LU分解的例子"},{"source":"/notes/2021/2021.11/MIT_18.065-Part_4-LU_Factorization","target":"/notes/2021/2021.11/LU%E5%88%86%E8%A7%A3%E7%9A%84%E4%BE%8B%E5%AD%90","text":"LU分解的例子"}],"/notes/2021/2021.11/SVD-Intuition":[{"source":"/notes/2021/2021.11/MIT_18.065-Part_9-Singular-Value-Decomposition-SVD","target":"/notes/2021/2021.11/SVD-Intuition","text":"SVD Intuition"},{"source":"/notes/2021/2021.11/MIT_18.065-Part_9-Singular-Value-Decomposition-SVD","target":"/notes/2021/2021.11/SVD-Intuition","text":"SVD Intuition"}],"/notes/2021/2021.12/%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83-Mutivariate_Gaussian":[{"source":"/notes/2021/2021.12/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E7%9A%84%E5%88%A4%E5%88%AB%E5%87%BD%E6%95%B0","target":"/notes/2021/2021.12/%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83-Mutivariate_Gaussian","text":"多元高斯分布-Mutivariate_Gaussian"}],"/notes/2021/2021.12/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E5%87%BD%E6%95%B0%E7%9A%84%E6%9C%9F%E6%9C%9B":[{"source":"/notes/2021/2021.12/Johnson-Lindenstrauss-Lemma-Publish-Version","target":"/notes/2021/2021.12/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E5%87%BD%E6%95%B0%E7%9A%84%E6%9C%9F%E6%9C%9B","text":"关于随机变量函数的期望"}],"/notes/2021/2021.12/Bayesian_EstimationInference%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1":[{"source":"/notes/2022/2022.2/D2L-12-Predication_or_Inference-Difference","target":"/notes/2021/2021.12/Bayesian_EstimationInference%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1","text":"Bayesian_Estimation(Inference)贝叶斯估计"}],"/notes/2021/2021.12/Chernoff-Bounds":[{"source":"/notes/2021/2021.12/Johnson-Lindenstrauss-Lemma-Publish-Version","target":"/notes/2021/2021.12/Chernoff-Bounds","text":"Chernoff Bounds"},{"source":"/notes/2021/2021.12/Johnson-Lindenstrauss-Lemma-Publish-Version","target":"/notes/2021/2021.12/Chernoff-Bounds","text":"Chernoff Bounds"}],"/notes/2021/2021.12/Chi-Squared_Distribution-%E5%8D%A1%E6%96%B9%E5%88%86%E5%B8%83":[{"source":"/notes/2021/2021.12/Johnson-Lindenstrauss-Lemma-Publish-Version","target":"/notes/2021/2021.12/Chi-Squared_Distribution-%E5%8D%A1%E6%96%B9%E5%88%86%E5%B8%83","text":"notes/2021/2021.12/Chi-Squared_Distribution-卡方分布"}],"/notes/2021/2021.12/Covariance-%E5%8D%8F%E6%96%B9%E5%B7%AE":[{"source":"/notes/2021/2021.10/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5_Covariance_Matrix","target":"/notes/2021/2021.12/Covariance-%E5%8D%8F%E6%96%B9%E5%B7%AE","text":"covariance"}],"/notes/2021/2021.12/Cross-Validation":[{"source":"/notes/2022/2022.2/D2L-20-%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE%E4%B8%8E%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE","target":"/notes/2021/2021.12/Cross-Validation","text":"Cross Validation"}],"/notes/2021/2021.12/Johnson-Lindenstrauss-Lemma-Publish-Version":[{"source":"/","target":"/notes/2021/2021.12/Johnson-Lindenstrauss-Lemma-Publish-Version","text":"Johnson Lindenstrauss Lemma"}],"/notes/2021/2021.12/Maximum_Likelihood_Estimation-%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1":[{"source":"/notes/2021/2021.12/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1-Parameter_Estimation","target":"/notes/2021/2021.12/Maximum_Likelihood_Estimation-%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1","text":"Maximum_Likelihood_Estimation-极大似然估计"},{"source":"/notes/2022/2022.2/Likelihood_Function-%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0","target":"/notes/2021/2021.12/Maximum_Likelihood_Estimation-%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1","text":"Maximum_Likelihood_Estimation-极大似然估计"}],"/notes/2021/2021.12/Understanding-Bayes-Theorem":[{"source":"/notes/2021/2021.12/Bayesian-Decision-Theory-Part1","target":"/notes/2021/2021.12/Understanding-Bayes-Theorem","text":"根据Bayes定理"}],"/notes/2021/2021.12/Union_Bound-%E5%B8%83%E5%B0%94%E4%B8%8D%E7%AD%89%E5%BC%8F-Booles_inequality":[{"source":"/notes/2021/2021.12/Johnson-Lindenstrauss-Lemma-Publish-Version","target":"/notes/2021/2021.12/Union_Bound-%E5%B8%83%E5%B0%94%E4%B8%8D%E7%AD%89%E5%BC%8F-Booles_inequality","text":"notes/2021/2021.12/Union_Bound-布尔不等式-Boole's_inequality"}],"/notes/2021/2021.6/%E5%9B%BE%E7%81%B5%E5%BD%92%E7%BA%A6-Turing-Reduction":[{"source":"/notes/2021/2021.6/Diffie-Hellman%E9%97%AE%E9%A2%98","target":"/notes/2021/2021.6/%E5%9B%BE%E7%81%B5%E5%BD%92%E7%BA%A6-Turing-Reduction","text":"图灵归约 Turing Reduction"}],"/notes/2021/2021.6/Diffie-Hellman%E9%97%AE%E9%A2%98":[{"source":"/","target":"/notes/2021/2021.6/Diffie-Hellman%E9%97%AE%E9%A2%98","text":"Diffie-Hellman问题"}],"/notes/2021/2021.8/%E5%87%B8%E4%BC%98%E5%8C%96%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98":[{"source":"/notes/2021/2021.8/Part.13_Cost_Function-Logistic_RegressionML_Andrew.Ng.","target":"/notes/2021/2021.8/%E5%87%B8%E4%BC%98%E5%8C%96%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98","text":"凸函数"},{"source":"/notes/2021/2021.8/Part.5_Gradient_DescentML_Andrew.Ng.","target":"/notes/2021/2021.8/%E5%87%B8%E4%BC%98%E5%8C%96%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98","text":"notes/2021/2021.8/凸优化与线性回归问题"},{"source":"/notes/2021/2021.8/Part.5_Gradient_DescentML_Andrew.Ng.","target":"/notes/2021/2021.8/%E5%87%B8%E4%BC%98%E5%8C%96%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98","text":"凸优化与线性回归问题"}],"/notes/2021/2021.8/%E5%A6%82%E4%BD%95%E5%8A%AA%E5%8A%9B%E5%B7%A5%E4%BD%9C_Paul_Graham":[{"source":"/","target":"/notes/2021/2021.8/%E5%A6%82%E4%BD%95%E5%8A%AA%E5%8A%9B%E5%B7%A5%E4%BD%9C_Paul_Graham","text":"如何努力工作_Paul_Graham"},{"source":"/notes/2021/2021.8/How_to_Work_Hard-Paul_Graham","target":"/notes/2021/2021.8/%E5%A6%82%E4%BD%95%E5%8A%AA%E5%8A%9B%E5%B7%A5%E4%BD%9C_Paul_Graham","text":"notes/2021/2021.8/如何努力工作_Paul_Graham"},{"source":"/notes/2021/2021.8/%E5%A6%82%E4%BD%95%E5%8A%AA%E5%8A%9B%E5%B7%A5%E4%BD%9C_Paul_Graham","target":"/notes/2021/2021.8/%E5%A6%82%E4%BD%95%E5%8A%AA%E5%8A%9B%E5%B7%A5%E4%BD%9C_Paul_Graham","text":"如何努力工作_Paul_Graham"}],"/notes/2021/2021.8/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%88%86%E5%B8%83%E4%B8%8E%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E7%9A%84%E8%81%94%E7%B3%BB_Relation_of_Laplace_distribution-_and_Gaussian_distribution":[{"source":"/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error","target":"/notes/2021/2021.8/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%88%86%E5%B8%83%E4%B8%8E%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E7%9A%84%E8%81%94%E7%B3%BB_Relation_of_Laplace_distribution-_and_Gaussian_distribution","text":"Related_Post"},{"source":"/notes/2021/2021.9/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83_%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_Normal_Distribution-Gaussian_Distribution","target":"/notes/2021/2021.8/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%88%86%E5%B8%83%E4%B8%8E%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E7%9A%84%E8%81%94%E7%B3%BB_Relation_of_Laplace_distribution-_and_Gaussian_distribution","text":"拉普拉斯分布与高斯分布的联系_Relation_of_Laplace_distribution _and_Gaussian_distribution"}],"/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E7%9A%84%E6%B1%82%E5%AF%BC":[{"source":"/","target":"/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E7%9A%84%E6%B1%82%E5%AF%BC","text":"矩阵的求导"},{"source":"/notes/2021/2021.8/Normal_Equation_Proof_2_Matrix_Method","target":"/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E7%9A%84%E6%B1%82%E5%AF%BC","text":"矩阵的求导"},{"source":"/notes/2022/2022.1/D2L-4-%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC","target":"/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E7%9A%84%E6%B1%82%E5%AF%BC","text":"矩阵的求导"}],"/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E8%BF%B9%E7%9A%84%E6%80%A7%E8%B4%A8":[{"source":"/notes/2021/2021.8/Normal_Equation_Proof_2_Matrix_Method","target":"/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E8%BF%B9%E7%9A%84%E6%80%A7%E8%B4%A8","text":"矩阵迹的性质"},{"source":"/notes/2021/2021.8/Normal_Equation_Proof_2_Matrix_Method","target":"/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E8%BF%B9%E7%9A%84%E6%80%A7%E8%B4%A8","text":"这里"}],"/notes/2021/2021.8/Different_Gradient_Descent_Methods":[{"source":"/notes/2021/2021.8/Linear_RegressionGradient_Descent","target":"/notes/2021/2021.8/Different_Gradient_Descent_Methods","text":"Batch Gradient Descent 批梯度下降 BGD"},{"source":"/notes/2021/2021.8/Part.15_Advanced_OptimizationML_Andrew.Ng.","target":"/notes/2021/2021.8/Different_Gradient_Descent_Methods","text":"Link:其他Gradient_Descent Different_Gradient_Descent_Methods"},{"source":"/notes/2021/2021.8/Part.5_Gradient_DescentML_Andrew.Ng.","target":"/notes/2021/2021.8/Different_Gradient_Descent_Methods","text":"Different_Gradient_Descent_Methods"},{"source":"/notes/2022/2022.1/D2L-10-%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D","target":"/notes/2021/2021.8/Different_Gradient_Descent_Methods","text":"Different_Gradient_Descent_Methods"}],"/notes/2021/2021.8/Linear_RegressionGradient_Descent":[{"source":"/notes/2021/2021.8/Part.5_Gradient_DescentML_Andrew.Ng.","target":"/notes/2021/2021.8/Linear_RegressionGradient_Descent","text":"Linear Regression \u0026 Gradient Descent"},{"source":"/notes/2021/2021.8/Part.5_Gradient_DescentML_Andrew.Ng.","target":"/notes/2021/2021.8/Linear_RegressionGradient_Descent","text":"Linear_Regression\u0026Gradient_Descent"},{"source":"/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error","target":"/notes/2021/2021.8/Linear_RegressionGradient_Descent","text":"因为平方项的2被约掉了"}],"/notes/2021/2021.8/Mean_Squared_Error_%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE":[{"source":"/notes/2021/2021.8/Part.13_Cost_Function-Logistic_RegressionML_Andrew.Ng.","target":"/notes/2021/2021.8/Mean_Squared_Error_%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE","text":"均方误差"},{"source":"/notes/2021/2021.8/Part.3_Linear_RegressionML_Andrew.Ng.","target":"/notes/2021/2021.8/Mean_Squared_Error_%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE","text":"平方误差函数/Squared Error Function/Mean Squared Error"},{"source":"/notes/2021/2021.8/Part.9_Normal_EquationML_Andrew.Ng.","target":"/notes/2021/2021.8/Mean_Squared_Error_%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE","text":"最小二乘法"},{"source":"/notes/2021/2021.8/Part.9_Normal_EquationML_Andrew.Ng.","target":"/notes/2021/2021.8/Mean_Squared_Error_%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE","text":"均方误差"},{"source":"/notes/2021/2021.9/Part.26_Probabilistic_Interpretation_of_MSEML_Andrew.Ng.","target":"/notes/2021/2021.8/Mean_Squared_Error_%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE","text":"Mean_Squared_Error-均方误差"},{"source":"/notes/2022/2022.4/D2L-77-BERT-Fine-tune","target":"/notes/2021/2021.8/Mean_Squared_Error_%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE","text":"Mean_Squared_Error_均方误差"}],"/notes/2021/2021.8/Normal_Equation_Proof_2_Matrix_Method":[{"source":"/notes/2021/2021.8/Part.9_Normal_EquationML_Andrew.Ng.","target":"/notes/2021/2021.8/Normal_Equation_Proof_2_Matrix_Method","text":"Normal_Equation_Proof_2_Matrix_Method"},{"source":"/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E7%9A%84%E6%B1%82%E5%AF%BC","target":"/notes/2021/2021.8/Normal_Equation_Proof_2_Matrix_Method","text":"Normal_Equation_Proof_2_Matrix_Method"}],"/notes/2021/2021.8/Relation_Between_Linear_RegressionGradient_Descent_%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%92%8C%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E5%85%B3%E7%B3%BB":[{"source":"/notes/2021/2021.8/Part.14_Logistic_RegressionGradient_DescentML_Andrew.Ng.","target":"/notes/2021/2021.8/Relation_Between_Linear_RegressionGradient_Descent_%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%92%8C%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E5%85%B3%E7%B3%BB","text":"Relation_Between_Linear_Regression\u0026Gradient_Descent_梯度下降和线性回归的关系"},{"source":"/notes/2021/2021.8/Part.5_Gradient_DescentML_Andrew.Ng.","target":"/notes/2021/2021.8/Relation_Between_Linear_RegressionGradient_Descent_%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%92%8C%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E5%85%B3%E7%B3%BB","text":"Relation_Between_Linear_Regression\u0026Gradient_Descent_梯度下降和线性回归的关系"}],"/notes/2021/2021.8/Sigmoid-Definition":[{"source":"/notes/2021/2021.8/Sigmoid_Function","target":"/notes/2021/2021.8/Sigmoid-Definition","text":"Sigmoid的含义是"}],"/notes/2021/2021.8/Sigmoid_Function":[{"source":"/notes/2021/2021.8/Part.14_Logistic_RegressionGradient_DescentML_Andrew.Ng.","target":"/notes/2021/2021.8/Sigmoid_Function","text":"Logistic的导函数"},{"source":"/notes/2021/2021.9/%E8%AF%81%E6%98%8ELogistic%E5%9B%9E%E5%BD%92%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%98%AF%E5%87%B8%E5%87%BD%E6%95%B0","target":"/notes/2021/2021.8/Sigmoid_Function","text":"Logistic Function"},{"source":"/notes/2022/2022.2/D2L-18-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-Activation_Functions","target":"/notes/2021/2021.8/Sigmoid_Function","text":"Sigmoid_Function"},{"source":"/notes/2022/2022.2/D2L-24-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7","target":"/notes/2021/2021.8/Sigmoid_Function","text":"$Sigmoid$ Function"},{"source":"/notes/2022/2022.2/Softmax%E5%87%BD%E6%95%B0","target":"/notes/2021/2021.8/Sigmoid_Function","text":"Sigmoid_Function"}],"/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error":[{"source":"/notes/2021/2021.8/Linear_RegressionGradient_Descent","target":"/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error","text":"Regarding the $\\frac1 2$ term"},{"source":"/notes/2021/2021.8/Mean_Squared_Error_%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE","target":"/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error","text":"Why_do_cost_functions_use_the_square_error/为什么损失函数要使用均方误差"},{"source":"/notes/2021/2021.8/Mean_Squared_Error_%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE","target":"/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error","text":"Regarding the frac1 2 term"},{"source":"/notes/2021/2021.8/Part.3_Linear_RegressionML_Andrew.Ng.","target":"/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error","text":"Why_do_cost_functions_use_the_square_error"},{"source":"/notes/2021/2021.8/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%88%86%E5%B8%83%E4%B8%8E%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E7%9A%84%E8%81%94%E7%B3%BB_Relation_of_Laplace_distribution-_and_Gaussian_distribution","target":"/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error","text":"Why_do_cost_functions_use_the_square_error"},{"source":"/notes/2021/2021.9/Part.26_Probabilistic_Interpretation_of_MSEML_Andrew.Ng.","target":"/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error","text":"Why_do_cost_functions_use_the_square_error"},{"source":"/notes/2022/2022.2/Norm-in-Regularization-Intuition","target":"/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error","text":"L1范数约束后的解很\"稀疏\", 这在特征选择时是很有用的"}],"/notes/2021/2021.9/%E6%AD%A3%E5%88%99%E9%A1%B9%E4%B8%8D%E5%BD%B1%E5%93%8D%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%87%B8%E6%80%A7":[{"source":"/notes/2021/2021.9/Part.19_Regularized_Linear_RegressionML_Andrew.Ng.","target":"/notes/2021/2021.9/%E6%AD%A3%E5%88%99%E9%A1%B9%E4%B8%8D%E5%BD%B1%E5%93%8D%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%87%B8%E6%80%A7","text":"正则项不影响线性回归损失函数的凸性"},{"source":"/notes/2022/2022.1/D2L-11-%E6%B3%9B%E5%8C%96Generalization","target":"/notes/2021/2021.9/%E6%AD%A3%E5%88%99%E9%A1%B9%E4%B8%8D%E5%BD%B1%E5%93%8D%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%87%B8%E6%80%A7","text":"正则项不影响线性回归损失函数的凸性"}],"/notes/2021/2021.9/%E6%AD%A3%E5%88%99%E9%A1%B9%E4%B8%8D%E5%BD%B1%E5%93%8DLogistic%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%87%B8%E6%80%A7":[{"source":"/notes/2021/2021.9/Part.20_Regularized_Logistic_RegressionML_Andrew.Ng.","target":"/notes/2021/2021.9/%E6%AD%A3%E5%88%99%E9%A1%B9%E4%B8%8D%E5%BD%B1%E5%93%8DLogistic%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%87%B8%E6%80%A7","text":"正则项不影响Logistic回归损失函数凸性"}],"/notes/2021/2021.9/%E6%AD%A3%E5%88%99%E9%A1%B9%E4%BC%9A%E6%B6%88%E9%99%A4%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E6%B3%95%E5%8F%AF%E8%83%BD%E7%9A%84%E4%B8%8D%E5%8F%AF%E9%80%86%E6%80%A7":[{"source":"/notes/2021/2021.9/Part.19_Regularized_Linear_RegressionML_Andrew.Ng.","target":"/notes/2021/2021.9/%E6%AD%A3%E5%88%99%E9%A1%B9%E4%BC%9A%E6%B6%88%E9%99%A4%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E6%B3%95%E5%8F%AF%E8%83%BD%E7%9A%84%E4%B8%8D%E5%8F%AF%E9%80%86%E6%80%A7","text":"notes/2021/2021.9/正则项会消除正规方程法可能的不可逆性"}],"/notes/2021/2021.9/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83_%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_Normal_Distribution-Gaussian_Distribution":[{"source":"/notes/2021/2021.12/Chi-Squared_Distribution-%E5%8D%A1%E6%96%B9%E5%88%86%E5%B8%83","target":"/notes/2021/2021.9/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83_%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_Normal_Distribution-Gaussian_Distribution","text":"正态分布_高斯分布_Normal_Distribution-Gaussian_Distribution"},{"source":"/notes/2021/2021.12/Johnson-Lindenstrauss-Lemma-Publish-Version","target":"/notes/2021/2021.9/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83_%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_Normal_Distribution-Gaussian_Distribution","text":"正态分布_高斯分布_Normal_Distribution-Gaussian_Distribution"},{"source":"/notes/2021/2021.12/%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83-Mutivariate_Gaussian","target":"/notes/2021/2021.9/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83_%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_Normal_Distribution-Gaussian_Distribution","text":"正态分布_高斯分布_Normal_Distribution-Gaussian_Distribution"},{"source":"/notes/2021/2021.9/Part.26_Probabilistic_Interpretation_of_MSEML_Andrew.Ng.","target":"/notes/2021/2021.9/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83_%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_Normal_Distribution-Gaussian_Distribution","text":"notes/2021/2021.9/正态分布_高斯分布_Normal_Distribution-Gaussian_Distribution"},{"source":"/notes/2021/2021.9/Part.26_Probabilistic_Interpretation_of_MSEML_Andrew.Ng.","target":"/notes/2021/2021.9/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83_%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_Normal_Distribution-Gaussian_Distribution","text":"正态分布的表达式"}],"/notes/2021/2021.9/%E7%8B%AC%E7%AB%8B%E5%90%8C%E5%88%86%E5%B8%83-IID":[{"source":"/notes/2021/2021.9/Part.26_Probabilistic_Interpretation_of_MSEML_Andrew.Ng.","target":"/notes/2021/2021.9/%E7%8B%AC%E7%AB%8B%E5%90%8C%E5%88%86%E5%B8%83-IID","text":"独立同分布-IID"}],"/notes/2021/2021.9/%E8%AF%81%E6%98%8ELogistic%E5%9B%9E%E5%BD%92%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%98%AF%E5%87%B8%E5%87%BD%E6%95%B0":[{"source":"/notes/2021/2021.8/Part.13_Cost_Function-Logistic_RegressionML_Andrew.Ng.","target":"/notes/2021/2021.9/%E8%AF%81%E6%98%8ELogistic%E5%9B%9E%E5%BD%92%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%98%AF%E5%87%B8%E5%87%BD%E6%95%B0","text":"证明Logistic回归的损失函数是凸函数"},{"source":"/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE","target":"/notes/2021/2021.9/%E8%AF%81%E6%98%8ELogistic%E5%9B%9E%E5%BD%92%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%98%AF%E5%87%B8%E5%87%BD%E6%95%B0","text":"证明Logistic回归的损失函数是凸函数"},{"source":"/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE","target":"/notes/2021/2021.9/%E8%AF%81%E6%98%8ELogistic%E5%9B%9E%E5%BD%92%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%98%AF%E5%87%B8%E5%87%BD%E6%95%B0","text":"详细证明见: 证明Logistic回归的损失函数是凸函数"}],"/notes/2021/2021.9/OK_should_be_in_capital-lettersor_okay":[{"source":"/notes/2021/2021.9/i.e._Meaning","target":"/notes/2021/2021.9/OK_should_be_in_capital-lettersor_okay","text":"notes/2021/2021.9/OK_should_be_in_capital letters(or_okay)"}],"/notes/2022/2022.1/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%AF%94%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%9B%B4%E9%AB%98%E6%95%88":[{"source":"/","target":"/notes/2022/2022.1/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%AF%94%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%9B%B4%E9%AB%98%E6%95%88","text":"为什么反向传播比前向传播更高效"},{"source":"/notes/2022/2022.1/D2L-7-%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC","target":"/notes/2022/2022.1/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%AF%94%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%9B%B4%E9%AB%98%E6%95%88","text":"为什么反向传播比前向传播更高效"}],"/notes/2022/2022.1/D2L-1-What_is_a_tensor":[{"source":"/","target":"/notes/2022/2022.1/D2L-1-What_is_a_tensor","text":"What_is_a_tensor"}],"/notes/2022/2022.1/D2L-10-%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D":[{"source":"/notes/2022/2022.2/Norm-in-Regularization-Intuition","target":"/notes/2022/2022.1/D2L-10-%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D","text":"小批量随机梯度下降"}],"/notes/2022/2022.1/D2L-4-%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC":[{"source":"/","target":"/notes/2022/2022.1/D2L-4-%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC","text":"D2L-4-矩阵求导"}],"/notes/2022/2022.1/D2L-5-%E6%8B%93%E5%B1%95%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99":[{"source":"/notes/2022/2022.1/D2L-6-%E8%AE%A1%E7%AE%97%E5%9B%BE","target":"/notes/2022/2022.1/D2L-5-%E6%8B%93%E5%B1%95%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99","text":"例子 线性回归"},{"source":"/notes/2022/2022.4/D2L-55-%E5%9C%A8%E6%97%B6%E9%97%B4%E4%B8%8A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD","target":"/notes/2022/2022.1/D2L-5-%E6%8B%93%E5%B1%95%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99","text":"D2L-5-拓展链式法则"}],"/notes/2022/2022.1/D2L-6-%E8%AE%A1%E7%AE%97%E5%9B%BE":[{"source":"/notes/2022/2022.1/D2L-7-%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC","target":"/notes/2022/2022.1/D2L-6-%E8%AE%A1%E7%AE%97%E5%9B%BE","text":"计算图"}],"/notes/2022/2022.1/D2L-7-%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC":[{"source":"/","target":"/notes/2022/2022.1/D2L-7-%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC","text":"自动求导"}],"/notes/2022/2022.1/Dummy_Variables":[{"source":"/","target":"/notes/2022/2022.1/Dummy_Variables","text":"Dummy_Variables"},{"source":"/notes/2022/2022.1/One-hot_Encoding-%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81","target":"/notes/2022/2022.1/Dummy_Variables","text":"notes/2022/2022.1/Dummy_Variables"}],"/notes/2022/2022.1/Einstein-Notation":[{"source":"/notes/2022/2022.1/D2L-4-%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC","target":"/notes/2022/2022.1/Einstein-Notation","text":"Einstein Notation"}],"/notes/2022/2022.1/Kronecker-delta-%E5%85%8B%E7%BD%97%E5%86%85%E5%85%8B%CE%B4%E5%87%BD%E6%95%B0":[{"source":"/notes/2022/2022.1/Dummy_Variables","target":"/notes/2022/2022.1/Kronecker-delta-%E5%85%8B%E7%BD%97%E5%86%85%E5%85%8B%CE%B4%E5%87%BD%E6%95%B0","text":"Kronecker delta"}],"/notes/2022/2022.1/One-hot_Encoding-%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81":[{"source":"/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5","target":"/notes/2022/2022.1/One-hot_Encoding-%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81","text":"One-hot_Encoding-独热编码"},{"source":"/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss","target":"/notes/2022/2022.1/One-hot_Encoding-%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81","text":"One-hot编码"},{"source":"/notes/2022/2022.6/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-%E5%91%A8%E6%9C%9F%E6%80%A7%E6%95%B0%E6%8D%AE%E6%97%B6%E9%97%B4%E7%AD%89","target":"/notes/2022/2022.1/One-hot_Encoding-%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81","text":"One-hot_Encoding-独热编码"}],"/notes/2022/2022.10/Gilbert-Strang-%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%9F%A9%E9%98%B5%E7%9F%A5%E8%AF%86":[{"source":"/","target":"/notes/2022/2022.10/Gilbert-Strang-%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%9F%A9%E9%98%B5%E7%9F%A5%E8%AF%86","text":"Gilbert Strang 深入浅出机器学习的矩阵知识"}],"/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%82%E6%95%B0%E4%B8%8D%E8%83%BD%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%BA%E5%90%8C%E4%B8%80%E4%B8%AA%E5%B8%B8%E6%95%B0":[{"source":"/notes/2022/2022.2/D2L-25-%E8%AE%A9%E8%AE%AD%E7%BB%83%E6%9B%B4%E5%8A%A0%E7%A8%B3%E5%AE%9A-Xavier%E5%88%9D%E5%A7%8B%E5%8C%96","target":"/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%82%E6%95%B0%E4%B8%8D%E8%83%BD%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%BA%E5%90%8C%E4%B8%80%E4%B8%AA%E5%B8%B8%E6%95%B0","text":"为什么参数不能初始化为同一个常数?"}],"/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE":[{"source":"/","target":"/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE","text":"为什么Softmax回归不用MSE"},{"source":"/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss","target":"/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE","text":"为什么Softmax回归不用MSE"},{"source":"/notes/2022/2022.2/%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%9B%B0%E9%9A%BE","target":"/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE","text":"为什么Softmax回归不用MSE"},{"source":"/notes/2022/2022.3/D2L-45-ResNet","target":"/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE","text":"为什么Softmax回归不用MSE"}],"/notes/2022/2022.2/%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2-Affine_Transformation":[{"source":"/notes/2022/2022.2/D2L-17-MLP-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA","target":"/notes/2022/2022.2/%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2-Affine_Transformation","text":"仿射变换"}],"/notes/2022/2022.2/%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%8F%82%E6%95%B0%E5%A4%A7%E5%B0%8F%E7%9A%84%E8%AE%A1%E7%AE%97":[{"source":"/notes/2022/2022.2/%E5%AF%B9%E4%BA%8E%E7%AD%89%E4%BB%B7%E7%9A%84%E7%BD%91%E7%BB%9C-%E5%B0%8F%E7%9A%84%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%8F%82%E6%95%B0%E6%9B%B4%E5%B0%91","target":"/notes/2022/2022.2/%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%8F%82%E6%95%B0%E5%A4%A7%E5%B0%8F%E7%9A%84%E8%AE%A1%E7%AE%97","text":"卷积层参数大小的计算"},{"source":"/notes/2022/2022.3/D2L-42-NiN","target":"/notes/2022/2022.2/%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%8F%82%E6%95%B0%E5%A4%A7%E5%B0%8F%E7%9A%84%E8%AE%A1%E7%AE%97","text":"卷积层参数大小的计算"}],"/notes/2022/2022.2/%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%9B%B0%E9%9A%BE":[{"source":"/notes/2021/2021.8/Part.4_Cost_Function_IntuitionML_Andrew.Ng.","target":"/notes/2022/2022.2/%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%9B%B0%E9%9A%BE","text":"可视化损失函数的困难"},{"source":"/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE","target":"/notes/2022/2022.2/%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%9B%B0%E9%9A%BE","text":"对于多个样本需要加上求和符号"}],"/notes/2022/2022.2/%E5%A5%BD%E7%9A%84%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%B9%E8%B4%A8":[{"source":"/","target":"/notes/2022/2022.2/%E5%A5%BD%E7%9A%84%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%B9%E8%B4%A8","text":"好的预测模型的特质"}],"/notes/2022/2022.2/%E5%AF%B9%E4%BA%8E%E7%AD%89%E4%BB%B7%E7%9A%84%E7%BD%91%E7%BB%9C-%E5%B0%8F%E7%9A%84%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%8F%82%E6%95%B0%E6%9B%B4%E5%B0%91":[{"source":"/notes/2022/2022.2/2%E4%B8%AA3x3%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%A0%86%E5%8F%A0%E5%90%8E%E7%AD%89%E4%BB%B7%E4%BA%8E%E4%B8%80%E4%B8%AA5x5%E5%8D%B7%E7%A7%AF%E6%A0%B8","target":"/notes/2022/2022.2/%E5%AF%B9%E4%BA%8E%E7%AD%89%E4%BB%B7%E7%9A%84%E7%BD%91%E7%BB%9C-%E5%B0%8F%E7%9A%84%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%8F%82%E6%95%B0%E6%9B%B4%E5%B0%91","text":"notes/2022/2022.2/对于等价的网络, 小的卷积核参数更少"}],"/notes/2022/2022.2/%E5%BD%92%E7%BA%B3%E5%81%8F%E7%BD%AE-Inductive-bias-learning-bias":[{"source":"/notes/2022/2022.2/D2L-33-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN","target":"/notes/2022/2022.2/%E5%BD%92%E7%BA%B3%E5%81%8F%E7%BD%AE-Inductive-bias-learning-bias","text":"归纳偏置 Inductive bias"},{"source":"/notes/2022/2022.5/%E7%AB%AF%E5%88%B0%E7%AB%AF%E5%AD%A6%E4%B9%A0-End_to_End_Learning-E2E","target":"/notes/2022/2022.2/%E5%BD%92%E7%BA%B3%E5%81%8F%E7%BD%AE-Inductive-bias-learning-bias","text":"归纳偏置"}],"/notes/2022/2022.2/2%E4%B8%AA3x3%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%A0%86%E5%8F%A0%E5%90%8E%E7%AD%89%E4%BB%B7%E4%BA%8E%E4%B8%80%E4%B8%AA5x5%E5%8D%B7%E7%A7%AF%E6%A0%B8":[{"source":"/notes/2022/2022.2/D2L-41-VGG","target":"/notes/2022/2022.2/2%E4%B8%AA3x3%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%A0%86%E5%8F%A0%E5%90%8E%E7%AD%89%E4%BB%B7%E4%BA%8E%E4%B8%80%E4%B8%AA5x5%E5%8D%B7%E7%A7%AF%E6%A0%B8","text":"notes/2022/2022.2/2个3x3卷积核堆叠后等价于一个5x5卷积核"},{"source":"/notes/2022/2022.2/%E5%AF%B9%E4%BA%8E%E7%AD%89%E4%BB%B7%E7%9A%84%E7%BD%91%E7%BB%9C-%E5%B0%8F%E7%9A%84%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%8F%82%E6%95%B0%E6%9B%B4%E5%B0%91","target":"/notes/2022/2022.2/2%E4%B8%AA3x3%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%A0%86%E5%8F%A0%E5%90%8E%E7%AD%89%E4%BB%B7%E4%BA%8E%E4%B8%80%E4%B8%AA5x5%E5%8D%B7%E7%A7%AF%E6%A0%B8","text":"2个3x3卷积核堆叠后等价于一个5x5卷积核"}],"/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5":[{"source":"/","target":"/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5","text":"Cross_Entropy-交叉熵"},{"source":"/notes/2021/2021.8/Part.13_Cost_Function-Logistic_RegressionML_Andrew.Ng.","target":"/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5","text":"Cross_Entropy-交叉熵"},{"source":"/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss","target":"/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5","text":"交叉熵"},{"source":"/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss","target":"/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5","text":"Cross Entropy - 交叉熵"},{"source":"/notes/2022/2022.4/D2L-53-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN","target":"/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5","text":"Cross_Entropy-交叉熵"}],"/notes/2022/2022.2/D2L-13-Softmax_Regression":[{"source":"/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5","target":"/notes/2022/2022.2/D2L-13-Softmax_Regression","text":"Softmax回归"},{"source":"/notes/2022/2022.2/D2L-17-MLP-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA","target":"/notes/2022/2022.2/D2L-13-Softmax_Regression","text":"Softmax"},{"source":"/notes/2022/2022.2/Relation_between_Softmax_and_Logistic_Regression","target":"/notes/2022/2022.2/D2L-13-Softmax_Regression","text":"Softmax回归"}],"/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss":[{"source":"/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5","target":"/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss","text":"交叉熵是怎样衡量输出和真实值的差别的呢?"},{"source":"/notes/2022/2022.2/D2L-13-Softmax_Regression","target":"/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss","text":"D2L-14-Cross Entropy as Loss"},{"source":"/notes/2022/2022.4/D2L-61-Sequence-to-Sequence-Learning-Seq2Seq","target":"/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss","text":"交叉熵损失函数"}],"/notes/2022/2022.2/D2L-17-MLP-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA":[{"source":"/","target":"/notes/2022/2022.2/D2L-17-MLP-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA","text":"MLP-多层感知机"},{"source":"/notes/2022/2022.4/D2L-53-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN","target":"/notes/2022/2022.2/D2L-17-MLP-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA","text":"MLP"}],"/notes/2022/2022.2/D2L-21-%E6%A8%A1%E5%9E%8B%E5%AE%B9%E9%87%8F":[{"source":"/notes/2022/2022.3/D2L-45-ResNet","target":"/notes/2022/2022.2/D2L-21-%E6%A8%A1%E5%9E%8B%E5%AE%B9%E9%87%8F","text":"模型容量"}],"/notes/2022/2022.2/D2L-22-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F":[{"source":"/","target":"/notes/2022/2022.2/D2L-22-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F","text":"权重衰减"},{"source":"/notes/2022/2022.2/D2L-40-AlexNet","target":"/notes/2022/2022.2/D2L-22-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F","text":"权重衰减"},{"source":"/notes/2022/2022.2/Regularization-%E6%AD%A3%E5%88%99%E5%8C%96","target":"/notes/2022/2022.2/D2L-22-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F","text":"D2L-22-权重衰减"},{"source":"/notes/2022/2022.2/%E5%A5%BD%E7%9A%84%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%B9%E8%B4%A8","target":"/notes/2022/2022.2/D2L-22-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F","text":"1995年，克里斯托弗·毕晓普证明了 具有输入噪声的训练等价于Tikhonov正则化。"}],"/notes/2022/2022.2/D2L-23-Dropout-%E4%B8%A2%E5%BC%83%E6%B3%95":[{"source":"/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%82%E6%95%B0%E4%B8%8D%E8%83%BD%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%BA%E5%90%8C%E4%B8%80%E4%B8%AA%E5%B8%B8%E6%95%B0","target":"/notes/2022/2022.2/D2L-23-Dropout-%E4%B8%A2%E5%BC%83%E6%B3%95","text":"(Dropout-丢弃法)"},{"source":"/notes/2022/2022.2/%E5%A5%BD%E7%9A%84%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%B9%E8%B4%A8","target":"/notes/2022/2022.2/D2L-23-Dropout-%E4%B8%A2%E5%BC%83%E6%B3%95","text":"Dropout"},{"source":"/notes/2022/2022.4/D2L-69-Scaled-Dot-Product-Attention","target":"/notes/2022/2022.2/D2L-23-Dropout-%E4%B8%A2%E5%BC%83%E6%B3%95","text":"Dropout"},{"source":"/notes/2022/2022.4/D2L-74-Transformer","target":"/notes/2022/2022.2/D2L-23-Dropout-%E4%B8%A2%E5%BC%83%E6%B3%95","text":"Dropout"}],"/notes/2022/2022.2/D2L-24-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7":[{"source":"/","target":"/notes/2022/2022.2/D2L-24-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7","text":"数值稳定性"},{"source":"/notes/2022/2022.2/D2L-25-%E8%AE%A9%E8%AE%AD%E7%BB%83%E6%9B%B4%E5%8A%A0%E7%A8%B3%E5%AE%9A-Xavier%E5%88%9D%E5%A7%8B%E5%8C%96","target":"/notes/2022/2022.2/D2L-24-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7","text":"梯度问题的根本原因"},{"source":"/notes/2022/2022.3/D2L-45-ResNet","target":"/notes/2022/2022.2/D2L-24-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7","text":"后一种问题"},{"source":"/notes/2022/2022.3/D2L-45-ResNet","target":"/notes/2022/2022.2/D2L-24-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7","text":"D2L-24-数值稳定性"},{"source":"/notes/2022/2022.4/D2L-55-%E5%9C%A8%E6%97%B6%E9%97%B4%E4%B8%8A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD","target":"/notes/2022/2022.2/D2L-24-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7","text":"D2L-24-数值稳定性"}],"/notes/2022/2022.2/D2L-25-%E8%AE%A9%E8%AE%AD%E7%BB%83%E6%9B%B4%E5%8A%A0%E7%A8%B3%E5%AE%9A-Xavier%E5%88%9D%E5%A7%8B%E5%8C%96":[{"source":"/","target":"/notes/2022/2022.2/D2L-25-%E8%AE%A9%E8%AE%AD%E7%BB%83%E6%9B%B4%E5%8A%A0%E7%A8%B3%E5%AE%9A-Xavier%E5%88%9D%E5%A7%8B%E5%8C%96","text":"D2L-25-让训练更加稳定-Xavier初始化"},{"source":"/notes/2022/2022.4/D2L-55-%E5%9C%A8%E6%97%B6%E9%97%B4%E4%B8%8A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD","target":"/notes/2022/2022.2/D2L-25-%E8%AE%A9%E8%AE%AD%E7%BB%83%E6%9B%B4%E5%8A%A0%E7%A8%B3%E5%AE%9A-Xavier%E5%88%9D%E5%A7%8B%E5%8C%96","text":"D2L-25-让训练更加稳定"},{"source":"/notes/2022/2022.4/D2L-55-%E5%9C%A8%E6%97%B6%E9%97%B4%E4%B8%8A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD","target":"/notes/2022/2022.2/D2L-25-%E8%AE%A9%E8%AE%AD%E7%BB%83%E6%9B%B4%E5%8A%A0%E7%A8%B3%E5%AE%9A-Xavier%E5%88%9D%E5%A7%8B%E5%8C%96","text":"梯度归一化"}],"/notes/2022/2022.2/D2L-29-Computation-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B1%82":[{"source":"/notes/2022/2022.2/D2L-27-Computation-%E5%B1%82%E5%92%8C%E5%9D%97","target":"/notes/2022/2022.2/D2L-29-Computation-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B1%82","text":"我们可以自定义一个层"}],"/notes/2022/2022.2/D2L-32-Convolution-%E5%8D%B7%E7%A7%AF":[{"source":"/","target":"/notes/2022/2022.2/D2L-32-Convolution-%E5%8D%B7%E7%A7%AF","text":"Convolution-卷积，数学与ML含义"},{"source":"/notes/2022/2022.2/D2L-37-CNN%E7%9A%84%E8%AE%A1%E7%AE%97%E5%A4%8D%E6%9D%82%E5%BA%A6","target":"/notes/2022/2022.2/D2L-32-Convolution-%E5%8D%B7%E7%A7%AF","text":"数值计算 卷积与傅里叶变换"}],"/notes/2022/2022.2/D2L-34-%E5%8D%B7%E7%A7%AF%E5%B1%82-%E5%A1%AB%E5%85%85-Padding":[{"source":"/notes/2022/2022.2/D2L-38-%E6%B1%A0%E5%8C%96%E5%B1%82-Pooling_Layer","target":"/notes/2022/2022.2/D2L-34-%E5%8D%B7%E7%A7%AF%E5%B1%82-%E5%A1%AB%E5%85%85-Padding","text":"填充"}],"/notes/2022/2022.2/D2L-35-%E5%8D%B7%E7%A7%AF%E5%B1%82-%E6%AD%A5%E5%B9%85-Stride":[{"source":"/notes/2022/2022.2/D2L-38-%E6%B1%A0%E5%8C%96%E5%B1%82-Pooling_Layer","target":"/notes/2022/2022.2/D2L-35-%E5%8D%B7%E7%A7%AF%E5%B1%82-%E6%AD%A5%E5%B9%85-Stride","text":"步幅"}],"/notes/2022/2022.2/D2L-36-1x1%E5%8D%B7%E7%A7%AF%E5%B1%82":[{"source":"/","target":"/notes/2022/2022.2/D2L-36-1x1%E5%8D%B7%E7%A7%AF%E5%B1%82","text":"1x1卷积层有什么用"},{"source":"/notes/2022/2022.3/D2L-42-NiN","target":"/notes/2022/2022.2/D2L-36-1x1%E5%8D%B7%E7%A7%AF%E5%B1%82","text":"D2L-36-1x1卷积层"},{"source":"/notes/2022/2022.3/D2L-43-GoogLeNetInception","target":"/notes/2022/2022.2/D2L-36-1x1%E5%8D%B7%E7%A7%AF%E5%B1%82","text":"1x1卷积层"}],"/notes/2022/2022.2/D2L-39-LeNet":[{"source":"/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","target":"/notes/2022/2022.2/D2L-39-LeNet","text":"LeNet"},{"source":"/notes/2022/2022.2/D2L-40-AlexNet","target":"/notes/2022/2022.2/D2L-39-LeNet","text":"LeNet"}],"/notes/2022/2022.2/D2L-40-AlexNet":[{"source":"/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","target":"/notes/2022/2022.2/D2L-40-AlexNet","text":"AlexNet"},{"source":"/notes/2022/2022.2/D2L-41-VGG","target":"/notes/2022/2022.2/D2L-40-AlexNet","text":"AlexNet"}],"/notes/2022/2022.2/D2L-41-VGG":[{"source":"/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","target":"/notes/2022/2022.2/D2L-41-VGG","text":"VGG"},{"source":"/notes/2022/2022.2/D2L-40-AlexNet","target":"/notes/2022/2022.2/D2L-41-VGG","text":"VGG"}],"/notes/2022/2022.2/Entropy-%E7%86%B5":[{"source":"/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5","target":"/notes/2022/2022.2/Entropy-%E7%86%B5","text":"熵(Entropy)"}],"/notes/2022/2022.2/KL_Divergence-KL%E6%95%A3%E5%BA%A6":[{"source":"/","target":"/notes/2022/2022.2/KL_Divergence-KL%E6%95%A3%E5%BA%A6","text":"KL_Divergence-KL散度"},{"source":"/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error","target":"/notes/2022/2022.2/KL_Divergence-KL%E6%95%A3%E5%BA%A6","text":"KL-散度（相对熵）"},{"source":"/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5","target":"/notes/2022/2022.2/KL_Divergence-KL%E6%95%A3%E5%BA%A6","text":"KL散度"},{"source":"/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss","target":"/notes/2022/2022.2/KL_Divergence-KL%E6%95%A3%E5%BA%A6","text":"KL_Divergence"},{"source":"/notes/2022/2022.6/JS%E6%95%A3%E5%BA%A6","target":"/notes/2022/2022.2/KL_Divergence-KL%E6%95%A3%E5%BA%A6","text":"Kullback–Leibler divergence"}],"/notes/2022/2022.2/Likelihood_Function-%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0":[{"source":"/notes/2021/2021.12/Maximum_Likelihood_Estimation-%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1","target":"/notes/2022/2022.2/Likelihood_Function-%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0","text":"Likelihood_Function-似然函数"}],"/notes/2022/2022.2/Logit":[{"source":"/","target":"/notes/2022/2022.2/Logit","text":"What on earth is Logit"},{"source":"/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss","target":"/notes/2022/2022.2/Logit","text":"Logit"},{"source":"/notes/2022/2022.3/D2L-42-NiN","target":"/notes/2022/2022.2/Logit","text":"Logit"}],"/notes/2022/2022.2/Norm-in-Regularization-Intuition":[{"source":"/","target":"/notes/2022/2022.2/Norm-in-Regularization-Intuition","text":"Norm in Regularization - Intuition"},{"source":"/notes/2022/2022.2/D2L-22-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F","target":"/notes/2022/2022.2/Norm-in-Regularization-Intuition","text":"Norm in Regularization - Intuition"}],"/notes/2022/2022.2/Regularization-%E6%AD%A3%E5%88%99%E5%8C%96":[{"source":"/notes/2022/2022.2/D2L-22-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F","target":"/notes/2022/2022.2/Regularization-%E6%AD%A3%E5%88%99%E5%8C%96","text":"正则化"},{"source":"/notes/2022/2022.4/D2L-55-%E5%9C%A8%E6%97%B6%E9%97%B4%E4%B8%8A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD","target":"/notes/2022/2022.2/Regularization-%E6%AD%A3%E5%88%99%E5%8C%96","text":"正则化"}],"/notes/2022/2022.2/Relation_between_Softmax_and_Logistic_Regression":[{"source":"/notes/2022/2022.2/D2L-13-Softmax_Regression","target":"/notes/2022/2022.2/Relation_between_Softmax_and_Logistic_Regression","text":"Relation_between_Softmax_and_Logistic_Regression"}],"/notes/2022/2022.2/Softmax%E5%87%BD%E6%95%B0":[{"source":"/notes/2022/2022.2/D2L-13-Softmax_Regression","target":"/notes/2022/2022.2/Softmax%E5%87%BD%E6%95%B0","text":"Softmax函数"}],"/notes/2022/2022.2/Softmax_Regression_is_Over-parameterized":[{"source":"/notes/2022/2022.2/Relation_between_Softmax_and_Logistic_Regression","target":"/notes/2022/2022.2/Softmax_Regression_is_Over-parameterized","text":"Softmax_Regression_is_Over-parameterized"},{"source":"/notes/2022/2022.2/Softmax%E5%87%BD%E6%95%B0","target":"/notes/2022/2022.2/Softmax_Regression_is_Over-parameterized","text":"notes/2022/2022.2/Softmax_Regression_is_Over-parameterized"}],"/notes/2022/2022.2/VC%E7%BB%B4-VC_Dimension":[{"source":"/notes/2022/2022.2/D2L-21-%E6%A8%A1%E5%9E%8B%E5%AE%B9%E9%87%8F","target":"/notes/2022/2022.2/VC%E7%BB%B4-VC_Dimension","text":"VC维"}],"/notes/2022/2022.2/Xavier%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E8%AF%A6%E7%BB%86%E4%BE%8B%E5%AD%90":[{"source":"/notes/2022/2022.2/D2L-25-%E8%AE%A9%E8%AE%AD%E7%BB%83%E6%9B%B4%E5%8A%A0%E7%A8%B3%E5%AE%9A-Xavier%E5%88%9D%E5%A7%8B%E5%8C%96","target":"/notes/2022/2022.2/Xavier%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E8%AF%A6%E7%BB%86%E4%BE%8B%E5%AD%90","text":"Xavier初始化的详细例子"}],"/notes/2022/2022.3/D2L-42-NiN":[{"source":"/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","target":"/notes/2022/2022.3/D2L-42-NiN","text":"NiN"},{"source":"/notes/2022/2022.2/D2L-40-AlexNet","target":"/notes/2022/2022.3/D2L-42-NiN","text":"NiN"},{"source":"/notes/2022/2022.3/D2L-43-GoogLeNetInception","target":"/notes/2022/2022.3/D2L-42-NiN","text":"NiN"},{"source":"/notes/2022/2022.3/D2L-45-ResNet","target":"/notes/2022/2022.3/D2L-42-NiN","text":"D2L-42-NiN"}],"/notes/2022/2022.3/D2L-43-GoogLeNetInception":[{"source":"/notes/2022/2022.2/D2L-40-AlexNet","target":"/notes/2022/2022.3/D2L-43-GoogLeNetInception","text":"GoogLeNet(Inception)"},{"source":"/notes/2022/2022.3/D2L-45-ResNet","target":"/notes/2022/2022.3/D2L-43-GoogLeNetInception","text":"D2L-43-GoogLeNet(Inception)"}],"/notes/2022/2022.3/D2L-44-Batch_Normalization-%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96":[{"source":"/notes/2022/2022.4/D2L-74-Transformer","target":"/notes/2022/2022.3/D2L-44-Batch_Normalization-%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96","text":"Batch_Normalization-批量归一化"}],"/notes/2022/2022.3/D2L-45-ResNet":[{"source":"/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","target":"/notes/2022/2022.3/D2L-45-ResNet","text":"ResNet"},{"source":"/notes/2022/2022.2/D2L-40-AlexNet","target":"/notes/2022/2022.3/D2L-45-ResNet","text":"ResNet"},{"source":"/notes/2022/2022.4/D2L-74-Transformer","target":"/notes/2022/2022.3/D2L-45-ResNet","text":"ResNet"}],"/notes/2022/2022.3/D2L-46-DenseNet":[{"source":"/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","target":"/notes/2022/2022.3/D2L-46-DenseNet","text":"DenseNet"}],"/notes/2022/2022.3/D2L-48-%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B-Sequence_Models":[{"source":"/notes/2022/2022.4/D2L-53-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN","target":"/notes/2022/2022.3/D2L-48-%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B-Sequence_Models","text":"隐变量自回归模型"},{"source":"/notes/2022/2022.4/D2L-59-%E5%8F%8C%E5%90%91%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C","target":"/notes/2022/2022.3/D2L-48-%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B-Sequence_Models","text":"之前的笔记 - 序列模型-Sequence_Models"}],"/notes/2022/2022.3/D2L-50-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-%E4%BC%A0%E7%BB%9F%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8D%E8%B6%B3":[{"source":"/notes/2022/2022.3/D2L-51-%E8%AF%AD%E8%A8%80%E7%9A%84%E7%BB%9F%E8%AE%A1%E7%89%B9%E5%BE%81","target":"/notes/2022/2022.3/D2L-50-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-%E4%BC%A0%E7%BB%9F%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8D%E8%B6%B3","text":"拉普拉斯平滑"},{"source":"/notes/2022/2022.4/D2L-76-BERT-Pretrain","target":"/notes/2022/2022.3/D2L-50-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-%E4%BC%A0%E7%BB%9F%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8D%E8%B6%B3","text":"语言模型(Language Model)"}],"/notes/2022/2022.4/%E5%87%B8%E7%BB%84%E5%90%88-Convex-Combination":[{"source":"/notes/2022/2022.4/D2L-56-%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83GRU","target":"/notes/2022/2022.4/%E5%87%B8%E7%BB%84%E5%90%88-Convex-Combination","text":"凸组合 - Convex Combination"},{"source":"/notes/2022/2022.4/D2L-57-LSTM-%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C","target":"/notes/2022/2022.4/%E5%87%B8%E7%BB%84%E5%90%88-Convex-Combination","text":"凸组合 - Convex Combination"}],"/notes/2022/2022.4/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CFeedforward-neural-network":[{"source":"/notes/2022/2022.4/D2L-53-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN","target":"/notes/2022/2022.4/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CFeedforward-neural-network","text":"notes/2022/2022.4/前馈神经网络(Feedforward neural network)"}],"/notes/2022/2022.4/%E9%80%92%E6%8E%A8%E5%85%AC%E5%BC%8F-a_tb_t+c_ta_t-1-%E8%BD%AC%E9%80%9A%E9%A1%B9%E5%85%AC%E5%BC%8F":[{"source":"/notes/2022/2022.4/D2L-55-%E5%9C%A8%E6%97%B6%E9%97%B4%E4%B8%8A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD","target":"/notes/2022/2022.4/%E9%80%92%E6%8E%A8%E5%85%AC%E5%BC%8F-a_tb_t+c_ta_t-1-%E8%BD%AC%E9%80%9A%E9%A1%B9%E5%85%AC%E5%BC%8F","text":"notes/2022/2022.4/递推公式 $a_{t}=b_{t}+c_{t}a_{t-1}$ 转通项公式"}],"/notes/2022/2022.4/D2L-53-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN":[{"source":"/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","target":"/notes/2022/2022.4/D2L-53-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN","text":"RNN"},{"source":"/notes/2022/2022.4/D2L-61-Sequence-to-Sequence-Learning-Seq2Seq","target":"/notes/2022/2022.4/D2L-53-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN","text":"困惑度 Perplexity"}],"/notes/2022/2022.4/D2L-54-Gradient-Clipping-%E6%A2%AF%E5%BA%A6%E5%89%AA%E8%A3%81":[{"source":"/notes/2022/2022.2/D2L-25-%E8%AE%A9%E8%AE%AD%E7%BB%83%E6%9B%B4%E5%8A%A0%E7%A8%B3%E5%AE%9A-Xavier%E5%88%9D%E5%A7%8B%E5%8C%96","target":"/notes/2022/2022.4/D2L-54-Gradient-Clipping-%E6%A2%AF%E5%BA%A6%E5%89%AA%E8%A3%81","text":"Gradient Clipping-梯度剪裁"}],"/notes/2022/2022.4/D2L-56-%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83GRU":[{"source":"/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","target":"/notes/2022/2022.4/D2L-56-%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83GRU","text":"GRU"},{"source":"/notes/2022/2022.4/D2L-57-LSTM-%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C","target":"/notes/2022/2022.4/D2L-56-%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83GRU","text":"D2L-56-门控循环单元GRU"}],"/notes/2022/2022.4/D2L-57-LSTM-%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C":[{"source":"/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","target":"/notes/2022/2022.4/D2L-57-LSTM-%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C","text":"LSTM"}],"/notes/2022/2022.4/D2L-60-Encoder-Decoder":[{"source":"/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","target":"/notes/2022/2022.4/D2L-60-Encoder-Decoder","text":"Encoder-Decoder"},{"source":"/notes/2022/2022.4/D2L-61-Sequence-to-Sequence-Learning-Seq2Seq","target":"/notes/2022/2022.4/D2L-60-Encoder-Decoder","text":"Encoder-Decoder"},{"source":"/notes/2022/2022.4/D2L-74-Transformer","target":"/notes/2022/2022.4/D2L-60-Encoder-Decoder","text":"Encoder Decoder"}],"/notes/2022/2022.4/D2L-61-Sequence-to-Sequence-Learning-Seq2Seq":[{"source":"/notes/2022/2022.4/D2L-63-Beam-Search","target":"/notes/2022/2022.4/D2L-61-Sequence-to-Sequence-Learning-Seq2Seq","text":"Seq2Seq"},{"source":"/notes/2022/2022.4/D2L-70-Seq2Seq-with-Attention-Bahdanau-Attention","target":"/notes/2022/2022.4/D2L-61-Sequence-to-Sequence-Learning-Seq2Seq","text":"Seq2Seq"}],"/notes/2022/2022.4/D2L-62-BLEU-Bilingual-Evaluation-Understudy":[{"source":"/notes/2022/2022.4/D2L-61-Sequence-to-Sequence-Learning-Seq2Seq","target":"/notes/2022/2022.4/D2L-62-BLEU-Bilingual-Evaluation-Understudy","text":"D2L-62-BLEU (Bilingual Evaluation Understudy)"},{"source":"/notes/2022/2022.4/D2L-63-Beam-Search","target":"/notes/2022/2022.4/D2L-62-BLEU-Bilingual-Evaluation-Understudy","text":"BLEU"}],"/notes/2022/2022.4/D2L-64-Kernel-Regression":[{"source":"/","target":"/notes/2022/2022.4/D2L-64-Kernel-Regression","text":"Kernel Regression"},{"source":"/notes/2022/2022.4/D2L-66-Kernel-Regression-and-Attention","target":"/notes/2022/2022.4/D2L-64-Kernel-Regression","text":"Kernel Regression 里面的采用高斯核的时候"}],"/notes/2022/2022.4/D2L-66-Kernel-Regression-and-Attention":[{"source":"/","target":"/notes/2022/2022.4/D2L-66-Kernel-Regression-and-Attention","text":"Kernel Regression and Attention"},{"source":"/notes/2022/2022.4/D2L-64-Kernel-Regression","target":"/notes/2022/2022.4/D2L-66-Kernel-Regression-and-Attention","text":"notes/2022/2022.4/D2L-66-Kernel Regression and Attention"}],"/notes/2022/2022.4/D2L-67-Attention-Scoring-Function":[{"source":"/","target":"/notes/2022/2022.4/D2L-67-Attention-Scoring-Function","text":"Attention Scoring Function"},{"source":"/notes/2022/2022.4/D2L-71-Multi-Head_Attention","target":"/notes/2022/2022.4/D2L-67-Attention-Scoring-Function","text":"Attention Scoring Function"}],"/notes/2022/2022.4/D2L-68-Additive-Attention":[{"source":"/","target":"/notes/2022/2022.4/D2L-68-Additive-Attention","text":"图解Additive Attention"},{"source":"/notes/2022/2022.4/D2L-67-Attention-Scoring-Function","target":"/notes/2022/2022.4/D2L-68-Additive-Attention","text":"D2L-68-Additive Attention"},{"source":"/notes/2022/2022.4/D2L-70-Seq2Seq-with-Attention-Bahdanau-Attention","target":"/notes/2022/2022.4/D2L-68-Additive-Attention","text":"Additive Attention"}],"/notes/2022/2022.4/D2L-69-Scaled-Dot-Product-Attention":[{"source":"/","target":"/notes/2022/2022.4/D2L-69-Scaled-Dot-Product-Attention","text":"Scaled Dot-Product Attention"},{"source":"/notes/2022/2022.4/D2L-67-Attention-Scoring-Function","target":"/notes/2022/2022.4/D2L-69-Scaled-Dot-Product-Attention","text":"D2L-69-Scaled Dot-Product Attention"}],"/notes/2022/2022.4/D2L-70-Seq2Seq-with-Attention-Bahdanau-Attention":[{"source":"/","target":"/notes/2022/2022.4/D2L-70-Seq2Seq-with-Attention-Bahdanau-Attention","text":"Seq2Seq with Attention - Bahdanau Attention"}],"/notes/2022/2022.4/D2L-71-Multi-Head_Attention":[{"source":"/","target":"/notes/2022/2022.4/D2L-71-Multi-Head_Attention","text":"图解Multi-Head_Attention"},{"source":"/notes/2022/2022.4/D2L-74-Transformer","target":"/notes/2022/2022.4/D2L-71-Multi-Head_Attention","text":"多头注意力"}],"/notes/2022/2022.4/D2L-72-Self-Attention":[{"source":"/","target":"/notes/2022/2022.4/D2L-72-Self-Attention","text":"图解Self-Attention"},{"source":"/notes/2022/2022.4/D2L-74-Transformer","target":"/notes/2022/2022.4/D2L-72-Self-Attention","text":"Self-Attention"},{"source":"/notes/2022/2022.4/D2L-74-Transformer","target":"/notes/2022/2022.4/D2L-72-Self-Attention","text":"自注意力忽略了原始序列的位置信息"}],"/notes/2022/2022.4/D2L-73-Positional_Encoding":[{"source":"/notes/2022/2022.4/D2L-72-Self-Attention","target":"/notes/2022/2022.4/D2L-73-Positional_Encoding","text":"D2L-73-Positional_Encoding"}],"/notes/2022/2022.4/D2L-74-Transformer":[{"source":"/","target":"/notes/2022/2022.4/D2L-74-Transformer","text":"智能版加权平均 is All You Need: 图解Transformer"}],"/notes/2022/2022.4/D2L-75-BERT":[{"source":"/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","target":"/notes/2022/2022.4/D2L-75-BERT","text":"BERT"}],"/notes/2022/2022.4/D2L-77-BERT-Fine-tune":[{"source":"/notes/2022/2022.4/D2L-75-BERT","target":"/notes/2022/2022.4/D2L-77-BERT-Fine-tune","text":"D2L-77-BERT - Fine-tune"}],"/notes/2022/2022.4/RNN%E4%B8%ADoutput%E5%92%8Chidden_state%E7%9A%84%E5%8C%BA%E5%88%AB":[{"source":"/notes/2022/2022.4/D2L-53-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN","target":"/notes/2022/2022.4/RNN%E4%B8%ADoutput%E5%92%8Chidden_state%E7%9A%84%E5%8C%BA%E5%88%AB","text":"RNN中output和hidden_state的区别"}],"/notes/2022/2022.4/Viterbi-Algorithm":[{"source":"/notes/2022/2022.4/D2L-63-Beam-Search","target":"/notes/2022/2022.4/Viterbi-Algorithm","text":"Viterbi Algorithm"}],"/notes/2022/2022.5/%E4%BB%8E%E4%BA%8C%E9%A1%B9%E5%88%86%E5%B8%83%E5%88%B0%E6%B3%8A%E6%9D%BE%E5%88%86%E5%B8%83%E5%86%8D%E5%88%B0%E6%8C%87%E6%95%B0%E5%88%86%E5%B8%83-From-Binomial-Distribution-to-Poisson-Distribution-to-Exponential-Distribution":[{"source":"/","target":"/notes/2022/2022.5/%E4%BB%8E%E4%BA%8C%E9%A1%B9%E5%88%86%E5%B8%83%E5%88%B0%E6%B3%8A%E6%9D%BE%E5%88%86%E5%B8%83%E5%86%8D%E5%88%B0%E6%8C%87%E6%95%B0%E5%88%86%E5%B8%83-From-Binomial-Distribution-to-Poisson-Distribution-to-Exponential-Distribution","text":"从二项分布到泊松分布再到指数分布-From Binomial Distribution to Poisson Distribution to Exponential Distribution"}],"/notes/2022/2022.5/%E7%AB%AF%E5%88%B0%E7%AB%AF%E5%AD%A6%E4%B9%A0-End_to_End_Learning-E2E":[{"source":"/","target":"/notes/2022/2022.5/%E7%AB%AF%E5%88%B0%E7%AB%AF%E5%AD%A6%E4%B9%A0-End_to_End_Learning-E2E","text":"端到端学习-End_to_End_Learning-E2E"}],"/notes/2022/2022.5/Cross_Entropy_Loss_Input_Format-%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%BE%93%E5%85%A5%E6%A0%BC%E5%BC%8F":[{"source":"/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss","target":"/notes/2022/2022.5/Cross_Entropy_Loss_Input_Format-%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%BE%93%E5%85%A5%E6%A0%BC%E5%BC%8F","text":"Cross_Entropy_Loss_Input_Format-交叉熵损失函数输入格式"}],"/notes/2022/2022.5/Harmonic_Mean-%E8%B0%83%E5%92%8C%E5%B9%B3%E5%9D%87%E6%95%B0":[{"source":"/notes/2022/2022.4/D2L-53-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN","target":"/notes/2022/2022.5/Harmonic_Mean-%E8%B0%83%E5%92%8C%E5%B9%B3%E5%9D%87%E6%95%B0","text":"调和平均数(Harmonic mean)"},{"source":"/notes/2022/2022.5/F1_Score","target":"/notes/2022/2022.5/Harmonic_Mean-%E8%B0%83%E5%92%8C%E5%B9%B3%E5%9D%87%E6%95%B0","text":"Harmonic_Mean-调和平均数"}],"/notes/2022/2022.5/Precision-Recall-and-Accuracy":[{"source":"/notes/2022/2022.5/F1_Score","target":"/notes/2022/2022.5/Precision-Recall-and-Accuracy","text":"Precision Recall"}],"/notes/2022/2022.7/%E6%9C%89%E6%95%88%E5%9C%B0%E8%83%8C%E8%AF%B5GRE%E5%8D%95%E8%AF%8D":[{"source":"/","target":"/notes/2022/2022.7/%E6%9C%89%E6%95%88%E5%9C%B0%E8%83%8C%E8%AF%B5GRE%E5%8D%95%E8%AF%8D","text":"有效地背诵GRE单词"}]}},"links":[{"source":"/","target":"/notes/2021/2021.12/Johnson-Lindenstrauss-Lemma-Publish-Version","text":"Johnson Lindenstrauss Lemma"},{"source":"/","target":"/notes/2022/2022.5/%E4%BB%8E%E4%BA%8C%E9%A1%B9%E5%88%86%E5%B8%83%E5%88%B0%E6%B3%8A%E6%9D%BE%E5%88%86%E5%B8%83%E5%86%8D%E5%88%B0%E6%8C%87%E6%95%B0%E5%88%86%E5%B8%83-From-Binomial-Distribution-to-Poisson-Distribution-to-Exponential-Distribution","text":"从二项分布到泊松分布再到指数分布-From Binomial Distribution to Poisson Distribution to Exponential Distribution"},{"source":"/","target":"/notes/2021/2021.10/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%96%B9%E5%B7%AE%E7%9A%84%E5%88%86%E6%AF%8D%E5%B8%B8%E5%B8%B8%E6%98%AFn-1","text":"为什么方差的分母常常是n-1"},{"source":"/","target":"/notes/2021/2021.11/%E9%85%89%E7%9F%A9%E9%98%B5%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%AB%E9%85%89%E7%9F%A9%E9%98%B5","text":"酉矩阵为什么叫酉矩阵"},{"source":"/","target":"/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E7%9A%84%E6%B1%82%E5%AF%BC","text":"矩阵的求导"},{"source":"/","target":"/notes/2022/2022.1/D2L-4-%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC","text":"D2L-4-矩阵求导"},{"source":"/","target":"/notes/2021/2021.6/Diffie-Hellman%E9%97%AE%E9%A2%98","text":"Diffie-Hellman问题"},{"source":"/","target":"/notes/2022/2022.1/Dummy_Variables","text":"Dummy_Variables"},{"source":"/","target":"/notes/2022/2022.2/KL_Divergence-KL%E6%95%A3%E5%BA%A6","text":"KL_Divergence-KL散度"},{"source":"/","target":"/notes/2022/2022.10/Gilbert-Strang-%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%9F%A9%E9%98%B5%E7%9F%A5%E8%AF%86","text":"Gilbert Strang 深入浅出机器学习的矩阵知识"},{"source":"/","target":"/notes/2022/2022.2/Logit","text":"What on earth is Logit"},{"source":"/","target":"/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE","text":"为什么Softmax回归不用MSE"},{"source":"/","target":"/notes/2022/2022.4/D2L-64-Kernel-Regression","text":"Kernel Regression"},{"source":"/","target":"/notes/2022/2022.2/Norm-in-Regularization-Intuition","text":"Norm in Regularization - Intuition"},{"source":"/","target":"/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5","text":"Cross_Entropy-交叉熵"},{"source":"/","target":"/notes/2022/2022.2/D2L-32-Convolution-%E5%8D%B7%E7%A7%AF","text":"Convolution-卷积，数学与ML含义"},{"source":"/","target":"/notes/2022/2022.2/D2L-36-1x1%E5%8D%B7%E7%A7%AF%E5%B1%82","text":"1x1卷积层有什么用"},{"source":"/","target":"/notes/2022/2022.4/D2L-66-Kernel-Regression-and-Attention","text":"Kernel Regression and Attention"},{"source":"/","target":"/notes/2022/2022.4/D2L-67-Attention-Scoring-Function","text":"Attention Scoring Function"},{"source":"/","target":"/notes/2022/2022.4/D2L-68-Additive-Attention","text":"图解Additive Attention"},{"source":"/","target":"/notes/2022/2022.4/D2L-69-Scaled-Dot-Product-Attention","text":"Scaled Dot-Product Attention"},{"source":"/","target":"/notes/2022/2022.4/D2L-70-Seq2Seq-with-Attention-Bahdanau-Attention","text":"Seq2Seq with Attention - Bahdanau Attention"},{"source":"/","target":"/notes/2022/2022.4/D2L-71-Multi-Head_Attention","text":"图解Multi-Head_Attention"},{"source":"/","target":"/notes/2022/2022.4/D2L-72-Self-Attention","text":"图解Self-Attention"},{"source":"/","target":"/notes/2022/2022.4/D2L-74-Transformer","text":"智能版加权平均 is All You Need: 图解Transformer"},{"source":"/","target":"/notes/2022/2022.5/%E7%AB%AF%E5%88%B0%E7%AB%AF%E5%AD%A6%E4%B9%A0-End_to_End_Learning-E2E","text":"端到端学习-End_to_End_Learning-E2E"},{"source":"/","target":"/notes/2022/2022.2/%E5%A5%BD%E7%9A%84%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%B9%E8%B4%A8","text":"好的预测模型的特质"},{"source":"/","target":"/notes/2022/2022.1/D2L-1-What_is_a_tensor","text":"What_is_a_tensor"},{"source":"/","target":"/notes/2022/2022.1/D2L-7-%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC","text":"自动求导"},{"source":"/","target":"/notes/2022/2022.1/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%AF%94%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%9B%B4%E9%AB%98%E6%95%88","text":"为什么反向传播比前向传播更高效"},{"source":"/","target":"/notes/2022/2022.2/D2L-17-MLP-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA","text":"MLP-多层感知机"},{"source":"/","target":"/notes/2022/2022.2/D2L-22-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F","text":"权重衰减"},{"source":"/","target":"/notes/2022/2022.2/D2L-24-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7","text":"数值稳定性"},{"source":"/","target":"/notes/2022/2022.2/D2L-25-%E8%AE%A9%E8%AE%AD%E7%BB%83%E6%9B%B4%E5%8A%A0%E7%A8%B3%E5%AE%9A-Xavier%E5%88%9D%E5%A7%8B%E5%8C%96","text":"D2L-25-让训练更加稳定-Xavier初始化"},{"source":"/","target":"/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","text":"经典模型"},{"source":"/","target":"/notes/2022/2022.7/%E6%9C%89%E6%95%88%E5%9C%B0%E8%83%8C%E8%AF%B5GRE%E5%8D%95%E8%AF%8D","text":"有效地背诵GRE单词"},{"source":"/","target":"/notes/2021/2021.8/%E5%A6%82%E4%BD%95%E5%8A%AA%E5%8A%9B%E5%B7%A5%E4%BD%9C_Paul_Graham","text":"如何努力工作_Paul_Graham"},{"source":"/notes/2021/2021.10/Compiler-2_Bottom-Up_Parsing-%E8%87%AA%E5%BA%95%E5%90%91%E4%B8%8A%E5%88%86%E6%9E%90","target":"/notes/2021/2021.10/Compiler-3_%E7%AE%97%E7%AC%A6%E4%BC%98%E5%85%88%E5%88%86%E6%9E%90","text":"\"算符优先分析\""},{"source":"/notes/2021/2021.10/Compiler-3_%E7%AE%97%E7%AC%A6%E4%BC%98%E5%85%88%E5%88%86%E6%9E%90","target":"/","text":"为什么?"},{"source":"/notes/2021/2021.10/Compiler-4_LR%E5%88%86%E6%9E%90","target":"/notes/2021/2021.10/Compiler-4-1_%E4%BB%80%E4%B9%88%E6%98%AF-LR-%E5%88%86%E6%9E%90","text":"Compiler-4-1_什么是 LR 分析"},{"source":"/notes/2021/2021.10/Compiler-4_LR%E5%88%86%E6%9E%90","target":"/notes/2021/2021.10/Compiler-4-2_LR0_Parse","text":"Compiler-4-2_LR(0)_Parse"},{"source":"/notes/2021/2021.10/Compiler-4_LR%E5%88%86%E6%9E%90","target":"/notes/2021/2021.10/Compiler-4-3_SLR_parse","text":"Compiler-4-3_SLR_parse"},{"source":"/notes/2021/2021.10/Compiler-4_LR%E5%88%86%E6%9E%90","target":"/notes/2021/2021.10/Compiler-4-4_LR1_%E5%88%86%E6%9E%90","text":"Compiler-4-4_LR(1)_分析"},{"source":"/notes/2021/2021.10/Compiler-4_LR%E5%88%86%E6%9E%90","target":"/notes/2021/2021.10/Compiler-4-5_LALR1","text":"Compiler-4-5_LALR(1)"},{"source":"/notes/2021/2021.10/Part.29_Fisher_Linear_DiscriminantPattern_Classification-Chapter_4","target":"/notes/2021/2021.10/Dot_Product_and_Linear_Transformation-%E5%90%91%E9%87%8F%E5%86%85%E7%A7%AF%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2","text":"Dot_Product_and_Linear_Transformation-向量内积与线性变换"},{"source":"/notes/2021/2021.10/Part.29_Fisher_Linear_DiscriminantPattern_Classification-Chapter_4","target":"/notes/2021/2021.10/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5_Covariance_Matrix","text":"协方差矩阵_Covariance_Matrix"},{"source":"/notes/2021/2021.10/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5_Covariance_Matrix","target":"/notes/2021/2021.10/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%96%B9%E5%B7%AE%E7%9A%84%E5%88%86%E6%AF%8D%E5%B8%B8%E5%B8%B8%E6%98%AFn-1","text":"为什么方差的分母常常是n-1"},{"source":"/notes/2021/2021.10/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5_Covariance_Matrix","target":"/notes/2021/2021.12/Covariance-%E5%8D%8F%E6%96%B9%E5%B7%AE","text":"covariance"},{"source":"/notes/2021/2021.11/MIT_18.065-Part_2-Matrix_Factorization","target":"/notes/2021/2021.11/LU%E5%88%86%E8%A7%A3%E7%9A%84%E4%BE%8B%E5%AD%90","text":"LU分解的例子"},{"source":"/notes/2021/2021.11/MIT_18.065-Part_4-LU_Factorization","target":"/notes/2021/2021.11/LU%E5%88%86%E8%A7%A3%E7%9A%84%E4%BE%8B%E5%AD%90","text":"LU分解的例子"},{"source":"/notes/2021/2021.11/MIT_18.065-Part_7-Eigenvalues-and-Eigenvectors","target":"/notes/2021/2021.11/%E7%90%86%E8%A7%A3%E7%9B%B8%E4%BC%BC%E7%9F%A9%E9%98%B5","text":"notes/2021/2021.11/理解相似矩阵"},{"source":"/notes/2021/2021.11/MIT_18.065-Part_9-Singular-Value-Decomposition-SVD","target":"/notes/2021/2021.11/%E5%85%B3%E4%BA%8E%E7%89%B9%E5%BE%81%E5%80%BC%E7%9A%84%E4%B8%80%E4%B8%AA%E7%BB%93%E8%AE%BA","text":"关于特征值的一个结论"},{"source":"/notes/2021/2021.11/MIT_18.065-Part_9-Singular-Value-Decomposition-SVD","target":"/notes/2021/2021.11/%E5%85%B3%E4%BA%8E%E7%A7%A9%E7%9A%84%E4%B8%80%E4%B8%AA%E7%BB%93%E8%AE%BA","text":"关于秩的一个结论"},{"source":"/notes/2021/2021.11/MIT_18.065-Part_9-Singular-Value-Decomposition-SVD","target":"/notes/2021/2021.11/SVD-Intuition","text":"SVD Intuition"},{"source":"/notes/2021/2021.11/MIT_18.065-Part_9-Singular-Value-Decomposition-SVD","target":"/notes/2021/2021.11/A-Fancy-Example-of-SVD","text":"notes/2021/2021.11/A Fancy Example of SVD"},{"source":"/notes/2021/2021.11/MIT_18.065-Part_9-Singular-Value-Decomposition-SVD","target":"/notes/2021/2021.11/SVD-Intuition","text":"SVD Intuition"},{"source":"/notes/2021/2021.11/OS-11_%E4%B8%AD%E6%96%AD","target":"/../../../1_Project/2021.6_CPU/CP0_CoProcessor0_in_CPU","text":"CP0_CoProcessor0_in_CPU"},{"source":"/notes/2021/2021.12/Bayesian-Decision-Theory-Part1","target":"/notes/2021/2021.12/Understanding-Bayes-Theorem","text":"根据Bayes定理"},{"source":"/notes/2021/2021.12/Chi-Squared_Distribution-%E5%8D%A1%E6%96%B9%E5%88%86%E5%B8%83","target":"/notes/2021/2021.9/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83_%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_Normal_Distribution-Gaussian_Distribution","text":"正态分布_高斯分布_Normal_Distribution-Gaussian_Distribution"},{"source":"/notes/2021/2021.12/Johnson-Lindenstrauss-Lemma-Publish-Version","target":"/notes/2021/2021.12/Union_Bound-%E5%B8%83%E5%B0%94%E4%B8%8D%E7%AD%89%E5%BC%8F-Booles_inequality","text":"notes/2021/2021.12/Union_Bound-布尔不等式-Boole's_inequality"},{"source":"/notes/2021/2021.12/Johnson-Lindenstrauss-Lemma-Publish-Version","target":"/notes/2021/2021.9/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83_%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_Normal_Distribution-Gaussian_Distribution","text":"正态分布_高斯分布_Normal_Distribution-Gaussian_Distribution"},{"source":"/notes/2021/2021.12/Johnson-Lindenstrauss-Lemma-Publish-Version","target":"/notes/2021/2021.12/Chi-Squared_Distribution-%E5%8D%A1%E6%96%B9%E5%88%86%E5%B8%83","text":"notes/2021/2021.12/Chi-Squared_Distribution-卡方分布"},{"source":"/notes/2021/2021.12/Johnson-Lindenstrauss-Lemma-Publish-Version","target":"/notes/2021/2021.12/Chernoff-Bounds","text":"Chernoff Bounds"},{"source":"/notes/2021/2021.12/Johnson-Lindenstrauss-Lemma-Publish-Version","target":"/notes/2021/2021.12/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E5%87%BD%E6%95%B0%E7%9A%84%E6%9C%9F%E6%9C%9B","text":"关于随机变量函数的期望"},{"source":"/notes/2021/2021.12/Johnson-Lindenstrauss-Lemma-Publish-Version","target":"/notes/2021/2021.12/Chernoff-Bounds","text":"Chernoff Bounds"},{"source":"/notes/2021/2021.12/Maximum_Likelihood_Estimation-%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1","target":"/notes/2022/2022.2/Likelihood_Function-%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0","text":"Likelihood_Function-似然函数"},{"source":"/notes/2021/2021.12/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1-Parameter_Estimation","target":"/notes/2021/2021.12/Maximum_Likelihood_Estimation-%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1","text":"Maximum_Likelihood_Estimation-极大似然估计"},{"source":"/notes/2021/2021.12/%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83-Mutivariate_Gaussian","target":"/notes/2021/2021.9/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83_%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_Normal_Distribution-Gaussian_Distribution","text":"正态分布_高斯分布_Normal_Distribution-Gaussian_Distribution"},{"source":"/notes/2021/2021.12/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E7%9A%84%E5%88%A4%E5%88%AB%E5%87%BD%E6%95%B0","target":"/notes/2021/2021.12/%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83-Mutivariate_Gaussian","text":"多元高斯分布-Mutivariate_Gaussian"},{"source":"/notes/2021/2021.6/Diffie-Hellman%E9%97%AE%E9%A2%98","target":"/notes/2021/2021.6/%E5%9B%BE%E7%81%B5%E5%BD%92%E7%BA%A6-Turing-Reduction","text":"图灵归约 Turing Reduction"},{"source":"/notes/2021/2021.8/How_to_Work_Hard-Paul_Graham","target":"/notes/2021/2021.8/%E5%A6%82%E4%BD%95%E5%8A%AA%E5%8A%9B%E5%B7%A5%E4%BD%9C_Paul_Graham","text":"notes/2021/2021.8/如何努力工作_Paul_Graham"},{"source":"/notes/2021/2021.8/Linear_RegressionGradient_Descent","target":"/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error","text":"Regarding the $\\frac1 2$ term"},{"source":"/notes/2021/2021.8/Linear_RegressionGradient_Descent","target":"/notes/2021/2021.8/Different_Gradient_Descent_Methods","text":"Batch Gradient Descent 批梯度下降 BGD"},{"source":"/notes/2021/2021.8/Mean_Squared_Error_%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE","target":"/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error","text":"Why_do_cost_functions_use_the_square_error/为什么损失函数要使用均方误差"},{"source":"/notes/2021/2021.8/Mean_Squared_Error_%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE","target":"/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error","text":"Regarding the frac1 2 term"},{"source":"/notes/2021/2021.8/Normal_Equation_Proof_2_Matrix_Method","target":"/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E7%9A%84%E6%B1%82%E5%AF%BC","text":"矩阵的求导"},{"source":"/notes/2021/2021.8/Normal_Equation_Proof_2_Matrix_Method","target":"/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E8%BF%B9%E7%9A%84%E6%80%A7%E8%B4%A8","text":"矩阵迹的性质"},{"source":"/notes/2021/2021.8/Normal_Equation_Proof_2_Matrix_Method","target":"/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E8%BF%B9%E7%9A%84%E6%80%A7%E8%B4%A8","text":"这里"},{"source":"/notes/2021/2021.8/Part.13_Cost_Function-Logistic_RegressionML_Andrew.Ng.","target":"/notes/2021/2021.8/Mean_Squared_Error_%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE","text":"均方误差"},{"source":"/notes/2021/2021.8/Part.13_Cost_Function-Logistic_RegressionML_Andrew.Ng.","target":"/notes/2021/2021.8/%E5%87%B8%E4%BC%98%E5%8C%96%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98","text":"凸函数"},{"source":"/notes/2021/2021.8/Part.13_Cost_Function-Logistic_RegressionML_Andrew.Ng.","target":"/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5","text":"Cross_Entropy-交叉熵"},{"source":"/notes/2021/2021.8/Part.13_Cost_Function-Logistic_RegressionML_Andrew.Ng.","target":"/notes/2021/2021.9/%E8%AF%81%E6%98%8ELogistic%E5%9B%9E%E5%BD%92%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%98%AF%E5%87%B8%E5%87%BD%E6%95%B0","text":"证明Logistic回归的损失函数是凸函数"},{"source":"/notes/2021/2021.8/Part.14_Logistic_RegressionGradient_DescentML_Andrew.Ng.","target":"/notes/2021/2021.8/Sigmoid_Function","text":"Logistic的导函数"},{"source":"/notes/2021/2021.8/Part.14_Logistic_RegressionGradient_DescentML_Andrew.Ng.","target":"/notes/2021/2021.8/Relation_Between_Linear_RegressionGradient_Descent_%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%92%8C%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E5%85%B3%E7%B3%BB","text":"Relation_Between_Linear_Regression\u0026Gradient_Descent_梯度下降和线性回归的关系"},{"source":"/notes/2021/2021.8/Part.15_Advanced_OptimizationML_Andrew.Ng.","target":"/notes/2021/2021.8/Different_Gradient_Descent_Methods","text":"Link:其他Gradient_Descent Different_Gradient_Descent_Methods"},{"source":"/notes/2021/2021.8/Part.3_Linear_RegressionML_Andrew.Ng.","target":"/notes/2021/2021.8/Mean_Squared_Error_%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE","text":"平方误差函数/Squared Error Function/Mean Squared Error"},{"source":"/notes/2021/2021.8/Part.3_Linear_RegressionML_Andrew.Ng.","target":"/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error","text":"Why_do_cost_functions_use_the_square_error"},{"source":"/notes/2021/2021.8/Part.4_Cost_Function_IntuitionML_Andrew.Ng.","target":"/notes/2022/2022.2/%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%9B%B0%E9%9A%BE","text":"可视化损失函数的困难"},{"source":"/notes/2021/2021.8/Part.5_Gradient_DescentML_Andrew.Ng.","target":"/notes/2021/2021.8/Different_Gradient_Descent_Methods","text":"Different_Gradient_Descent_Methods"},{"source":"/notes/2021/2021.8/Part.5_Gradient_DescentML_Andrew.Ng.","target":"/notes/2021/2021.8/Linear_RegressionGradient_Descent","text":"Linear Regression \u0026 Gradient Descent"},{"source":"/notes/2021/2021.8/Part.5_Gradient_DescentML_Andrew.Ng.","target":"/notes/2021/2021.8/Linear_RegressionGradient_Descent","text":"Linear_Regression\u0026Gradient_Descent"},{"source":"/notes/2021/2021.8/Part.5_Gradient_DescentML_Andrew.Ng.","target":"/notes/2021/2021.8/Relation_Between_Linear_RegressionGradient_Descent_%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%92%8C%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E5%85%B3%E7%B3%BB","text":"Relation_Between_Linear_Regression\u0026Gradient_Descent_梯度下降和线性回归的关系"},{"source":"/notes/2021/2021.8/Part.5_Gradient_DescentML_Andrew.Ng.","target":"/notes/2021/2021.8/%E5%87%B8%E4%BC%98%E5%8C%96%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98","text":"notes/2021/2021.8/凸优化与线性回归问题"},{"source":"/notes/2021/2021.8/Part.5_Gradient_DescentML_Andrew.Ng.","target":"/notes/2021/2021.8/%E5%87%B8%E4%BC%98%E5%8C%96%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98","text":"凸优化与线性回归问题"},{"source":"/notes/2021/2021.8/Part.9_Normal_EquationML_Andrew.Ng.","target":"/notes/2021/2021.8/Normal_Equation_Proof_2_Matrix_Method","text":"Normal_Equation_Proof_2_Matrix_Method"},{"source":"/notes/2021/2021.8/Part.9_Normal_EquationML_Andrew.Ng.","target":"/notes/2021/2021.8/Mean_Squared_Error_%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE","text":"最小二乘法"},{"source":"/notes/2021/2021.8/Part.9_Normal_EquationML_Andrew.Ng.","target":"/notes/2021/2021.8/Mean_Squared_Error_%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE","text":"均方误差"},{"source":"/notes/2021/2021.8/Sigmoid_Function","target":"/notes/2021/2021.8/Sigmoid-Definition","text":"Sigmoid的含义是"},{"source":"/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error","target":"/notes/2022/2022.2/KL_Divergence-KL%E6%95%A3%E5%BA%A6","text":"KL-散度（相对熵）"},{"source":"/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error","target":"/notes/2021/2021.8/Linear_RegressionGradient_Descent","text":"因为平方项的2被约掉了"},{"source":"/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error","target":"/notes/2021/2021.8/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%88%86%E5%B8%83%E4%B8%8E%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E7%9A%84%E8%81%94%E7%B3%BB_Relation_of_Laplace_distribution-_and_Gaussian_distribution","text":"Related_Post"},{"source":"/notes/2021/2021.8/%E5%A6%82%E4%BD%95%E5%8A%AA%E5%8A%9B%E5%B7%A5%E4%BD%9C_Paul_Graham","target":"/notes/2021/2021.8/%E5%A6%82%E4%BD%95%E5%8A%AA%E5%8A%9B%E5%B7%A5%E4%BD%9C_Paul_Graham","text":"如何努力工作_Paul_Graham"},{"source":"/notes/2021/2021.8/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%88%86%E5%B8%83%E4%B8%8E%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E7%9A%84%E8%81%94%E7%B3%BB_Relation_of_Laplace_distribution-_and_Gaussian_distribution","target":"/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error","text":"Why_do_cost_functions_use_the_square_error"},{"source":"/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E7%9A%84%E6%B1%82%E5%AF%BC","target":"/notes/2021/2021.8/Normal_Equation_Proof_2_Matrix_Method","text":"Normal_Equation_Proof_2_Matrix_Method"},{"source":"/notes/2021/2021.9/Part.19_Regularized_Linear_RegressionML_Andrew.Ng.","target":"/notes/2021/2021.9/%E6%AD%A3%E5%88%99%E9%A1%B9%E4%B8%8D%E5%BD%B1%E5%93%8D%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%87%B8%E6%80%A7","text":"正则项不影响线性回归损失函数的凸性"},{"source":"/notes/2021/2021.9/Part.19_Regularized_Linear_RegressionML_Andrew.Ng.","target":"/notes/2021/2021.9/%E6%AD%A3%E5%88%99%E9%A1%B9%E4%BC%9A%E6%B6%88%E9%99%A4%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E6%B3%95%E5%8F%AF%E8%83%BD%E7%9A%84%E4%B8%8D%E5%8F%AF%E9%80%86%E6%80%A7","text":"notes/2021/2021.9/正则项会消除正规方程法可能的不可逆性"},{"source":"/notes/2021/2021.9/Part.20_Regularized_Logistic_RegressionML_Andrew.Ng.","target":"/notes/2021/2021.9/%E6%AD%A3%E5%88%99%E9%A1%B9%E4%B8%8D%E5%BD%B1%E5%93%8DLogistic%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%87%B8%E6%80%A7","text":"正则项不影响Logistic回归损失函数凸性"},{"source":"/notes/2021/2021.9/Part.26_Probabilistic_Interpretation_of_MSEML_Andrew.Ng.","target":"/notes/2021/2021.8/Mean_Squared_Error_%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE","text":"Mean_Squared_Error-均方误差"},{"source":"/notes/2021/2021.9/Part.26_Probabilistic_Interpretation_of_MSEML_Andrew.Ng.","target":"/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error","text":"Why_do_cost_functions_use_the_square_error"},{"source":"/notes/2021/2021.9/Part.26_Probabilistic_Interpretation_of_MSEML_Andrew.Ng.","target":"/notes/2021/2021.9/%E7%8B%AC%E7%AB%8B%E5%90%8C%E5%88%86%E5%B8%83-IID","text":"独立同分布-IID"},{"source":"/notes/2021/2021.9/Part.26_Probabilistic_Interpretation_of_MSEML_Andrew.Ng.","target":"/notes/2021/2021.9/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83_%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_Normal_Distribution-Gaussian_Distribution","text":"notes/2021/2021.9/正态分布_高斯分布_Normal_Distribution-Gaussian_Distribution"},{"source":"/notes/2021/2021.9/Part.26_Probabilistic_Interpretation_of_MSEML_Andrew.Ng.","target":"/notes/2021/2021.9/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83_%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_Normal_Distribution-Gaussian_Distribution","text":"正态分布的表达式"},{"source":"/notes/2021/2021.9/i.e._Meaning","target":"/notes/2021/2021.9/OK_should_be_in_capital-lettersor_okay","text":"notes/2021/2021.9/OK_should_be_in_capital letters(or_okay)"},{"source":"/notes/2021/2021.9/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83_%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_Normal_Distribution-Gaussian_Distribution","target":"/notes/2021/2021.8/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%88%86%E5%B8%83%E4%B8%8E%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E7%9A%84%E8%81%94%E7%B3%BB_Relation_of_Laplace_distribution-_and_Gaussian_distribution","text":"拉普拉斯分布与高斯分布的联系_Relation_of_Laplace_distribution _and_Gaussian_distribution"},{"source":"/notes/2021/2021.9/%E8%AF%81%E6%98%8ELogistic%E5%9B%9E%E5%BD%92%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%98%AF%E5%87%B8%E5%87%BD%E6%95%B0","target":"/notes/2021/2021.8/Sigmoid_Function","text":"Logistic Function"},{"source":"/notes/2022/2022.1/D2L-10-%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D","target":"/notes/2021/2021.8/Different_Gradient_Descent_Methods","text":"Different_Gradient_Descent_Methods"},{"source":"/notes/2022/2022.1/D2L-11-%E6%B3%9B%E5%8C%96Generalization","target":"/notes/2021/2021.9/%E6%AD%A3%E5%88%99%E9%A1%B9%E4%B8%8D%E5%BD%B1%E5%93%8D%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%87%B8%E6%80%A7","text":"正则项不影响线性回归损失函数的凸性"},{"source":"/notes/2022/2022.1/D2L-4-%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC","target":"/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E7%9A%84%E6%B1%82%E5%AF%BC","text":"矩阵的求导"},{"source":"/notes/2022/2022.1/D2L-4-%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC","target":"/notes/2022/2022.1/Einstein-Notation","text":"Einstein Notation"},{"source":"/notes/2022/2022.1/D2L-6-%E8%AE%A1%E7%AE%97%E5%9B%BE","target":"/notes/2022/2022.1/D2L-5-%E6%8B%93%E5%B1%95%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99","text":"例子 线性回归"},{"source":"/notes/2022/2022.1/D2L-7-%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC","target":"/notes/2022/2022.1/D2L-6-%E8%AE%A1%E7%AE%97%E5%9B%BE","text":"计算图"},{"source":"/notes/2022/2022.1/D2L-7-%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC","target":"/notes/2022/2022.1/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%AF%94%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%9B%B4%E9%AB%98%E6%95%88","text":"为什么反向传播比前向传播更高效"},{"source":"/notes/2022/2022.1/Dummy_Variables","target":"/notes/2022/2022.1/Kronecker-delta-%E5%85%8B%E7%BD%97%E5%86%85%E5%85%8B%CE%B4%E5%87%BD%E6%95%B0","text":"Kronecker delta"},{"source":"/notes/2022/2022.1/One-hot_Encoding-%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81","target":"/notes/2022/2022.1/Dummy_Variables","text":"notes/2022/2022.1/Dummy_Variables"},{"source":"/notes/2022/2022.1/%E7%9F%A9%E9%98%B5%E7%9A%84%E4%B8%8D%E5%90%8C%E4%B9%98%E6%B3%95-Hadamard-Kronecker","target":"/D2L-1-What_is_a_tensor","text":"Tensor Product"},{"source":"/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","target":"/notes/2022/2022.2/D2L-39-LeNet","text":"LeNet"},{"source":"/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","target":"/notes/2022/2022.2/D2L-40-AlexNet","text":"AlexNet"},{"source":"/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","target":"/notes/2022/2022.2/D2L-41-VGG","text":"VGG"},{"source":"/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","target":"/notes/2022/2022.3/D2L-42-NiN","text":"NiN"},{"source":"/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","target":"/notes/2022/2022.3/D2L-45-ResNet","text":"ResNet"},{"source":"/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","target":"/notes/2022/2022.3/D2L-46-DenseNet","text":"DenseNet"},{"source":"/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","target":"/notes/2022/2022.4/D2L-53-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN","text":"RNN"},{"source":"/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","target":"/notes/2022/2022.4/D2L-57-LSTM-%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C","text":"LSTM"},{"source":"/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","target":"/notes/2022/2022.4/D2L-56-%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83GRU","text":"GRU"},{"source":"/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","target":"/notes/2022/2022.4/D2L-60-Encoder-Decoder","text":"Encoder-Decoder"},{"source":"/notes/2022/2022.10/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B","target":"/notes/2022/2022.4/D2L-75-BERT","text":"BERT"},{"source":"/notes/2022/2022.2/2%E4%B8%AA3x3%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%A0%86%E5%8F%A0%E5%90%8E%E7%AD%89%E4%BB%B7%E4%BA%8E%E4%B8%80%E4%B8%AA5x5%E5%8D%B7%E7%A7%AF%E6%A0%B8","target":"/notes/2022/2022.2/%E5%AF%B9%E4%BA%8E%E7%AD%89%E4%BB%B7%E7%9A%84%E7%BD%91%E7%BB%9C-%E5%B0%8F%E7%9A%84%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%8F%82%E6%95%B0%E6%9B%B4%E5%B0%91","text":"notes/2022/2022.2/对于等价的网络, 小的卷积核参数更少"},{"source":"/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5","target":"/notes/2022/2022.2/KL_Divergence-KL%E6%95%A3%E5%BA%A6","text":"KL散度"},{"source":"/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5","target":"/notes/2022/2022.2/Entropy-%E7%86%B5","text":"熵(Entropy)"},{"source":"/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5","target":"/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss","text":"交叉熵是怎样衡量输出和真实值的差别的呢?"},{"source":"/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5","target":"/notes/2022/2022.2/D2L-13-Softmax_Regression","text":"Softmax回归"},{"source":"/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5","target":"/notes/2022/2022.1/One-hot_Encoding-%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81","text":"One-hot_Encoding-独热编码"},{"source":"/notes/2022/2022.2/D2L-12-Predication_or_Inference-Difference","target":"/notes/2021/2021.12/Bayesian_EstimationInference%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1","text":"Bayesian_Estimation(Inference)贝叶斯估计"},{"source":"/notes/2022/2022.2/D2L-13-Softmax_Regression","target":"/notes/2022/2022.2/Softmax%E5%87%BD%E6%95%B0","text":"Softmax函数"},{"source":"/notes/2022/2022.2/D2L-13-Softmax_Regression","target":"/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss","text":"D2L-14-Cross Entropy as Loss"},{"source":"/notes/2022/2022.2/D2L-13-Softmax_Regression","target":"/notes/2022/2022.2/Relation_between_Softmax_and_Logistic_Regression","text":"Relation_between_Softmax_and_Logistic_Regression"},{"source":"/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss","target":"/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5","text":"交叉熵"},{"source":"/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss","target":"/notes/2022/2022.2/KL_Divergence-KL%E6%95%A3%E5%BA%A6","text":"KL_Divergence"},{"source":"/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss","target":"/notes/2022/2022.2/Logit","text":"Logit"},{"source":"/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss","target":"/notes/2022/2022.1/One-hot_Encoding-%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81","text":"One-hot编码"},{"source":"/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss","target":"/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5","text":"Cross Entropy - 交叉熵"},{"source":"/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss","target":"/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE","text":"为什么Softmax回归不用MSE"},{"source":"/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss","target":"/notes/2022/2022.5/Cross_Entropy_Loss_Input_Format-%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%BE%93%E5%85%A5%E6%A0%BC%E5%BC%8F","text":"Cross_Entropy_Loss_Input_Format-交叉熵损失函数输入格式"},{"source":"/notes/2022/2022.2/D2L-17-MLP-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA","target":"/notes/2022/2022.2/%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2-Affine_Transformation","text":"仿射变换"},{"source":"/notes/2022/2022.2/D2L-17-MLP-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA","target":"/notes/2022/2022.2/D2L-13-Softmax_Regression","text":"Softmax"},{"source":"/notes/2022/2022.2/D2L-18-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-Activation_Functions","target":"/notes/2021/2021.8/Sigmoid_Function","text":"Sigmoid_Function"},{"source":"/notes/2022/2022.2/D2L-20-%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE%E4%B8%8E%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE","target":"/notes/2021/2021.12/Cross-Validation","text":"Cross Validation"},{"source":"/notes/2022/2022.2/D2L-21-%E6%A8%A1%E5%9E%8B%E5%AE%B9%E9%87%8F","target":"/notes/2022/2022.2/VC%E7%BB%B4-VC_Dimension","text":"VC维"},{"source":"/notes/2022/2022.2/D2L-22-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F","target":"/notes/2022/2022.2/Regularization-%E6%AD%A3%E5%88%99%E5%8C%96","text":"正则化"},{"source":"/notes/2022/2022.2/D2L-22-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F","target":"/notes/2022/2022.2/Norm-in-Regularization-Intuition","text":"Norm in Regularization - Intuition"},{"source":"/notes/2022/2022.2/D2L-24-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7","target":"/notes/2021/2021.8/Sigmoid_Function","text":"$Sigmoid$ Function"},{"source":"/notes/2022/2022.2/D2L-25-%E8%AE%A9%E8%AE%AD%E7%BB%83%E6%9B%B4%E5%8A%A0%E7%A8%B3%E5%AE%9A-Xavier%E5%88%9D%E5%A7%8B%E5%8C%96","target":"/notes/2022/2022.2/D2L-24-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7","text":"梯度问题的根本原因"},{"source":"/notes/2022/2022.2/D2L-25-%E8%AE%A9%E8%AE%AD%E7%BB%83%E6%9B%B4%E5%8A%A0%E7%A8%B3%E5%AE%9A-Xavier%E5%88%9D%E5%A7%8B%E5%8C%96","target":"/notes/2022/2022.4/D2L-54-Gradient-Clipping-%E6%A2%AF%E5%BA%A6%E5%89%AA%E8%A3%81","text":"Gradient Clipping-梯度剪裁"},{"source":"/notes/2022/2022.2/D2L-25-%E8%AE%A9%E8%AE%AD%E7%BB%83%E6%9B%B4%E5%8A%A0%E7%A8%B3%E5%AE%9A-Xavier%E5%88%9D%E5%A7%8B%E5%8C%96","target":"/notes/2022/2022.2/Xavier%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E8%AF%A6%E7%BB%86%E4%BE%8B%E5%AD%90","text":"Xavier初始化的详细例子"},{"source":"/notes/2022/2022.2/D2L-25-%E8%AE%A9%E8%AE%AD%E7%BB%83%E6%9B%B4%E5%8A%A0%E7%A8%B3%E5%AE%9A-Xavier%E5%88%9D%E5%A7%8B%E5%8C%96","target":"/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%82%E6%95%B0%E4%B8%8D%E8%83%BD%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%BA%E5%90%8C%E4%B8%80%E4%B8%AA%E5%B8%B8%E6%95%B0","text":"为什么参数不能初始化为同一个常数?"},{"source":"/notes/2022/2022.2/D2L-27-Computation-%E5%B1%82%E5%92%8C%E5%9D%97","target":"/notes/2022/2022.2/D2L-29-Computation-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B1%82","text":"我们可以自定义一个层"},{"source":"/notes/2022/2022.2/D2L-33-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN","target":"/notes/2022/2022.2/%E5%BD%92%E7%BA%B3%E5%81%8F%E7%BD%AE-Inductive-bias-learning-bias","text":"归纳偏置 Inductive bias"},{"source":"/notes/2022/2022.2/D2L-37-CNN%E7%9A%84%E8%AE%A1%E7%AE%97%E5%A4%8D%E6%9D%82%E5%BA%A6","target":"/notes/2022/2022.2/D2L-32-Convolution-%E5%8D%B7%E7%A7%AF","text":"数值计算 卷积与傅里叶变换"},{"source":"/notes/2022/2022.2/D2L-38-%E6%B1%A0%E5%8C%96%E5%B1%82-Pooling_Layer","target":"/notes/2022/2022.2/D2L-34-%E5%8D%B7%E7%A7%AF%E5%B1%82-%E5%A1%AB%E5%85%85-Padding","text":"填充"},{"source":"/notes/2022/2022.2/D2L-38-%E6%B1%A0%E5%8C%96%E5%B1%82-Pooling_Layer","target":"/notes/2022/2022.2/D2L-35-%E5%8D%B7%E7%A7%AF%E5%B1%82-%E6%AD%A5%E5%B9%85-Stride","text":"步幅"},{"source":"/notes/2022/2022.2/D2L-40-AlexNet","target":"/notes/2022/2022.2/D2L-22-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F","text":"权重衰减"},{"source":"/notes/2022/2022.2/D2L-40-AlexNet","target":"/notes/2022/2022.2/D2L-39-LeNet","text":"LeNet"},{"source":"/notes/2022/2022.2/D2L-40-AlexNet","target":"/notes/2022/2022.2/D2L-41-VGG","text":"VGG"},{"source":"/notes/2022/2022.2/D2L-40-AlexNet","target":"/notes/2022/2022.3/D2L-42-NiN","text":"NiN"},{"source":"/notes/2022/2022.2/D2L-40-AlexNet","target":"/notes/2022/2022.3/D2L-43-GoogLeNetInception","text":"GoogLeNet(Inception)"},{"source":"/notes/2022/2022.2/D2L-40-AlexNet","target":"/notes/2022/2022.3/D2L-45-ResNet","text":"ResNet"},{"source":"/notes/2022/2022.2/D2L-41-VGG","target":"/notes/2022/2022.2/D2L-40-AlexNet","text":"AlexNet"},{"source":"/notes/2022/2022.2/D2L-41-VGG","target":"/notes/2022/2022.2/2%E4%B8%AA3x3%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%A0%86%E5%8F%A0%E5%90%8E%E7%AD%89%E4%BB%B7%E4%BA%8E%E4%B8%80%E4%B8%AA5x5%E5%8D%B7%E7%A7%AF%E6%A0%B8","text":"notes/2022/2022.2/2个3x3卷积核堆叠后等价于一个5x5卷积核"},{"source":"/notes/2022/2022.2/Likelihood_Function-%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0","target":"/notes/2021/2021.12/Maximum_Likelihood_Estimation-%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1","text":"Maximum_Likelihood_Estimation-极大似然估计"},{"source":"/notes/2022/2022.2/Norm-in-Regularization-Intuition","target":"/notes/2022/2022.1/D2L-10-%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D","text":"小批量随机梯度下降"},{"source":"/notes/2022/2022.2/Norm-in-Regularization-Intuition","target":"/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error","text":"L1范数约束后的解很\"稀疏\", 这在特征选择时是很有用的"},{"source":"/notes/2022/2022.2/Regularization-%E6%AD%A3%E5%88%99%E5%8C%96","target":"/notes/2022/2022.2/D2L-22-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F","text":"D2L-22-权重衰减"},{"source":"/notes/2022/2022.2/Relation_between_Softmax_and_Logistic_Regression","target":"/notes/2022/2022.2/D2L-13-Softmax_Regression","text":"Softmax回归"},{"source":"/notes/2022/2022.2/Relation_between_Softmax_and_Logistic_Regression","target":"/notes/2022/2022.2/Softmax_Regression_is_Over-parameterized","text":"Softmax_Regression_is_Over-parameterized"},{"source":"/notes/2022/2022.2/Softmax%E5%87%BD%E6%95%B0","target":"/notes/2022/2022.2/Softmax_Regression_is_Over-parameterized","text":"notes/2022/2022.2/Softmax_Regression_is_Over-parameterized"},{"source":"/notes/2022/2022.2/Softmax%E5%87%BD%E6%95%B0","target":"/notes/2021/2021.8/Sigmoid_Function","text":"Sigmoid_Function"},{"source":"/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE","target":"/notes/2021/2021.9/%E8%AF%81%E6%98%8ELogistic%E5%9B%9E%E5%BD%92%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%98%AF%E5%87%B8%E5%87%BD%E6%95%B0","text":"证明Logistic回归的损失函数是凸函数"},{"source":"/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE","target":"/notes/2022/2022.2/%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%9B%B0%E9%9A%BE","text":"对于多个样本需要加上求和符号"},{"source":"/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE","target":"/notes/2021/2021.9/%E8%AF%81%E6%98%8ELogistic%E5%9B%9E%E5%BD%92%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%98%AF%E5%87%B8%E5%87%BD%E6%95%B0","text":"详细证明见: 证明Logistic回归的损失函数是凸函数"},{"source":"/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%82%E6%95%B0%E4%B8%8D%E8%83%BD%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%BA%E5%90%8C%E4%B8%80%E4%B8%AA%E5%B8%B8%E6%95%B0","target":"/notes/2022/2022.2/D2L-23-Dropout-%E4%B8%A2%E5%BC%83%E6%B3%95","text":"(Dropout-丢弃法)"},{"source":"/notes/2022/2022.2/%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%9B%B0%E9%9A%BE","target":"/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE","text":"为什么Softmax回归不用MSE"},{"source":"/notes/2022/2022.2/%E5%A5%BD%E7%9A%84%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%B9%E8%B4%A8","target":"/notes/2022/2022.2/D2L-22-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F","text":"1995年，克里斯托弗·毕晓普证明了 具有输入噪声的训练等价于Tikhonov正则化。"},{"source":"/notes/2022/2022.2/%E5%A5%BD%E7%9A%84%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%B9%E8%B4%A8","target":"/notes/2022/2022.2/D2L-23-Dropout-%E4%B8%A2%E5%BC%83%E6%B3%95","text":"Dropout"},{"source":"/notes/2022/2022.2/%E5%AF%B9%E4%BA%8E%E7%AD%89%E4%BB%B7%E7%9A%84%E7%BD%91%E7%BB%9C-%E5%B0%8F%E7%9A%84%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%8F%82%E6%95%B0%E6%9B%B4%E5%B0%91","target":"/notes/2022/2022.2/2%E4%B8%AA3x3%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%A0%86%E5%8F%A0%E5%90%8E%E7%AD%89%E4%BB%B7%E4%BA%8E%E4%B8%80%E4%B8%AA5x5%E5%8D%B7%E7%A7%AF%E6%A0%B8","text":"2个3x3卷积核堆叠后等价于一个5x5卷积核"},{"source":"/notes/2022/2022.2/%E5%AF%B9%E4%BA%8E%E7%AD%89%E4%BB%B7%E7%9A%84%E7%BD%91%E7%BB%9C-%E5%B0%8F%E7%9A%84%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%8F%82%E6%95%B0%E6%9B%B4%E5%B0%91","target":"/notes/2022/2022.2/%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%8F%82%E6%95%B0%E5%A4%A7%E5%B0%8F%E7%9A%84%E8%AE%A1%E7%AE%97","text":"卷积层参数大小的计算"},{"source":"/notes/2022/2022.3/D2L-42-NiN","target":"/notes/2022/2022.2/%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%8F%82%E6%95%B0%E5%A4%A7%E5%B0%8F%E7%9A%84%E8%AE%A1%E7%AE%97","text":"卷积层参数大小的计算"},{"source":"/notes/2022/2022.3/D2L-42-NiN","target":"/notes/2022/2022.2/D2L-36-1x1%E5%8D%B7%E7%A7%AF%E5%B1%82","text":"D2L-36-1x1卷积层"},{"source":"/notes/2022/2022.3/D2L-42-NiN","target":"/notes/2022/2022.2/Logit","text":"Logit"},{"source":"/notes/2022/2022.3/D2L-43-GoogLeNetInception","target":"/notes/2022/2022.3/D2L-42-NiN","text":"NiN"},{"source":"/notes/2022/2022.3/D2L-43-GoogLeNetInception","target":"/notes/2022/2022.2/D2L-36-1x1%E5%8D%B7%E7%A7%AF%E5%B1%82","text":"1x1卷积层"},{"source":"/notes/2022/2022.3/D2L-45-ResNet","target":"/notes/2022/2022.2/D2L-21-%E6%A8%A1%E5%9E%8B%E5%AE%B9%E9%87%8F","text":"模型容量"},{"source":"/notes/2022/2022.3/D2L-45-ResNet","target":"/notes/2022/2022.2/D2L-24-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7","text":"后一种问题"},{"source":"/notes/2022/2022.3/D2L-45-ResNet","target":"/notes/2022/2022.2/D2L-24-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7","text":"D2L-24-数值稳定性"},{"source":"/notes/2022/2022.3/D2L-45-ResNet","target":"/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE","text":"为什么Softmax回归不用MSE"},{"source":"/notes/2022/2022.3/D2L-45-ResNet","target":"/notes/2022/2022.3/D2L-43-GoogLeNetInception","text":"D2L-43-GoogLeNet(Inception)"},{"source":"/notes/2022/2022.3/D2L-45-ResNet","target":"/notes/2022/2022.3/D2L-42-NiN","text":"D2L-42-NiN"},{"source":"/notes/2022/2022.3/D2L-51-%E8%AF%AD%E8%A8%80%E7%9A%84%E7%BB%9F%E8%AE%A1%E7%89%B9%E5%BE%81","target":"/notes/2022/2022.3/D2L-50-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-%E4%BC%A0%E7%BB%9F%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8D%E8%B6%B3","text":"拉普拉斯平滑"},{"source":"/notes/2022/2022.4/D2L-53-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN","target":"/notes/2022/2022.3/D2L-48-%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B-Sequence_Models","text":"隐变量自回归模型"},{"source":"/notes/2022/2022.4/D2L-53-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN","target":"/notes/2022/2022.2/D2L-17-MLP-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA","text":"MLP"},{"source":"/notes/2022/2022.4/D2L-53-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN","target":"/notes/2022/2022.4/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CFeedforward-neural-network","text":"notes/2022/2022.4/前馈神经网络(Feedforward neural network)"},{"source":"/notes/2022/2022.4/D2L-53-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN","target":"/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5","text":"Cross_Entropy-交叉熵"},{"source":"/notes/2022/2022.4/D2L-53-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN","target":"/notes/2022/2022.5/Harmonic_Mean-%E8%B0%83%E5%92%8C%E5%B9%B3%E5%9D%87%E6%95%B0","text":"调和平均数(Harmonic mean)"},{"source":"/notes/2022/2022.4/D2L-53-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN","target":"/notes/2022/2022.4/RNN%E4%B8%ADoutput%E5%92%8Chidden_state%E7%9A%84%E5%8C%BA%E5%88%AB","text":"RNN中output和hidden_state的区别"},{"source":"/notes/2022/2022.4/D2L-55-%E5%9C%A8%E6%97%B6%E9%97%B4%E4%B8%8A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD","target":"/notes/2022/2022.2/D2L-24-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7","text":"D2L-24-数值稳定性"},{"source":"/notes/2022/2022.4/D2L-55-%E5%9C%A8%E6%97%B6%E9%97%B4%E4%B8%8A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD","target":"/notes/2022/2022.2/D2L-25-%E8%AE%A9%E8%AE%AD%E7%BB%83%E6%9B%B4%E5%8A%A0%E7%A8%B3%E5%AE%9A-Xavier%E5%88%9D%E5%A7%8B%E5%8C%96","text":"D2L-25-让训练更加稳定"},{"source":"/notes/2022/2022.4/D2L-55-%E5%9C%A8%E6%97%B6%E9%97%B4%E4%B8%8A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD","target":"/notes/2022/2022.2/Regularization-%E6%AD%A3%E5%88%99%E5%8C%96","text":"正则化"},{"source":"/notes/2022/2022.4/D2L-55-%E5%9C%A8%E6%97%B6%E9%97%B4%E4%B8%8A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD","target":"/notes/2022/2022.4/%E9%80%92%E6%8E%A8%E5%85%AC%E5%BC%8F-a_tb_t+c_ta_t-1-%E8%BD%AC%E9%80%9A%E9%A1%B9%E5%85%AC%E5%BC%8F","text":"notes/2022/2022.4/递推公式 $a_{t}=b_{t}+c_{t}a_{t-1}$ 转通项公式"},{"source":"/notes/2022/2022.4/D2L-55-%E5%9C%A8%E6%97%B6%E9%97%B4%E4%B8%8A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD","target":"/notes/2022/2022.2/D2L-25-%E8%AE%A9%E8%AE%AD%E7%BB%83%E6%9B%B4%E5%8A%A0%E7%A8%B3%E5%AE%9A-Xavier%E5%88%9D%E5%A7%8B%E5%8C%96","text":"梯度归一化"},{"source":"/notes/2022/2022.4/D2L-55-%E5%9C%A8%E6%97%B6%E9%97%B4%E4%B8%8A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD","target":"/notes/2022/2022.1/D2L-5-%E6%8B%93%E5%B1%95%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99","text":"D2L-5-拓展链式法则"},{"source":"/notes/2022/2022.4/D2L-56-%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83GRU","target":"/notes/2022/2022.4/%E5%87%B8%E7%BB%84%E5%90%88-Convex-Combination","text":"凸组合 - Convex Combination"},{"source":"/notes/2022/2022.4/D2L-57-LSTM-%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C","target":"/notes/2022/2022.4/D2L-56-%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83GRU","text":"D2L-56-门控循环单元GRU"},{"source":"/notes/2022/2022.4/D2L-57-LSTM-%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C","target":"/notes/2022/2022.4/%E5%87%B8%E7%BB%84%E5%90%88-Convex-Combination","text":"凸组合 - Convex Combination"},{"source":"/notes/2022/2022.4/D2L-59-%E5%8F%8C%E5%90%91%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C","target":"/notes/2022/2022.3/D2L-48-%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B-Sequence_Models","text":"之前的笔记 - 序列模型-Sequence_Models"},{"source":"/notes/2022/2022.4/D2L-61-Sequence-to-Sequence-Learning-Seq2Seq","target":"/notes/2022/2022.4/D2L-60-Encoder-Decoder","text":"Encoder-Decoder"},{"source":"/notes/2022/2022.4/D2L-61-Sequence-to-Sequence-Learning-Seq2Seq","target":"/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss","text":"交叉熵损失函数"},{"source":"/notes/2022/2022.4/D2L-61-Sequence-to-Sequence-Learning-Seq2Seq","target":"/notes/2022/2022.4/D2L-53-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN","text":"困惑度 Perplexity"},{"source":"/notes/2022/2022.4/D2L-61-Sequence-to-Sequence-Learning-Seq2Seq","target":"/notes/2022/2022.4/D2L-62-BLEU-Bilingual-Evaluation-Understudy","text":"D2L-62-BLEU (Bilingual Evaluation Understudy)"},{"source":"/notes/2022/2022.4/D2L-63-Beam-Search","target":"/notes/2022/2022.4/D2L-61-Sequence-to-Sequence-Learning-Seq2Seq","text":"Seq2Seq"},{"source":"/notes/2022/2022.4/D2L-63-Beam-Search","target":"/notes/2022/2022.4/Viterbi-Algorithm","text":"Viterbi Algorithm"},{"source":"/notes/2022/2022.4/D2L-63-Beam-Search","target":"/notes/2022/2022.4/D2L-62-BLEU-Bilingual-Evaluation-Understudy","text":"BLEU"},{"source":"/notes/2022/2022.4/D2L-64-Kernel-Regression","target":"/notes/2022/2022.4/D2L-66-Kernel-Regression-and-Attention","text":"notes/2022/2022.4/D2L-66-Kernel Regression and Attention"},{"source":"/notes/2022/2022.4/D2L-66-Kernel-Regression-and-Attention","target":"/notes/2022/2022.4/D2L-64-Kernel-Regression","text":"Kernel Regression 里面的采用高斯核的时候"},{"source":"/notes/2022/2022.4/D2L-67-Attention-Scoring-Function","target":"/notes/2022/2022.4/D2L-68-Additive-Attention","text":"D2L-68-Additive Attention"},{"source":"/notes/2022/2022.4/D2L-67-Attention-Scoring-Function","target":"/notes/2022/2022.4/D2L-69-Scaled-Dot-Product-Attention","text":"D2L-69-Scaled Dot-Product Attention"},{"source":"/notes/2022/2022.4/D2L-69-Scaled-Dot-Product-Attention","target":"/notes/2021/2021.11/%E5%86%85%E7%A7%AF%E5%92%8C%E7%9B%B8%E5%85%B3%E6%80%A7%E7%9A%84%E8%81%94%E7%B3%BB-DotInner_Product__Correlation","text":"内积可以衡量两个向量之间的相似程度"},{"source":"/notes/2022/2022.4/D2L-69-Scaled-Dot-Product-Attention","target":"/notes/2022/2022.2/D2L-23-Dropout-%E4%B8%A2%E5%BC%83%E6%B3%95","text":"Dropout"},{"source":"/notes/2022/2022.4/D2L-70-Seq2Seq-with-Attention-Bahdanau-Attention","target":"/notes/2022/2022.4/D2L-61-Sequence-to-Sequence-Learning-Seq2Seq","text":"Seq2Seq"},{"source":"/notes/2022/2022.4/D2L-70-Seq2Seq-with-Attention-Bahdanau-Attention","target":"/notes/2022/2022.4/D2L-68-Additive-Attention","text":"Additive Attention"},{"source":"/notes/2022/2022.4/D2L-71-Multi-Head_Attention","target":"/notes/2022/2022.4/D2L-67-Attention-Scoring-Function","text":"Attention Scoring Function"},{"source":"/notes/2022/2022.4/D2L-72-Self-Attention","target":"/notes/2022/2022.4/D2L-73-Positional_Encoding","text":"D2L-73-Positional_Encoding"},{"source":"/notes/2022/2022.4/D2L-74-Transformer","target":"/notes/2022/2022.4/D2L-60-Encoder-Decoder","text":"Encoder Decoder"},{"source":"/notes/2022/2022.4/D2L-74-Transformer","target":"/notes/2022/2022.4/D2L-72-Self-Attention","text":"Self-Attention"},{"source":"/notes/2022/2022.4/D2L-74-Transformer","target":"/notes/2022/2022.4/D2L-71-Multi-Head_Attention","text":"多头注意力"},{"source":"/notes/2022/2022.4/D2L-74-Transformer","target":"/notes/2022/2022.3/D2L-45-ResNet","text":"ResNet"},{"source":"/notes/2022/2022.4/D2L-74-Transformer","target":"/notes/2022/2022.3/D2L-44-Batch_Normalization-%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96","text":"Batch_Normalization-批量归一化"},{"source":"/notes/2022/2022.4/D2L-74-Transformer","target":"/notes/2022/2022.2/D2L-23-Dropout-%E4%B8%A2%E5%BC%83%E6%B3%95","text":"Dropout"},{"source":"/notes/2022/2022.4/D2L-74-Transformer","target":"/notes/2022/2022.4/D2L-72-Self-Attention","text":"自注意力忽略了原始序列的位置信息"},{"source":"/notes/2022/2022.4/D2L-75-BERT","target":"/notes/2022/2022.4/D2L-77-BERT-Fine-tune","text":"D2L-77-BERT - Fine-tune"},{"source":"/notes/2022/2022.4/D2L-76-BERT-Pretrain","target":"/notes/2022/2022.3/D2L-50-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-%E4%BC%A0%E7%BB%9F%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8D%E8%B6%B3","text":"语言模型(Language Model)"},{"source":"/notes/2022/2022.4/D2L-77-BERT-Fine-tune","target":"/notes/2021/2021.8/Mean_Squared_Error_%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE","text":"Mean_Squared_Error_均方误差"},{"source":"/notes/2022/2022.5/F1_Score","target":"/notes/2022/2022.5/Harmonic_Mean-%E8%B0%83%E5%92%8C%E5%B9%B3%E5%9D%87%E6%95%B0","text":"Harmonic_Mean-调和平均数"},{"source":"/notes/2022/2022.5/F1_Score","target":"/notes/2022/2022.5/Precision-Recall-and-Accuracy","text":"Precision Recall"},{"source":"/notes/2022/2022.5/%E7%AB%AF%E5%88%B0%E7%AB%AF%E5%AD%A6%E4%B9%A0-End_to_End_Learning-E2E","target":"/notes/2022/2022.2/%E5%BD%92%E7%BA%B3%E5%81%8F%E7%BD%AE-Inductive-bias-learning-bias","text":"归纳偏置"},{"source":"/notes/2022/2022.6/JS%E6%95%A3%E5%BA%A6","target":"/notes/2022/2022.2/KL_Divergence-KL%E6%95%A3%E5%BA%A6","text":"Kullback–Leibler divergence"},{"source":"/notes/2022/2022.6/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-%E5%91%A8%E6%9C%9F%E6%80%A7%E6%95%B0%E6%8D%AE%E6%97%B6%E9%97%B4%E7%AD%89","target":"/notes/2022/2022.1/One-hot_Encoding-%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81","text":"One-hot_Encoding-独热编码"},{"source":"/notes/2022/2022.7/%E6%9C%89%E6%95%88%E5%9C%B0%E8%83%8C%E8%AF%B5GRE%E5%8D%95%E8%AF%8D","target":"/","text":"incontrovertible"},{"source":"/notes/2022/2022.7/%E6%9C%89%E6%95%88%E5%9C%B0%E8%83%8C%E8%AF%B5GRE%E5%8D%95%E8%AF%8D","target":"/","text":"unimpeachable"},{"source":"/notes/2022/2022.7/%E6%9C%89%E6%95%88%E5%9C%B0%E8%83%8C%E8%AF%B5GRE%E5%8D%95%E8%AF%8D","target":"/","text":"unimpeachable"},{"source":"/notes/2022/2022.7/%E6%9C%89%E6%95%88%E5%9C%B0%E8%83%8C%E8%AF%B5GRE%E5%8D%95%E8%AF%8D","target":"/","text":"incontrovertible"},{"source":"/notes/2022/2022.7/%E6%9C%89%E6%95%88%E5%9C%B0%E8%83%8C%E8%AF%B5GRE%E5%8D%95%E8%AF%8D","target":"/","text":"impeccable"}]}