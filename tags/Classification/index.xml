<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Classification on</title><link>https://alonelysheep.github.io/quartz-blog/tags/Classification/</link><description>Recent content in Classification on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 11 Feb 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://alonelysheep.github.io/quartz-blog/tags/Classification/index.xml" rel="self" type="application/rss+xml"/><item><title>D2L-13-Softmax_Regression</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-13-Softmax_Regression/</link><pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-13-Softmax_Regression/</guid><description>Softmax 回归 2022-02-11 Tags: #SoftmaxRegression #MachineLearning #Classification #MulticlassClassification
Softmax回归解决的是多分类问题1, 它可以看作是二分类的 Logistic_Regression的推广. Softmax回归 Softmax回归就是在线性回归的基础上套上一个Softmax函数, 取输出结果中概率最大的项作为预测结果. 交叉熵作为损失函数 D2L-14-Cross Entropy as Loss</description></item><item><title>Relation_between_Softmax_and_Logistic_Regression</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Relation_between_Softmax_and_Logistic_Regression/</link><pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Relation_between_Softmax_and_Logistic_Regression/</guid><description>Softmax 与 Logistic 回归的联系 2022-02-11 Tags: #SoftmaxRegression #LogisticRegression #Classification #MulticlassClassification
Ref: Unsupervised Feature Learning and Deep Learning Tutorial
二分类的 Softmax回归形式如下: $$h_{\theta}(x)=\frac{1}{\exp \left(\theta^{(1) \top} x\right)+\exp \left(\theta^{(2) \top} x^{(i)}\right)}\left[\begin{array}{c} \exp \left(\theta^{(1) \top} x\right) \ \exp \left(\theta^{(2) \top} x\right) \end{array}\right]$$</description></item><item><title>Part.11_Classification(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.11_ClassificationML_Andrew.Ng./</link><pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.11_ClassificationML_Andrew.Ng./</guid><description>Classification 2021-08-19 Tags: #MachineLearning #Classification
分类问题最简单的情况是二分类问题(Binary Classification), 更一般的情况是多分类问题.
分类问题与回归问题最大的不同是其对输出的要求是离散的, 线性函数/回归在分类问题上面不适用.</description></item><item><title>Part.12_Logistic_Regression(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.12_Logistic_RegressionML_Andrew.Ng./</link><pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.12_Logistic_RegressionML_Andrew.Ng./</guid><description>Logistic Regression 2021-08-19 Tags: #LogisticRegression #MachineLearning #Classification
Logistic Function Hypothesis Representation 我们可以通过对线性回归的方法进行一些小改动来匹配回归问题, 在线性回归的时候, $h(x)$的输出与分类问题的&amp;quot;值域&amp;quot;偏差较大, 比如在二分类问题里面, 要求$y=0\space or\space 1$, 但是$h(x)$会输出大于一或者小于零的数.</description></item><item><title>Part.16_MulticlassClassification-One_vs_Rest(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.16_MulticlassClassification-One_vs_RestML_Andrew.Ng./</link><pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.16_MulticlassClassification-One_vs_RestML_Andrew.Ng./</guid><description>One vs Rest 2021-08-19 Tags: #MulticlassClassification #Classification #MachineLearning
AKA: One vs All MulticlassClassification $$\begin{aligned} &amp;amp;y \in{0,1 \ldots n} \ &amp;amp;h_{\theta}^{(0)}(x)=P(y=0 \mid x ; \theta) \ &amp;amp;h_{\theta}^{(1)}(x)=P(y=1 \mid x ; \theta) \ &amp;amp;\cdots \ &amp;amp;h_{\theta}^{(n)}(x)=P(y=n \mid x ; \theta) \ &amp;amp;\text { prediction }=\max {i}\left(h{\theta}^{(i)}(x)\right) \end{aligned}$$</description></item></channel></rss>