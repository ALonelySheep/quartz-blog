<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>StatisticalLearning on</title><link>https://alonelysheep.github.io/quartz-blog/tags/StatisticalLearning/</link><description>Recent content in StatisticalLearning on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 12 Feb 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://alonelysheep.github.io/quartz-blog/tags/StatisticalLearning/index.xml" rel="self" type="application/rss+xml"/><item><title>VC维-VC_Dimension</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/VC%E7%BB%B4-VC_Dimension/</link><pubDate>Sat, 12 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/VC%E7%BB%B4-VC_Dimension/</guid><description>Vapnik–Chervonenkis dimension 2022-02-12 Tags: #DeepLearning #StatisticalLearning
在VC理论中，VC维是对一个可学习分类函数空间的能力（复杂度，表示能力等）的衡量。它定义为算法能“打散”的点集的势的最大值。
对于线性分类器:
VC维可以衡量训练误差和泛化误差的间隔, 但是在深度学习中, 我们很难计算一个模型的VC维</description></item></channel></rss>