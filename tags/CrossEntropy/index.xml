<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>CrossEntropy on</title><link>https://alonelysheep.github.io/quartz-blog/tags/CrossEntropy/</link><description>Recent content in CrossEntropy on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 28 Feb 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://alonelysheep.github.io/quartz-blog/tags/CrossEntropy/index.xml" rel="self" type="application/rss+xml"/><item><title>为什么Softmax回归不用MSE</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE/</link><pubDate>Mon, 28 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE/</guid><description>为什么Softmax (或者Logistic) 不用MSE作为损失函数? 2022-02-28 Tags: #DeepLearning #MachineLearning #SoftmaxRegression #LogisticRegression #CostFunction #MeanSquareError #CrossEntropy
回顾:
MSE假设样本误差i.i.d., 并且服从正态分布, 最小化MSE等价于极大似然估计. 通常用于回归问题. MSE基于输出与真实值的欧氏距离.</description></item><item><title>D2L-14-Cross Entropy as Loss</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss/</link><pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss/</guid><description>交叉熵作为损失函数 2022-02-11 Tags: #CostFunction #DeepLearning #CrossEntropy
在作为损失函数的时候, 构成 交叉熵的概率分布为: 真实分布: $P^*$ 模型输出: $P$ 作为损失函数, 交叉熵的作用是 衡量模型输出与真实值的差距, 作为优化算法的优化对象, 还需要尽量简洁, 减少训练模型的开销.</description></item></channel></rss>