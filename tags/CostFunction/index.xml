<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>CostFunction on</title><link>https://alonelysheep.github.io/quartz-blog/tags/CostFunction/</link><description>Recent content in CostFunction on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 28 Feb 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://alonelysheep.github.io/quartz-blog/tags/CostFunction/index.xml" rel="self" type="application/rss+xml"/><item><title>为什么Softmax回归不用MSE</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE/</link><pubDate>Mon, 28 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE/</guid><description>为什么Softmax (或者Logistic) 不用MSE作为损失函数? 2022-02-28 Tags: #DeepLearning #MachineLearning #SoftmaxRegression #LogisticRegression #CostFunction #MeanSquareError #CrossEntropy
回顾:
MSE假设样本误差i.i.d., 并且服从正态分布, 最小化MSE等价于极大似然估计. 通常用于回归问题. MSE基于输出与真实值的欧氏距离.</description></item><item><title>D2L-14-Cross Entropy as Loss</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss/</link><pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss/</guid><description>交叉熵作为损失函数 2022-02-11 Tags: #CostFunction #DeepLearning #CrossEntropy
在作为损失函数的时候, 构成 交叉熵的概率分布为: 真实分布: $P^*$ 模型输出: $P$ 作为损失函数, 交叉熵的作用是 衡量模型输出与真实值的差距, 作为优化算法的优化对象, 还需要尽量简洁, 减少训练模型的开销.</description></item><item><title>Part.28_Cost_Function-Neural_Network(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Part.28_Cost_Function-Neural_NetworkML_Andrew.Ng./</link><pubDate>Sat, 09 Oct 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Part.28_Cost_Function-Neural_NetworkML_Andrew.Ng./</guid><description>Cost Function - Neural Network 2021-10-09 Tags: #MachineLearning #NeuralNetwork #CostFunction
Basic Concepts $$\left{\left(x^{(1)}, y^{(1)}\right), \left(x^{(2)}, y^{(2)}\right), \ldots, \left(x^{(m)}, y^{(m)}\right)\right}$$</description></item><item><title>Part.26_Probabilistic_Interpretation_of_MSE(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.26_Probabilistic_Interpretation_of_MSEML_Andrew.Ng./</link><pubDate>Thu, 16 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.26_Probabilistic_Interpretation_of_MSEML_Andrew.Ng./</guid><description>均方差的合理性 - 概率解释 2021-09-16 Tags: #MachineLearning #Math/Statistics #MeanSquareError #CostFunction
之前的一些讨论 Mean_Squared_Error-均方误差 Why_do_cost_functions_use_the_square_error CS229 - Probabilistic Interpretation 独立同分布-IID [[notes/2021/2021.</description></item><item><title>正则项不影响Logistic回归损失函数凸性</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/%E6%AD%A3%E5%88%99%E9%A1%B9%E4%B8%8D%E5%BD%B1%E5%93%8DLogistic%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%87%B8%E6%80%A7/</link><pubDate>Sat, 11 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/%E6%AD%A3%E5%88%99%E9%A1%B9%E4%B8%8D%E5%BD%B1%E5%93%8DLogistic%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%87%B8%E6%80%A7/</guid><description>正则项不影响Logistic回归损失函数凸性 2021-09-11 Tags: #MachineLearning #LogisticRegression #Regularization #ConvexOptimization #CostFunction
首先, 没有加正则项的二阶导数如下 那么只需要计算正则项的二阶导数 $$\begin{align} J(\theta)&amp;amp;=P(\theta)+\frac\lambda{2m}\sum^n_{i=1}\theta_i^2 \end{align}$$
$$\begin{aligned} \frac{\partial^2}{\partial \theta_{j}^2} \left(\frac\lambda{2m}\sum^n_{i=1}\theta_i^2\right)&amp;amp;= \frac{\lambda}{m}\frac{\partial}{\partial \theta_{j}} \theta_{j}\ &amp;amp;=\frac{\lambda}{m}&amp;gt;0 \end{aligned}$$ 所以损失函数还是凸的</description></item><item><title>证明Logistic回归的损失函数是凸函数</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/%E8%AF%81%E6%98%8ELogistic%E5%9B%9E%E5%BD%92%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%98%AF%E5%87%B8%E5%87%BD%E6%95%B0/</link><pubDate>Sat, 11 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/%E8%AF%81%E6%98%8ELogistic%E5%9B%9E%E5%BD%92%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%98%AF%E5%87%B8%E5%87%BD%E6%95%B0/</guid><description>证明Logistic回归的损失函数是凸函数 2021-09-11 Tags: #MachineLearning #LogisticRegression #ConvexOptimization #CostFunction
证明 原函数 Part.13_Cost_Function-Logistic_Regression(ML_Andrew.Ng.) $$\begin{aligned} h&amp;amp;=g(X \theta) \ J(\theta)&amp;amp;=-\frac{1}{m} \cdot\left[y^{T} \log (h)+(1-y)^{T} \log (1-h)\right] \end{aligned}$$</description></item><item><title>Part.13_Cost_Function-Logistic_Regression(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.13_Cost_Function-Logistic_RegressionML_Andrew.Ng./</link><pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.13_Cost_Function-Logistic_RegressionML_Andrew.Ng./</guid><description>Cost Function - Logistic Regression 2021-08-19 Tags: #CostFunction #LogisticRegression #MachineLearning
Representation 如果我们采用 线性回归的损失函数: 均方误差, 那么因为Logistic 回归的$h(x)$里面有形式很复杂的Logistic函数, 损失函数将不再是 凸函数, 将会很难最小化, 所以我们需要考虑另外的损失函数形式:</description></item><item><title>Part.4_Cost_Function_Intuition(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.4_Cost_Function_IntuitionML_Andrew.Ng./</link><pubDate>Mon, 02 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.4_Cost_Function_IntuitionML_Andrew.Ng./</guid><description>Cost Function Intuition: Linear Regression 2021-08-02 Tags: #MachineLearning #CostFunction #LinearRegression
2-Dimension Intuition 首先简化一下我们的问题, 现在只有三个数据点$(1,1),(2,2),(3,3)$, 我们的Hypothesis Function$:h=\theta_1 x$ 只有一个参数$\theta_1$表示斜率, Cost Function还是: $$ J\left(\theta_{0}, \theta_{1}\right)=\frac{1}{2 m} \sum_{i=1}^{m}\left(\hat{y}^{(i)}-y^{(i)}\right)^{2}=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2} $$ 则我们大概可以把Cost Function的变化过程表示成这样: 可以看出, 斜率为1的时候Cost Function有最小值1, 此时Hypothesis最优.</description></item><item><title>Mean_Squared_Error_均方误差</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Mean_Squared_Error_%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE/</link><pubDate>Sat, 31 Jul 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Mean_Squared_Error_%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE/</guid><description>Mean Squared Error 2021-07-31 Tags: #MachineLearning #CostFunction
Mean Square Error: 平均平方误差, 简称均方差, MSE, 又称 Mean Squared Deviation (MSD)
均方差的形式很简单, 但是也有许多问题值得思考</description></item><item><title>Why_do_cost_functions_use_the_square_error</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error/</link><pubDate>Sat, 31 Jul 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error/</guid><description>Why do cost functions use the square error? 2021-07-31 Tags: #MachineLearning #CostFunction #MeanSquareError
Reference: StackExchange: why-do-cost-functions-use-the-square-error?
StackExchange上面一个关于均方差的一个很好的解释, 翻译如下:
Question: I&amp;rsquo;m just getting started with some machine learning, and until now I have been dealing with linear regression over one variable.</description></item></channel></rss>