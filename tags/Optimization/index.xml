<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Optimization on</title><link>https://alonelysheep.github.io/quartz-blog/tags/Optimization/</link><description>Recent content in Optimization on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 02 Feb 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://alonelysheep.github.io/quartz-blog/tags/Optimization/index.xml" rel="self" type="application/rss+xml"/><item><title>D2L-10-小批量随机梯度下降</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-10-%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</link><pubDate>Wed, 02 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-10-%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</guid><description>小批量随机梯度下降(Mini-Batch)是深度学习默认的求解方法 2022-02-02 Tags: #MachineLearning #GradientDescent #DeepLearning #Optimization
Different_Gradient_Descent_Methods 注意有两个点: 小批量(Mini-Batch), 随机(Stochastic) 梯度下降 其中:
小批量是因为在整个数据集上面训练一次又慢又贵 同时小批量还能从多个相似的数据点中选一个代表来计算, 节约了计算资源 但是样本不能太小, 太小的样本不适合用GPU并行计算 随机是选取小样本的方法: 随机选取</description></item><item><title>拉格朗日乘数-Lagrange_Multiplier</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0-Lagrange_Multiplier/</link><pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0-Lagrange_Multiplier/</guid><description>拉格朗日乘数 2021-10-29 Tags: #Math #Optimization
Intuition $$\mathcal{L}(x, y, \lambda)=f(x, y)-\lambda g(x, y)$$ 在一个三维曲面($f(x,y)$)上面画了一条曲线($g(x,y)$), 求这条曲线上面的最低点.
Explanation 中英文的维基百科已经解释的十分直观与清晰了:
Chinese English</description></item></channel></rss>