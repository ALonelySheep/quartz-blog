<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>InformationTheory on</title><link>https://alonelysheep.github.io/quartz-blog/tags/InformationTheory/</link><description>Recent content in InformationTheory on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 11 Feb 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://alonelysheep.github.io/quartz-blog/tags/InformationTheory/index.xml" rel="self" type="application/rss+xml"/><item><title>Cross_Entropy-交叉熵</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5/</link><pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5/</guid><description>Cross Entropy - 交叉熵 2022-02-11 Tags: #InformationTheory #DeepLearning
Intuition 熵是编码一个事件所需要的最短平均长度 $$\begin{aligned}H(p)&amp;amp;=\sum_{x_{i}} p\left(x_{i}\right) \log \frac{1}{p\left(x_{i}\right)} \ &amp;amp;=-\sum_{x} p(x) \log p(x) \end{aligned}$$</description></item><item><title>Hamming_Distance_汉明距离</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/Hamming_Distance_%E6%B1%89%E6%98%8E%E8%B7%9D%E7%A6%BB/</link><pubDate>Fri, 25 Jun 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/Hamming_Distance_%E6%B1%89%E6%98%8E%E8%B7%9D%E7%A6%BB/</guid><description>Hamming Distance / 汉明距离 汉明距离是对于两个相同长度的字符串而言, the number of positions at which the corresponding symbols are different(相同的位置上对应字符不同的位置个数)
图例 带色线条是路径示意
Two example distances: 100→011 has distance 3; 010→111 has distance 2</description></item><item><title>Entropy-熵</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Entropy-%E7%86%B5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Entropy-%E7%86%B5/</guid><description>Entropy - 熵 2022-02-11
Tags: #InformationTheory
理解 熵是Surprise的期望 Entropy (for data science) Clearly Explained!!! - YouTube
熵是对事件复杂度的衡量, 即我们最少需要多少信息才能完整地描述这个事件 Intuitively Understanding the Shannon Entropy - YouTube</description></item></channel></rss>