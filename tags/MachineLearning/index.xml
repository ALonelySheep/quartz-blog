<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>MachineLearning on</title><link>https://alonelysheep.github.io/quartz-blog/tags/MachineLearning/</link><description>Recent content in MachineLearning on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 14 Jun 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://alonelysheep.github.io/quartz-blog/tags/MachineLearning/index.xml" rel="self" type="application/rss+xml"/><item><title>数据预处理-周期性数据(时间等)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.6/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-%E5%91%A8%E6%9C%9F%E6%80%A7%E6%95%B0%E6%8D%AE%E6%97%B6%E9%97%B4%E7%AD%89/</link><pubDate>Tue, 14 Jun 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.6/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-%E5%91%A8%E6%9C%9F%E6%80%A7%E6%95%B0%E6%8D%AE%E6%97%B6%E9%97%B4%E7%AD%89/</guid><description>周期性数据的预处理 2022-06-14 Tags: #MachineLearning #DataPreprocessing #CyclicFeatureEncoding
Source: Three Approaches to Encoding Time Information as Features for ML Models | NVIDIA Technical Blog</description></item><item><title>端到端学习-End_to_End_Learning-E2E</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.5/%E7%AB%AF%E5%88%B0%E7%AB%AF%E5%AD%A6%E4%B9%A0-End_to_End_Learning-E2E/</link><pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.5/%E7%AB%AF%E5%88%B0%E7%AB%AF%E5%AD%A6%E4%B9%A0-End_to_End_Learning-E2E/</guid><description>端到端学习 2022-05-05 Tags: #EndtoEndLearning #MachineLearning #DeepLearning
端到端的学习就是省略中间步骤，直接从输入得到输出结果。 Pro &amp;amp; Con Pro 不用人为设计中间步骤, 减少了工作量, 并且避免人为添加的步骤给模型带来不好的 归纳偏置. Con 需要足够多的数据才能达到较好的效果</description></item><item><title>D2L-64-Kernel Regression</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-64-Kernel-Regression/</link><pubDate>Wed, 20 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-64-Kernel-Regression/</guid><description>Nadaraya-Watson Kernel Regression 2022-04-20 Tags: #KernelRegression #Nonparametric #Attention #MachineLearning
Intuition Definition $$f(x) = \sum_{i=1}^n \frac{K(x - x_i)}{\sum_{j=1}^n K(x - x_j)} y_i$$</description></item><item><title>为什么Softmax回归不用MSE</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE/</link><pubDate>Mon, 28 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE/</guid><description>为什么Softmax (或者Logistic) 不用MSE作为损失函数? 2022-02-28 Tags: #DeepLearning #MachineLearning #SoftmaxRegression #LogisticRegression #CostFunction #MeanSquareError #CrossEntropy
回顾:
MSE假设样本误差i.i.d., 并且服从正态分布, 最小化MSE等价于极大似然估计. 通常用于回归问题. MSE基于输出与真实值的欧氏距离.</description></item><item><title>归纳偏置-Inductive bias - learning bias</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E5%BD%92%E7%BA%B3%E5%81%8F%E7%BD%AE-Inductive-bias-learning-bias/</link><pubDate>Sat, 26 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E5%BD%92%E7%BA%B3%E5%81%8F%E7%BD%AE-Inductive-bias-learning-bias/</guid><description>Inductive Bias - 归纳偏置 / 归纳偏好 2022-02-26 Tags: #DeepLearning #MachineLearning
当学习器去预测其未遇到过的输入的结果时，会做一些假设（Mitchell, 1980）。而学习算法中的归纳偏置（Inductive bias）则是这些假设的集合。1
一个典型的归纳偏置例子是 奥卡姆剃刀，它假设最简单而又一致的假设是最佳的。这里的一致是指学习器的假设会对所有样本产生正确的结果。
Machine Learning - Mitchell Chapter 2.</description></item><item><title>Norm in Regularization - Intuition</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Norm-in-Regularization-Intuition/</link><pubDate>Mon, 14 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Norm-in-Regularization-Intuition/</guid><description>Norm in Regularization - Intuition 2022-02-14 Tags: #Norm #Regularization #DeepLearning #MachineLearning
L2 Norm $\ell_{2}$ in Regularization L2 Norm 的等高线是圆形的</description></item><item><title>D2L-15-Perceptron</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-15-Perceptron/</link><pubDate>Sat, 12 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-15-Perceptron/</guid><description>Perceptron - 感知机 2022-02-12 Tags: #MachineLearning #Perceptron</description></item><item><title>D2L-13-Softmax_Regression</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-13-Softmax_Regression/</link><pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-13-Softmax_Regression/</guid><description>Softmax 回归 2022-02-11 Tags: #SoftmaxRegression #MachineLearning #Classification #MulticlassClassification
Softmax回归解决的是多分类问题1, 它可以看作是二分类的 Logistic_Regression的推广. Softmax回归 Softmax回归就是在线性回归的基础上套上一个Softmax函数, 取输出结果中概率最大的项作为预测结果. 交叉熵作为损失函数 D2L-14-Cross Entropy as Loss</description></item><item><title>Likelihood_Function-似然函数</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Likelihood_Function-%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0/</link><pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Likelihood_Function-%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0/</guid><description>Likelihood Function - 似然函数 2022-02-11 Tags: #Math/Statistics #MachineLearning
对于某个(某组)随机变量 $X$, 我们通过采样获得了数据集 $x$ :
似然函数$\mathcal{L}(\theta \mid x)$就是在某个参数(parameter) $\theta$ 下, 现有数据 $x$ 出现的概率大小, 也就是说: $$\mathcal{L}(\theta \mid x) = P(X=x\mid\theta)$$ $P(X=x\mid\theta)$ 也常常写作 $p_{\theta}(x)=P_{\theta}(X=x)=P(X=x\space ;\theta)$</description></item><item><title>D2L-11-泛化(Generalization)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-11-%E6%B3%9B%E5%8C%96Generalization/</link><pubDate>Tue, 08 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-11-%E6%B3%9B%E5%8C%96Generalization/</guid><description>Generalization: 泛化 2022-02-08 Tags: #MachineLearning #DeepLearning
线性回归恰好是一个在整个域中只有一个最小值的学习问题。 1但是对于像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。
深度学习实践者很少会去花费大力气寻找这样一组参数，使得在_训练集_上的损失达到最小。 事实上，更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失， 这一挑战被称为_泛化_（generalization）。
可以证明, 正则后的线性回归损失函数MSE依然是凸的: 正则项不影响线性回归损失函数的凸性&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>D2L-10-小批量随机梯度下降</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-10-%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</link><pubDate>Wed, 02 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-10-%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</guid><description>小批量随机梯度下降(Mini-Batch)是深度学习默认的求解方法 2022-02-02 Tags: #MachineLearning #GradientDescent #DeepLearning #Optimization
Different_Gradient_Descent_Methods 注意有两个点: 小批量(Mini-Batch), 随机(Stochastic) 梯度下降 其中:
小批量是因为在整个数据集上面训练一次又慢又贵 同时小批量还能从多个相似的数据点中选一个代表来计算, 节约了计算资源 但是样本不能太小, 太小的样本不适合用GPU并行计算 随机是选取小样本的方法: 随机选取</description></item><item><title>D2L-6-计算图</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-6-%E8%AE%A1%E7%AE%97%E5%9B%BE/</link><pubDate>Wed, 02 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-6-%E8%AE%A1%E7%AE%97%E5%9B%BE/</guid><description>计算图 2022-02-02 Tags: #MachineLearning #DeepLearning
将计算表示为一个无环图 例子 线性回归: 计算图有两种构造方法: 显式构造 主要应用于: Tensorflow/Theano/MXNet 例子: 1 2 3 4 5 from mxnet import sym a = sym.</description></item><item><title>D2L-9-梯度下降的方向</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-9-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E6%96%B9%E5%90%91/</link><pubDate>Wed, 02 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-9-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E6%96%B9%E5%90%91/</guid><description>梯度下降的方向是梯度的反方向 2022-02-02 Tags: #GradientDescent #DeepLearning #MachineLearning
梯度是一个函数增长最快的方向, 通常我们都是想获得损失函数的最小值, 所以需要沿着梯度的反方向来移动.
注意这并不是一定的, 梯度下降/上升只是一种优化方法而已, 如果我们想要优化的目标函数取得最大值, 那么就应该沿着梯度的方向变化.</description></item><item><title>Bayesian_Estimation(Inference)贝叶斯估计</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Bayesian_EstimationInference%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/</link><pubDate>Mon, 27 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Bayesian_EstimationInference%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/</guid><description>贝叶斯估计 2021-12-27 Tags: #Bayes #MachineLearning
贝叶斯估计 有点难, 还要进一步学习
贝叶斯估计是一种参数估计方法, 不只局限于设计贝叶斯分类器 贝叶斯估计的核心在于用新的样本来更新旧的Prior, 一起得到一个PostPrior的参数的概率分布, 合并的过程利用的是贝叶斯分布. 贝叶斯估计和极大似然估计的最大不同就是贝叶斯估计的是参数可能的概率分布, 而不是一个确定的值. 通过对这个概率分布进行积分, 我们可以平均地得到所有情况下最可能出现的参数. 这两篇文章写的很好:</description></item><item><title>Maximum_Likelihood_Estimation-极大似然估计</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Maximum_Likelihood_Estimation-%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/</link><pubDate>Sat, 25 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Maximum_Likelihood_Estimation-%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/</guid><description>极大似然估计 MLE 2021-12-25 Tags: #MachineLearning #Math/Statistics
Links: Likelihood_Function-似然函数
假设样本 $X$ 服从已知的概率分布(比如正态分布)
极大似然估计就是要找一个参数 $\hat\theta$, 使似然函数 $\mathcal{L}(\theta \mid X)$ 取得最大值$$i.e.\quad \hat{\theta}=\operatorname{argmax}_{\theta \in \Theta} \mathcal{L}(\theta \mid X)$$ 极大似然估计认为: 最佳的参数 $\hat\theta$ 最可能使取样结果为现在的 $x$, 也就是说, 概率$P(X=x\mid \theta)$最大: $$\hat{\theta}=\operatorname{argmax}_{\theta \in \Theta} P(X=x\mid \theta)$$</description></item><item><title>参数估计-Parameter_Estimation</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1-Parameter_Estimation/</link><pubDate>Sat, 25 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1-Parameter_Estimation/</guid><description>参数估计 2021-12-25 Tags: #MachineLearning #ParameterEstimation #Math/Statistics
在设计分类器或者进行回归预测的时候, 我们需要知道目标问题的概率分布情况. 但是通常我们能得到的数据只是一些特例(即训练样本). 为了对问题进行建模, 我们不仅需要确定合适的概率分布模型, 还需要根据训练样本确定模型里面的具体参数. 参数估计就是在模型已知的情况下得到最优参数的过程.
对于贝叶斯分类器, 估计先验概率$P(\omega_i)$通常不是很困难. 难点在于估计类条件概率密度$p(x|\omega_i)$, 这是因为:</description></item><item><title>正态分布的判别函数</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E7%9A%84%E5%88%A4%E5%88%AB%E5%87%BD%E6%95%B0/</link><pubDate>Fri, 24 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E7%9A%84%E5%88%A4%E5%88%AB%E5%87%BD%E6%95%B0/</guid><description>Discriminant Function of Gaussian 2021-12-24 Tags: #MachineLearning #DiscriminantFunction #GaussianDistribution
下面是学习Duda模式分类第二章做的简单的笔记, 有时间应该进一步梳理
高斯分布的判别函数(贝叶斯分类器)的一个常见形式是把Bayes定理的分子取下来, 再取对数.
即以下形式: $$g_{i}(\mathbf{x})=\ln p\left(\mathbf{x} \mid \omega_{i}\right)+\ln P\left(\omega_{i}\right)$$</description></item><item><title>Bayesian Decision Theory - Part1</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Bayesian-Decision-Theory-Part1/</link><pubDate>Tue, 21 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Bayesian-Decision-Theory-Part1/</guid><description>贝叶斯决策论 - Part1 2021-12-21 Tags: #MachineLearning #Bayes
贝叶斯决策其实就是把生活中我们基于直觉和常识的决策方法形式化了, 并加以进一步地推广. 贝叶斯决策综合考量每种情况的概率和决策带来的代价. 贝叶斯决策假设问题可以用概率分布的形式来刻画, 属于贝叶斯学派的一种方法. Intro A Sad Case 假如圣诞老人打包了100盒糖果, 其中20盒是巧克力($\omega_1$), 80盒是水果硬糖($\omega_2$), 现在他从里面随机挑了一盒给小企鹅, 盒子的颜色和重量都一样, 那么小企鹅得到的是巧克力还是水果硬糖呢?</description></item><item><title>Cross Validation</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Cross-Validation/</link><pubDate>Sun, 19 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Cross-Validation/</guid><description>Cross Validation 2021-12-19 Tags: #MachineLearning
交叉验证是一种评价模型好坏的方法. 设置验证集的目的在于减少样本给模型带来的Bias, 即我们想要找到一个普遍适用的模型, 而不是只在训练集上表现很好的模型. 交叉验证是一种增大验证集, 充分利用数据的方法. 交叉验证的方法 Cross-validation (statistics) - Wikipedia Machine Learning Fundamentals: Cross Validation - YouTube 简单的来说, 交叉验证会把训练集随机分成几个部分.</description></item><item><title>Johnson Lindenstrauss Lemma - Publish Version</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Johnson-Lindenstrauss-Lemma-Publish-Version/</link><pubDate>Fri, 03 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Johnson-Lindenstrauss-Lemma-Publish-Version/</guid><description>Johnson Lindenstrauss Lemma 2021-12-03 Tags: #MachineLearning #Math
对于高维数据，我们能够在降维的过程中保留其大部分的几何特征，即使降维的幅度非常大。 这是徐亦达老师让我们学习的第一个主题
1
Study Materials MIT 6.</description></item><item><title>Part.30_Dimensionality_Reduction(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/Part.30_Dimensionality_ReductionML_Andrew.Ng./</link><pubDate>Thu, 11 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/Part.30_Dimensionality_ReductionML_Andrew.Ng./</guid><description>Dimensionality Reduction - 降维 2021-11-11 Tags: #MachineLearning #DimensionalityReduction
Motivation Data Compression 将数据维数减少后, 可以节约存储数据的空间
Visualization 高维数据降维后才能可视化, 而可视化有助于我们理解高维数据的内在含义.
Principal Component Analysis [[notes/2021/2021.</description></item><item><title>Part.31_Principal_Component_Analysis(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/Part.31_Principal_Component_AnalysisML_Andrew.Ng./</link><pubDate>Thu, 11 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/Part.31_Principal_Component_AnalysisML_Andrew.Ng./</guid><description>Principal Component Analysis - 主成分分析 2021-11-11 Tags: #MachineLearning #DimensionalityReduction #PCA
基本步骤 Step 0 - Data Preprocessing Normalization 归一化 Standardization 标准化 PCA依赖于欧氏距离, 所以预处理数据可以让降维效果更好.</description></item><item><title>Part.29_Fisher_Linear_Discriminant(Pattern_Classification-Chapter_4)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Part.29_Fisher_Linear_DiscriminantPattern_Classification-Chapter_4/</link><pubDate>Thu, 28 Oct 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Part.29_Fisher_Linear_DiscriminantPattern_Classification-Chapter_4/</guid><description>Fisher Linear Discriminant 2021-10-28 Tags: #MachineLearning #PatternClassification #Course #DimensionalityReduction
通过降维进行分类, 降到一维即为线性判别. 其实, 线性判别分析 (LDA)就是对Fisher线性判别的归纳.1 Motivation Curse of Dimensionality - 模型的表现随着维度的增加而变坏, 而且根据设计者的3维直觉, 无法很好的解决高维度的问题.</description></item><item><title>Part.28_Cost_Function-Neural_Network(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Part.28_Cost_Function-Neural_NetworkML_Andrew.Ng./</link><pubDate>Sat, 09 Oct 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Part.28_Cost_Function-Neural_NetworkML_Andrew.Ng./</guid><description>Cost Function - Neural Network 2021-10-09 Tags: #MachineLearning #NeuralNetwork #CostFunction
Basic Concepts $$\left{\left(x^{(1)}, y^{(1)}\right), \left(x^{(2)}, y^{(2)}\right), \ldots, \left(x^{(m)}, y^{(m)}\right)\right}$$</description></item><item><title>Part.27_Locally_Weighted_Linear_Regression(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.27_Locally_Weighted_Linear_RegressionML_Andrew.Ng./</link><pubDate>Thu, 30 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.27_Locally_Weighted_Linear_RegressionML_Andrew.Ng./</guid><description>Locally Weighted Linear Regression 2021-09-30 Tags: #MachineLearning #LinearRegression
Abbreviation: LWR
上图展现了Underfitting &amp;amp; Overfitting的情况，而 Locally weighted linear regression (LWR) is an algorithm which, assuming there is sufficient training data, makes the choice of features less critical.</description></item><item><title>ROC_and_AUC_Graph</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/ROC_and_AUC_Graph/</link><pubDate>Thu, 30 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/ROC_and_AUC_Graph/</guid><description>ROC AUC Clearly Explained 2021-09-30 Tags: #MachineLearning #ROC #AUC
ROC: Receiver Operator Characteristic 用来判断哪一个是最好的Classification Threshold. AUC: the area under the curve, 用来判断哪一个是最好的模型</description></item><item><title>Part.26_Probabilistic_Interpretation_of_MSE(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.26_Probabilistic_Interpretation_of_MSEML_Andrew.Ng./</link><pubDate>Thu, 16 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.26_Probabilistic_Interpretation_of_MSEML_Andrew.Ng./</guid><description>均方差的合理性 - 概率解释 2021-09-16 Tags: #MachineLearning #Math/Statistics #MeanSquareError #CostFunction
之前的一些讨论 Mean_Squared_Error-均方误差 Why_do_cost_functions_use_the_square_error CS229 - Probabilistic Interpretation 独立同分布-IID [[notes/2021/2021.</description></item><item><title>Part.23_Forward_Propagation-Neural_Network(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.23_Forward_Propagation-Neural_NetworkML_Andrew.Ng./</link><pubDate>Sun, 12 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.23_Forward_Propagation-Neural_NetworkML_Andrew.Ng./</guid><description>Forward Propagation 2021-09-12 Tags: #NeuralNetwork #MachineLearning
Vectorized Implementation $$\begin{aligned} a_{1}^{(2)} &amp;amp;=g\left(\Theta_{10}^{(1)} x_{0}+\Theta_{11}^{(1)} x_{1}+\Theta_{12}^{(1)} x_{2}+\Theta_{13}^{(1)} x_{3}\right) \ a_{2}^{(2)} &amp;amp;=g\left(\Theta_{20}^{(1)} x_{0}+\Theta_{21}^{(1)} x_{1}+\Theta_{22}^{(1)} x_{2}+\Theta_{23}^{(1)} x_{3}\right) \ a_{3}^{(2)} &amp;amp;=g\left(\Theta_{30}^{(1)} x_{0}+\Theta_{31}^{(1)} x_{1}+\Theta_{32}^{(1)} x_{2}+\Theta_{33}^{(1)} x_{3}\right) \ \end{aligned}$$ 我们把Sigmoid函数里面的部分用$z$代替: $$\begin{aligned} &amp;amp;a_{1}^{(2)}=g\left(z_{1}^{(2)}\right) \ &amp;amp;a_{2}^{(2)}=g\left(z_{2}^{(2)}\right) \ &amp;amp;a_{3}^{(2)}=g\left(z_{3}^{(2)}\right) \end{aligned}$$ 同时: $$\begin{aligned} &amp;amp;X = \left[\begin{array}{cccc} x_0 \x_1 \x_2 \x_3 \ \end{array}\right] =a^{(1)} \end{aligned}$$</description></item><item><title>Part.24_Neural_Network-Examples(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.24_Neural_Network-ExamplesML_Andrew.Ng./</link><pubDate>Sun, 12 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.24_Neural_Network-ExamplesML_Andrew.Ng./</guid><description>Examples of Neural Network 2021-09-12 Tags: #NeuralNetwork #MachineLearning
我们可以通过以下直观的组合过程, 体会神经网络利用Linearity构建Non-Linearity的过程. OR function AND function NOT function Putting together -&amp;gt; XNOR Function</description></item><item><title>Part.25_Multiclass_Classification-Neural_Network(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.25_Multiclass_Classification-Neural_NetworkML_Andrew.Ng./</link><pubDate>Sun, 12 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.25_Multiclass_Classification-Neural_NetworkML_Andrew.Ng./</guid><description>Multiclass Classification 2021-09-12 Tags: #MachineLearning #NeuralNetwork
We can define our set of resulting classes as y: $$ y^{(i)}=\left[\begin{array}{l} 1 \ 0 \ 0 \ 0 \end{array}\right],\left[\begin{array}{l} 0 \ 1 \ 0 \ 0 \end{array}\right],\left[\begin{array}{l} 0 \ 0 \ 1 \ 0 \end{array}\right],\left[\begin{array}{l} 0 \ 0 \ 0 \ 1 \end{array}\right] $$ Each $y^{(i)}$ represents a different image corresponding to either a car, pedestrian, truck, or motorcycle.</description></item><item><title>Part.20_Regularized_Logistic_Regression(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.20_Regularized_Logistic_RegressionML_Andrew.Ng./</link><pubDate>Sat, 11 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.20_Regularized_Logistic_RegressionML_Andrew.Ng./</guid><description>Regularized Logistic Regression 2021-09-11 Tags: #MachineLearning #LogisticRegression #Regularization
回顾一下没有正则化的情况 损失函数 更简洁的形式 $$\begin{align} J(\theta) &amp;amp;=-\frac{1}{m} \sum_{i=1}^{m}\left[y^{(i)} \log \left(h_{\theta}\left(x^{(i)}\right)\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]\ \end{align}$$ or 向量化的 $$\begin{aligned} h&amp;amp;=g(X \theta) \ J(\theta)&amp;amp;=-\frac{1}{m} \cdot\left[y^{T} \log (h)+(1-y)^{T} \log (1-h)\right] \end{aligned}$$</description></item><item><title>Part.21_Neural_Network_Introduction(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.21_Neural_Network_IntroductionML_Andrew.Ng./</link><pubDate>Sat, 11 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.21_Neural_Network_IntroductionML_Andrew.Ng./</guid><description>Neural Network - Introduction 2021-09-11 Tags: #NeuralNetwork #MachineLearning
Non-linear Hypotheses 所以随着特征数量的提高, 如果需要更加复杂的假设函数, 特征数量会爆炸式地增长. 比如在图片处理的时候, 每一个像素都是一个特征(RGB彩色像素甚至是三个特征), 那么特征的数目将会是十分庞大的: The “one learning algorithm” hypothesis Neurons in the Brain 树突: Dendrite 轴突: Axon 神经脉冲: Spike</description></item><item><title>Part.22_Model_Representation-Neural_Network(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.22_Model_Representation-Neural_NetworkML_Andrew.Ng./</link><pubDate>Sat, 11 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.22_Model_Representation-Neural_NetworkML_Andrew.Ng./</guid><description>Model Representation - NN 2021-09-11 Tags: #NeuralNetwork #MachineLearning
Hypothesis 一个神经元(Neuron / Activation Unit)的输出计算公式由如下公式给出: $$h_\Theta(x)= a = g(x_0\theta_0+x_1\theta_1+\cdots+x_n\theta_n)$$ 是线性的. ($g(x)$是Sigmoid Function)</description></item><item><title>正则项不影响Logistic回归损失函数凸性</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/%E6%AD%A3%E5%88%99%E9%A1%B9%E4%B8%8D%E5%BD%B1%E5%93%8DLogistic%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%87%B8%E6%80%A7/</link><pubDate>Sat, 11 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/%E6%AD%A3%E5%88%99%E9%A1%B9%E4%B8%8D%E5%BD%B1%E5%93%8DLogistic%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%87%B8%E6%80%A7/</guid><description>正则项不影响Logistic回归损失函数凸性 2021-09-11 Tags: #MachineLearning #LogisticRegression #Regularization #ConvexOptimization #CostFunction
首先, 没有加正则项的二阶导数如下 那么只需要计算正则项的二阶导数 $$\begin{align} J(\theta)&amp;amp;=P(\theta)+\frac\lambda{2m}\sum^n_{i=1}\theta_i^2 \end{align}$$
$$\begin{aligned} \frac{\partial^2}{\partial \theta_{j}^2} \left(\frac\lambda{2m}\sum^n_{i=1}\theta_i^2\right)&amp;amp;= \frac{\lambda}{m}\frac{\partial}{\partial \theta_{j}} \theta_{j}\ &amp;amp;=\frac{\lambda}{m}&amp;gt;0 \end{aligned}$$ 所以损失函数还是凸的</description></item><item><title>证明Logistic回归的损失函数是凸函数</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/%E8%AF%81%E6%98%8ELogistic%E5%9B%9E%E5%BD%92%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%98%AF%E5%87%B8%E5%87%BD%E6%95%B0/</link><pubDate>Sat, 11 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/%E8%AF%81%E6%98%8ELogistic%E5%9B%9E%E5%BD%92%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%98%AF%E5%87%B8%E5%87%BD%E6%95%B0/</guid><description>证明Logistic回归的损失函数是凸函数 2021-09-11 Tags: #MachineLearning #LogisticRegression #ConvexOptimization #CostFunction
证明 原函数 Part.13_Cost_Function-Logistic_Regression(ML_Andrew.Ng.) $$\begin{aligned} h&amp;amp;=g(X \theta) \ J(\theta)&amp;amp;=-\frac{1}{m} \cdot\left[y^{T} \log (h)+(1-y)^{T} \log (1-h)\right] \end{aligned}$$</description></item><item><title>Part.18_Regularization_Intuition(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.18_Regularization_IntuitionML_Andrew.Ng./</link><pubDate>Fri, 10 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.18_Regularization_IntuitionML_Andrew.Ng./</guid><description>Regularization: Intuition 2021-09-10 Tags: #MachineLearning #Regularization
如果我们约束的参数&amp;quot;加大权重&amp;quot;, 那么在优化的时候就会重点最小化那些加了权重的参数. E.g. $$ \theta_{0}+\theta_{1} x+\theta_{2} x^{2}+\theta_{3} x^{3}+\theta_{4} x^{4} $$ We&amp;rsquo;ll want to eliminate the influence of $\theta_{3} x^{3}$ and $\theta_{4} x^{4}$.</description></item><item><title>Part.19_Regularized_Linear_Regression(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.19_Regularized_Linear_RegressionML_Andrew.Ng./</link><pubDate>Fri, 10 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.19_Regularized_Linear_RegressionML_Andrew.Ng./</guid><description>Regularization &amp;amp; Linear Regression 2021-09-10 Tags: #MachineLearning #Regularization #GradientDescent #LinearRegression #NormalEquation
Regularization &amp;amp; Gradient Descent 添加了正则项之后有两点需要注意:
$\theta_0$需要单独处理 (不需要正则约束, 损失函数不一样) $\theta_1 \sim \theta_n$ 因为需要正则化, 损失函数$J(\theta)$发生了变化, 梯度需要重新计算 正则项不影响线性回归损失函数的凸性 同时考虑上面两点, 梯度下降更新公式变为了:</description></item><item><title>正则项不影响线性回归损失函数的凸性</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/%E6%AD%A3%E5%88%99%E9%A1%B9%E4%B8%8D%E5%BD%B1%E5%93%8D%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%87%B8%E6%80%A7/</link><pubDate>Fri, 10 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/%E6%AD%A3%E5%88%99%E9%A1%B9%E4%B8%8D%E5%BD%B1%E5%93%8D%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%87%B8%E6%80%A7/</guid><description>正则项不影响线性回归损失函数的凸性 2021-09-10 Tags: #MachineLearning #Regularization #GradientDescent #LinearRegression #ConvexOptimization
Question: 加上正则项以后函数还是凸的吗? 梯度下降还适用吗? 还是适用的, 证明如下 首先, 如何证明一个函数为凸函数? 如果$f$是二阶可微的，那么如果$f$的定义域是凸集，并且$\forall x\in dom(f), \nabla^2 f(x)\geqslant0$，那么$f$ 就是一个凸函数.</description></item><item><title>Norm_of_a_Vector-Matrix</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Norm_of_a_Vector-Matrix/</link><pubDate>Fri, 20 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Norm_of_a_Vector-Matrix/</guid><description>向量/矩阵的范数 2021-08-20 Tags: #Norm #Math #MachineLearning #Regularization
https://zh.wikipedia.org/wiki/%E8%8C%83%E6%95%B0</description></item><item><title>Part.17_Overfitting_Underfitting(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.17_Overfitting_UnderfittingML_Andrew.Ng./</link><pubDate>Fri, 20 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.17_Overfitting_UnderfittingML_Andrew.Ng./</guid><description>Overfitting Underfitting 2021-08-20 Tags: #Overfitting #Underfitting #MachineLearning
Underfitting Underfitting 的另一 种表述是这个模型有 &amp;ldquo;High Bias&amp;rdquo;, 直观上理解, 这个模型对数据集有着先入为主的&amp;quot;偏见&amp;quot;, &amp;ldquo;不允许&amp;quot;数据集为二次的, 导致预测效果不好. Bias=Preconception
原因: 模型太简单/使用的特征太少</description></item><item><title>Part.11_Classification(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.11_ClassificationML_Andrew.Ng./</link><pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.11_ClassificationML_Andrew.Ng./</guid><description>Classification 2021-08-19 Tags: #MachineLearning #Classification
分类问题最简单的情况是二分类问题(Binary Classification), 更一般的情况是多分类问题.
分类问题与回归问题最大的不同是其对输出的要求是离散的, 线性函数/回归在分类问题上面不适用.</description></item><item><title>Part.12_Logistic_Regression(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.12_Logistic_RegressionML_Andrew.Ng./</link><pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.12_Logistic_RegressionML_Andrew.Ng./</guid><description>Logistic Regression 2021-08-19 Tags: #LogisticRegression #MachineLearning #Classification
Logistic Function Hypothesis Representation 我们可以通过对线性回归的方法进行一些小改动来匹配回归问题, 在线性回归的时候, $h(x)$的输出与分类问题的&amp;quot;值域&amp;quot;偏差较大, 比如在二分类问题里面, 要求$y=0\space or\space 1$, 但是$h(x)$会输出大于一或者小于零的数.</description></item><item><title>Part.13_Cost_Function-Logistic_Regression(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.13_Cost_Function-Logistic_RegressionML_Andrew.Ng./</link><pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.13_Cost_Function-Logistic_RegressionML_Andrew.Ng./</guid><description>Cost Function - Logistic Regression 2021-08-19 Tags: #CostFunction #LogisticRegression #MachineLearning
Representation 如果我们采用 线性回归的损失函数: 均方误差, 那么因为Logistic 回归的$h(x)$里面有形式很复杂的Logistic函数, 损失函数将不再是 凸函数, 将会很难最小化, 所以我们需要考虑另外的损失函数形式:</description></item><item><title>Part.14_Logistic_Regression&amp;Gradient_Descent(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.14_Logistic_RegressionGradient_DescentML_Andrew.Ng./</link><pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.14_Logistic_RegressionGradient_DescentML_Andrew.Ng./</guid><description>Logistic Regression &amp;amp; Gradient Descent 2021-08-19 Tags: #LogisticRegression #GradientDescent #MachineLearning
Gradient Descent: Cost Function: 推导 损失函数里面的$g(x)$为Logistic函数, Logistic的导函数为: $$\begin{aligned} \frac {d}{dx}g(x)&amp;amp;=g(x)\left(1-g(x)\right)\ \end{aligned}$$</description></item><item><title>Part.15_Advanced_Optimization(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.15_Advanced_OptimizationML_Andrew.Ng./</link><pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.15_Advanced_OptimizationML_Andrew.Ng./</guid><description>Advanced Optimization 2021-08-19 Tags: #Octave #MachineLearning #GradientDescent #LinearRegression #LogisticRegression
More sophisticated, faster way to optimize parameters: Conjugate gradient BFGS L-BFGS Link:其他Gradient_Descent Different_Gradient_Descent_Methods</description></item><item><title>Part.16_MulticlassClassification-One_vs_Rest(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.16_MulticlassClassification-One_vs_RestML_Andrew.Ng./</link><pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.16_MulticlassClassification-One_vs_RestML_Andrew.Ng./</guid><description>One vs Rest 2021-08-19 Tags: #MulticlassClassification #Classification #MachineLearning
AKA: One vs All MulticlassClassification $$\begin{aligned} &amp;amp;y \in{0,1 \ldots n} \ &amp;amp;h_{\theta}^{(0)}(x)=P(y=0 \mid x ; \theta) \ &amp;amp;h_{\theta}^{(1)}(x)=P(y=1 \mid x ; \theta) \ &amp;amp;\cdots \ &amp;amp;h_{\theta}^{(n)}(x)=P(y=n \mid x ; \theta) \ &amp;amp;\text { prediction }=\max {i}\left(h{\theta}^{(i)}(x)\right) \end{aligned}$$</description></item><item><title>Sigmoid_Function</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Sigmoid_Function/</link><pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Sigmoid_Function/</guid><description>Sigmoid Function 2021-08-19 Tags: #Sigmoid #MachineLearning #ActivationFunction
什么是Sigmoid函数? Sigmoid的含义是像S型的, 所以Sigmoid函数便是具有S形状的一类函数.
Sigmoid函数把整个实数域上的任意数映射到一个有限的区间里面: $(0,1)$
在分类问题里面, it&amp;rsquo;s useful for transforming an arbitrary-valued function into a function better suited for classification.</description></item><item><title>Part.10_Octave_Tutorial(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.10_Octave_TutorialML_Andrew.Ng./</link><pubDate>Wed, 18 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.10_Octave_TutorialML_Andrew.Ng./</guid><description>Octave Tutorial 2021-08-18 Tags: #Octave #MachineLearning
还是要在实践中学习Octave 为什么吴恩达说Octave比Python好呢? 或许这里涉及到编程与建模的区别? 建模的目的是快速实现一个模型, 像 Matlab Octave Labview之类的软件的目标就是快速建模, 而编程语言像是Python之类的, 他们的目的偏向于 建立一个模型的可靠的应用实例, 不仅要实现, 还需要可靠, 性能需要优化 但是像IPython Console, Jupyter Notebook之类的交互式编程界面是否已经打破了这两个之间的隔阂?</description></item><item><title>矩阵的求导</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E7%9A%84%E6%B1%82%E5%AF%BC/</link><pubDate>Mon, 16 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E7%9A%84%E6%B1%82%E5%AF%BC/</guid><description>对矩阵的求导_Matrix_Derivative 2021-08-16 Tags: #Matrix #Derivative #Calculus #MachineLearning
在学习吴恩达机器学习CS229的时候为了推导Normal Equation的公式, 接触到了函数对于矩阵的求导, 因为许久没有接触微积分, 并且知识跨度太大, 许久没有看懂, 故在此笔记中慢慢梳理. Learning Materials: Pili HU, Matrix Calculus, https://github.</description></item><item><title>Part.9_Normal_Equation(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.9_Normal_EquationML_Andrew.Ng./</link><pubDate>Sat, 14 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.9_Normal_EquationML_Andrew.Ng./</guid><description>Normal Equation 2021-08-14 Tags: #MachineLearning #NormalEquation #LinearRegression
Normal Equation 是解 线性回归(Linear Regression) 问题的一种代数方法. Definition The value of $\theta$ that minimizes $J(\theta)$ can be given in closed form by the equation $$ \theta=\left(X^{T} X\right)^{-1} X^{T} \vec{y} $$</description></item><item><title>Part.7_Feature_Scaling(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.7_Feature_ScalingML_Andrew.Ng./</link><pubDate>Fri, 06 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.7_Feature_ScalingML_Andrew.Ng./</guid><description>Feature Scaling 2021-08-06 Tags: #MachineLearning #FeatureEngineering
1
深入阅读的链接: https://sebastianraschka.com/Articles/2014_about_feature_scaling.html
When to Use 在梯度下降的时候, 缩放数据可以让梯度变化更平滑 If an algorithm uses gradient descent, then the difference in ranges of features will cause different step sizes for each feature.</description></item><item><title>Part.8_Train_Gradient_Descent(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.8_Train_Gradient_DescentML_Andrew.Ng./</link><pubDate>Fri, 06 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.8_Train_Gradient_DescentML_Andrew.Ng./</guid><description>Train Gradient Descent 2021-08-06 Tags: #GradientDescent #MachineLearning
判断收敛(Convergence)的方法 画出Cost Function - Iteration图, 平缓后收敛 相邻周期变化值小于一个很小的值$\Delta$ 寻找正常的学习率 只要学习率$\alpha$足够小, 损失函数一定是递减的(可以严格证明) 如果学习率波动或者递增, 常常是因为学习率过大 学习率过大也有一定几率导致收敛缓慢 学习率过小会导致收敛过慢 合适的方法是类似于二分法的思路, 用一系列的值去尝试, e.</description></item><item><title>Relation_Between_Linear_Regression&amp;Gradient_Descent_梯度下降和线性回归的关系</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Relation_Between_Linear_RegressionGradient_Descent_%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%92%8C%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E5%85%B3%E7%B3%BB/</link><pubDate>Thu, 05 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Relation_Between_Linear_RegressionGradient_Descent_%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%92%8C%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E5%85%B3%E7%B3%BB/</guid><description>梯度下降法和线性回归的关系 2021-08-05 Tags: #MachineLearning #LinearRegression #GradientDescent
1 2 graph TD A([梯度下降])--&amp;gt;B([梯度下降+平方损失])--&amp;gt;C([梯度下降+平方损失+线性回归]) 梯度下降法公式 $$ \begin{array}{l} \text { repeat until convergence }{\ \begin{array}{cc} \theta_{j}:=\theta_{j}-\alpha \frac{\Large\partial}{\Large\partial \Large\theta_{j}} J\left(\theta_{0},\cdots ,\theta_{n}\right) &amp;amp; \text { (simultaneously update } j=0, \cdots ,j=n) \end{array}\ \text { } } \end{array} $$ 梯度下降 + Cost Function=平方损失 $$J\left(\theta_{0},\cdots ,\theta_{n}\right)=\frac{1}{2 m} \sum_{i=1}^{m}\left(\hat{y}^{(i)}-y^{(i)}\right)^{2}=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}$$ 所以</description></item><item><title>Part.4_Cost_Function_Intuition(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.4_Cost_Function_IntuitionML_Andrew.Ng./</link><pubDate>Mon, 02 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.4_Cost_Function_IntuitionML_Andrew.Ng./</guid><description>Cost Function Intuition: Linear Regression 2021-08-02 Tags: #MachineLearning #CostFunction #LinearRegression
2-Dimension Intuition 首先简化一下我们的问题, 现在只有三个数据点$(1,1),(2,2),(3,3)$, 我们的Hypothesis Function$:h=\theta_1 x$ 只有一个参数$\theta_1$表示斜率, Cost Function还是: $$ J\left(\theta_{0}, \theta_{1}\right)=\frac{1}{2 m} \sum_{i=1}^{m}\left(\hat{y}^{(i)}-y^{(i)}\right)^{2}=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2} $$ 则我们大概可以把Cost Function的变化过程表示成这样: 可以看出, 斜率为1的时候Cost Function有最小值1, 此时Hypothesis最优.</description></item><item><title>Part.5_Gradient_Descent(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.5_Gradient_DescentML_Andrew.Ng./</link><pubDate>Mon, 02 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.5_Gradient_DescentML_Andrew.Ng./</guid><description>Gradient Descent 2021-08-02 Tags: #MachineLearning #GradientDescent
梯度下降是一种最小化损失函数的标准方法 So we have our hypothesis function and we have a way of measuring how well it fits into the data.</description></item><item><title>凸优化与线性回归问题</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/%E5%87%B8%E4%BC%98%E5%8C%96%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98/</link><pubDate>Mon, 02 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/%E5%87%B8%E4%BC%98%E5%8C%96%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98/</guid><description>Gradient Descent &amp;amp; Convex Optimization / 凸优化 2021-08-02 Tags: #MachineLearning #ConvexOptimization #Math
在 这里(和下面的引用里面), 我们特殊的线性规划的损失函数一定是一个凸函数, 那么在其他情况下, 线性规划还是凸函数吗, 线性规划问题会陷入局部最优的问题中去吗?
Note that, while gradient descent can be susceptible to local minima in general, the optimization problem we have posed here for linear regression has only one global, and no other local, optima; thus gradient descent always converges (assuming the learning rate α is not too large) to the global minimum.</description></item><item><title>Mean_Squared_Error_均方误差</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Mean_Squared_Error_%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE/</link><pubDate>Sat, 31 Jul 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Mean_Squared_Error_%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE/</guid><description>Mean Squared Error 2021-07-31 Tags: #MachineLearning #CostFunction
Mean Square Error: 平均平方误差, 简称均方差, MSE, 又称 Mean Squared Deviation (MSD)
均方差的形式很简单, 但是也有许多问题值得思考</description></item><item><title>Part.3_Linear_Regression(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.3_Linear_RegressionML_Andrew.Ng./</link><pubDate>Sat, 31 Jul 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.3_Linear_RegressionML_Andrew.Ng./</guid><description>Linear Regression 2021-07-31 Tags: #MachineLearning #SelfLearning
Model Representation Supervised Learning Regression Problem Structure 基于训练集, 我们希望通过学习算法得到一个Hypothesis函数$h$, 在房价预测问题上. 输入房子的大小, 得到估计的价格.</description></item><item><title>Why_do_cost_functions_use_the_square_error</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error/</link><pubDate>Sat, 31 Jul 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Why_do_cost_functions_use_the_square_error/</guid><description>Why do cost functions use the square error? 2021-07-31 Tags: #MachineLearning #CostFunction #MeanSquareError
Reference: StackExchange: why-do-cost-functions-use-the-square-error?
StackExchange上面一个关于均方差的一个很好的解释, 翻译如下:
Question: I&amp;rsquo;m just getting started with some machine learning, and until now I have been dealing with linear regression over one variable.</description></item><item><title>Part.1_Supervised_Learning(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.1_Supervised_LearningML_Andrew.Ng./</link><pubDate>Tue, 27 Jul 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.1_Supervised_LearningML_Andrew.Ng./</guid><description>Supervised Learning-Introduction 2021-07-27 Tags: #MachineLearning #SupervisedLearning
What is supervised learning? Supervised learning refers to the fact that we gave the algorithm a dataset in which &amp;ldquo;Right Answers&amp;rdquo; were given.</description></item><item><title>Part.2_Unsupervised_Learning(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.2_Unsupervised_LearningML_Andrew.Ng./</link><pubDate>Tue, 27 Jul 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.2_Unsupervised_LearningML_Andrew.Ng./</guid><description>Unsupervised Learning-Introduction 2021-07-27 Tags: #MachineLearning #UnsupervisedLearning
What is unsupervised learning? In Unsupervised Learning, we&amp;rsquo;re given data that looks different than data that looks like this that doesn&amp;rsquo;t have any labels or that all has the same label or really no labels.</description></item></channel></rss>