<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>DeepLearning on</title><link>https://alonelysheep.github.io/quartz-blog/tags/DeepLearning/</link><description>Recent content in DeepLearning on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 05 May 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://alonelysheep.github.io/quartz-blog/tags/DeepLearning/index.xml" rel="self" type="application/rss+xml"/><item><title>端到端学习-End_to_End_Learning-E2E</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.5/%E7%AB%AF%E5%88%B0%E7%AB%AF%E5%AD%A6%E4%B9%A0-End_to_End_Learning-E2E/</link><pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.5/%E7%AB%AF%E5%88%B0%E7%AB%AF%E5%AD%A6%E4%B9%A0-End_to_End_Learning-E2E/</guid><description>端到端学习 2022-05-05 Tags: #EndtoEndLearning #MachineLearning #DeepLearning
端到端的学习就是省略中间步骤，直接从输入得到输出结果。 Pro &amp;amp; Con Pro 不用人为设计中间步骤, 减少了工作量, 并且避免人为添加的步骤给模型带来不好的 归纳偏置. Con 需要足够多的数据才能达到较好的效果</description></item><item><title>D2L-75-BERT</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-75-BERT/</link><pubDate>Sat, 30 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-75-BERT/</guid><description>Bidirectional Encoder Representations from Transformers (BERT) 2022-04-30 Tags: #BERT #Transformer #DeepLearning
Motivation 构建一个通用的语言模型 在计算机视觉领域中, 我们能对一个已经训练好的大型网络进行微调(Fine-tune), 以较小的计算成本和网络改动就能获得很好的模型. BERT就是期望能够构建一个足够强大的预训练模型(Pretrained), 来适配各种各样的任务. 结合两个现有架构的优点: ELMo &amp;amp; GPT GPT: task-agnostic 其实在 BERT 以前, OpenAI 已经提出了 GPT （Generative Pre-Training，生成式预训练）模型, 试图提供一种既考虑上下文语意(context-sensitive), 又能适配多任务(task-agnostic1)的模型</description></item><item><title>D2L-76-BERT - Pretrain</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-76-BERT-Pretrain/</link><pubDate>Sat, 30 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-76-BERT-Pretrain/</guid><description>BERT: Pretrain 2022-04-30 Tags: #BERT #Pretrain #DeepLearning #Transformer
Pretrain Tasks Task 1 - Masked Language Modeling Motivation 语言模型(Language Model) 在输出时是从左到右进行的, 使用左侧的上下文来预测未知词元。</description></item><item><title>D2L-77-BERT - Fine-tune</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-77-BERT-Fine-tune/</link><pubDate>Sat, 30 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-77-BERT-Fine-tune/</guid><description>BERT: Fine-tune 2022-04-30 Tags: #BERT #Fine-tune #DeepLearning #Transformer
预训练好 BERT 以后, 我们只需要对模型进行很小的改动即可适配很多任务. 在 Finetuning 的时候, 新增的输出部分是从头开始训练的, 而 BERT 主体部分是在 pre-train 的基础上进行训练的.</description></item><item><title>D2L-71-Multi-Head_Attention</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-71-Multi-Head_Attention/</link><pubDate>Wed, 27 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-71-Multi-Head_Attention/</guid><description>多头注意力 2022-04-27 Tags: #Attention #Multi-headAttention #DeepLearning
多头注意力就是对 Query, Key, Value 进行一些线性变换, 并行地计算多个注意力, 期望模型能学习到多样化的依赖关系. Another way of seeing it: 1 模型构建 下面我们给出 Multi-Head Attention 的形象化表示:</description></item><item><title>D2L-73-Positional_Encoding</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-73-Positional_Encoding/</link><pubDate>Wed, 27 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-73-Positional_Encoding/</guid><description>位置编码: 将位置信息加入数据 2022-04-27 Tags: #PositionalEncoding #Self-Attention #DeepLearning
为了使用序列的顺序信息，我们通过在输入表示中添加 位置编码（positional encoding）来注入绝对的或相对的位置信息。
我觉得D2L讲的很深入很好了: 10.6. Self-Attention and Positional Encoding</description></item><item><title>D2L-74-Transformer</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-74-Transformer/</link><pubDate>Wed, 27 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-74-Transformer/</guid><description>Transformer 2022-04-27 Tags: #Transformer #Attention #DeepLearning
1
Transformer 是一个纯基于 Attention 的 Encoder Decoder 架构模型
Hugging Face Explorable Transformer: exBERT</description></item><item><title>D2L-72-Self-Attention</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-72-Self-Attention/</link><pubDate>Tue, 26 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-72-Self-Attention/</guid><description>自注意力 2022-04-26 Tags: #Self-Attention #Attention #DeepLearning
Attention 机制可以抽象为:1 $$\begin{align} \textit{Attention}(Q,K,V) = V\cdot\textit{softmax}\space (\textit{score}(Q, K)) \end{align}$$ 自注意力就是 $Q = K = V$ , 也就是同一个序列同时作为 Query, Key 和 Value.</description></item><item><title>D2L-70-Seq2Seq with Attention - Bahdanau Attention</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-70-Seq2Seq-with-Attention-Bahdanau-Attention/</link><pubDate>Fri, 22 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-70-Seq2Seq-with-Attention-Bahdanau-Attention/</guid><description>含注意力机制的Seq2Seq 2022-04-22 Tags: #Seq2Seq #Attention #DeepLearning #RNN
Motivation 在 Seq2Seq模型里面, Encoder向Decoder传递的仅仅是最后一个时间步的隐状态, 也就是上下文变量 $\mathbf c= \mathbf{h}_T$, 我们假设里面已经包含了输入序列的所有信息: 但这样每一步Decoder的输入都是原序列的一个&amp;quot;全局, 笼统的总结&amp;quot;, 这是不太合理的: 在下图中, 在翻译&amp;quot;Knowledge&amp;quot;的的时候, 显然&amp;quot;力量&amp;quot;这个词是不太重要的.</description></item><item><title>D2L-67-Attention Scoring Function</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-67-Attention-Scoring-Function/</link><pubDate>Thu, 21 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-67-Attention-Scoring-Function/</guid><description>注意力评分函数 2022-04-21 Tags: #Attention #DeepLearning
抽取出Attention Pooling里面都有的Softmax部分, 我们可以将注意力机制的设计简化为Attention Scoring Function的设计. 形式化的表达如下:
query $\mathbf{q} \in \mathbb{R}^q$,</description></item><item><title>D2L-68-Additive Attention</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-68-Additive-Attention/</link><pubDate>Thu, 21 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-68-Additive-Attention/</guid><description>加性注意力 2022-04-21 Tags: #Attention #DeepLearning
一般来说，当Query和Key是不同长度的矢量时，我们可以使用Additive Attention来作为Scoring Function。
给定查询 $\mathbf{q} \in \mathbb{R}^q$ 和键 $\mathbf{k} \in \mathbb{R}^k$，加性注意力（additive attention）的评分函数(Scoring Function)为 $$a(\mathbf q, \mathbf k) = \mathbf w_v^\top \text{tanh}(\mathbf W_q\mathbf q + \mathbf W_k \mathbf k) \in \mathbb{R},$$</description></item><item><title>D2L-69-Scaled Dot-Product Attention</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-69-Scaled-Dot-Product-Attention/</link><pubDate>Thu, 21 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-69-Scaled-Dot-Product-Attention/</guid><description>缩放的点积注意力 2022-04-21 Tags: #Attention #DeepLearning
1
相比Additive Attention, 使用点积可以得到计算效率更高的Scoring Function. 但是点积操作要求查询和键具有相同的长度 $d$。
我们知道 内积可以衡量两个向量之间的相似程度, 所以我们可以这样解读缩放点积注意力:</description></item><item><title>D2L-62-BLEU (Bilingual Evaluation Understudy)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-62-BLEU-Bilingual-Evaluation-Understudy/</link><pubDate>Wed, 20 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-62-BLEU-Bilingual-Evaluation-Understudy/</guid><description>BLEU (Bilingual Evaluation Understudy) 2022-04-20 Tags: #BLEU #DeepLearning
BLEU 是一种用于评价输出序列质量的评价指标, 其特点在于它考虑到了序列长度和预测难度的关系.
BLEU 通过综合&amp;quot;不同n-gram在结果中的成功次数&amp;quot;来评价最终质量的好坏.
定义 $$ \exp\left(\min\left(0, 1 - \frac{\mathrm{len}{\text{label}}}{\mathrm{len}{\text{pred}}}\right)\right) \prod_{n=1}^k p_n^{1/2^n}$$</description></item><item><title>D2L-65-Attention Cues &amp; Attention Mechanisms</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-65-Attention-Cues-Attention-Mechanisms/</link><pubDate>Wed, 20 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-65-Attention-Cues-Attention-Mechanisms/</guid><description>注意力信号 与 注意力机制 2022-04-20 Tags: #Attention #DeepLearning
Attention Cue Attention Cue分为两种: nonvolitional cue 和 volitional cue. Your volition is the power you have to decide something for yourself.</description></item><item><title>D2L-66-Kernel Regression and Attention</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-66-Kernel-Regression-and-Attention/</link><pubDate>Wed, 20 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-66-Kernel-Regression-and-Attention/</guid><description>Kernel Regression And Attention 2022-04-20 Tags: #KernelRegression #Attention #DeepLearning
Nadaraya-Watson kernel regression is an example of machine learning with attention mechanisms.</description></item><item><title>D2L-60-Encoder-Decoder</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-60-Encoder-Decoder/</link><pubDate>Tue, 19 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-60-Encoder-Decoder/</guid><description>编码器-解码器架构 2022-04-19 Tags: #EncoderDecoder #RNN #DeepLearning
Encoder-Decoder将模型分为两部分, 使得我们可以先用编码器处理不规则的输入, 然后再将输出送入Decoder得到最终结果.
Encoder-Decoder是一种抽象的模型架构, 可以有许多不同的实现方式.
有的时候Decoder也需要Input, 所以上图也可以表示成下面的样子:</description></item><item><title>D2L-61-Sequence to Sequence Learning - Seq2Seq</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-61-Sequence-to-Sequence-Learning-Seq2Seq/</link><pubDate>Tue, 19 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-61-Sequence-to-Sequence-Learning-Seq2Seq/</guid><description>Seq2Seq: 序列到序列模型 2022-04-19 Tags: #Seq2Seq #EncoderDecoder #DeepLearning #RNN
Seq2Seq也就是Sequence to Sequence, 顾名思义, 它实现的是一种序列到另一种序列的转换(比如从英语到中文). Seq2Seq符合 Encoder-Decoder架构 总览 如上图所示, 首先Encoder输入长度可变的序列， 并将其转换为固定形状的隐状态。然后隐状态输入Decoder, 解码器根据隐状态和输入来生成最后的输出.</description></item><item><title>D2L-57-LSTM-长短期记忆网络</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-57-LSTM-%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C/</link><pubDate>Mon, 18 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-57-LSTM-%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C/</guid><description>Long Short-Term Memory 2022-04-18 Tags: #LSTM #DeepLearning #RNN
LSTM是最早用于解决长期依赖问题的一种RNN. 它比GRU复杂, 但是设计思想是一样的. 有趣的是, LSTM(1997)比GRU(2014)早出现近20年.
LSTM和GRU一样, 使用了不同的门(Gate)来控制上一个隐状态在下一个隐状态里面的占比, 也就是有选择地来混合&amp;quot;长期记忆&amp;quot;和&amp;quot;短期记忆&amp;quot;, 这也是其名称的由来.</description></item><item><title>D2L-58-深度循环神经网络</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-58-%E6%B7%B1%E5%BA%A6%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link><pubDate>Mon, 18 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-58-%E6%B7%B1%E5%BA%A6%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</guid><description>Deep Recurrent Neural Networks 2022-04-18 Tags: #RNN #DeepLearning
和MLP与CNN中一样, 我们可以通过添加更多的层来增强网络的表达能力. 但不同的是, 增加的每一层都需要在时间步上展开, 就像上图一样. 具体的来说, 除了边缘部分外, 每一个隐状态 $H^{(l)}_t$ 同时接受上一层同一时间步的 $\textcolor{red}{H^{(l-1)}t}$ 和同一层上一时间步的 $\textcolor{red}{H^{(l)}{t-1}}$ 作为输入, 并且输出到下一层同一时间步的 $\textcolor{royalblue}{H^{(l+1)}t}$ 和同一层下一时间步的 $\textcolor{royalblue}{H^{(l)}{t+1}}$ 用GRU或LSTM的隐状态代替上图中的隐状态，便得到深度GRU或深度LSTM。 形式化定义 假设在时间步 $t$ 有一个小批量的输入数据 $\mathbf{X}_t \in \mathbb{R}^{n \times d}$（样本数：$n$，每个样本中的输入数：$d$）。</description></item><item><title>D2L-59-双向循环神经网络</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-59-%E5%8F%8C%E5%90%91%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link><pubDate>Mon, 18 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-59-%E5%8F%8C%E5%90%91%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</guid><description>Bidirectional Recurrent Neural Networks 2022-04-18 Tags: #RNN #DeepLearning #BidirectionalRNN
双向神经网络增加了反向扫描的隐藏层, 使网络拥有了&amp;quot;前瞻能力&amp;quot; 正向层和反向层的输入是相同的, 是并行进行的, 最后正向和反向的结果一起生成输出. 在D2L教程里面将正向反向扫描的过程和隐马尔科夫模型动态规划的正向与反向传递1进行了类比: 这种转变集中体现了现代深度网络的设计原则： 首先使用经典统计模型的函数依赖类型，然后将其参数化为通用形式。</description></item><item><title>D2L-56-门控循环单元GRU</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-56-%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83GRU/</link><pubDate>Sun, 03 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-56-%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83GRU/</guid><description>Gated Recurrent Units (GRU) 2022-04-03 Tags: #GRU #RNN #DeepLearning
GRU在RNN的基础上添加了&amp;quot;门&amp;quot;(Gate), 针对性地解决了RNN里面存在的以下问题: 长期依赖问题: 序列早期的部分可能对未来所有观测值都有非常重要的影响, 我们需要能够保留序列早期信息的网络结构. GRU里面体现在: 重置门减少重置, 更新门更多地保留上一个隐状态 序列里面可能有干扰信息, 我们需要能够跳过(遗忘)这些信息的机制 GRU里面体现在: 更新门更多地保留上一个隐状态 序列里面可能有逻辑中断, 比如一本书里面章节的变化往往会导致主题的变化.</description></item><item><title>D2L-53-循环神经网络RNN</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-53-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN/</link><pubDate>Fri, 01 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-53-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN/</guid><description>Recurrent Neural Networks 2022-04-01 Tags: #RNN #DeepLearning #NeuralNetwork
Motivation 基于马尔可夫假设的N元语法（n-gram）需要存储大量的参数。在 $n$ 逐渐增大的过程中，n-gram模型的参数大小 $|W|$ 与序列长度 $n$ 是指数关系：$$|W|=|\mathcal{V}|^n $$ ($|\mathcal{V}|$ 是单词的数目) 因此, 我们将目光转向了 隐变量自回归模型.</description></item><item><title>D2L-46-DenseNet</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-46-DenseNet/</link><pubDate>Mon, 07 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-46-DenseNet/</guid><description>DenseNet 2022-03-07 Tags: #DenseNet #DeepLearning #CNN ResNet极大地改变了如何参数化深层网络中函数的观点。 稠密连接网络（DenseNet）在某种程度上是ResNet的逻辑扩展。
PDF(zotero://select/items/@huang2017densely)
数学直觉: 从ResNet到DenseNet 某个函数在 $x=0$ 处的泰勒展开为: $$f(x)=f(0)+f^{\prime}(0) x+\frac{f^{\prime \prime}(0)}{2 !</description></item><item><title>D2L-45-ResNet</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-45-ResNet/</link><pubDate>Sun, 06 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-45-ResNet/</guid><description>ResNet 残差网络 2022-03-06 Tags: #ResNet #CNN #DeepLearning
ResNet在网络中引入了残差连接的思想, 简单的改变带来了很棒的效果.
残差连接让每一层很容易地包含了原始函数1, 这样能保证新增加的每一层都能包含原来的最优解, 进一步在原来的基础上继续改进.
Motivation 函数类的角度 我们定义 $\mathcal{F}$ 是某个模型能够拟合的所有函数构成的函数类.</description></item><item><title>D2L-43-GoogLeNet(Inception)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-43-GoogLeNetInception/</link><pubDate>Sat, 05 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-43-GoogLeNetInception/</guid><description>GoogLeNet 2022-03-05 Tags: #DeepLearning #CNN #GoogLeNet-Inception GoogLeNet是一个含并行连结的网络, 其核心组成部分为&amp;quot;Inception块&amp;quot;.
Inception块组合使用了不同大小的卷积核, 试图用现有的稠密结构(Dense Components)来构建一个&amp;quot;最佳的局部稀疏网络&amp;quot;.
局部: 多个Inception块拼接构成最后的GoogLeNet 稀疏: 也就是具有随机性的结构1 GoogLeNet还具有较高的计算效率, 这主要得益于Inception块里面不含全连接层.</description></item><item><title>D2L-44-Batch_Normalization-批量归一化</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-44-Batch_Normalization-%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96/</link><pubDate>Sat, 05 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-44-Batch_Normalization-%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96/</guid><description>Batch Normalization 2022-03-05 Tags: #BatchNormalization #Normalization #DeepLearning #Regularization
批量归一化是一种加速收敛的方法.
批量归一化作用于每一个mini-Batch, 先将这个Batch归一化, 然后再做一个统一的偏移与拉伸.
最后这个偏移和拉伸的量是一个可以学习的超参数 对于全连接层, BN作用于每一个特征</description></item><item><title>D2L-42-NiN</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-42-NiN/</link><pubDate>Fri, 04 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-42-NiN/</guid><description>Network in Network - NiN 2022-03-04 Tags: #DeepLearning #NiN #CNN
用卷积代替全连接 动机 全连接层很贵 (参数很多) 一层卷积层需要的参数为:
卷积层参数大小的计算 卷积层后面第一个全连接层的参数为: $$in_Channel\times in_Height\times in_Width\times num_of_Hidden_Units$$</description></item><item><title>D2L-41-VGG</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-41-VGG/</link><pubDate>Thu, 03 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-41-VGG/</guid><description>VGG 2022-03-03 Tags: #DeepLearning #VGG #CNN
模块化是VGG网络最重要的思想. 模块化进一步带来了自由性, 不同的块配置可以带来不同的模型表现. 规范化 - 模块化 与芯片设计中工程师从放置晶体管到逻辑元件再到逻辑块的过程类似，神经网络架构的设计也逐渐变得更加抽象。研究人员开始从单个神经元的角度思考问题，发展到整个层，现在又转向块，重复层的模式。 使用块的设计同样让模型更加简洁. VGG块 VGG将 AlexNet 里面三层连续的卷积拿出来, 抽象成VGG块, 作为构建网络的基础模式.</description></item><item><title>D2L-40-AlexNet</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-40-AlexNet/</link><pubDate>Wed, 02 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-40-AlexNet/</guid><description>AlexNet 2022-03-02 Tags: #DeepLearning #AlexNet #CNN #ImageNet
模型解析 对比LeNet 最重要的是, AlexNet导致了计算机视觉方法论的改变: 从核方法到深度神经网络, 开启了神经网络的第二次热潮 1 对比LeNet, AlexNet的主要特点有: 输入图片更 &amp;ldquo;大&amp;rdquo;, 网络结构更 &amp;ldquo;深&amp;rdquo;, 每层通道更 &amp;ldquo;多&amp;rdquo;, 滑动窗口更 &amp;ldquo;大&amp;rdquo;(核函数和池化层) 使用了ReLU作为激活函数 池化层采用了Max Pooling 使用了丢弃法(Dropout) 作为正则化方法2, 而 LeNet只采用了 权重衰减 AlexNet在训练前进行了数据增强 3</description></item><item><title>D2L-39-LeNet</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-39-LeNet/</link><pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-39-LeNet/</guid><description>LeNet 2022-03-01 Tags: #LeNet #DeepLearning #CNN #NeuralNetwork
架构 1
总体来看，LeNet（LeNet-5）由两个部分组成：
卷积编码器：由两个卷积层组成; 全连接层密集块：由三个全连接层组成。 每个卷积块中的基本单元是一个卷积层(含一个sigmoid激活函数) 和一个Avg Pooling 层。</description></item><item><title>为什么Softmax回归不用MSE</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE/</link><pubDate>Mon, 28 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE/</guid><description>为什么Softmax (或者Logistic) 不用MSE作为损失函数? 2022-02-28 Tags: #DeepLearning #MachineLearning #SoftmaxRegression #LogisticRegression #CostFunction #MeanSquareError #CrossEntropy
回顾:
MSE假设样本误差i.i.d., 并且服从正态分布, 最小化MSE等价于极大似然估计. 通常用于回归问题. MSE基于输出与真实值的欧氏距离.</description></item><item><title>D2L-34-卷积层 - 填充 - Padding</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-34-%E5%8D%B7%E7%A7%AF%E5%B1%82-%E5%A1%AB%E5%85%85-Padding/</link><pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-34-%E5%8D%B7%E7%A7%AF%E5%B1%82-%E5%A1%AB%E5%85%85-Padding/</guid><description>Padding - 填充 2022-02-27 Tags: #DeepLearning #CNN #Padding
It&amp;rsquo;s always nice to have an interactive example:
Convolution Visualizer CNN Explainer 卷积操作会使图像尺寸变小, 填充 (Padding) 可以减缓这个过程.</description></item><item><title>D2L-35-卷积层 - 步幅 - Stride</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-35-%E5%8D%B7%E7%A7%AF%E5%B1%82-%E6%AD%A5%E5%B9%85-Stride/</link><pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-35-%E5%8D%B7%E7%A7%AF%E5%B1%82-%E6%AD%A5%E5%B9%85-Stride/</guid><description>Stride - 步幅 2022-02-27 Tags: #DeepLearning #CNN #Stride
It&amp;rsquo;s always nice to have an interactive example:
Convolution Visualizer CNN Explainer 卷积操作会使图像尺寸变小, 增大步幅 (Stride) 可以加快这个过程.</description></item><item><title>D2L-36-1x1卷积层</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-36-1x1%E5%8D%B7%E7%A7%AF%E5%B1%82/</link><pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-36-1x1%E5%8D%B7%E7%A7%AF%E5%B1%82/</guid><description>$1×1$ 卷积层 2022-02-27 Tags: #CNN #DeepLearning #Convolution
$1×1$ 卷积，即 $k_h=k_w=1$，它虽然不能提取相关特征, 但是却能融合图像的不同通道, 也是一种很受欢迎的网络结构.
它相当于输入形状为 $n_{h} n_{w} \times c_{i}$ , 权重为 $c_{o} \times c_{i}$ 的全连接层</description></item><item><title>D2L-37-CNN的计算复杂度</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-37-CNN%E7%9A%84%E8%AE%A1%E7%AE%97%E5%A4%8D%E6%9D%82%E5%BA%A6/</link><pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-37-CNN%E7%9A%84%E8%AE%A1%E7%AE%97%E5%A4%8D%E6%9D%82%E5%BA%A6/</guid><description>CNN的计算复杂度 2022-02-27 Tags: #DeepLearning #CNN #ComputationalComplexity
前面我们提到过利用傅里叶变换可以快速地计算卷积: 数值计算 卷积与傅里叶变换
动手学深度学习里面给出了一个例子, 说明卷积的计算复杂度其实还是很高的, 只是参数的存储开销较小.</description></item><item><title>D2L-38-池化层-Pooling_Layer</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-38-%E6%B1%A0%E5%8C%96%E5%B1%82-Pooling_Layer/</link><pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-38-%E6%B1%A0%E5%8C%96%E5%B1%82-Pooling_Layer/</guid><description>池化层/汇聚层 - Pooling Layer 2022-02-27 Tags: #PoolingLayer #DeepLearning #CNN
首先，池化层为什么叫“池化层” Collins Dictionary - Pool 7. verb
If a group of people or organizations pool their money, knowledge, or equipment, they share it or put it together so that it can be used for a particular purpose.</description></item><item><title>D2L-32-Convolution-卷积</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-32-Convolution-%E5%8D%B7%E7%A7%AF/</link><pubDate>Sat, 26 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-32-Convolution-%E5%8D%B7%E7%A7%AF/</guid><description>卷积 - Convolution 2022-02-26 Tags: #DeepLearning #Convolution
关键点:
Convolution Determines the Output of a System for any Input1</description></item><item><title>D2L-33-卷积神经网络CNN</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-33-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/</link><pubDate>Sat, 26 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-33-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/</guid><description>Convolutional Neural Network - 卷积神经网络 2022-02-26 Tags: #CNN #DeepLearning #Convolution
MLP的不足 随着图像分辨率的提高, MLP显露出以下不足:
假设我们的图像分辨率为 $1920\times 1080$, 那么一张图片就有 $2,073,600$ 个像素点, 假设和输入层相连的隐藏层有 $1000$ 个单元, 那么光是第一个全连接层就有大约 $2\times10^9$ ($20$ 亿) 个参数, 训练这样的网络是难以想象的, 况且这还只是网络的第一层.</description></item><item><title>归纳偏置-Inductive bias - learning bias</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E5%BD%92%E7%BA%B3%E5%81%8F%E7%BD%AE-Inductive-bias-learning-bias/</link><pubDate>Sat, 26 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E5%BD%92%E7%BA%B3%E5%81%8F%E7%BD%AE-Inductive-bias-learning-bias/</guid><description>Inductive Bias - 归纳偏置 / 归纳偏好 2022-02-26 Tags: #DeepLearning #MachineLearning
当学习器去预测其未遇到过的输入的结果时，会做一些假设（Mitchell, 1980）。而学习算法中的归纳偏置（Inductive bias）则是这些假设的集合。1
一个典型的归纳偏置例子是 奥卡姆剃刀，它假设最简单而又一致的假设是最佳的。这里的一致是指学习器的假设会对所有样本产生正确的结果。
Machine Learning - Mitchell Chapter 2.</description></item><item><title>D2L-26-环境和分布偏移</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-26-%E7%8E%AF%E5%A2%83%E5%92%8C%E5%88%86%E5%B8%83%E5%81%8F%E7%A7%BB/</link><pubDate>Sat, 19 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-26-%E7%8E%AF%E5%A2%83%E5%92%8C%E5%88%86%E5%B8%83%E5%81%8F%E7%A7%BB/</guid><description>Environment and Distribution Shift 2022-02-19 Tags: #DeepLearning #DistributionShift #CovariateShift
环境是变化的，数据也是。有时通过将基于模型的决策引入环境，我们可能会破坏模型1。我们需要合理地调整模型来适应这种可能的变化。 分布偏移的类型 Covariate Shift - 协变量偏移 顾名思义，就是输入数据（特征、协变量）的分布发生了偏移。也就是说，输入变得不一样了，原来是真实的猫猫狗狗， 现在变成了卡通的猫猫狗狗！ 形式化地说, 就是输入数据的分布 $P(X)$ 发生了变化, 但是在输入确定之后, 最终输出的标签分布 $P(y\space |\space X)$ 不变.</description></item><item><title>D2L-27-Computation-层和块</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-27-Computation-%E5%B1%82%E5%92%8C%E5%9D%97/</link><pubDate>Sat, 19 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-27-Computation-%E5%B1%82%E5%92%8C%E5%9D%97/</guid><description>深度学习计算: 使用层 和块 2022-02-19 Tags: #DeepLearning #Computation #PyTorch
这一章主要介绍框架的使用细节, 最好的方法就是结合代码示例, 边运行边理解. 这里我们记录一些容易忽略的要点. 在线代码实例: 5.1. 层和块 层和块: 定义 &amp;ldquo;层&amp;quot;具有三个特征:</description></item><item><title>D2L-28-Computation-参数管理</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-28-Computation-%E5%8F%82%E6%95%B0%E7%AE%A1%E7%90%86/</link><pubDate>Sat, 19 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-28-Computation-%E5%8F%82%E6%95%B0%E7%AE%A1%E7%90%86/</guid><description>深度学习计算: 参数管理 2022-02-19 Tags: #DeepLearning #Computation #Parameter #PyTorch
在线代码实例: 5.2. 参数管理 本节主要有以下内容：
访问参数，用于调试、诊断和可视化。 参数初始化。 在不同模型组件间共享参数。(保持某几个层的参数是同步的) 延后初始化 ¶ 深度学习框架无法判断网络的输入维度是什么。 这里的诀窍是框架的 延后初始化（defers initialization）， 即直到数据第一次通过模型传递时，框架才会动态地推断出每个层的大小。 这个在MXNET 和 Tensorflow 里面有, PyTorch还不太完善, 不过LazyLinear可以达到类似的功能</description></item><item><title>D2L-29-Computation-自定义层</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-29-Computation-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B1%82/</link><pubDate>Sat, 19 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-29-Computation-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B1%82/</guid><description>深度学习计算: 自定义一个层 2022-02-19 Tags: #DeepLearning #Computation #PyTorch
在线代码实例: 5.4. 自定义层 我们可以通过基本层类设计自定义层。这允许我们定义灵活的新层。 在自定义层定义完成后，我们就可以在任意环境和网络架构中调用该自定义层。 层可以有局部参数，这些参数可以通过内置函数创建。</description></item><item><title>D2L-30-Computation-读写文件</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-30-Computation-%E8%AF%BB%E5%86%99%E6%96%87%E4%BB%B6/</link><pubDate>Sat, 19 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-30-Computation-%E8%AF%BB%E5%86%99%E6%96%87%E4%BB%B6/</guid><description>深度学习计算: 读写文件 2022-02-19 Tags: #DeepLearning #Computation #PyTorch
在线代码实例: 5.5. 读写文件 我们可以保存一个张量, 或者张量的字典和列表 我们可以通过参数字典保存和加载网络的全部参数, 但是Pytorch中, 模型的定义需要用其他方法来保存.</description></item><item><title>D2L-31-Computation-购买与使用GPU</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-31-Computation-%E8%B4%AD%E4%B9%B0%E4%B8%8E%E4%BD%BF%E7%94%A8GPU/</link><pubDate>Sat, 19 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-31-Computation-%E8%B4%AD%E4%B9%B0%E4%B8%8E%E4%BD%BF%E7%94%A8GPU/</guid><description>深度学习计算: 购买与使用GPU 2022-02-19 Tags: #DeepLearning #Computation #PyTorch #GPU
购买与搭建计算平台: 16.4. 选择服务器和GPU
Pytorch使用GPU: 5.6. GPU — 动手学深度学习</description></item><item><title>为什么参数不能初始化为同一个常数</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%82%E6%95%B0%E4%B8%8D%E8%83%BD%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%BA%E5%90%8C%E4%B8%80%E4%B8%AA%E5%B8%B8%E6%95%B0/</link><pubDate>Sat, 19 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%82%E6%95%B0%E4%B8%8D%E8%83%BD%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%BA%E5%90%8C%E4%B8%80%E4%B8%AA%E5%B8%B8%E6%95%B0/</guid><description>深度学习: 参数化所固有的对称性 2022-02-19 Tags: #DeepLearning
4.8.1.3. 打破对称性 对于一个多层感知机, 假设隐藏层只有两个单元, 输出层只有一个输出单元。 想象一下，如果我们将隐藏层的所有参数初始化为 $W^{(1)}=c$， $c$ 为常量，会发生什么？ 在这种情况下，在前向传播期间，两个隐藏单元采用相同的输入和参数， 产生相同的激活，该激活被送到输出单元。 在反向传播期间，根据参数 $W^{(1)}$ 对输出单元进行微分， 得到一个梯度，其元素都取相同的值。 因此，在一次梯度下降（例如，小批量随机梯度下降）之后， $W^{(1)}$ 的所有元素仍然有相同的值。 而每一次迭代永远都不会打破对称性，这也意味着隐藏层的行为就好像只有一个单元, 我们永远也无法利用多层网络强大的表达能力。 请注意，虽然小批量随机梯度下降不会打破这种对称性，但暂退法 (Dropout-丢弃法) 正则化可以。</description></item><item><title>D2L-25-让训练更加稳定-Xavier初始化</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-25-%E8%AE%A9%E8%AE%AD%E7%BB%83%E6%9B%B4%E5%8A%A0%E7%A8%B3%E5%AE%9A-Xavier%E5%88%9D%E5%A7%8B%E5%8C%96/</link><pubDate>Fri, 18 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-25-%E8%AE%A9%E8%AE%AD%E7%BB%83%E6%9B%B4%E5%8A%A0%E7%A8%B3%E5%AE%9A-Xavier%E5%88%9D%E5%A7%8B%E5%8C%96/</guid><description>让训练更加稳定 2022-02-18 Tags: #DeepLearning
要点 因为我们无法改变 梯度问题的根本原因, 所以我们的目标是将梯度的值控制在一个合理的范围内. 改进方向 将乘法变加法: 如ResNet, LSTM
归一化: 梯度归一化, Gradient Clipping-梯度剪裁</description></item><item><title>D2L-24-数值稳定性</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-24-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7/</link><pubDate>Thu, 17 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-24-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7/</guid><description>深度学习里面的数值稳定性 2022-02-17 Tags: #DeepLearning #NumericalComputing
问题的由来 数值稳定性的问题发生在反向传播的时候. 对于一个很深的模型, 计算在损失 $\ell$ 关于第 $t$ 层权重 $\mathbf{W_t}$ 的梯度的时候, 如果第 $t$ 层关于输出较远, 则结果由许多矩阵乘法构成, 这会导致梯度爆炸或者梯度消失.</description></item><item><title>D2L-23-Dropout-丢弃法</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-23-Dropout-%E4%B8%A2%E5%BC%83%E6%B3%95/</link><pubDate>Mon, 14 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-23-Dropout-%E4%B8%A2%E5%BC%83%E6%B3%95/</guid><description>Dropout - 丢弃法(暂退法) 2022-02-14 Tags: #Dropout #Regularization #DeepLearning
1
Dropout就是在前向传播过程计算每一内部层的同时注入噪声, 从而提高模型的平滑性, 减少过拟合. 实现方式 实现的关键是要以一种无偏(不改变期望)的方式注入噪声.</description></item><item><title>Norm in Regularization - Intuition</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Norm-in-Regularization-Intuition/</link><pubDate>Mon, 14 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Norm-in-Regularization-Intuition/</guid><description>Norm in Regularization - Intuition 2022-02-14 Tags: #Norm #Regularization #DeepLearning #MachineLearning
L2 Norm $\ell_{2}$ in Regularization L2 Norm 的等高线是圆形的</description></item><item><title>Regularization-正则化</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Regularization-%E6%AD%A3%E5%88%99%E5%8C%96/</link><pubDate>Mon, 14 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Regularization-%E6%AD%A3%E5%88%99%E5%8C%96/</guid><description>Regularization 2022-02-14 Tags: #Regularization #DeepLearning
Definition Regularization: any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error.</description></item><item><title>好的预测模型的特质</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E5%A5%BD%E7%9A%84%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%B9%E8%B4%A8/</link><pubDate>Mon, 14 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E5%A5%BD%E7%9A%84%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%B9%E8%B4%A8/</guid><description>&amp;ldquo;好&amp;quot;的预测模型的特征 2022-02-14 Tags: #DeepLearning
泛化性的角度:
我们期待“好”的预测模型能在未知的数据上有很好的表现： 经典泛化理论认为，为了缩小训练和测试性能之间的差距，应该以简单的模型为目标。 简单性以较小维度的形式展现. 此外, $L_2$ 正则化的有效性也说明, 参数的范数也代表了一种有用的简单性度量。 简单性的另一个角度是平滑性，即函数不应该对其输入的微小变化敏感。 例如，当我们对图像进行分类时，我们预计向像素添加一些随机噪声应该是基本无影响的。 1995年，克里斯托弗·毕晓普证明了 具有输入噪声的训练等价于Tikhonov正则化。 这项工作用数学证实了“要求函数光滑”和“要求函数对输入的随机噪声具有适应性”之间的联系。</description></item><item><title>D2L-16-线性模型的问题</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-16-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%97%AE%E9%A2%98/</link><pubDate>Sat, 12 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-16-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%97%AE%E9%A2%98/</guid><description>线性模型存在的问题 2022-02-12 Tags: #DeepLearning
线性意味着 &amp;ldquo;单调性&amp;rdquo; 假设： 输出和输入以相同的速度变化.
但是有很多问题虽然是单调的, 但是并不是线性&amp;quot;匀速&amp;quot;变化的 对策: 对数据进行预处理，使线性变得更合理，如进行对数变换。 但是很多情况也不是单调的。 例如，我们想要根据体温预测死亡率。 对于体温高于37摄氏度的人来说，温度越高风险越大。 然而，对于体温低于37摄氏度的人来说，温度越高风险就越低。</description></item><item><title>D2L-17-MLP-多层感知机</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-17-MLP-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/</link><pubDate>Sat, 12 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-17-MLP-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/</guid><description>Multilayer Perceptron 2022-02-12 Tags: #MultilayerPerceptron #DeepLearning #Perceptron
隐藏层 从线性到非线性 用$\mathbf{X, H, O}$ 分别代表输入层, 隐藏层和输出层, 带偏置的模型可以表示如下: $$\begin{aligned} &amp;amp;\mathbf{H}=\mathbf{X} \mathbf{W}^{(1)}+\mathbf{b}^{(1)} \ &amp;amp;\mathbf{O}=\mathbf{H} \mathbf{W}^{(2)}+\mathbf{b}^{(2)} \end{aligned}$$</description></item><item><title>D2L-18-激活函数-Activation_Functions</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-18-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-Activation_Functions/</link><pubDate>Sat, 12 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-18-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-Activation_Functions/</guid><description>常用激活函数 2022-02-12 Tags: #DeepLearning #ActivationFunction
1
ReLU 修正线性单元（Rectified Linear Unit，ReLU） ReLU就是一个 $max(0,x)$ 函数. ReLU是分段线性的 ReLU的变体通过设置一个线性项, 使得负轴的一些信息得到保留(Parameterized ReLU) $\mathbf{pReLU}(x)=max(0,x)+α\space min(0,x).</description></item><item><title>D2L-20-训练误差与泛化误差</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-20-%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE%E4%B8%8E%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE/</link><pubDate>Sat, 12 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-20-%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE%E4%B8%8E%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE/</guid><description>训练误差与泛化误差 2022-02-12 Tags: #DeepLearning
这一小节写的挺好的: 训练误差和泛化误差¶ 定义 训练误差（training error）是指， 模型在训练数据集上计算得到的误差。
泛化误差（generalization error）是指， 模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。
因为我们不能得到无限多的样本, 所以我们只能估计泛化误差.</description></item><item><title>D2L-21-模型容量</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-21-%E6%A8%A1%E5%9E%8B%E5%AE%B9%E9%87%8F/</link><pubDate>Sat, 12 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-21-%E6%A8%A1%E5%9E%8B%E5%AE%B9%E9%87%8F/</guid><description>模型容量(复杂度) 2022-02-12 Tags: #DeepLearning
Link: Part.17_Overfitting_Underfitting(ML_Andrew.Ng.)
概念 模型容量就是模型的复杂度, 也就是一个模型的Variance. 模型容量和数据的复杂度应该对应, 两者的不匹配容易导致过拟合与欠拟合问题 下图是两种损失随着模型复杂度的变化(注意横轴代表的是不同复杂度的模型, 不要和训练的Loss曲线搞混). 可以看到对于同一个数据集, 过于复杂的模型虽然有着更小的训练损失, 但是有了更大的泛化损失, 则说明模型过分契合训练集了, 学习到了训练集的一些误差等等, 太过于灵活(High variance), 出现了过拟合 而如果模型过于简单, 则训练损失和泛化损失都很高, 说明模型能力不够, 学习不到足够的知识, 模型对于数据的偏见(Bias)太高了 模型容量 估计模型容量 我们很难比较不同类型模型的容量差别: 比如树模型和神经网络.</description></item><item><title>D2L-22-权重衰减</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-22-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/</link><pubDate>Sat, 12 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-22-%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F/</guid><description>权重衰减 2022-02-12 Tags: #Regularization #DeepLearning
权重衰减就是利用 $\ell_{2}$ 范数进行 正则化, 避免过拟合 权重衰减是通过减小目标参数(weights)的大小来实现正则化的, 这也是其名称的由来. 参数的范数代表了一种有用的简单性度量。1 Links:</description></item><item><title>VC维-VC_Dimension</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/VC%E7%BB%B4-VC_Dimension/</link><pubDate>Sat, 12 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/VC%E7%BB%B4-VC_Dimension/</guid><description>Vapnik–Chervonenkis dimension 2022-02-12 Tags: #DeepLearning #StatisticalLearning
在VC理论中，VC维是对一个可学习分类函数空间的能力（复杂度，表示能力等）的衡量。它定义为算法能“打散”的点集的势的最大值。
对于线性分类器:
VC维可以衡量训练误差和泛化误差的间隔, 但是在深度学习中, 我们很难计算一个模型的VC维</description></item><item><title>Cross_Entropy-交叉熵</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5/</link><pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Cross_Entropy-%E4%BA%A4%E5%8F%89%E7%86%B5/</guid><description>Cross Entropy - 交叉熵 2022-02-11 Tags: #InformationTheory #DeepLearning
Intuition 熵是编码一个事件所需要的最短平均长度 $$\begin{aligned}H(p)&amp;amp;=\sum_{x_{i}} p\left(x_{i}\right) \log \frac{1}{p\left(x_{i}\right)} \ &amp;amp;=-\sum_{x} p(x) \log p(x) \end{aligned}$$</description></item><item><title>D2L-14-Cross Entropy as Loss</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss/</link><pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-14-Cross-Entropy-as-Loss/</guid><description>交叉熵作为损失函数 2022-02-11 Tags: #CostFunction #DeepLearning #CrossEntropy
在作为损失函数的时候, 构成 交叉熵的概率分布为: 真实分布: $P^*$ 模型输出: $P$ 作为损失函数, 交叉熵的作用是 衡量模型输出与真实值的差距, 作为优化算法的优化对象, 还需要尽量简洁, 减少训练模型的开销.</description></item><item><title>KL_Divergence-KL散度</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/KL_Divergence-KL%E6%95%A3%E5%BA%A6/</link><pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/KL_Divergence-KL%E6%95%A3%E5%BA%A6/</guid><description>Kullback–Leibler divergence 2022-02-11 Tags: #Math/Probability #DeepLearning
1
KL散度可以衡量两个概率分布之间的相似性
KL散度也称为相对熵
Wikipedia: In mathematical statistics, the Kullback–Leibler divergence, $D _{KL} ( P ∥ Q )$ (also called relative entropy), is a statistical distance: a measure of how one probability distribution Q is different from a second, reference probability distribution P.</description></item><item><title>Logit</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Logit/</link><pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Logit/</guid><description>Logit: a confusing term 2022-02-11 Tags: #Math #DeepLearning #SoftmaxRegression
Ref: machine learning - What is the meaning of the word logits in TensorFlow?</description></item><item><title>One-hot_Encoding-独热编码</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/One-hot_Encoding-%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81/</link><pubDate>Wed, 09 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/One-hot_Encoding-%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81/</guid><description>独热编码 One-hot Encoding 2022-02-09 Tags: #One-hot #DeepLearning #Encoding
$$\begin{array}{ll} apple &amp;amp;=\quad [\space 1\quad 0\quad 0\space ] \ banana &amp;amp;=\quad [\space 0\quad 1\quad 0\space ] \ pineapple &amp;amp;=\quad [\space 0\quad 0\quad 1\space ] \end{array}$$</description></item><item><title>D2L-11-泛化(Generalization)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-11-%E6%B3%9B%E5%8C%96Generalization/</link><pubDate>Tue, 08 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-11-%E6%B3%9B%E5%8C%96Generalization/</guid><description>Generalization: 泛化 2022-02-08 Tags: #MachineLearning #DeepLearning
线性回归恰好是一个在整个域中只有一个最小值的学习问题。 1但是对于像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。
深度学习实践者很少会去花费大力气寻找这样一组参数，使得在_训练集_上的损失达到最小。 事实上，更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失， 这一挑战被称为_泛化_（generalization）。
可以证明, 正则后的线性回归损失函数MSE依然是凸的: 正则项不影响线性回归损失函数的凸性&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>D2L-12-Predication_or_Inference-Difference</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-12-Predication_or_Inference-Difference/</link><pubDate>Tue, 08 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-12-Predication_or_Inference-Difference/</guid><description>Prediction or Inference? The Difference 2022-02-08 Tags: #DeepLearning #Math/Statistics #Inference #Prediction
在深度学习里面, 给定特征估计目标的过程通常称为_预测_（prediction）或_推断_（inference）。 但是, 虽然 推断 这个词已经成为深度学习的标准术语，但其实 推断 这个词有些用词不当。 在统计学中，推断 更多地表示基于数据集估计参数。1 当深度学习从业者与统计学家交谈时，术语的误用经常导致一些误解。2 如Bayesian Inference: Bayesian_Estimation(Inference)贝叶斯估计&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>D2L-10-小批量随机梯度下降</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-10-%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</link><pubDate>Wed, 02 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-10-%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</guid><description>小批量随机梯度下降(Mini-Batch)是深度学习默认的求解方法 2022-02-02 Tags: #MachineLearning #GradientDescent #DeepLearning #Optimization
Different_Gradient_Descent_Methods 注意有两个点: 小批量(Mini-Batch), 随机(Stochastic) 梯度下降 其中:
小批量是因为在整个数据集上面训练一次又慢又贵 同时小批量还能从多个相似的数据点中选一个代表来计算, 节约了计算资源 但是样本不能太小, 太小的样本不适合用GPU并行计算 随机是选取小样本的方法: 随机选取</description></item><item><title>D2L-6-计算图</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-6-%E8%AE%A1%E7%AE%97%E5%9B%BE/</link><pubDate>Wed, 02 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-6-%E8%AE%A1%E7%AE%97%E5%9B%BE/</guid><description>计算图 2022-02-02 Tags: #MachineLearning #DeepLearning
将计算表示为一个无环图 例子 线性回归: 计算图有两种构造方法: 显式构造 主要应用于: Tensorflow/Theano/MXNet 例子: 1 2 3 4 5 from mxnet import sym a = sym.</description></item><item><title>D2L-7-自动求导</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-7-%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC/</link><pubDate>Wed, 02 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-7-%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC/</guid><description>自动求导 2022-02-02 Tags: #DeepLearning
在机器学习里面, 深度学习框架可以帮我们自动求导, 计算梯度.
自动求导的两种方式 基于链式法则, 求导有两种顺序:
正向累积 $$\frac{\partial y}{\partial x}=\frac{\partial y}{\partial u_{n}}\left(\frac{\partial u_{n}}{\partial u_{n-1}}\left(\ldots\left(\frac{\partial u_{2}}{\partial u_{1}} \frac{\partial u_{1}}{\partial x}\right)\right)\right)$$ 反向累积、又称反向传递 $$ \frac{\partial y}{\partial x}=\left(\left(\left(\frac{\partial y}{\partial u_{n}} \frac{\partial u_{n}}{\partial u_{n-1}}\right) \ldots\right) \frac{\partial u_{2}}{\partial u_{1}}\right) \frac{\partial u_{1}}{\partial x} $$ 反向累积/传播 反向传播分为两个阶段: 正向阶段和反向阶段</description></item><item><title>D2L-9-梯度下降的方向</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-9-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E6%96%B9%E5%90%91/</link><pubDate>Wed, 02 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-9-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E6%96%B9%E5%90%91/</guid><description>梯度下降的方向是梯度的反方向 2022-02-02 Tags: #GradientDescent #DeepLearning #MachineLearning
梯度是一个函数增长最快的方向, 通常我们都是想获得损失函数的最小值, 所以需要沿着梯度的反方向来移动.
注意这并不是一定的, 梯度下降/上升只是一种优化方法而已, 如果我们想要优化的目标函数取得最大值, 那么就应该沿着梯度的方向变化.</description></item><item><title>D2L-2-Tensor数据操作</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-2-Tensor%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C/</link><pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-2-Tensor%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C/</guid><description>Tensor数据操作 2022-02-01 Tags: #Tensor #DeepLearning
[行, 列] 用冒号可以表示范围, 即一个子区域 注意还可以用双冒号间隔选择, 双冒号后的数字为间隔的周期</description></item><item><title>D2L-1-What_is_a_tensor</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-1-What_is_a_tensor/</link><pubDate>Tue, 25 Jan 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-1-What_is_a_tensor/</guid><description>What is a tensor? 2022-01-25 Tags: #Tensor #DeepLearning
最初, 张量是在物理和数学里面的一个概念, 后来深度学习借用了这个名词, 但是意义有所改变.
在数学与物理学的语境里面, &amp;ldquo;Tensor&amp;quot;是一个抽象的概念, 用于表示在坐标变换下的一种不变量, 比如广义相对论中, 坐标的变换会引起观测的时空的变换。而爱因斯坦张量（Einstein tensor）是广义相对论中用来描述时空曲率的一个张量, 不随坐标的变换而变换.</description></item></channel></rss>