<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>LogisticRegression on</title><link>https://alonelysheep.github.io/quartz-blog/tags/LogisticRegression/</link><description>Recent content in LogisticRegression on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 28 Feb 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://alonelysheep.github.io/quartz-blog/tags/LogisticRegression/index.xml" rel="self" type="application/rss+xml"/><item><title>为什么Softmax回归不用MSE</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE/</link><pubDate>Mon, 28 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE/</guid><description>为什么Softmax (或者Logistic) 不用MSE作为损失函数? 2022-02-28 Tags: #DeepLearning #MachineLearning #SoftmaxRegression #LogisticRegression #CostFunction #MeanSquareError #CrossEntropy
回顾:
MSE假设样本误差i.i.d., 并且服从正态分布, 最小化MSE等价于极大似然估计. 通常用于回归问题. MSE基于输出与真实值的欧氏距离.</description></item><item><title>Relation_between_Softmax_and_Logistic_Regression</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Relation_between_Softmax_and_Logistic_Regression/</link><pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Relation_between_Softmax_and_Logistic_Regression/</guid><description>Softmax 与 Logistic 回归的联系 2022-02-11 Tags: #SoftmaxRegression #LogisticRegression #Classification #MulticlassClassification
Ref: Unsupervised Feature Learning and Deep Learning Tutorial
二分类的 Softmax回归形式如下: $$h_{\theta}(x)=\frac{1}{\exp \left(\theta^{(1) \top} x\right)+\exp \left(\theta^{(2) \top} x^{(i)}\right)}\left[\begin{array}{c} \exp \left(\theta^{(1) \top} x\right) \ \exp \left(\theta^{(2) \top} x\right) \end{array}\right]$$</description></item><item><title>Part.20_Regularized_Logistic_Regression(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.20_Regularized_Logistic_RegressionML_Andrew.Ng./</link><pubDate>Sat, 11 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.20_Regularized_Logistic_RegressionML_Andrew.Ng./</guid><description>Regularized Logistic Regression 2021-09-11 Tags: #MachineLearning #LogisticRegression #Regularization
回顾一下没有正则化的情况 损失函数 更简洁的形式 $$\begin{align} J(\theta) &amp;amp;=-\frac{1}{m} \sum_{i=1}^{m}\left[y^{(i)} \log \left(h_{\theta}\left(x^{(i)}\right)\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]\ \end{align}$$ or 向量化的 $$\begin{aligned} h&amp;amp;=g(X \theta) \ J(\theta)&amp;amp;=-\frac{1}{m} \cdot\left[y^{T} \log (h)+(1-y)^{T} \log (1-h)\right] \end{aligned}$$</description></item><item><title>正则项不影响Logistic回归损失函数凸性</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/%E6%AD%A3%E5%88%99%E9%A1%B9%E4%B8%8D%E5%BD%B1%E5%93%8DLogistic%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%87%B8%E6%80%A7/</link><pubDate>Sat, 11 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/%E6%AD%A3%E5%88%99%E9%A1%B9%E4%B8%8D%E5%BD%B1%E5%93%8DLogistic%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%87%B8%E6%80%A7/</guid><description>正则项不影响Logistic回归损失函数凸性 2021-09-11 Tags: #MachineLearning #LogisticRegression #Regularization #ConvexOptimization #CostFunction
首先, 没有加正则项的二阶导数如下 那么只需要计算正则项的二阶导数 $$\begin{align} J(\theta)&amp;amp;=P(\theta)+\frac\lambda{2m}\sum^n_{i=1}\theta_i^2 \end{align}$$
$$\begin{aligned} \frac{\partial^2}{\partial \theta_{j}^2} \left(\frac\lambda{2m}\sum^n_{i=1}\theta_i^2\right)&amp;amp;= \frac{\lambda}{m}\frac{\partial}{\partial \theta_{j}} \theta_{j}\ &amp;amp;=\frac{\lambda}{m}&amp;gt;0 \end{aligned}$$ 所以损失函数还是凸的</description></item><item><title>证明Logistic回归的损失函数是凸函数</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/%E8%AF%81%E6%98%8ELogistic%E5%9B%9E%E5%BD%92%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%98%AF%E5%87%B8%E5%87%BD%E6%95%B0/</link><pubDate>Sat, 11 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/%E8%AF%81%E6%98%8ELogistic%E5%9B%9E%E5%BD%92%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%98%AF%E5%87%B8%E5%87%BD%E6%95%B0/</guid><description>证明Logistic回归的损失函数是凸函数 2021-09-11 Tags: #MachineLearning #LogisticRegression #ConvexOptimization #CostFunction
证明 原函数 Part.13_Cost_Function-Logistic_Regression(ML_Andrew.Ng.) $$\begin{aligned} h&amp;amp;=g(X \theta) \ J(\theta)&amp;amp;=-\frac{1}{m} \cdot\left[y^{T} \log (h)+(1-y)^{T} \log (1-h)\right] \end{aligned}$$</description></item><item><title>Part.12_Logistic_Regression(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.12_Logistic_RegressionML_Andrew.Ng./</link><pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.12_Logistic_RegressionML_Andrew.Ng./</guid><description>Logistic Regression 2021-08-19 Tags: #LogisticRegression #MachineLearning #Classification
Logistic Function Hypothesis Representation 我们可以通过对线性回归的方法进行一些小改动来匹配回归问题, 在线性回归的时候, $h(x)$的输出与分类问题的&amp;quot;值域&amp;quot;偏差较大, 比如在二分类问题里面, 要求$y=0\ or\ 1$, 但是$h(x)$会输出大于一或者小于零的数.</description></item><item><title>Part.13_Cost_Function-Logistic_Regression(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.13_Cost_Function-Logistic_RegressionML_Andrew.Ng./</link><pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.13_Cost_Function-Logistic_RegressionML_Andrew.Ng./</guid><description>Cost Function - Logistic Regression 2021-08-19 Tags: #CostFunction #LogisticRegression #MachineLearning
Representation 如果我们采用 线性回归的损失函数: 均方误差, 那么因为Logistic 回归的$h(x)$里面有形式很复杂的Logistic函数, 损失函数将不再是 凸函数, 将会很难最小化, 所以我们需要考虑另外的损失函数形式:</description></item><item><title>Part.14_Logistic_Regression&amp;Gradient_Descent(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.14_Logistic_RegressionGradient_DescentML_Andrew.Ng./</link><pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.14_Logistic_RegressionGradient_DescentML_Andrew.Ng./</guid><description>Logistic Regression &amp;amp; Gradient Descent 2021-08-19 Tags: #LogisticRegression #GradientDescent #MachineLearning
Gradient Descent: Cost Function: 推导 损失函数里面的$g(x)$为Logistic函数, Logistic的导函数为: $$\begin{aligned} \frac {d}{dx}g(x)&amp;amp;=g(x)\left(1-g(x)\right)\ \end{aligned}$$</description></item><item><title>Part.15_Advanced_Optimization(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.15_Advanced_OptimizationML_Andrew.Ng./</link><pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.15_Advanced_OptimizationML_Andrew.Ng./</guid><description>Advanced Optimization 2021-08-19 Tags: #Octave #MachineLearning #GradientDescent #LinearRegression #LogisticRegression
More sophisticated, faster way to optimize parameters: Conjugate gradient BFGS L-BFGS Link:其他Gradient_Descent Different_Gradient_Descent_Methods</description></item></channel></rss>