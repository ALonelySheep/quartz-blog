<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Transformer on</title><link>https://alonelysheep.github.io/quartz-blog/tags/Transformer/</link><description>Recent content in Transformer on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 30 Apr 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://alonelysheep.github.io/quartz-blog/tags/Transformer/index.xml" rel="self" type="application/rss+xml"/><item><title>D2L-75-BERT</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-75-BERT/</link><pubDate>Sat, 30 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-75-BERT/</guid><description>Bidirectional Encoder Representations from Transformers (BERT) 2022-04-30 Tags: #BERT #Transformer #DeepLearning
Motivation 构建一个通用的语言模型 在计算机视觉领域中, 我们能对一个已经训练好的大型网络进行微调(Fine-tune), 以较小的计算成本和网络改动就能获得很好的模型. BERT就是期望能够构建一个足够强大的预训练模型(Pretrained), 来适配各种各样的任务. 结合两个现有架构的优点: ELMo &amp;amp; GPT GPT: task-agnostic 其实在 BERT 以前, OpenAI 已经提出了 GPT （Generative Pre-Training，生成式预训练）模型, 试图提供一种既考虑上下文语意(context-sensitive), 又能适配多任务(task-agnostic1)的模型</description></item><item><title>D2L-76-BERT - Pretrain</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-76-BERT-Pretrain/</link><pubDate>Sat, 30 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-76-BERT-Pretrain/</guid><description>BERT: Pretrain 2022-04-30 Tags: #BERT #Pretrain #DeepLearning #Transformer
Pretrain Tasks Task 1 - Masked Language Modeling Motivation 语言模型(Language Model) 在输出时是从左到右进行的, 使用左侧的上下文来预测未知词元。</description></item><item><title>D2L-77-BERT - Fine-tune</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-77-BERT-Fine-tune/</link><pubDate>Sat, 30 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-77-BERT-Fine-tune/</guid><description>BERT: Fine-tune 2022-04-30 Tags: #BERT #Fine-tune #DeepLearning #Transformer
预训练好 BERT 以后, 我们只需要对模型进行很小的改动即可适配很多任务. 在 Finetuning 的时候, 新增的输出部分是从头开始训练的, 而 BERT 主体部分是在 pre-train 的基础上进行训练的.</description></item><item><title>D2L-74-Transformer</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-74-Transformer/</link><pubDate>Wed, 27 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-74-Transformer/</guid><description>Transformer 2022-04-27 Tags: #Transformer #Attention #DeepLearning
1
Transformer 是一个纯基于 Attention 的 Encoder Decoder 架构模型
Hugging Face Explorable Transformer: exBERT</description></item></channel></rss>