<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>SoftmaxRegression on</title><link>https://alonelysheep.github.io/quartz-blog/tags/SoftmaxRegression/</link><description>Recent content in SoftmaxRegression on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 28 Feb 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://alonelysheep.github.io/quartz-blog/tags/SoftmaxRegression/index.xml" rel="self" type="application/rss+xml"/><item><title>为什么Softmax回归不用MSE</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE/</link><pubDate>Mon, 28 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E4%B8%BA%E4%BB%80%E4%B9%88Softmax%E5%9B%9E%E5%BD%92%E4%B8%8D%E7%94%A8MSE/</guid><description>为什么Softmax (或者Logistic) 不用MSE作为损失函数? 2022-02-28 Tags: #DeepLearning #MachineLearning #SoftmaxRegression #LogisticRegression #CostFunction #MeanSquareError #CrossEntropy
回顾:
MSE假设样本误差i.i.d., 并且服从正态分布, 最小化MSE等价于极大似然估计. 通常用于回归问题. MSE基于输出与真实值的欧氏距离.</description></item><item><title>D2L-13-Softmax_Regression</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-13-Softmax_Regression/</link><pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-13-Softmax_Regression/</guid><description>Softmax 回归 2022-02-11 Tags: #SoftmaxRegression #MachineLearning #Classification #MulticlassClassification
Softmax回归解决的是多分类问题1, 它可以看作是二分类的 Logistic_Regression的推广. Softmax函数
Softmax回归 Softmax回归就是在线性回归的基础上套上一个Softmax函数, 取输出结果中概率最大的项作为预测结果. 交叉熵作为损失函数 D2L-14-Cross Entropy as Loss</description></item><item><title>Logit</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Logit/</link><pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Logit/</guid><description>Logit: a confusing term 2022-02-11 Tags: #Math #DeepLearning #SoftmaxRegression
Ref: machine learning - What is the meaning of the word logits in TensorFlow?</description></item><item><title>Relation_between_Softmax_and_Logistic_Regression</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Relation_between_Softmax_and_Logistic_Regression/</link><pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Relation_between_Softmax_and_Logistic_Regression/</guid><description>Softmax 与 Logistic 回归的联系 2022-02-11 Tags: #SoftmaxRegression #LogisticRegression #Classification #MulticlassClassification
Ref: Unsupervised Feature Learning and Deep Learning Tutorial
二分类的 Softmax回归形式如下: $$h_{\theta}(x)=\frac{1}{\exp \left(\theta^{(1) \top} x\right)+\exp \left(\theta^{(2) \top} x^{(i)}\right)}\left[\begin{array}{c} \exp \left(\theta^{(1) \top} x\right) \ \exp \left(\theta^{(2) \top} x\right) \end{array}\right]$$</description></item><item><title>Softmax_Regression_is_Over-parameterized</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Softmax_Regression_is_Over-parameterized/</link><pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Softmax_Regression_is_Over-parameterized/</guid><description>Softmax Regression is Over-parameterized 2022-02-11 Tags: #SoftmaxRegression
Ref: Unsupervised Feature Learning and Deep Learning Tutorial
将Softmax回归里面的参数全部变化一个相同的值, 结果不变: $$\begin{aligned} P\left(y^{(i)}=k \mid x^{(i)} ; \theta\right) &amp;amp;=\frac{\exp \left(\left(\theta^{(k)}-\psi\right)^{\top} x^{(i)}\right)}{\sum_{j=1}^{K} \exp \left(\left(\theta^{(j)}-\psi\right)^{\top} x^{(i)}\right)} \ &amp;amp;=\frac{\exp \left(\theta^{(k) \top} x^{(i)}\right) \exp \left(-\psi^{\top} x^{(i)}\right)}{\sum_{j=1}^{K} \exp \left(\theta^{(j) \top} x^{(i)}\right) \exp \left(-\psi^{\top} x^{(i)}\right)} \ &amp;amp;=\frac{\exp \left(\theta^{(k) \top} x^{(i)}\right)}{\sum_{j=1}^{K} \exp \left(\theta^{(j) \top} x^{(i)}\right)} \end{aligned}$$</description></item></channel></rss>