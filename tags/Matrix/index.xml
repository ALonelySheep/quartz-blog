<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Matrix on</title><link>https://alonelysheep.github.io/quartz-blog/tags/Matrix/</link><description>Recent content in Matrix on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 01 Feb 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://alonelysheep.github.io/quartz-blog/tags/Matrix/index.xml" rel="self" type="application/rss+xml"/><item><title>D2L-4-矩阵求导</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-4-%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/</link><pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-4-%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/</guid><description>矩阵求导 2022-02-01 Tags: #Math #Matrix
矩阵的求导一直很让人头疼😖 之前的笔记: 矩阵的求导 李沐这次的讲解方式不太一样，是从标量逐步推广到矩阵，还蛮清晰的。 从标量到向量 其中 $\Large{\frac{\partial y}{\partial x}, \frac{\partial \mathbf y}{\partial x}}$都很好理解, 尤其需要注意的是当求导的自变量$\mathbf x$为向量的时候, 为 $$\mathbf{x}=\left[\begin{array}{c} x_{1} \ x_{2} \ \vdots \ x_{n} \end{array}\right] \quad \frac{\partial y}{\partial \mathbf{x}}=\left[\frac{\partial y}{\partial x_{1}}, \frac{\partial y}{\partial x_{2}}, \ldots, \frac{\partial y}{\partial x_{n}}\right]$$ 结果变成了一个行向量.</description></item><item><title>矩阵的不同乘法-Hadamard-Kronecker</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/%E7%9F%A9%E9%98%B5%E7%9A%84%E4%B8%8D%E5%90%8C%E4%B9%98%E6%B3%95-Hadamard-Kronecker/</link><pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/%E7%9F%A9%E9%98%B5%E7%9A%84%E4%B8%8D%E5%90%8C%E4%B9%98%E6%B3%95-Hadamard-Kronecker/</guid><description>矩阵的不同乘积 2022-02-01 Tags: #Matrix #Math
一般的矩阵乘法 Hadamard Product $\odot$ 对应位置的元素相乘 $$ \begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\ a_{21} &amp;amp; a_{22} &amp;amp; a_{23}\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33} \end{bmatrix} \circ \begin{bmatrix} b_{11} &amp;amp; b_{12} &amp;amp; b_{13}\ b_{21} &amp;amp; b_{22} &amp;amp; b_{23}\ b_{31} &amp;amp; b_{32} &amp;amp; b_{33} \end{bmatrix} = \begin{bmatrix} a_{11}, b_{11} &amp;amp; a_{12}, b_{12} &amp;amp; a_{13}, b_{13}\ a_{21}, b_{21} &amp;amp; a_{22}, b_{22} &amp;amp; a_{23}, b_{23}\ a_{31}, b_{31} &amp;amp; a_{32}, b_{32} &amp;amp; a_{33}, b_{33} \end{bmatrix}$$</description></item><item><title>MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning,</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT-18.065-Matrix-Methods-in-Data-Analysis-Signal-Processing-and-Machine-Learning/</link><pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT-18.065-Matrix-Methods-in-Data-Analysis-Signal-Processing-and-Machine-Learning/</guid><description>MIT 18.065 - Matrix Methods in Data Analysis, Signal Processing, and Machine Learning, 2021-11-12 Tags: #Matrix #Math/LinearAlgebra #Math
学习这门课的主要动力是线性代数的知识在大二的一年内已经有所遗忘了, 并且在学习机器学习期间常常设计线性代数与矩阵的相关知识, 所以想要有针对性地深入学习与复习一下.</description></item><item><title>MIT_18.065-Part_2-Matrix_Factorization</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_2-Matrix_Factorization/</link><pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_2-Matrix_Factorization/</guid><description>矩阵分解 2021-11-12 Tags: #Matrix #Math/LinearAlgebra #Math
我们看待矩阵乘积的新方式有助于我们理解数据科学里面对矩阵的各种分解. 我们常常需要发掘一个矩阵$A$里面隐藏的信息, 而通过将$A$分解为$CR$, 我们可以观察A里面最基本的组成部分: 秩为1的矩阵: $col_k(C)\space row_k(R)$ 下面列举重要的分解, 在详细论述后将补充相应细节</description></item><item><title>MIT_18.065-Part_4-LU_Factorization</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_4-LU_Factorization/</link><pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_4-LU_Factorization/</guid><description>LU 分解 2021-11-12 Tags: #Math/LinearAlgebra #Math #Matrix
$A=L U$
Key Idea: 怎样从A = Sum of Rank 1 Matrices的角度来理解这个分解?</description></item><item><title>协方差矩阵_Covariance_Matrix</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5_Covariance_Matrix/</link><pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5_Covariance_Matrix/</guid><description>Covariance Matrix 2021-10-29 Tags: #Matrix #Math/Statistics
https://janakiev.com/blog/covariance-matrix/
Variance, Covariance Variance measures the variation of a single random variable (like height of a person in a population) $$\sigma_{x}^{2}=\mathbb E \left(\frac{1}{n-1} \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}\right)$$ Link: 为什么方差的分母常常是n-1</description></item><item><title>矩阵的求导</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E7%9A%84%E6%B1%82%E5%AF%BC/</link><pubDate>Mon, 16 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E7%9A%84%E6%B1%82%E5%AF%BC/</guid><description>对矩阵的求导_Matrix_Derivative 2021-08-16 Tags: #Matrix #Derivative #Calculus #MachineLearning
在学习吴恩达机器学习CS229的时候为了推导Normal Equation的公式, 接触到了函数对于矩阵的求导, 因为许久没有接触微积分, 并且知识跨度太大, 许久没有看懂, 故在此笔记中慢慢梳理. Learning Materials: Pili HU, Matrix Calculus, https://github.</description></item><item><title>矩阵迹的性质</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E8%BF%B9%E7%9A%84%E6%80%A7%E8%B4%A8/</link><pubDate>Mon, 16 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E8%BF%B9%E7%9A%84%E6%80%A7%E8%B4%A8/</guid><description>矩阵迹的性质 2021-08-16 Tags: #Trace #Matrix #Math
标量可以直接套上迹： $a=\operatorname{tr}(a)$
$\mathrm{tr}AB = \mathrm{tr}BA$ ^tracecommutative
左边: $$ \begin{align} &amp;amp;\sum^n_i a_{1i}b_{i1}+\sum^n_i a_{2i}b_{i2}+\cdots+\sum^n_i a_{mi}b_{im} \ = &amp;amp;\sum^m_j\sum^n_ia_{ji}b_{ij} \ = &amp;amp;\sum^m_i\sum^n_j a_{ij}b_{ji} \end{align} $$ 右边: $$ \begin{align} &amp;amp;\sum^m_i b_{1i}a_{i1}+\sum^m_i b_{2i}a_{i2}+\cdots+\sum^m_i b_{ni}a_{in} \ = &amp;amp;\sum^n_j\sum^m_i b_{ji}a_{ij} \ = &amp;amp;\sum^m_i\sum^n_j a_{ij}b_{ji} \end{align} $$</description></item><item><title>MIT_18.065-Part_3-A_Different_Perspectvie_of_Matrix_Multiplication-An_Example</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_3-A_Different_Perspectvie_of_Matrix_Multiplication-An_Example/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_3-A_Different_Perspectvie_of_Matrix_Multiplication-An_Example/</guid><description>Math/LinearAlgebra #Matrix #Math $$S=Q \Lambda Q^{\mathrm{T}}$$
其中S是一个对称矩阵, $S=S^{\mathrm{T}}$
Q的行向量是S的特征向量, 这些特征向量相互正交 $$Q=\left[\begin{array}{ccc} \mid &amp;amp; &amp;amp; \mid \ q_{1} &amp;amp; \ldots &amp;amp; q_{n} \ \mid &amp;amp; &amp;amp; \mid \end{array}\right]$$</description></item></channel></rss>