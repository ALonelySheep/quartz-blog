<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Fine-tune on</title><link>https://alonelysheep.github.io/quartz-blog/tags/Fine-tune/</link><description>Recent content in Fine-tune on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 30 Apr 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://alonelysheep.github.io/quartz-blog/tags/Fine-tune/index.xml" rel="self" type="application/rss+xml"/><item><title>D2L-77-BERT - Fine-tune</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-77-BERT-Fine-tune/</link><pubDate>Sat, 30 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-77-BERT-Fine-tune/</guid><description>BERT: Fine-tune 2022-04-30 Tags: #BERT #Fine-tune #DeepLearning #Transformer
预训练好 BERT 以后, 我们只需要对模型进行很小的改动即可适配很多任务. 在 Finetuning 的时候, 新增的输出部分是从头开始训练的, 而 BERT 主体部分是在 pre-train 的基础上进行训练的.</description></item></channel></rss>