<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Self-Attention on</title><link>https://alonelysheep.github.io/quartz-blog/tags/Self-Attention/</link><description>Recent content in Self-Attention on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 27 Apr 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://alonelysheep.github.io/quartz-blog/tags/Self-Attention/index.xml" rel="self" type="application/rss+xml"/><item><title>D2L-73-Positional_Encoding</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-73-Positional_Encoding/</link><pubDate>Wed, 27 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-73-Positional_Encoding/</guid><description>位置编码: 将位置信息加入数据 2022-04-27 Tags: #PositionalEncoding #Self-Attention #DeepLearning
为了使用序列的顺序信息，我们通过在输入表示中添加 位置编码（positional encoding）来注入绝对的或相对的位置信息。
我觉得D2L讲的很深入很好了: 10.6. Self-Attention and Positional Encoding</description></item><item><title>D2L-72-Self-Attention</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-72-Self-Attention/</link><pubDate>Tue, 26 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-72-Self-Attention/</guid><description>自注意力 2022-04-26 Tags: #Self-Attention #Attention #DeepLearning
Attention 机制可以抽象为:1 $$\begin{align} \textit{Attention}(Q,K,V) = V\cdot\textit{softmax}\space (\textit{score}(Q, K)) \end{align}$$ 自注意力就是 $Q = K = V$ , 也就是同一个序列同时作为 Query, Key 和 Value.</description></item></channel></rss>