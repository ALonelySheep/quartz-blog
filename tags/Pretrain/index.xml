<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Pretrain on</title><link>https://alonelysheep.github.io/quartz-blog/tags/Pretrain/</link><description>Recent content in Pretrain on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 30 Apr 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://alonelysheep.github.io/quartz-blog/tags/Pretrain/index.xml" rel="self" type="application/rss+xml"/><item><title>D2L-76-BERT - Pretrain</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-76-BERT-Pretrain/</link><pubDate>Sat, 30 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-76-BERT-Pretrain/</guid><description>BERT: Pretrain 2022-04-30 Tags: #BERT #Pretrain #DeepLearning #Transformer
Pretrain Tasks Task 1 - Masked Language Modeling Motivation 语言模型(Language Model) 在输出时是从左到右进行的, 使用左侧的上下文来预测未知词元。</description></item></channel></rss>