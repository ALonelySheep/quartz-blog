<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>CNN on</title><link>https://alonelysheep.github.io/quartz-blog/tags/CNN/</link><description>Recent content in CNN on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 07 Mar 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://alonelysheep.github.io/quartz-blog/tags/CNN/index.xml" rel="self" type="application/rss+xml"/><item><title>D2L-46-DenseNet</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-46-DenseNet/</link><pubDate>Mon, 07 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-46-DenseNet/</guid><description>DenseNet 2022-03-07 Tags: #DenseNet #DeepLearning #CNN ResNet极大地改变了如何参数化深层网络中函数的观点。 稠密连接网络（DenseNet）在某种程度上是ResNet的逻辑扩展。
PDF(zotero://select/items/@huang2017densely)
数学直觉: 从ResNet到DenseNet 某个函数在 $x=0$ 处的泰勒展开为: $$f(x)=f(0)+f^{\prime}(0) x+\frac{f^{\prime \prime}(0)}{2 !</description></item><item><title>D2L-45-ResNet</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-45-ResNet/</link><pubDate>Sun, 06 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-45-ResNet/</guid><description>ResNet 残差网络 2022-03-06 Tags: #ResNet #CNN #DeepLearning
ResNet在网络中引入了残差连接的思想, 简单的改变带来了很棒的效果.
残差连接让每一层很容易地包含了原始函数1, 这样能保证新增加的每一层都能包含原来的最优解, 进一步在原来的基础上继续改进.
Motivation 函数类的角度 我们定义 $\mathcal{F}$ 是某个模型能够拟合的所有函数构成的函数类.</description></item><item><title>D2L-43-GoogLeNet(Inception)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-43-GoogLeNetInception/</link><pubDate>Sat, 05 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-43-GoogLeNetInception/</guid><description>GoogLeNet 2022-03-05 Tags: #DeepLearning #CNN #GoogLeNet-Inception GoogLeNet是一个含并行连结的网络, 其核心组成部分为&amp;quot;Inception块&amp;quot;.
Inception块组合使用了不同大小的卷积核, 试图用现有的稠密结构(Dense Components)来构建一个&amp;quot;最佳的局部稀疏网络&amp;quot;.
局部: 多个Inception块拼接构成最后的GoogLeNet 稀疏: 也就是具有随机性的结构1 GoogLeNet还具有较高的计算效率, 这主要得益于Inception块里面不含全连接层.</description></item><item><title>D2L-42-NiN</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-42-NiN/</link><pubDate>Fri, 04 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-42-NiN/</guid><description>Network in Network - NiN 2022-03-04 Tags: #DeepLearning #NiN #CNN
用卷积代替全连接 动机 全连接层很贵 (参数很多) 一层卷积层需要的参数为:
卷积层参数大小的计算 卷积层后面第一个全连接层的参数为: $$in_Channel\times in_Height\times in_Width\times num_of_Hidden_Units$$</description></item><item><title>D2L-41-VGG</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-41-VGG/</link><pubDate>Thu, 03 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-41-VGG/</guid><description>VGG 2022-03-03 Tags: #DeepLearning #VGG #CNN
模块化是VGG网络最重要的思想. 模块化进一步带来了自由性, 不同的块配置可以带来不同的模型表现. 规范化 - 模块化 与芯片设计中工程师从放置晶体管到逻辑元件再到逻辑块的过程类似，神经网络架构的设计也逐渐变得更加抽象。研究人员开始从单个神经元的角度思考问题，发展到整个层，现在又转向块，重复层的模式。 使用块的设计同样让模型更加简洁. VGG块 VGG将 AlexNet 里面三层连续的卷积拿出来, 抽象成VGG块, 作为构建网络的基础模式.</description></item><item><title>D2L-40-AlexNet</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-40-AlexNet/</link><pubDate>Wed, 02 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-40-AlexNet/</guid><description>AlexNet 2022-03-02 Tags: #DeepLearning #AlexNet #CNN #ImageNet
模型解析 对比LeNet 最重要的是, AlexNet导致了计算机视觉方法论的改变: 从核方法到深度神经网络, 开启了神经网络的第二次热潮 1 对比LeNet, AlexNet的主要特点有: 输入图片更 &amp;ldquo;大&amp;rdquo;, 网络结构更 &amp;ldquo;深&amp;rdquo;, 每层通道更 &amp;ldquo;多&amp;rdquo;, 滑动窗口更 &amp;ldquo;大&amp;rdquo;(核函数和池化层) 使用了ReLU作为激活函数 池化层采用了Max Pooling 使用了丢弃法(Dropout) 作为正则化方法2, 而 LeNet只采用了 权重衰减 AlexNet在训练前进行了数据增强 3</description></item><item><title>D2L-39-LeNet</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-39-LeNet/</link><pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-39-LeNet/</guid><description>LeNet 2022-03-01 Tags: #LeNet #DeepLearning #CNN #NeuralNetwork
架构 1
总体来看，LeNet（LeNet-5）由两个部分组成：
卷积编码器：由两个卷积层组成; 全连接层密集块：由三个全连接层组成。 每个卷积块中的基本单元是一个卷积层(含一个sigmoid激活函数) 和一个Avg Pooling 层。</description></item><item><title>D2L-34-卷积层 - 填充 - Padding</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-34-%E5%8D%B7%E7%A7%AF%E5%B1%82-%E5%A1%AB%E5%85%85-Padding/</link><pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-34-%E5%8D%B7%E7%A7%AF%E5%B1%82-%E5%A1%AB%E5%85%85-Padding/</guid><description>Padding - 填充 2022-02-27 Tags: #DeepLearning #CNN #Padding
It&amp;rsquo;s always nice to have an interactive example:
Convolution Visualizer CNN Explainer 卷积操作会使图像尺寸变小, 填充 (Padding) 可以减缓这个过程.</description></item><item><title>D2L-35-卷积层 - 步幅 - Stride</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-35-%E5%8D%B7%E7%A7%AF%E5%B1%82-%E6%AD%A5%E5%B9%85-Stride/</link><pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-35-%E5%8D%B7%E7%A7%AF%E5%B1%82-%E6%AD%A5%E5%B9%85-Stride/</guid><description>Stride - 步幅 2022-02-27 Tags: #DeepLearning #CNN #Stride
It&amp;rsquo;s always nice to have an interactive example:
Convolution Visualizer CNN Explainer 卷积操作会使图像尺寸变小, 增大步幅 (Stride) 可以加快这个过程.</description></item><item><title>D2L-36-1x1卷积层</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-36-1x1%E5%8D%B7%E7%A7%AF%E5%B1%82/</link><pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-36-1x1%E5%8D%B7%E7%A7%AF%E5%B1%82/</guid><description>$1×1$ 卷积层 2022-02-27 Tags: #CNN #DeepLearning #Convolution
$1×1$ 卷积，即 $k_h=k_w=1$，它虽然不能提取相关特征, 但是却能融合图像的不同通道, 也是一种很受欢迎的网络结构.
它相当于输入形状为 $n_{h} n_{w} \times c_{i}$ , 权重为 $c_{o} \times c_{i}$ 的全连接层</description></item><item><title>D2L-37-CNN的计算复杂度</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-37-CNN%E7%9A%84%E8%AE%A1%E7%AE%97%E5%A4%8D%E6%9D%82%E5%BA%A6/</link><pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-37-CNN%E7%9A%84%E8%AE%A1%E7%AE%97%E5%A4%8D%E6%9D%82%E5%BA%A6/</guid><description>CNN的计算复杂度 2022-02-27 Tags: #DeepLearning #CNN #ComputationalComplexity
前面我们提到过利用傅里叶变换可以快速地计算卷积: 数值计算 卷积与傅里叶变换
动手学深度学习里面给出了一个例子, 说明卷积的计算复杂度其实还是很高的, 只是参数的存储开销较小.</description></item><item><title>D2L-38-池化层-Pooling_Layer</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-38-%E6%B1%A0%E5%8C%96%E5%B1%82-Pooling_Layer/</link><pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-38-%E6%B1%A0%E5%8C%96%E5%B1%82-Pooling_Layer/</guid><description>池化层/汇聚层 - Pooling Layer 2022-02-27 Tags: #PoolingLayer #DeepLearning #CNN
首先，池化层为什么叫“池化层” Collins Dictionary - Pool 7. verb
If a group of people or organizations pool their money, knowledge, or equipment, they share it or put it together so that it can be used for a particular purpose.</description></item><item><title>D2L-33-卷积神经网络CNN</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-33-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/</link><pubDate>Sat, 26 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-33-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/</guid><description>Convolutional Neural Network - 卷积神经网络 2022-02-26 Tags: #CNN #DeepLearning #Convolution
MLP的不足 随着图像分辨率的提高, MLP显露出以下不足:
假设我们的图像分辨率为 $1920\times 1080$, 那么一张图片就有 $2,073,600$ 个像素点, 假设和输入层相连的隐藏层有 $1000$ 个单元, 那么光是第一个全连接层就有大约 $2\times10^9$ ($20$ 亿) 个参数, 训练这样的网络是难以想象的, 况且这还只是网络的第一层.</description></item></channel></rss>