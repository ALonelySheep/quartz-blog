<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>NeuralNetwork on</title><link>https://alonelysheep.github.io/quartz-blog/tags/NeuralNetwork/</link><description>Recent content in NeuralNetwork on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 01 Apr 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://alonelysheep.github.io/quartz-blog/tags/NeuralNetwork/index.xml" rel="self" type="application/rss+xml"/><item><title>D2L-53-循环神经网络RNN</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-53-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN/</link><pubDate>Fri, 01 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-53-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN/</guid><description>Recurrent Neural Networks 2022-04-01 Tags: #RNN #DeepLearning #NeuralNetwork
Motivation 基于马尔可夫假设的N元语法（n-gram）需要存储大量的参数。在 $n$ 逐渐增大的过程中，n-gram模型的参数大小 $|W|$ 与序列长度 $n$ 是指数关系：$$|W|=|\mathcal{V}|^n $$ ($|\mathcal{V}|$ 是单词的数目) 因此, 我们将目光转向了 隐变量自回归模型.</description></item><item><title>D2L-39-LeNet</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-39-LeNet/</link><pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-39-LeNet/</guid><description>LeNet 2022-03-01 Tags: #LeNet #DeepLearning #CNN #NeuralNetwork
架构 1
总体来看，LeNet（LeNet-5）由两个部分组成：
卷积编码器：由两个卷积层组成; 全连接层密集块：由三个全连接层组成。 每个卷积块中的基本单元是一个卷积层(含一个sigmoid激活函数) 和一个Avg Pooling 层。</description></item><item><title>Part.28_Cost_Function-Neural_Network(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Part.28_Cost_Function-Neural_NetworkML_Andrew.Ng./</link><pubDate>Sat, 09 Oct 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Part.28_Cost_Function-Neural_NetworkML_Andrew.Ng./</guid><description>Cost Function - Neural Network 2021-10-09 Tags: #MachineLearning #NeuralNetwork #CostFunction
Basic Concepts $$\left{\left(x^{(1)}, y^{(1)}\right), \left(x^{(2)}, y^{(2)}\right), \ldots, \left(x^{(m)}, y^{(m)}\right)\right}$$</description></item><item><title>Part.23_Forward_Propagation-Neural_Network(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.23_Forward_Propagation-Neural_NetworkML_Andrew.Ng./</link><pubDate>Sun, 12 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.23_Forward_Propagation-Neural_NetworkML_Andrew.Ng./</guid><description>Forward Propagation 2021-09-12 Tags: #NeuralNetwork #MachineLearning
Vectorized Implementation $$\begin{aligned} a_{1}^{(2)} &amp;amp;=g\left(\Theta_{10}^{(1)} x_{0}+\Theta_{11}^{(1)} x_{1}+\Theta_{12}^{(1)} x_{2}+\Theta_{13}^{(1)} x_{3}\right) \ a_{2}^{(2)} &amp;amp;=g\left(\Theta_{20}^{(1)} x_{0}+\Theta_{21}^{(1)} x_{1}+\Theta_{22}^{(1)} x_{2}+\Theta_{23}^{(1)} x_{3}\right) \ a_{3}^{(2)} &amp;amp;=g\left(\Theta_{30}^{(1)} x_{0}+\Theta_{31}^{(1)} x_{1}+\Theta_{32}^{(1)} x_{2}+\Theta_{33}^{(1)} x_{3}\right) \ \end{aligned}$$ 我们把Sigmoid函数里面的部分用$z$代替: $$\begin{aligned} &amp;amp;a_{1}^{(2)}=g\left(z_{1}^{(2)}\right) \ &amp;amp;a_{2}^{(2)}=g\left(z_{2}^{(2)}\right) \ &amp;amp;a_{3}^{(2)}=g\left(z_{3}^{(2)}\right) \end{aligned}$$ 同时: $$\begin{aligned} &amp;amp;X = \left[\begin{array}{cccc} x_0 \x_1 \x_2 \x_3 \ \end{array}\right] =a^{(1)} \end{aligned}$$</description></item><item><title>Part.24_Neural_Network-Examples(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.24_Neural_Network-ExamplesML_Andrew.Ng./</link><pubDate>Sun, 12 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.24_Neural_Network-ExamplesML_Andrew.Ng./</guid><description>Examples of Neural Network 2021-09-12 Tags: #NeuralNetwork #MachineLearning
我们可以通过以下直观的组合过程, 体会神经网络利用Linearity构建Non-Linearity的过程. OR function AND function NOT function Putting together -&amp;gt; XNOR Function</description></item><item><title>Part.25_Multiclass_Classification-Neural_Network(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.25_Multiclass_Classification-Neural_NetworkML_Andrew.Ng./</link><pubDate>Sun, 12 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.25_Multiclass_Classification-Neural_NetworkML_Andrew.Ng./</guid><description>Multiclass Classification 2021-09-12 Tags: #MachineLearning #NeuralNetwork
We can define our set of resulting classes as y: $$ y^{(i)}=\left[\begin{array}{l} 1 \ 0 \ 0 \ 0 \end{array}\right],\left[\begin{array}{l} 0 \ 1 \ 0 \ 0 \end{array}\right],\left[\begin{array}{l} 0 \ 0 \ 1 \ 0 \end{array}\right],\left[\begin{array}{l} 0 \ 0 \ 0 \ 1 \end{array}\right] $$ Each $y^{(i)}$ represents a different image corresponding to either a car, pedestrian, truck, or motorcycle.</description></item><item><title>Part.21_Neural_Network_Introduction(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.21_Neural_Network_IntroductionML_Andrew.Ng./</link><pubDate>Sat, 11 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.21_Neural_Network_IntroductionML_Andrew.Ng./</guid><description>Neural Network - Introduction 2021-09-11 Tags: #NeuralNetwork #MachineLearning
Non-linear Hypotheses 所以随着特征数量的提高, 如果需要更加复杂的假设函数, 特征数量会爆炸式地增长. 比如在图片处理的时候, 每一个像素都是一个特征(RGB彩色像素甚至是三个特征), 那么特征的数目将会是十分庞大的: The “one learning algorithm” hypothesis Neurons in the Brain 树突: Dendrite 轴突: Axon 神经脉冲: Spike</description></item><item><title>Part.22_Model_Representation-Neural_Network(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.22_Model_Representation-Neural_NetworkML_Andrew.Ng./</link><pubDate>Sat, 11 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.22_Model_Representation-Neural_NetworkML_Andrew.Ng./</guid><description>Model Representation - NN 2021-09-11 Tags: #NeuralNetwork #MachineLearning
Hypothesis 一个神经元(Neuron / Activation Unit)的输出计算公式由如下公式给出: $$h_\Theta(x)= a = g(x_0\theta_0+x_1\theta_1+\cdots+x_n\theta_n)$$ 是线性的. ($g(x)$是Sigmoid Function)</description></item></channel></rss>