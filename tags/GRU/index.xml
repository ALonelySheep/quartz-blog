<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>GRU on</title><link>https://alonelysheep.github.io/quartz-blog/tags/GRU/</link><description>Recent content in GRU on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 03 Apr 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://alonelysheep.github.io/quartz-blog/tags/GRU/index.xml" rel="self" type="application/rss+xml"/><item><title>D2L-56-门控循环单元GRU</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-56-%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83GRU/</link><pubDate>Sun, 03 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-56-%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83GRU/</guid><description>Gated Recurrent Units (GRU) 2022-04-03 Tags: #GRU #RNN #DeepLearning
GRU在RNN的基础上添加了&amp;quot;门&amp;quot;(Gate), 针对性地解决了RNN里面存在的以下问题: 长期依赖问题: 序列早期的部分可能对未来所有观测值都有非常重要的影响, 我们需要能够保留序列早期信息的网络结构. GRU里面体现在: 重置门减少重置, 更新门更多地保留上一个隐状态 序列里面可能有干扰信息, 我们需要能够跳过(遗忘)这些信息的机制 GRU里面体现在: 更新门更多地保留上一个隐状态 序列里面可能有逻辑中断, 比如一本书里面章节的变化往往会导致主题的变化.</description></item></channel></rss>