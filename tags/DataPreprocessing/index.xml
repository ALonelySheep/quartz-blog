<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>DataPreprocessing on</title><link>https://alonelysheep.github.io/quartz-blog/tags/DataPreprocessing/</link><description>Recent content in DataPreprocessing on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 14 Jun 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://alonelysheep.github.io/quartz-blog/tags/DataPreprocessing/index.xml" rel="self" type="application/rss+xml"/><item><title>数据预处理-周期性数据(时间等)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.6/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-%E5%91%A8%E6%9C%9F%E6%80%A7%E6%95%B0%E6%8D%AE%E6%97%B6%E9%97%B4%E7%AD%89/</link><pubDate>Tue, 14 Jun 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.6/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-%E5%91%A8%E6%9C%9F%E6%80%A7%E6%95%B0%E6%8D%AE%E6%97%B6%E9%97%B4%E7%AD%89/</guid><description>周期性数据的预处理 2022-06-14 Tags: #MachineLearning #DataPreprocessing #CyclicFeatureEncoding
Source: Three Approaches to Encoding Time Information as Features for ML Models | NVIDIA Technical Blog</description></item><item><title>Imbalanced Data - 数据不均衡</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.6/Imbalanced-Data-%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%9D%87%E8%A1%A1/</link><pubDate>Tue, 07 Jun 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.6/Imbalanced-Data-%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%9D%87%E8%A1%A1/</guid><description>数据不均衡 2022-06-07 Tags: #DataPreprocessing
什么是数据不均衡 A classification data set with skewed class proportions is called imbalanced. Classes that make up a large proportion of the data set are called majority classes.</description></item><item><title>D2L-52-读取长序列数据的两种方法</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-52-%E8%AF%BB%E5%8F%96%E9%95%BF%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95/</link><pubDate>Tue, 08 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-52-%E8%AF%BB%E5%8F%96%E9%95%BF%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95/</guid><description>读取长序列数据的两种方法 2022-03-08 Tags: #SequentialData #DataPreprocessing
尽管序列数据本质上是连续的, 我们在处理数据的时候也希望将其分为小批量, 方便模型读取。 设我们将使用神经网络来训练语言模型， 模型中的网络一次处理具有预定义长度 （例如n个时间步）的一个小批量序列。 我们应该从上图中选择哪一个序列呢？ 事实上，他们都一样好。 然而，如果我们只选择一个偏移量， 那么用于训练网络的、所有可能的子序列的覆盖范围将是有限的。 因此，我们可以从随机偏移量开始划分序列， 以同时获得覆盖性（coverage）和随机性（randomness）。 下面，我们将描述如何实现随机采样（random sampling）和 顺序分区（sequential partitioning）策略。 随机采样 下图中 Batchsize=2, 也就是说每个Batch里面有两个序列, 一个红色的, 一个蓝色的.</description></item></channel></rss>