<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Multi-headAttention on</title><link>https://alonelysheep.github.io/quartz-blog/tags/Multi-headAttention/</link><description>Recent content in Multi-headAttention on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 27 Apr 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://alonelysheep.github.io/quartz-blog/tags/Multi-headAttention/index.xml" rel="self" type="application/rss+xml"/><item><title>D2L-71-Multi-Head_Attention</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-71-Multi-Head_Attention/</link><pubDate>Wed, 27 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-71-Multi-Head_Attention/</guid><description>多头注意力 2022-04-27 Tags: #Attention #Multi-headAttention #DeepLearning
多头注意力就是对 Query, Key, Value 进行一些线性变换, 并行地计算多个注意力, 期望模型能学习到多样化的依赖关系. Another way of seeing it: 1 模型构建 下面我们给出 Multi-Head Attention 的形象化表示:</description></item></channel></rss>