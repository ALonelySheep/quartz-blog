<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ActivationFunction on</title><link>https://alonelysheep.github.io/quartz-blog/tags/ActivationFunction/</link><description>Recent content in ActivationFunction on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 12 Feb 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://alonelysheep.github.io/quartz-blog/tags/ActivationFunction/index.xml" rel="self" type="application/rss+xml"/><item><title>D2L-18-激活函数-Activation_Functions</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-18-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-Activation_Functions/</link><pubDate>Sat, 12 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-18-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-Activation_Functions/</guid><description>常用激活函数 2022-02-12 Tags: #DeepLearning #ActivationFunction
1
ReLU 修正线性单元（Rectified Linear Unit，ReLU） ReLU就是一个 $max(0,x)$ 函数. ReLU是分段线性的 ReLU的变体通过设置一个线性项, 使得负轴的一些信息得到保留(Parameterized ReLU) $\mathbf{pReLU}(x)=max(0,x)+α\ min(0,x).</description></item><item><title>Sigmoid_Function</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Sigmoid_Function/</link><pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Sigmoid_Function/</guid><description>Sigmoid Function 2021-08-19 Tags: #Sigmoid #MachineLearning #ActivationFunction
什么是Sigmoid函数? Sigmoid的含义是像S型的, 所以Sigmoid函数便是具有S形状的一类函数.
Sigmoid函数把整个实数域上的任意数映射到一个有限的区间里面: $(0,1)$
在分类问题里面, it&amp;rsquo;s useful for transforming an arbitrary-valued function into a function better suited for classification.</description></item></channel></rss>