<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>GradientDescent on</title><link>https://alonelysheep.github.io/quartz-blog/tags/GradientDescent/</link><description>Recent content in GradientDescent on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 02 Feb 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://alonelysheep.github.io/quartz-blog/tags/GradientDescent/index.xml" rel="self" type="application/rss+xml"/><item><title>D2L-10-小批量随机梯度下降</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-10-%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</link><pubDate>Wed, 02 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-10-%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</guid><description>小批量随机梯度下降(Mini-Batch)是深度学习默认的求解方法 2022-02-02 Tags: #MachineLearning #GradientDescent #DeepLearning #Optimization
Different_Gradient_Descent_Methods 注意有两个点: 小批量(Mini-Batch), 随机(Stochastic) 梯度下降 其中:
小批量是因为在整个数据集上面训练一次又慢又贵 同时小批量还能从多个相似的数据点中选一个代表来计算, 节约了计算资源 但是样本不能太小, 太小的样本不适合用GPU并行计算 随机是选取小样本的方法: 随机选取</description></item><item><title>D2L-9-梯度下降的方向</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-9-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E6%96%B9%E5%90%91/</link><pubDate>Wed, 02 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-9-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E6%96%B9%E5%90%91/</guid><description>梯度下降的方向是梯度的反方向 2022-02-02 Tags: #GradientDescent #DeepLearning #MachineLearning
梯度是一个函数增长最快的方向, 通常我们都是想获得损失函数的最小值, 所以需要沿着梯度的反方向来移动.
注意这并不是一定的, 梯度下降/上升只是一种优化方法而已, 如果我们想要优化的目标函数取得最大值, 那么就应该沿着梯度的方向变化.</description></item><item><title>Part.19_Regularized_Linear_Regression(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.19_Regularized_Linear_RegressionML_Andrew.Ng./</link><pubDate>Fri, 10 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.19_Regularized_Linear_RegressionML_Andrew.Ng./</guid><description>Regularization &amp;amp; Linear Regression 2021-09-10 Tags: #MachineLearning #Regularization #GradientDescent #LinearRegression #NormalEquation
Regularization &amp;amp; Gradient Descent 添加了正则项之后有两点需要注意:
$\theta_0$需要单独处理 (不需要正则约束, 损失函数不一样) $\theta_1 \sim \theta_n$ 因为需要正则化, 损失函数$J(\theta)$发生了变化, 梯度需要重新计算 正则项不影响线性回归损失函数的凸性 同时考虑上面两点, 梯度下降更新公式变为了:</description></item><item><title>正则项不影响线性回归损失函数的凸性</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/%E6%AD%A3%E5%88%99%E9%A1%B9%E4%B8%8D%E5%BD%B1%E5%93%8D%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%87%B8%E6%80%A7/</link><pubDate>Fri, 10 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/%E6%AD%A3%E5%88%99%E9%A1%B9%E4%B8%8D%E5%BD%B1%E5%93%8D%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%87%B8%E6%80%A7/</guid><description>正则项不影响线性回归损失函数的凸性 2021-09-10 Tags: #MachineLearning #Regularization #GradientDescent #LinearRegression #ConvexOptimization
Question: 加上正则项以后函数还是凸的吗? 梯度下降还适用吗? 还是适用的, 证明如下 首先, 如何证明一个函数为凸函数? 如果$f$是二阶可微的，那么如果$f$的定义域是凸集，并且$\forall x\in dom(f), \nabla^2 f(x)\geqslant0$，那么$f$ 就是一个凸函数.</description></item><item><title>Part.14_Logistic_Regression&amp;Gradient_Descent(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.14_Logistic_RegressionGradient_DescentML_Andrew.Ng./</link><pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.14_Logistic_RegressionGradient_DescentML_Andrew.Ng./</guid><description>Logistic Regression &amp;amp; Gradient Descent 2021-08-19 Tags: #LogisticRegression #GradientDescent #MachineLearning
Gradient Descent: Cost Function: 推导 损失函数里面的$g(x)$为Logistic函数, Logistic的导函数为: $$\begin{aligned} \frac {d}{dx}g(x)&amp;amp;=g(x)\left(1-g(x)\right)\ \end{aligned}$$</description></item><item><title>Part.15_Advanced_Optimization(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.15_Advanced_OptimizationML_Andrew.Ng./</link><pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.15_Advanced_OptimizationML_Andrew.Ng./</guid><description>Advanced Optimization 2021-08-19 Tags: #Octave #MachineLearning #GradientDescent #LinearRegression #LogisticRegression
More sophisticated, faster way to optimize parameters: Conjugate gradient BFGS L-BFGS Link:其他Gradient_Descent Different_Gradient_Descent_Methods</description></item><item><title>Part.8_Train_Gradient_Descent(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.8_Train_Gradient_DescentML_Andrew.Ng./</link><pubDate>Fri, 06 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.8_Train_Gradient_DescentML_Andrew.Ng./</guid><description>Train Gradient Descent 2021-08-06 Tags: #GradientDescent #MachineLearning
判断收敛(Convergence)的方法 画出Cost Function - Iteration图, 平缓后收敛 相邻周期变化值小于一个很小的值$\Delta$ 寻找正常的学习率 只要学习率$\alpha$足够小, 损失函数一定是递减的(可以严格证明) 如果学习率波动或者递增, 常常是因为学习率过大 学习率过大也有一定几率导致收敛缓慢 学习率过小会导致收敛过慢 合适的方法是类似于二分法的思路, 用一系列的值去尝试, e.</description></item><item><title>Relation_Between_Linear_Regression&amp;Gradient_Descent_梯度下降和线性回归的关系</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Relation_Between_Linear_RegressionGradient_Descent_%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%92%8C%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E5%85%B3%E7%B3%BB/</link><pubDate>Thu, 05 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Relation_Between_Linear_RegressionGradient_Descent_%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%92%8C%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E5%85%B3%E7%B3%BB/</guid><description>梯度下降法和线性回归的关系 2021-08-05 Tags: #MachineLearning #LinearRegression #GradientDescent
1 2 graph TD A([梯度下降])--&amp;gt;B([梯度下降+平方损失])--&amp;gt;C([梯度下降+平方损失+线性回归]) 梯度下降法公式 $$ \begin{array}{l} \text { repeat until convergence }{\ \begin{array}{cc} \theta_{j}:=\theta_{j}-\alpha \frac{\Large\partial}{\Large\partial \Large\theta_{j}} J\left(\theta_{0},\cdots ,\theta_{n}\right) &amp;amp; \text { (simultaneously update } j=0, \cdots ,j=n) \end{array}\ \text { } } \end{array} $$ 梯度下降 + Cost Function=平方损失 $$J\left(\theta_{0},\cdots ,\theta_{n}\right)=\frac{1}{2 m} \sum_{i=1}^{m}\left(\hat{y}^{(i)}-y^{(i)}\right)^{2}=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}$$ 所以</description></item><item><title>Part.5_Gradient_Descent(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.5_Gradient_DescentML_Andrew.Ng./</link><pubDate>Mon, 02 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.5_Gradient_DescentML_Andrew.Ng./</guid><description>Gradient Descent 2021-08-02 Tags: #MachineLearning #GradientDescent
梯度下降是一种最小化损失函数的标准方法 So we have our hypothesis function and we have a way of measuring how well it fits into the data.</description></item></channel></rss>