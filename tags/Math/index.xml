<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Math on</title><link>https://alonelysheep.github.io/quartz-blog/tags/Math/</link><description>Recent content in Math on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 22 May 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://alonelysheep.github.io/quartz-blog/tags/Math/index.xml" rel="self" type="application/rss+xml"/><item><title>从二项分布到泊松分布再到指数分布-From Binomial Distribution to Poisson Distribution to Exponential Distribution</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.5/%E4%BB%8E%E4%BA%8C%E9%A1%B9%E5%88%86%E5%B8%83%E5%88%B0%E6%B3%8A%E6%9D%BE%E5%88%86%E5%B8%83%E5%86%8D%E5%88%B0%E6%8C%87%E6%95%B0%E5%88%86%E5%B8%83-From-Binomial-Distribution-to-Poisson-Distribution-to-Exponential-Distribution/</link><pubDate>Sun, 22 May 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.5/%E4%BB%8E%E4%BA%8C%E9%A1%B9%E5%88%86%E5%B8%83%E5%88%B0%E6%B3%8A%E6%9D%BE%E5%88%86%E5%B8%83%E5%86%8D%E5%88%B0%E6%8C%87%E6%95%B0%E5%88%86%E5%B8%83-From-Binomial-Distribution-to-Poisson-Distribution-to-Exponential-Distribution/</guid><description>From Binomial Distribution to Poisson Distribution to Exponential Distribution 2022-05-22 Tags: #Math/Probability #PoissonDistribution #BinomialDistribution #ExponentialDistribution
这两个回答讲的挺好: 泊松分布的现实意义是什么，为什么现实生活多数服从于泊松分布？ - 马同学的回答 - 知乎 https://www.</description></item><item><title>Harmonic_Mean-调和平均数</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.5/Harmonic_Mean-%E8%B0%83%E5%92%8C%E5%B9%B3%E5%9D%87%E6%95%B0/</link><pubDate>Tue, 03 May 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.5/Harmonic_Mean-%E8%B0%83%E5%92%8C%E5%B9%B3%E5%9D%87%E6%95%B0/</guid><description>Harmonic Mean 2022-05-03 Tags: #HarmonicMean #Math
用 $H$ 表示两个数的调和平均数, 则: $$\frac{1}{H}=\frac{1}{2}\left(\frac{1}{x_{1}}+\frac{1}{x_{2}}\right)$$ 显式地表示为: $$H=\frac{2 x_{1} x_{2}}{x_{1}+x_{2}}$$
直观理解 用紫色线段 $H$ 表示 $a, b$ 的Harmonic Mean: 其中:</description></item><item><title>凸组合 - Convex Combination</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/%E5%87%B8%E7%BB%84%E5%90%88-Convex-Combination/</link><pubDate>Mon, 18 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/%E5%87%B8%E7%BB%84%E5%90%88-Convex-Combination/</guid><description>Convex Combination 2022-04-18 Tags: #NonlinearProgreamming #Math #ConvexCombination
A convex combination of points $\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \ldots, \mathbf{x}^{(k)} \in \mathbb{R}^{n}$ is a &amp;ldquo;weighted average&amp;rdquo;: a linear combination $$ \lambda_{1} \mathbf{x}^{(1)}+\lambda_{2} \mathbf{x}^{(2)}+\cdots+\lambda_{k} \mathbf{x}^{(k)} $$ where $\lambda_{1}+\lambda_{2}+\cdots+\lambda_{k}=1$ and $\lambda_{1}, \ldots, \lambda_{k} \geq 0$</description></item><item><title>递推公式 $a_{t}=b_{t}+c_{t}a_{t-1}$ 转通项公式</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/%E9%80%92%E6%8E%A8%E5%85%AC%E5%BC%8F-a_tb_t+c_ta_t-1-%E8%BD%AC%E9%80%9A%E9%A1%B9%E5%85%AC%E5%BC%8F/</link><pubDate>Sat, 02 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/%E9%80%92%E6%8E%A8%E5%85%AC%E5%BC%8F-a_tb_t+c_ta_t-1-%E8%BD%AC%E9%80%9A%E9%A1%B9%E5%85%AC%E5%BC%8F/</guid><description>递推公式 $a_{t}=b_{t}+c_{t}a_{t-1}$ 转通项公式 2022-04-02 Tags: #Math
$$\begin{aligned} a_{t}=b_{t} &amp;amp;+c_{t} a_{t-1} \ &amp;amp;+ c_{t}\left(b_{t-1}+c_{t-1} a_{t-2}\right) \ &amp;amp;\hspace{4.25em}+c_{t-1}\left(b_{t-2}+c_{t-2} a_{t-3}\right) \ &amp;amp;\hspace{13em}\vdots \ &amp;amp;\hspace{12.</description></item><item><title>KL_Divergence-KL散度</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/KL_Divergence-KL%E6%95%A3%E5%BA%A6/</link><pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/KL_Divergence-KL%E6%95%A3%E5%BA%A6/</guid><description>Kullback–Leibler divergence 2022-02-11 Tags: #Math/Probability #DeepLearning
1
KL散度可以衡量两个概率分布之间的相似性
KL散度也称为相对熵
Wikipedia: In mathematical statistics, the Kullback–Leibler divergence, $D _{KL} ( P ∥ Q )$ (also called relative entropy), is a statistical distance: a measure of how one probability distribution Q is different from a second, reference probability distribution P.</description></item><item><title>Likelihood_Function-似然函数</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Likelihood_Function-%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0/</link><pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Likelihood_Function-%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0/</guid><description>Likelihood Function - 似然函数 2022-02-11 Tags: #Math/Statistics #MachineLearning
对于某个(某组)随机变量 $X$, 我们通过采样获得了数据集 $x$ :
似然函数$\mathcal{L}(\theta \mid x)$就是在某个参数(parameter) $\theta$ 下, 现有数据 $x$ 出现的概率大小, 也就是说: $$\mathcal{L}(\theta \mid x) = P(X=x\mid\theta)$$ $P(X=x\mid\theta)$ 也常常写作 $p_{\theta}(x)=P_{\theta}(X=x)=P(X=x\ ;\theta)$</description></item><item><title>Logit</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Logit/</link><pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/Logit/</guid><description>Logit: a confusing term 2022-02-11 Tags: #Math #DeepLearning #SoftmaxRegression
Ref: machine learning - What is the meaning of the word logits in TensorFlow?</description></item><item><title>Dummy_Variables</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/Dummy_Variables/</link><pubDate>Thu, 10 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/Dummy_Variables/</guid><description>Dummy Variable 2022-02-10 Tags: #Math/Statistics
Dummy variable (statistics) - Wikipedia
In statistics and econometrics, particularly in regression analysis, a dummy variable is one that takes only the value 0 or 1 to indicate the absence or presence of some categorical effect that may be expected to shift the outcome.</description></item><item><title>Kronecker delta - 克罗内克δ函数</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/Kronecker-delta-%E5%85%8B%E7%BD%97%E5%86%85%E5%85%8B%CE%B4%E5%87%BD%E6%95%B0/</link><pubDate>Thu, 10 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/Kronecker-delta-%E5%85%8B%E7%BD%97%E5%86%85%E5%85%8B%CE%B4%E5%87%BD%E6%95%B0/</guid><description>Kronecker delta 2022-02-10 Tags: #Math
$$\delta_{ij} = \left{\begin{matrix} 1 &amp;amp; (i=j) \ 0 &amp;amp; (i \ne j) \end{matrix}\right.$$
在线性代数中，单位矩阵可以写作 $\left(\delta_{i j}\right)_{i, j=1}^{n}$</description></item><item><title>仿射变换-Affine_Transformation</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2-Affine_Transformation/</link><pubDate>Thu, 10 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2-Affine_Transformation/</guid><description>仿射变换 Affine Transformation 2022-02-10 Tags: #Math/LinearAlgebra
仿射变换就是平移后的线性变换:
1 Here is an Interaction: Affine transformations / Kjerand Pedersen / Observable 有趣的是, 我们可以在高维度通过线性变换来完成仿射变换2 3</description></item><item><title>D2L-12-Predication_or_Inference-Difference</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-12-Predication_or_Inference-Difference/</link><pubDate>Tue, 08 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-12-Predication_or_Inference-Difference/</guid><description>Prediction or Inference? The Difference 2022-02-08 Tags: #DeepLearning #Math/Statistics #Inference #Prediction
在深度学习里面, 给定特征估计目标的过程通常称为_预测_（prediction）或_推断_（inference）。 但是, 虽然 推断 这个词已经成为深度学习的标准术语，但其实 推断 这个词有些用词不当。 在统计学中，推断 更多地表示基于数据集估计参数。1 当深度学习从业者与统计学家交谈时，术语的误用经常导致一些误解。2 如Bayesian Inference: Bayesian_Estimation(Inference)贝叶斯估计&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>D2L-5-拓展链式法则</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-5-%E6%8B%93%E5%B1%95%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99/</link><pubDate>Wed, 02 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-5-%E6%8B%93%E5%B1%95%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99/</guid><description>拓展的求导链式法则 2022-02-02 Tags: #Math #Derivative
从标量到向量, 不仅符号需要对应上, 矩阵的形状也需要对应上 $$\begin{align} &amp;amp;\frac{\partial y}{\partial \mathbf{x}}=\frac{\partial y}{\partial u} \frac{\partial u}{\partial \mathbf{x}}\ &amp;amp;\small{(1, n)}\quad{(1,1)(1,n)} \end{align}$$</description></item><item><title>D2L-3-亚导数</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-3-%E4%BA%9A%E5%AF%BC%E6%95%B0/</link><pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-3-%E4%BA%9A%E5%AF%BC%E6%95%B0/</guid><description>亚导数 2022-02-01 Tags: #Math
将导数拓展到了不可微的函数: 举例:</description></item><item><title>D2L-4-矩阵求导</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-4-%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/</link><pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/D2L-4-%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/</guid><description>矩阵求导 2022-02-01 Tags: #Math #Matrix
矩阵的求导一直很让人头疼😖 之前的笔记: 矩阵的求导 李沐这次的讲解方式不太一样，是从标量逐步推广到矩阵，还蛮清晰的。 从标量到向量 其中 $\Large{\frac{\partial y}{\partial x}, \frac{\partial \mathbf y}{\partial x}}$都很好理解, 尤其需要注意的是当求导的自变量$\mathbf x$为向量的时候, 为 $$\mathbf{x}=\left[\begin{array}{c} x_{1} \ x_{2} \ \vdots \ x_{n} \end{array}\right] \quad \frac{\partial y}{\partial \mathbf{x}}=\left[\frac{\partial y}{\partial x_{1}}, \frac{\partial y}{\partial x_{2}}, \ldots, \frac{\partial y}{\partial x_{n}}\right]$$ 结果变成了一个行向量.</description></item><item><title>矩阵的不同乘法-Hadamard-Kronecker</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/%E7%9F%A9%E9%98%B5%E7%9A%84%E4%B8%8D%E5%90%8C%E4%B9%98%E6%B3%95-Hadamard-Kronecker/</link><pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.1/%E7%9F%A9%E9%98%B5%E7%9A%84%E4%B8%8D%E5%90%8C%E4%B9%98%E6%B3%95-Hadamard-Kronecker/</guid><description>矩阵的不同乘积 2022-02-01 Tags: #Matrix #Math
一般的矩阵乘法 Hadamard Product $\odot$ 对应位置的元素相乘 $$ \begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13}\ a_{21} &amp;amp; a_{22} &amp;amp; a_{23}\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33} \end{bmatrix} \circ \begin{bmatrix} b_{11} &amp;amp; b_{12} &amp;amp; b_{13}\ b_{21} &amp;amp; b_{22} &amp;amp; b_{23}\ b_{31} &amp;amp; b_{32} &amp;amp; b_{33} \end{bmatrix} = \begin{bmatrix} a_{11}, b_{11} &amp;amp; a_{12}, b_{12} &amp;amp; a_{13}, b_{13}\ a_{21}, b_{21} &amp;amp; a_{22}, b_{22} &amp;amp; a_{23}, b_{23}\ a_{31}, b_{31} &amp;amp; a_{32}, b_{32} &amp;amp; a_{33}, b_{33} \end{bmatrix}$$</description></item><item><title>Maximum_Likelihood_Estimation-极大似然估计</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Maximum_Likelihood_Estimation-%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/</link><pubDate>Sat, 25 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Maximum_Likelihood_Estimation-%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/</guid><description>极大似然估计 MLE 2021-12-25 Tags: #MachineLearning #Math/Statistics
Links: Likelihood_Function-似然函数
假设样本 $X$ 服从已知的概率分布(比如正态分布)
极大似然估计就是要找一个参数 $\hat\theta$, 使似然函数 $\mathcal{L}(\theta \mid X)$ 取得最大值$$i.e.\quad \hat{\theta}=\operatorname{argmax}_{\theta \in \Theta} \mathcal{L}(\theta \mid X)$$ 极大似然估计认为: 最佳的参数 $\hat\theta$ 最可能使取样结果为现在的 $x$, 也就是说, 概率$P(X=x\mid \theta)$最大: $$\hat{\theta}=\operatorname{argmax}_{\theta \in \Theta} P(X=x\mid \theta)$$</description></item><item><title>参数估计-Parameter_Estimation</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1-Parameter_Estimation/</link><pubDate>Sat, 25 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1-Parameter_Estimation/</guid><description>参数估计 2021-12-25 Tags: #MachineLearning #ParameterEstimation #Math/Statistics
在设计分类器或者进行回归预测的时候, 我们需要知道目标问题的概率分布情况. 但是通常我们能得到的数据只是一些特例(即训练样本). 为了对问题进行建模, 我们不仅需要确定合适的概率分布模型, 还需要根据训练样本确定模型里面的具体参数. 参数估计就是在模型已知的情况下得到最优参数的过程.
对于贝叶斯分类器, 估计先验概率$P(\omega_i)$通常不是很困难. 难点在于估计类条件概率密度$p(x|\omega_i)$, 这是因为:</description></item><item><title>多元高斯分布-Mutivariate_Gaussian</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83-Mutivariate_Gaussian/</link><pubDate>Fri, 24 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83-Mutivariate_Gaussian/</guid><description>Multivariate Gaussian 2021-12-24 Tags: #GaussianDistribution #Math/Probability
正态分布_高斯分布_Normal_Distribution-Gaussian_Distribution
$$p(x ; \mu, \Sigma)=\frac{1}{(2 \pi)^{n / 2}|\Sigma|^{1 / 2}} \exp \left(-\frac{1}{2}(x-\mu)^{T} \Sigma^{-1}(x-\mu)\right)$$</description></item><item><title>Understanding Bayes' Theorem</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Understanding-Bayes-Theorem/</link><pubDate>Sun, 19 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Understanding-Bayes-Theorem/</guid><description>Understanding Bayes&amp;rsquo; Theorem 2021-12-19 Tags: #Math/Probability #Bayes
单从形式上来说, Bayes定理是十分简单的. 但是如果我们结合实际问题与一点几何直觉, Bayes定理可以从两个独特的角度来直观理解:
Update of Prior Beliefs &amp;amp; Change of Perspective Bayes&amp;rsquo; Theorem: Statement $$P(A \mid B)=\frac{P(B \mid A) P(A)}{P(B)}$$</description></item><item><title>Covariance-协方差</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Covariance-%E5%8D%8F%E6%96%B9%E5%B7%AE/</link><pubDate>Sat, 11 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Covariance-%E5%8D%8F%E6%96%B9%E5%B7%AE/</guid><description>Covariance 2021-12-11 Tags: #Math/Statistics
期望值分别为 $E(X)=\mu$ 与 $E(Y)=\nu$ 的两个随机变量 X 与 Y 之间的协方差定义为: $$\begin{aligned} \operatorname{cov}(X,Y)&amp;amp;=\mathrm{E}((X-\mu)(Y-\nu))\ &amp;amp;=\mathrm{E}(X \cdot Y-\nu X-\mu Y +\mu\nu)\ &amp;amp;=\mathrm{E}(X \cdot Y)-\nu \mathrm{E}(X)-\mu \mathrm{E}(Y) +\mu\nu\ &amp;amp;=\mathrm{E}(X \cdot Y)-\mu\nu-\mu\nu +\mu\nu\ &amp;amp;=\mathrm{E}(X \cdot Y)-\mu \nu \end{aligned}$$</description></item><item><title>Chernoff Bounds</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Chernoff-Bounds/</link><pubDate>Sat, 04 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Chernoff-Bounds/</guid><description>Chernoff Bounds 2021-12-04 Tags: #Math/Statistics
probabilitycourse.com - Chernoff Bounds
zotero:@ChernoffBounds(zotero://select/items/@ChernoffBounds)
The generic Chernoff bound for a random variable $X$ is attained by applying Markov&amp;rsquo;s inequality to $e^{tX}$.</description></item><item><title>Chi-Squared_Distribution-卡方分布</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Chi-Squared_Distribution-%E5%8D%A1%E6%96%B9%E5%88%86%E5%B8%83/</link><pubDate>Sat, 04 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Chi-Squared_Distribution-%E5%8D%A1%E6%96%B9%E5%88%86%E5%B8%83/</guid><description>Chi-Squared Distribution 2021-12-04 Tags: #Math/Statistics
A very Good Website Chi-squared distribution - Wikipedia
重要结论 $Z_{1},Z_{2},\cdots,Z_{n}$ are independent standard normal random variables,</description></item><item><title>Markov's and Chebyshev's Inequalities</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Markovs-and-Chebyshevs-Inequalities/</link><pubDate>Sat, 04 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Markovs-and-Chebyshevs-Inequalities/</guid><description>Markov and Chebyshev Inequalities 2021-12-04 Tags: #Math/Statistics
FileLink(zotero://select/items/@InequalitiesMarkov)
Markov&amp;rsquo;s Inequality $X$ 是一个非负的随机变量. 对于任意正实数 $a$ , 有 $$ P(X \geq a) \leq \frac{E(X)}{a} $$</description></item><item><title>Moment Generating Function-MGF</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Moment-Generating-Function-MGF/</link><pubDate>Sat, 04 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Moment-Generating-Function-MGF/</guid><description>Moment Generating Function - MGF 2021-12-04 Tags: #Math/Statistics
This article covers it all. Moment Generating Function Explained | by Aerin Kim | Towards Data Science</description></item><item><title>Union_Bound-布尔不等式-Boole's_inequality</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Union_Bound-%E5%B8%83%E5%B0%94%E4%B8%8D%E7%AD%89%E5%BC%8F-Booles_inequality/</link><pubDate>Sat, 04 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Union_Bound-%E5%B8%83%E5%B0%94%E4%B8%8D%E7%AD%89%E5%BC%8F-Booles_inequality/</guid><description>Union Bound: 布尔不等式 2021-12-04 Tags: #Math/Statistics
This website explained it well: The Union Bound and Extension Intuition $$\begin{aligned} P(A \cup B) &amp;amp;=P(A)+P(B)-P(A \cap B) \ &amp;amp; \leq P(A)+P(B) .</description></item><item><title>Johnson Lindenstrauss Lemma - Publish Version</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Johnson-Lindenstrauss-Lemma-Publish-Version/</link><pubDate>Fri, 03 Dec 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.12/Johnson-Lindenstrauss-Lemma-Publish-Version/</guid><description>Johnson Lindenstrauss Lemma 2021-12-03 Tags: #MachineLearning #Math
对于高维数据，我们能够在降维的过程中保留其大部分的几何特征，即使降维的幅度非常大。 这是徐亦达老师让我们学习的第一个主题
1
Study Materials MIT 6.</description></item><item><title>MIT_18.065-Part_11-SVD_&amp;_Linear_System</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_11-SVD__Linear_System/</link><pubDate>Thu, 18 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_11-SVD__Linear_System/</guid><description>SVD &amp;amp; Linear System 2021-11-18 Tags: #LinearRegression #SVD #Math/LinearAlgebra
$Ax=b$ 对于这个线性约束方程组: $$Ax=b$$
只有在A可逆的方阵的时候, 才有唯一解: $$x=A^{-1}b$$
而在A为其他形状的时候, 常常不能够简单的利用$A^{-1}$来求解这个方程组
Under-determined: (不定方程) 这时我们没有足够的约束来限制x, x常常有无穷解 换一个看法, 这可以看作因为$Row(A)$没有填满$R^n$, 所以我们可以在每一个解里面加上一部分核空间里面的向量$x_{kernel}$, 同时不影响方程的成立: $$\begin{aligned}&amp;amp;A(x+x_{kernel})=b \\Rightarrow &amp;amp;Ax+Ax_{kernel}=b\\Rightarrow &amp;amp;Ax+0=b\end{aligned}$$ Over-determined: (超定方程) 在这个情况下, 我们有太多限制来限制x, 所以有可能出现矛盾, 导致x没有解.</description></item><item><title>MIT_18.065-Part_10-SVD_in_Action</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_10-SVD_in_Action/</link><pubDate>Wed, 17 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_10-SVD_in_Action/</guid><description>SVD in Action 2021-11-17 Tags: #SVD #Math/LinearAlgebra
Economy SVD 在被分解的矩阵A特别&amp;quot;瘦高&amp;quot;的时候(m&amp;raquo;n), 我们可以只取$U$的前n列, 因为后面的&amp;quot;重要性&amp;quot;不大.
Application Digital Watermark Hands-on Tips Plot how the information varies SVD Method of Snapshots A different way to compute SVD if the data is so large that you can&amp;rsquo;t store it into memory at once.</description></item><item><title>内积和相关性的联系-Dot(Inner)_Product_&amp;_Correlation</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/%E5%86%85%E7%A7%AF%E5%92%8C%E7%9B%B8%E5%85%B3%E6%80%A7%E7%9A%84%E8%81%94%E7%B3%BB-DotInner_Product__Correlation/</link><pubDate>Tue, 16 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/%E5%86%85%E7%A7%AF%E5%92%8C%E7%9B%B8%E5%85%B3%E6%80%A7%E7%9A%84%E8%81%94%E7%B3%BB-DotInner_Product__Correlation/</guid><description>Inner Product &amp;amp; Correlation 2021-11-16 Tags: #Math #InnerProduct
Source: David Joyce&amp;rsquo;s answer to Is there any relation between &amp;lsquo;correlation of two signals&amp;rsquo; and &amp;lsquo;dot product of two vectors&amp;rsquo;?</description></item><item><title>矩阵相乘-关于维度的新视角</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%98-%E5%85%B3%E4%BA%8E%E7%BB%B4%E5%BA%A6%E7%9A%84%E6%96%B0%E8%A7%86%E8%A7%92/</link><pubDate>Tue, 16 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%98-%E5%85%B3%E4%BA%8E%E7%BB%B4%E5%BA%A6%E7%9A%84%E6%96%B0%E8%A7%86%E8%A7%92/</guid><description>关于矩阵相乘维度关系的新视角 2021-11-16 Tags: #Math/LinearAlgebra
!</description></item><item><title>酉矩阵为什么叫酉矩阵</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/%E9%85%89%E7%9F%A9%E9%98%B5%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%AB%E9%85%89%E7%9F%A9%E9%98%B5/</link><pubDate>Tue, 16 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/%E9%85%89%E7%9F%A9%E9%98%B5%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%AB%E9%85%89%E7%9F%A9%E9%98%B5/</guid><description>酉矩阵为什么叫酉矩阵 2021-11-16 Tags: #Math/LinearAlgebra #English
酉矩阵里面的&amp;quot;酉&amp;ldquo;其实是字母$U$的音译 （又译作幺正矩阵，英语：unitary matrix）
unitary adjective uk /ˈjuː.nɪ.tər.i/ us /ˈjuː.nɪ.ter.i/
of a system of local government in the UK in which official power is given to one organization that deals with all matters in a local area instead of to several organizations that each deal with only a few matters （英国地方政府）集权制的，单一自治体的</description></item><item><title>MIT_18.065-Part_7-Eigenvalues and Eigenvectors</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_7-Eigenvalues-and-Eigenvectors/</link><pubDate>Sun, 14 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_7-Eigenvalues-and-Eigenvectors/</guid><description>Eigenvalues and Eigenvectors 2021-11-14 Tags: #Math/LinearAlgebra #Math/LinearAlgebra/Eigenvalue
特征值的一些性质 相似矩阵 [[notes/2021/2021.11/理解相似矩阵]]
相似矩阵有相同的特征值 $$P^{-1}AP = B$$ 假设矩阵$B$有特征值$\lambda$: $$Bx=\lambda x$$ 则 $$\begin{aligned}P^{-1}APx&amp;amp;=\lambda x \ &amp;amp;\Rightarrow \ APx&amp;amp;=P\lambda x \ &amp;amp;\Rightarrow \ A(Px)&amp;amp;=\lambda (Px)\end{aligned}$$</description></item><item><title>MIT_18.065-Part_7-Eigenvalues and Eigenvectors</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_7-Eigenvalues-and-Eigenvectors/</link><pubDate>Sun, 14 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_7-Eigenvalues-and-Eigenvectors/</guid><description>Eigenvalues and Eigenvectors 2021-11-14 Tags: #Math/LinearAlgebra #Math/LinearAlgebra/Eigenvalue
特征值的一些性质 相似矩阵 [[notes/2021/2021.11/理解相似矩阵]]
相似矩阵有相同的特征值 $$P^{-1}AP = B$$ 假设矩阵$B$有特征值$\lambda$: $$Bx=\lambda x$$ 则 $$\begin{aligned}P^{-1}APx&amp;amp;=\lambda x \ &amp;amp;\Rightarrow \ APx&amp;amp;=P\lambda x \ &amp;amp;\Rightarrow \ A(Px)&amp;amp;=\lambda (Px)\end{aligned}$$</description></item><item><title>MIT_18.065-Part_8-Positive Definite and Semidefinite Matrices</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_8-Positive-Definite-and-Semidefinite-Matrices/</link><pubDate>Sun, 14 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_8-Positive-Definite-and-Semidefinite-Matrices/</guid><description>Positive Definite and Semi-definite Matrices 2021-11-14 Tags: #Math/LinearAlgebra
Positive Definite Matrices are the Best of the Symmetric Matrices. 五个判别条件 同时也是正定矩阵的重要性质: Positive Eigenvalues 所有的特征值都是正数 Energy $x^TSx&amp;gt;0$, $\forall x\neq 0$ 有正的&amp;quot;能量&amp;quot;, 这点后面会详述 $S=A^TA$, A has Independent Columns S可以被分解为一个矩阵的转置与自己的乘积 All leading Determinants &amp;gt; 0 All Pivots in Elimination &amp;gt; 0 Energy (在视频里面没有找到详细的定义, 网上也没有相关的资料, 应当是一个直观的概念)</description></item><item><title>MIT_18.065-Part_9-Singular Value Decomposition-SVD</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_9-Singular-Value-Decomposition-SVD/</link><pubDate>Sun, 14 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_9-Singular-Value-Decomposition-SVD/</guid><description>Singular Value Decomposition 2021-11-14 Tags: #Math/LinearAlgebra #SVD
1
SVD将任意矩阵$A$分解成了三个部分: $U$ 的列是相互正交的. - Left Singular Vectors $\Sigma$ 的对角线上面是递减的奇异值. - Singular Values $V^T$ 的行是相互正交的.</description></item><item><title>理解相似矩阵</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/%E7%90%86%E8%A7%A3%E7%9B%B8%E4%BC%BC%E7%9F%A9%E9%98%B5/</link><pubDate>Sun, 14 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/%E7%90%86%E8%A7%A3%E7%9B%B8%E4%BC%BC%E7%9F%A9%E9%98%B5/</guid><description>理解相似矩阵 2021-11-14 Tags: #Math/LinearAlgebra
设 $A,B$ 都是 $n$ 阶矩阵，若有可逆矩阵 $P$ , 使得 $B=P^{-1}AP$ , 则称$B$是$A$的相似矩阵。
相似矩阵是同一个线性变换在不同基向量下的不同矩阵表示.</description></item><item><title>MIT_18.065-Part_5-Four_Subspaces</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_5-Four_Subspaces/</link><pubDate>Sat, 13 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_5-Four_Subspaces/</guid><description>4 Subspaces 2021-11-13 Tags: #Math/LinearAlgebra
What are they? The Four Subspaces are:
$A$的列空间 $Col(A)$ $A$的行空间 $Row(A)=Col(A^T)$ $A$的核(零空间) $N(A) \Rightarrow Ax=0$ $A^T$的核 $N(A^T) \Rightarrow A^Ty=0$ 为了与课本一致, 我们将$Col(A)$简称为$C(A)$</description></item><item><title>MIT_18.065-Part_6-Orthonormal Columns in Q Give Q'Q = I</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_6-Orthonormal-Columns-in-Q-Give-QQ-I/</link><pubDate>Sat, 13 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_6-Orthonormal-Columns-in-Q-Give-QQ-I/</guid><description>Orthonormal Columns in Q Give Q&amp;rsquo;Q = I 2021-11-13 Tags: #Math/LinearAlgebra
orthogonal: 正交的 orthonormal: Ortho(gonal) + normal, 即又正交又是单位向量, 长度为1.</description></item><item><title>MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning,</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT-18.065-Matrix-Methods-in-Data-Analysis-Signal-Processing-and-Machine-Learning/</link><pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT-18.065-Matrix-Methods-in-Data-Analysis-Signal-Processing-and-Machine-Learning/</guid><description>MIT 18.065 - Matrix Methods in Data Analysis, Signal Processing, and Machine Learning, 2021-11-12 Tags: #Matrix #Math/LinearAlgebra #Math
学习这门课的主要动力是线性代数的知识在大二的一年内已经有所遗忘了, 并且在学习机器学习期间常常设计线性代数与矩阵的相关知识, 所以想要有针对性地深入学习与复习一下.</description></item><item><title>MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning,</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT-18.065-Matrix-Methods-in-Data-Analysis-Signal-Processing-and-Machine-Learning/</link><pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT-18.065-Matrix-Methods-in-Data-Analysis-Signal-Processing-and-Machine-Learning/</guid><description>MIT 18.065 - Matrix Methods in Data Analysis, Signal Processing, and Machine Learning, 2021-11-12 Tags: #Matrix #Math/LinearAlgebra #Math
学习这门课的主要动力是线性代数的知识在大二的一年内已经有所遗忘了, 并且在学习机器学习期间常常设计线性代数与矩阵的相关知识, 所以想要有针对性地深入学习与复习一下.</description></item><item><title>MIT_18.065-Part_1-A_Column_Space_Perspective</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_1-A_Column_Space_Perspective/</link><pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_1-A_Column_Space_Perspective/</guid><description>The Column Space of A Contains All Vectors Ax 2021-11-12 Tags: #Math/LinearAlgebra
Video Link: 1. The Column Space of A Contains All Vectors Ax - YouTube</description></item><item><title>MIT_18.065-Part_2-Matrix_Factorization</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_2-Matrix_Factorization/</link><pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_2-Matrix_Factorization/</guid><description>矩阵分解 2021-11-12 Tags: #Matrix #Math/LinearAlgebra #Math
我们看待矩阵乘积的新方式有助于我们理解数据科学里面对矩阵的各种分解. 我们常常需要发掘一个矩阵$A$里面隐藏的信息, 而通过将$A$分解为$CR$, 我们可以观察A里面最基本的组成部分: 秩为1的矩阵: $col_k(C)\ row_k(R)$ 下面列举重要的分解, 在详细论述后将补充相应细节</description></item><item><title>MIT_18.065-Part_2-Matrix_Factorization</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_2-Matrix_Factorization/</link><pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_2-Matrix_Factorization/</guid><description>矩阵分解 2021-11-12 Tags: #Matrix #Math/LinearAlgebra #Math
我们看待矩阵乘积的新方式有助于我们理解数据科学里面对矩阵的各种分解. 我们常常需要发掘一个矩阵$A$里面隐藏的信息, 而通过将$A$分解为$CR$, 我们可以观察A里面最基本的组成部分: 秩为1的矩阵: $col_k(C)\ row_k(R)$ 下面列举重要的分解, 在详细论述后将补充相应细节</description></item><item><title>MIT_18.065-Part_4-LU_Factorization</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_4-LU_Factorization/</link><pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_4-LU_Factorization/</guid><description>LU 分解 2021-11-12 Tags: #Math/LinearAlgebra #Math #Matrix
$A=L U$
Key Idea: 怎样从A = Sum of Rank 1 Matrices的角度来理解这个分解?</description></item><item><title>MIT_18.065-Part_4-LU_Factorization</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_4-LU_Factorization/</link><pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_4-LU_Factorization/</guid><description>LU 分解 2021-11-12 Tags: #Math/LinearAlgebra #Math #Matrix
$A=L U$
Key Idea: 怎样从A = Sum of Rank 1 Matrices的角度来理解这个分解?</description></item><item><title>Dot_Product_and_Linear_Transformation-向量内积与线性变换</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Dot_Product_and_Linear_Transformation-%E5%90%91%E9%87%8F%E5%86%85%E7%A7%AF%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2/</link><pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/Dot_Product_and_Linear_Transformation-%E5%90%91%E9%87%8F%E5%86%85%E7%A7%AF%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2/</guid><description>内积与线性变换 2021-10-29 Tags: #Math/LinearAlgebra #LinearTransformation #DotProduct #Vector
向量内积即多维空间到一维空间的线性变换. 将第二个向量变换到第一个向量的方向上去 如果第一个向量的长度为1, 那么就是第二个向量到第一个向量方向上的投影变换 Highlights 视频</description></item><item><title>为什么方差的分母常常是n-1</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%96%B9%E5%B7%AE%E7%9A%84%E5%88%86%E6%AF%8D%E5%B8%B8%E5%B8%B8%E6%98%AFn-1/</link><pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%96%B9%E5%B7%AE%E7%9A%84%E5%88%86%E6%AF%8D%E5%B8%B8%E5%B8%B8%E6%98%AFn-1/</guid><description>为什么方差的分母常常是$n-1$? 2021-10-29 Tags: #Math/Statistics #Variance
按照定义, 方差的分母的确应该是$n$
但是因为我们用样本的均值$\overline X$代替了数学期望$\mu$, 而这个$\overline X$是有误差的, $\frac 1 n \rightarrow \frac 1 {(n-1)}$是为了消除这个误差.</description></item><item><title>协方差矩阵_Covariance_Matrix</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5_Covariance_Matrix/</link><pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5_Covariance_Matrix/</guid><description>Covariance Matrix 2021-10-29 Tags: #Matrix #Math/Statistics
https://janakiev.com/blog/covariance-matrix/
Variance, Covariance Variance measures the variation of a single random variable (like height of a person in a population) $$\sigma_{x}^{2}=\mathbb E \left(\frac{1}{n-1} \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}\right)$$ Link: 为什么方差的分母常常是n-1</description></item><item><title>拉格朗日乘数-Lagrange_Multiplier</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0-Lagrange_Multiplier/</link><pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0-Lagrange_Multiplier/</guid><description>拉格朗日乘数 2021-10-29 Tags: #Math #Optimization
Intuition $$\mathcal{L}(x, y, \lambda)=f(x, y)-\lambda g(x, y)$$ 在一个三维曲面($f(x,y)$)上面画了一条曲线($g(x,y)$), 求这条曲线上面的最低点.
Explanation 中英文的维基百科已经解释的十分直观与清晰了:
Chinese English</description></item><item><title>梯度的方向是哪个方向-为什么梯度是函数增长最快的方向</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/%E6%A2%AF%E5%BA%A6%E7%9A%84%E6%96%B9%E5%90%91%E6%98%AF%E5%93%AA%E4%B8%AA%E6%96%B9%E5%90%91-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%A2%AF%E5%BA%A6%E6%98%AF%E5%87%BD%E6%95%B0%E5%A2%9E%E9%95%BF%E6%9C%80%E5%BF%AB%E7%9A%84%E6%96%B9%E5%90%91/</link><pubDate>Thu, 14 Oct 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.10/%E6%A2%AF%E5%BA%A6%E7%9A%84%E6%96%B9%E5%90%91%E6%98%AF%E5%93%AA%E4%B8%AA%E6%96%B9%E5%90%91-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%A2%AF%E5%BA%A6%E6%98%AF%E5%87%BD%E6%95%B0%E5%A2%9E%E9%95%BF%E6%9C%80%E5%BF%AB%E7%9A%84%E6%96%B9%E5%90%91/</guid><description>为什么梯度是函数增长最快的方向 2021-10-14 Tags: #Math
看这篇文章可快速回顾高数: https://zhuanlan.zhihu.com/p/38525412
Key Idea 对于任意方向一个增量 $\Delta z=f\left(x_{0}+t \cos \alpha, y_{0}+t \sin \alpha\right)-f\left(x_{0}, y_{0}\right)$
函数沿此方向的变化率为: $$ \lim {t \rightarrow 0^{+}} \frac{f\left(x{0}+t \cos \alpha, y_{0}+t \sin \alpha\right)-f\left(x_{0}, y_{0}\right)}{t}=f_{x}\left(x_{0}, y_{0}\right) \cos \alpha+f_{y}\left(x_{0}, y_{0}\right) \sin \alpha $$</description></item><item><title>Part.26_Probabilistic_Interpretation_of_MSE(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.26_Probabilistic_Interpretation_of_MSEML_Andrew.Ng./</link><pubDate>Thu, 16 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/Part.26_Probabilistic_Interpretation_of_MSEML_Andrew.Ng./</guid><description>均方差的合理性 - 概率解释 2021-09-16 Tags: #MachineLearning #Math/Statistics #MeanSquareError #CostFunction
之前的一些讨论 Mean_Squared_Error-均方误差 Why_do_cost_functions_use_the_square_error CS229 - Probabilistic Interpretation 独立同分布-IID [[notes/2021/2021.</description></item><item><title>正态分布_高斯分布_Normal_Distribution-Gaussian_Distribution</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83_%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_Normal_Distribution-Gaussian_Distribution/</link><pubDate>Thu, 16 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83_%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_Normal_Distribution-Gaussian_Distribution/</guid><description>正态分布 2021-09-16 Tags: #Math/Statistics
概率密度函数 正态分布, 概率密度函数: $$f(x)=\frac{1}{\sigma \sqrt{2 \pi}} e^{\Large -\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{2}}$$ or $$f(x)=\frac{1}{\sigma \sqrt{2 \pi}} \exp\left(-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{2}\right)$$
重要性质 Mean $(\mu)$ and standard deviation $(\sigma)$ $$ \begin{aligned} &amp;amp;\mu=E(X)=\int_{-\infty}^{\infty} x p(x) d x \ &amp;amp;\sigma^{2}=E\left{(X-\mu)^{2}\right}=\int_{-\infty}^{\infty}(x-\mu)^{2} p(x) d x \end{aligned} $$</description></item><item><title>独立同分布-IID</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/%E7%8B%AC%E7%AB%8B%E5%90%8C%E5%88%86%E5%B8%83-IID/</link><pubDate>Thu, 16 Sep 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.9/%E7%8B%AC%E7%AB%8B%E5%90%8C%E5%88%86%E5%B8%83-IID/</guid><description>独立同分布 Independent and identically distributed 2021-09-16 Tags: #Math/Statistics
定义 在概率论与统计学中，独立同分布（英语：Independent and identically distributed，或称独立同分配，缩写为iid、 i.i.d.、IID）是指一组随机变量中每个变量的概率分布都相同，且这些随机变量互相独立.</description></item><item><title>Norm_of_a_Vector-Matrix</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Norm_of_a_Vector-Matrix/</link><pubDate>Fri, 20 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Norm_of_a_Vector-Matrix/</guid><description>向量/矩阵的范数 2021-08-20 Tags: #Norm #Math #MachineLearning #Regularization
https://zh.wikipedia.org/wiki/%E8%8C%83%E6%95%B0</description></item><item><title>矩阵迹的性质</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E8%BF%B9%E7%9A%84%E6%80%A7%E8%B4%A8/</link><pubDate>Mon, 16 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/%E7%9F%A9%E9%98%B5%E8%BF%B9%E7%9A%84%E6%80%A7%E8%B4%A8/</guid><description>矩阵迹的性质 2021-08-16 Tags: #Trace #Matrix #Math
标量可以直接套上迹： $a=\operatorname{tr}(a)$
$\mathrm{tr}AB = \mathrm{tr}BA$ ^tracecommutative
左边: $$ \begin{align} &amp;amp;\sum^n_i a_{1i}b_{i1}+\sum^n_i a_{2i}b_{i2}+\cdots+\sum^n_i a_{mi}b_{im} \ = &amp;amp;\sum^m_j\sum^n_ia_{ji}b_{ij} \ = &amp;amp;\sum^m_i\sum^n_j a_{ij}b_{ji} \end{align} $$ 右边: $$ \begin{align} &amp;amp;\sum^m_i b_{1i}a_{i1}+\sum^m_i b_{2i}a_{i2}+\cdots+\sum^m_i b_{ni}a_{in} \ = &amp;amp;\sum^n_j\sum^m_i b_{ji}a_{ij} \ = &amp;amp;\sum^m_i\sum^n_j a_{ij}b_{ji} \end{align} $$</description></item><item><title>Part.6_Linear_Algerba_Review(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.6_Linear_Algerba_ReviewML_Andrew.Ng./</link><pubDate>Wed, 04 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.6_Linear_Algerba_ReviewML_Andrew.Ng./</guid><description>Linear Algebra Review 2021-08-04 Tags: #Math #Math/LinearAlgebra
Scalar: 标量, A physical quantity that is completely described by its magnitude.</description></item><item><title>Part.6_Linear_Algerba_Review(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.6_Linear_Algerba_ReviewML_Andrew.Ng./</link><pubDate>Wed, 04 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.6_Linear_Algerba_ReviewML_Andrew.Ng./</guid><description>Linear Algebra Review 2021-08-04 Tags: #Math #Math/LinearAlgebra
Scalar: 标量, A physical quantity that is completely described by its magnitude.</description></item><item><title>凸优化与线性回归问题</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/%E5%87%B8%E4%BC%98%E5%8C%96%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98/</link><pubDate>Mon, 02 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/%E5%87%B8%E4%BC%98%E5%8C%96%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98/</guid><description>Gradient Descent &amp;amp; Convex Optimization / 凸优化 2021-08-02 Tags: #MachineLearning #ConvexOptimization #Math
在 这里(和下面的引用里面), 我们特殊的线性规划的损失函数一定是一个凸函数, 那么在其他情况下, 线性规划还是凸函数吗, 线性规划问题会陷入局部最优的问题中去吗?
Note that, while gradient descent can be susceptible to local minima in general, the optimization problem we have posed here for linear regression has only one global, and no other local, optima; thus gradient descent always converges (assuming the learning rate α is not too large) to the global minimum.</description></item><item><title>拉普拉斯分布与高斯分布的联系_Relation_of_Laplace_distribution _and_Gaussian_distribution</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%88%86%E5%B8%83%E4%B8%8E%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E7%9A%84%E8%81%94%E7%B3%BB_Relation_of_Laplace_distribution-_and_Gaussian_distribution/</link><pubDate>Sat, 31 Jul 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%88%86%E5%B8%83%E4%B8%8E%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E7%9A%84%E8%81%94%E7%B3%BB_Relation_of_Laplace_distribution-_and_Gaussian_distribution/</guid><description>Gaussian distribution, Laplace distribution: The Relation 2021-07-31 Tags: #Math/Statistics #GaussianDistribution #LaplaceDistribution
拉普拉斯分布, 概率密度函数: Look at the formula for the PDF in the infobox &amp;ndash; it&amp;rsquo;s just the Gaussian with $|\boldsymbol{x}-\boldsymbol{\mu}|$ instead of $(\boldsymbol{x}-\boldsymbol{\mu})^{2}$)1</description></item><item><title>Hamming_Distance_汉明距离</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/Hamming_Distance_%E6%B1%89%E6%98%8E%E8%B7%9D%E7%A6%BB/</link><pubDate>Fri, 25 Jun 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/Hamming_Distance_%E6%B1%89%E6%98%8E%E8%B7%9D%E7%A6%BB/</guid><description>Hamming Distance / 汉明距离 汉明距离是对于两个相同长度的字符串而言, the number of positions at which the corresponding symbols are different(相同的位置上对应字符不同的位置个数)
图例 带色线条是路径示意
Two example distances: 100→011 has distance 3; 010→111 has distance 2</description></item><item><title>Diffie-Hellman问题</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/Diffie-Hellman%E9%97%AE%E9%A2%98/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/Diffie-Hellman%E9%97%AE%E9%A2%98/</guid><description>Diffie-Hellman问题 两个Diffie-Hellman问题 Computation Diffie-Hellman / CDH 即给定一个基数与两个指数, 计算合并的指数
给定$(\alpha, \alpha^b, \alpha^c)$, 求 $\alpha^{bc}$ Decision Diffie-Hellman / DDH 即给你三个指数, 让你判断最后一个是不是前两个的合并</description></item><item><title>Hash函数_Pt.4_安全Hash算法_SHA-1</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/Hash%E5%87%BD%E6%95%B0_Pt.4_%E5%AE%89%E5%85%A8Hash%E7%AE%97%E6%B3%95_SHA-1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/Hash%E5%87%BD%E6%95%B0_Pt.4_%E5%AE%89%E5%85%A8Hash%E7%AE%97%E6%B3%95_SHA-1/</guid><description>SHA-1 Tags: #Cryptography #Math #Course #Hash
SHA-1是一个具有160bit消息摘要的迭代Hash函数
主要思想 SHA-1 的分组大小是 512bit, 意味着每一次迭代处理 512bit 的数据
SHA-1建立在对比特串面向字的操作上, 意味着在处理512bit的时候是每次32bit, 一共16次, 一共80次. (为什么变多了? 因为在循环里面需要将16个字扩充到80个字, 如下图)</description></item><item><title>Hash函数_Pt.5_消息认证码_MAC</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/Hash%E5%87%BD%E6%95%B0_Pt.5_%E6%B6%88%E6%81%AF%E8%AE%A4%E8%AF%81%E7%A0%81_MAC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/Hash%E5%87%BD%E6%95%B0_Pt.5_%E6%B6%88%E6%81%AF%E8%AE%A4%E8%AF%81%E7%A0%81_MAC/</guid><description>消息认证码 MAC Message Authentication Code 消息认证码是什么 生成方式: 带密钥的Hash函数, Hash值可以不用在安全的信道上传输, 只需要开始的时候协商一个密钥$K$就可以了
用于核验在不安全信道上传输的消息有没有被修改
不安全的构造方式 一个很直观的方式便是把密钥$K$加到明文x里面,一起Hash, 既然攻击者不知道$K$是什么, 他也应该无法计算$h_K(x)=h(x_K). (x_K$为加入了K的x)
但是以下论证告诉我们一些简单的构造方式并不安全, 攻击者即使不知道K是什么, 也可以利用一个有效对$(x,h_K(x))$计算新的有效对$(x^\prime,h_K(x^\prime)$
直接把K作为$IV$ (IV 即 Compress 的初始输入 见迭代Hash函数的基本结构)</description></item><item><title>MIT_18.065-Part_3-A_Different_Perspectvie_of_Matrix_Multiplication-An_Example</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_3-A_Different_Perspectvie_of_Matrix_Multiplication-An_Example/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_3-A_Different_Perspectvie_of_Matrix_Multiplication-An_Example/</guid><description>#Math/LinearAlgebra #Matrix #Math
$$S=Q \Lambda Q^{\mathrm{T}}$$
其中S是一个对称矩阵, $S=S^{\mathrm{T}}$
Q的行向量是S的特征向量, 这些特征向量相互正交 $$Q=\left[\begin{array}{ccc} \mid &amp;amp; &amp;amp; \mid \ q_{1} &amp;amp; \ldots &amp;amp; q_{n} \ \mid &amp;amp; &amp;amp; \mid \end{array}\right]$$</description></item><item><title>MIT_18.065-Part_3-A_Different_Perspectvie_of_Matrix_Multiplication-An_Example</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_3-A_Different_Perspectvie_of_Matrix_Multiplication-An_Example/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.11/MIT_18.065-Part_3-A_Different_Perspectvie_of_Matrix_Multiplication-An_Example/</guid><description>#Math/LinearAlgebra #Matrix #Math
$$S=Q \Lambda Q^{\mathrm{T}}$$
其中S是一个对称矩阵, $S=S^{\mathrm{T}}$
Q的行向量是S的特征向量, 这些特征向量相互正交 $$Q=\left[\begin{array}{ccc} \mid &amp;amp; &amp;amp; \mid \ q_{1} &amp;amp; \ldots &amp;amp; q_{n} \ \mid &amp;amp; &amp;amp; \mid \end{array}\right]$$</description></item><item><title>代换密码（Substitution Cipher）与置换密码（Permutation Cipher）</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/%E4%BB%A3%E6%8D%A2%E5%AF%86%E7%A0%81Substitution-Cipher%E4%B8%8E%E7%BD%AE%E6%8D%A2%E5%AF%86%E7%A0%81Permutation-Cipher/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/%E4%BB%A3%E6%8D%A2%E5%AF%86%E7%A0%81Substitution-Cipher%E4%B8%8E%E7%BD%AE%E6%8D%A2%E5%AF%86%E7%A0%81Permutation-Cipher/</guid><description>代换密码与置换密码（Substitution Cipher &amp;amp; Permutation Cipher） Tags: #Math #Cryptography #Course
分不清楚这两个完全是翻译的锅
置换（不是置换密码）Permutation（Not Permutation Cipher) 首先置换是数学上的一种操作，是对一组确定的元素进行重新排列
元素不变 只改变顺序 Wikipedia:</description></item><item><title>图灵归约 Turing Reduction</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/%E5%9B%BE%E7%81%B5%E5%BD%92%E7%BA%A6-Turing-Reduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/%E5%9B%BE%E7%81%B5%E5%BD%92%E7%BA%A6-Turing-Reduction/</guid><description>把还没解决的问题归约到已经解决的问题上
用已经解决的问题去解决还没解决的问题
密码学原理与实践 Page 167
假设我们已经存在一个解决问题 A 的算法 $G(x)$
一个A到B的图灵归约即利用$G(x)$构造一个解决问题B的算法$H(x)$, 并且$H(x)$是多项式时间的.
https://zhuanlan.zhihu.com/p/194313998 这篇文章译自 reductions-and-jokes</description></item></channel></rss>