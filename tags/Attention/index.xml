<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Attention on</title><link>https://alonelysheep.github.io/quartz-blog/tags/Attention/</link><description>Recent content in Attention on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 27 Apr 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://alonelysheep.github.io/quartz-blog/tags/Attention/index.xml" rel="self" type="application/rss+xml"/><item><title>D2L-71-Multi-Head_Attention</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-71-Multi-Head_Attention/</link><pubDate>Wed, 27 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-71-Multi-Head_Attention/</guid><description>多头注意力 2022-04-27 Tags: #Attention #Multi-headAttention #DeepLearning
多头注意力就是对 Query, Key, Value 进行一些线性变换, 并行地计算多个注意力, 期望模型能学习到多样化的依赖关系. Another way of seeing it: 1 模型构建 下面我们给出 Multi-Head Attention 的形象化表示:</description></item><item><title>D2L-74-Transformer</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-74-Transformer/</link><pubDate>Wed, 27 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-74-Transformer/</guid><description>Transformer 2022-04-27 Tags: #Transformer #Attention #DeepLearning
1
Transformer 是一个纯基于 Attention 的 Encoder Decoder 架构模型
Hugging Face Explorable Transformer: exBERT</description></item><item><title>D2L-72-Self-Attention</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-72-Self-Attention/</link><pubDate>Tue, 26 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-72-Self-Attention/</guid><description>自注意力 2022-04-26 Tags: #Self-Attention #Attention #DeepLearning
Attention 机制可以抽象为:1 $$\begin{align} \textit{Attention}(Q,K,V) = V\cdot\textit{softmax}\ (\textit{score}(Q, K)) \end{align}$$ 自注意力就是 $Q = K = V$ , 也就是同一个序列同时作为 Query, Key 和 Value.</description></item><item><title>D2L-70-Seq2Seq with Attention - Bahdanau Attention</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-70-Seq2Seq-with-Attention-Bahdanau-Attention/</link><pubDate>Fri, 22 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-70-Seq2Seq-with-Attention-Bahdanau-Attention/</guid><description>含注意力机制的Seq2Seq 2022-04-22 Tags: #Seq2Seq #Attention #DeepLearning #RNN
Motivation 在 Seq2Seq模型里面, Encoder向Decoder传递的仅仅是最后一个时间步的隐状态, 也就是上下文变量 $\mathbf c= \mathbf{h}_T$, 我们假设里面已经包含了输入序列的所有信息: 但这样每一步Decoder的输入都是原序列的一个&amp;quot;全局, 笼统的总结&amp;quot;, 这是不太合理的: 在下图中, 在翻译&amp;quot;Knowledge&amp;quot;的的时候, 显然&amp;quot;力量&amp;quot;这个词是不太重要的.</description></item><item><title>D2L-67-Attention Scoring Function</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-67-Attention-Scoring-Function/</link><pubDate>Thu, 21 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-67-Attention-Scoring-Function/</guid><description>注意力评分函数 2022-04-21 Tags: #Attention #DeepLearning
抽取出Attention Pooling里面都有的Softmax部分, 我们可以将注意力机制的设计简化为Attention Scoring Function的设计. 形式化的表达如下:
query $\mathbf{q} \in \mathbb{R}^q$,</description></item><item><title>D2L-68-Additive Attention</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-68-Additive-Attention/</link><pubDate>Thu, 21 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-68-Additive-Attention/</guid><description>加性注意力 2022-04-21 Tags: #Attention #DeepLearning
一般来说，当Query和Key是不同长度的矢量时，我们可以使用Additive Attention来作为Scoring Function。
给定查询 $\mathbf{q} \in \mathbb{R}^q$ 和键 $\mathbf{k} \in \mathbb{R}^k$，加性注意力（additive attention）的评分函数(Scoring Function)为 $$a(\mathbf q, \mathbf k) = \mathbf w_v^\top \text{tanh}(\mathbf W_q\mathbf q + \mathbf W_k \mathbf k) \in \mathbb{R},$$</description></item><item><title>D2L-69-Scaled Dot-Product Attention</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-69-Scaled-Dot-Product-Attention/</link><pubDate>Thu, 21 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-69-Scaled-Dot-Product-Attention/</guid><description>缩放的点积注意力 2022-04-21 Tags: #Attention #DeepLearning
1
相比Additive Attention, 使用点积可以得到计算效率更高的Scoring Function. 但是点积操作要求查询和键具有相同的长度 $d$。
我们知道 内积可以衡量两个向量之间的相似程度, 所以我们可以这样解读缩放点积注意力:</description></item><item><title>D2L-64-Kernel Regression</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-64-Kernel-Regression/</link><pubDate>Wed, 20 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-64-Kernel-Regression/</guid><description>Nadaraya-Watson Kernel Regression 2022-04-20 Tags: #KernelRegression #Nonparametric #Attention #MachineLearning
Intuition Definition $$f(x) = \sum_{i=1}^n \frac{K(x - x_i)}{\sum_{j=1}^n K(x - x_j)} y_i$$</description></item><item><title>D2L-65-Attention Cues &amp; Attention Mechanisms</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-65-Attention-Cues-Attention-Mechanisms/</link><pubDate>Wed, 20 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-65-Attention-Cues-Attention-Mechanisms/</guid><description>注意力信号 与 注意力机制 2022-04-20 Tags: #Attention #DeepLearning
Attention Cue Attention Cue分为两种: nonvolitional cue 和 volitional cue. Your volition is the power you have to decide something for yourself.</description></item><item><title>D2L-66-Kernel Regression and Attention</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-66-Kernel-Regression-and-Attention/</link><pubDate>Wed, 20 Apr 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.4/D2L-66-Kernel-Regression-and-Attention/</guid><description>Kernel Regression And Attention 2022-04-20 Tags: #KernelRegression #Attention #DeepLearning
Nadaraya-Watson kernel regression is an example of machine learning with attention mechanisms.</description></item></channel></rss>