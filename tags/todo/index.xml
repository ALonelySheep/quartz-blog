<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>todo on</title><link>https://alonelysheep.github.io/quartz-blog/tags/todo/</link><description>Recent content in todo on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 07 Mar 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://alonelysheep.github.io/quartz-blog/tags/todo/index.xml" rel="self" type="application/rss+xml"/><item><title>D2L-47-序列信息</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-47-%E5%BA%8F%E5%88%97%E4%BF%A1%E6%81%AF/</link><pubDate>Mon, 07 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.3/D2L-47-%E5%BA%8F%E5%88%97%E4%BF%A1%E6%81%AF/</guid><description>序列信息 2022-03-07 Tags: #SequentialData
数据分布的不同 对于图像或者表格数据， 我们通常都假设所有样本是独立同分布的1。 然而，大多数的数据都有序列性。 例如，文章中的单词是按顺序写的，如果顺序被随机地重排，就很难理解文章原始的意思。 同样，视频中的图像帧、对话中的音频信号以及网站上的浏览行为都是有顺序的。 因此，针对此类数据而设计特定模型，可能效果会更好。 实际情景 在接收一个序列作为输入的时候， 我们通常期望猜测这个序列的后续。 例如预测股市的波动、 患者的体温曲线或者赛车所需的加速度。 我们需要能够处理这些数据的特定模型。 相关模型 如果说卷积神经网络可以有效地处理空间信息， 那么 循环神经网络（recurrent neural network，RNN）则可以更好地处理序列信息。 #todo</description></item><item><title>D2L-41-VGG</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-41-VGG/</link><pubDate>Thu, 03 Mar 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-41-VGG/</guid><description>VGG 2022-03-03 Tags: #DeepLearning #VGG #CNN
模块化是VGG网络最重要的思想. 模块化进一步带来了自由性, 不同的块配置可以带来不同的模型表现. 规范化 - 模块化 与芯片设计中工程师从放置晶体管到逻辑元件再到逻辑块的过程类似，神经网络架构的设计也逐渐变得更加抽象。研究人员开始从单个神经元的角度思考问题，发展到整个层，现在又转向块，重复层的模式。 使用块的设计同样让模型更加简洁. VGG块 VGG将 AlexNet 里面三层连续的卷积拿出来, 抽象成VGG块, 作为构建网络的基础模式.</description></item><item><title>D2L-24-数值稳定性</title><link>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-24-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7/</link><pubDate>Thu, 17 Feb 2022 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2022/2022.2/D2L-24-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7/</guid><description>深度学习里面的数值稳定性 2022-02-17 Tags: #DeepLearning #NumericalComputing
问题的由来 数值稳定性的问题发生在反向传播的时候. 对于一个很深的模型, 计算在损失 $\ell$ 关于第 $t$ 层权重 $\mathbf{W_t}$ 的梯度的时候, 如果第 $t$ 层关于输出较远, 则结果由许多矩阵乘法构成, 这会导致梯度爆炸或者梯度消失.</description></item><item><title>Part.7_Feature_Scaling(ML_Andrew.Ng.)</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.7_Feature_ScalingML_Andrew.Ng./</link><pubDate>Fri, 06 Aug 2021 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Part.7_Feature_ScalingML_Andrew.Ng./</guid><description>Feature Scaling 2021-08-06 Tags: #MachineLearning #FeatureEngineering
1
深入阅读的链接: https://sebastianraschka.com/Articles/2014_about_feature_scaling.html
When to Use 在梯度下降的时候, 缩放数据可以让梯度变化更平滑 If an algorithm uses gradient descent, then the difference in ranges of features will cause different step sizes for each feature.</description></item><item><title>Different_Gradient_Descent_Methods</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Different_Gradient_Descent_Methods/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.8/Different_Gradient_Descent_Methods/</guid><description>Batch Gradient Descent, 批梯度下降, BGD 每一次把所有数据都用来更新参数, Use ALL the training examples. $$ \begin{array}{l} \text { repeat until convergence }{\ \begin{array}{cc} &amp;amp;\theta_{j}:=\theta_{j}-\alpha \frac 1 m \sum_{i=1}^{m} \left(h_{\theta}(x^{(i)})-y^{(i)}\right) x_j^{(i)} \end{array}\ \text { } } \\ \text { (simultaneously update } j=0, \cdots ,j=n) \end{array} $$</description></item><item><title>Hash函数_Pt.4_安全Hash算法_SHA-1</title><link>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/Hash%E5%87%BD%E6%95%B0_Pt.4_%E5%AE%89%E5%85%A8Hash%E7%AE%97%E6%B3%95_SHA-1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://alonelysheep.github.io/quartz-blog/notes/2021/2021.6/Hash%E5%87%BD%E6%95%B0_Pt.4_%E5%AE%89%E5%85%A8Hash%E7%AE%97%E6%B3%95_SHA-1/</guid><description>SHA-1 Tags: #Cryptography #Math #Course #Hash
SHA-1是一个具有160bit消息摘要的迭代Hash函数
主要思想 SHA-1 的分组大小是 512bit, 意味着每一次迭代处理 512bit 的数据
SHA-1建立在对比特串面向字的操作上, 意味着在处理512bit的时候是每次32bit, 一共16次, 一共80次. (为什么变多了? 因为在循环里面需要将16个字扩充到80个字, 如下图)</description></item></channel></rss>